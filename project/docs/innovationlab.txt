<page>
  <title>Fetch.ai Innovation Lab</title>
  <url>https://innovationlab.fetch.ai</url>
  <content>> Fetch.ai Innovation Lab is an initiative set up by Fetch.ai to bring the technology weâ€™ve been building to the market. It enables students, entrepreneurs, and businesses to build solutions using this new technology, which is a paradigm shift in how we develop software. We provide tools to build, educate, and provide funding through every stageâ€”from proof of concept to MVP to launchâ€”helping businesses launch their products and succeed in the market.
> 
> Humayun Sheikh
> 
> Founder and CEO, Fetch.ai

Innovation is at the heart of everything we do.
-----------------------------------------------

Ambassador Innovator Club
-------------------------

TheÂ Fetch.aiÂ Ambassador Innovator Club fosters innovation within theÂ Fetch.aiÂ ecosystem. Ambassadors and innovators develop new applications usingÂ Fetch.aiâ€™s AI agents. Members access resources, support, and networking opportunities, driving groundbreaking ideas and growth. Participate in hackathons, code nights, and workshops to cultivate a forward-thinking community of tech innovators.

Internship Incubator Program
----------------------------

The Fetch.ai Internship Incubator Program engages exceptional individuals in high-impact projects, offering valuable experience and resources to accelerate careers in artificial intelligence and innovation. Interns collaborate on real-world challenges, fostering innovation and entrepreneurship within the Fetch.ai ecosystem, leveraging Fetch.ai Agents Technology for hands-on learning, mentorship, and accelerated professional growth.

Startup Accelerator
-------------------

The Startup Accelerator funds early-stage startups built on Fetch.ai, offering resources, mentorship, and support. Founders get ideas to build, compete in innovation lab competitions, and benefit from fast-track development. This comprehensive approach ensures startups can scale and launch their products efficiently, fostering AI-driven innovation and helping startups achieve remarkable success.

Leaders in Innovation.
----------------------

Meet our Senior Vice President.

Under our Senior Vice President, Sana Wajid, Fetch.ai Innovation Lab has empowered start-ups founded by the new generation of tech enthusiasts, pioneering multi-stakeholder AI solutions within a diverse array of sectors and business scopes. Her vision and her holistic contribution as a strategist, business mentor, and facilitator has helped countless AI developers and startups build their AI agent projects. As Chief Development Officer at Fetch.ai, Sana actively oversees the firm's strategy, connecting tech and entrepreneurial talents to the company's decentralized AI solutions. Under Sana's vision and her holistic contribution as strategist, business mentor, and facilitator of the Fetch.ai Innovation Lab, the company has empowered start ups founded by the new generation of tech enthusiasts, pioneering multi-stakeholder AI solutions within a diverse array of sectors and business scopes. Prior to joining Fetch.ai, she held key leadership roles in the rail industry, overseeing the launch of successful products that revolutionized asset management. With over 15 years of experience in product management and development, her expertise spans product strategy, agile design & development methodologies, customer relationships and operations.

Meet Our Team.
--------------

Our leadership team is committed to fostering innovation, creativity, and collaboration in the field of Artificial Intelligence. Meet the visionary minds driving our mission forward.

*   ### Humayun Sheikh
    
    CEO & Founder
    
    Fetch.ai
    
*   ### Sana Wajid
    
    Senior Vice President Fetch.ai Innovation Lab and CDO Fetch.ai
    
*   ### Rishank Jhavar
    
    Program Manager (Developer Advocacy & Marketing)
    
    Fetch.ai
    
*   ### Lisa Condon
    
    Operations & HR Director
    
    Fetch.ai
    
*   ### Jason Coleman
    
    Chief Financial Officer
    
    Fetch.ai
    
*   ### Abhi Gangani
    
    Senior Developer Advocate
    
    Fetch.ai
    
*   ### Kshipra Dhame
    
    Senior Developer Advocate
    
    Fetch.ai
    
*   ### Dev Chauhan
    
    Junior Developer Advocate
    
    Fetch.ai Innovation Lab
    
*   ### Gautam Kumar
    
    Junior Developer Advocate
    
    Fetch.ai Innovation Lab
    
*   ### Rajashekar V
    
    AI Engineer
    
    Fetch.ai Innovation Lab
    
*   ### Chinmay Mahagaonkar
    
    Software Engineer
    
    Fetch.ai Innovation Lab
    

Explore Applications of Agents Powered by Fetch.ai.
---------------------------------------------------

### [Fetch.ai](#)

Fetch.ai revolutionizes search and discovery, enabling seamless interaction with digital services through AI-powered agents. This decentralized platform allows anyone to deploy AI services at scale, fostering innovation and creating limitless possibilities in a connected world.

### [Mettalex](#)

Mettalex is the worldâ€™s first P2P Order book and agent-based DEX. Mettalex offers a next-generation trading experience in DeFi, combining Fetch.ai's advanced agent-based technology with a seamless, secure platform for efficient chain agnostic trades.

### [QConnect](#)

Qconnect transforms travel by seamlessly integrating AI-driven agents to plan, book, and enhance your journey. Powered by Fetch.ai, it offers a smarter, personalized travel experience, elevating convenience and efficiency.

### [AutoMate](#)

AutoMate uses Fetch.ai-powered agents to automate tedious tasks with minimal human input. It listens to meetings, identifies tasks, executes them across software, and delivers actionable insights, streamlining workflows efficiently.

### [Synergy](#)

Synergy powered by Fetch.ai, revolutionizes manufacturing by integrating AI agents to collect, clean, and analyze machinery data. It identifies trends, optimizes processes, and enhances efficiency without requiring hardware changes.

### [Rewise4](#)

Rewise4 leverages Fetch.ai's AI agents to provide personalized tutoring for students, allowing them to learn at their own pace. It supports teachers and parents in tracking progress and enhancing engagement.

### [DineSync](#)

DineSync, powered by Fetch.ai, automates restaurant operations with AI agents handling reservations, staffing, and real-time menu updates. It provides insights through analytics, optimizing efficiency and improving the dining experience.

Our Core Philosophy
-------------------

Innovate, Educate and Inspire

Collaborations
--------------

At Fetch AI Innovation Lab, we collaborate with world-class universities and premier hackathons to drive innovative AI projects.

Glimpses from our past events
-----------------------------

Find Us Where Great Ideas Are.
------------------------------</content>
</page>

<page>
  <title>Hello from Innovation Lab Resources | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources</url>
  <content>Welcome
-------

Innovation Lab Resources

E-book
------

Understanding AI Agents  
by Rajashekar Vennavelli</content>
</page>

<page>
  <title>Hello from Innovation Lab Resources | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/</url>
  <content>Welcome
-------

Innovation Lab Resources

E-book
------

Understanding AI Agents  
by Rajashekar Vennavelli</content>
</page>

<page>
  <title>Introduction | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/intro</url>
  <content>**Build, Connect, Communicate** and **Transact** between AI agents.

What is Fetch.ai?[â€‹](#what-is-fetchai "Direct link to What is Fetch.ai?")
-------------------------------------------------------------------------

Fetch.ai's vision is to create a marketplace of dynamic applications. We are empowering developers to build on our platform that can connect services and APIs without any domain knowledge. Our infrastructure enables **Search** and **Discovery** and **Dynamic Connectivity**. It offers an open, modular, UI-agnostic, self-assembling of services.

âš™ï¸ Core Components[â€‹](#ï¸-core-components "Direct link to âš™ï¸ Core Components")
-----------------------------------------------------------------------------

ğŸ”‘ Key Principles[â€‹](#-key-principles "Direct link to ğŸ”‘ Key Principles")
-------------------------------------------------------------------------

### ğŸš€ Build

Build Microservices using uagents, or make your own AI Agents using SDK to enable agentverse connectivity.

### ğŸ”— Connect

Deploy uAgents and register agents on Agentverse to discover and collaborate with other agents.

### ğŸ’¬ Communicate

Enable agents to exchange secure messages and share data seamlessly.

### ğŸ’° Transact

Perform decentralized transactions using blockchain to ensure trust, transparency, and accountability.

  

* * *

ğŸ”¥ How is Fetch.ai Different from Other AI Agent Frameworks?[â€‹](#-how-is-fetchai-different-from-other-ai-agent-frameworks "Direct link to ğŸ”¥ How is Fetch.ai Different from Other AI Agent Frameworks?")
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

### ğŸ”— Decentralisation

Unlike many frameworks, Fetch.ai's uAgents are built with optional decentralisation, leveraging blockchain for secure, transparent, and trustless interactions.

### ğŸ›’ Agent Marketplace

Fetch.ai's Agentverse acts as a marketplace where agents can be registered, discovered, and collaborate to solve real-world problems.

### ğŸ“¨ Unified Messaging System

uAgents provide a generalized messaging structure, removing the need to define custom data models for agent communication.

### ğŸ› ï¸ Seamless Integration

uAgents and the Fetch.ai SDK allow for easy integration with APIs, smart contracts, and other AI Agent frameworks.

### âš–ï¸ Lightweight and Scalable

The uAgents Framework is designed to be lightweight, making it ideal for deploying scalable, task-specific agents.

### ğŸ”— Blockchain-Powered Economy

Fetch.ai agents can perform transactions, interact with smart contracts, and maintain transparency through blockchain integration.

### ğŸ› ï¸ Predefined Templates and Tools

Fetch.ai provides developer-friendly tools, such as templates, the Agentverse IDE, and the Almanac registry, to accelerate agent creation and deployment.

### ğŸ”„ Interoperability

Agents can interact with multiple frameworks and other decentralized networks, enabling cross-platform collaboration.

Fetch.ai is not just a framework for building AI agentsâ€”it's an ecosystem that combines the power of AI, blockchain, and decentralisation to create autonomous, intelligent, and collaborative solutions.</content>
</page>

<page>
  <title>Introduction | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/intro</url>
  <content>**Build, Connect, Communicate** and **Transact** between AI agents.

What is Fetch.ai?[â€‹](#what-is-fetchai "Direct link to What is Fetch.ai?")
-------------------------------------------------------------------------

Fetch.ai's vision is to create a marketplace of dynamic applications. We are empowering developers to build on our platform that can connect services and APIs without any domain knowledge. Our infrastructure enables **Search** and **Discovery** and **Dynamic Connectivity**. It offers an open, modular, UI-agnostic, self-assembling of services.

âš™ï¸ Core Components[â€‹](#ï¸-core-components "Direct link to âš™ï¸ Core Components")
-----------------------------------------------------------------------------

ğŸ”‘ Key Principles[â€‹](#-key-principles "Direct link to ğŸ”‘ Key Principles")
-------------------------------------------------------------------------

### ğŸš€ Build

Build Microservices using uagents, or make your own AI Agents using SDK to enable agentverse connectivity.

### ğŸ”— Connect

Deploy uAgents and register agents on Agentverse to discover and collaborate with other agents.

### ğŸ’¬ Communicate

Enable agents to exchange secure messages and share data seamlessly.

### ğŸ’° Transact

Perform decentralized transactions using blockchain to ensure trust, transparency, and accountability.

  

* * *

ğŸ”¥ How is Fetch.ai Different from Other AI Agent Frameworks?[â€‹](#-how-is-fetchai-different-from-other-ai-agent-frameworks "Direct link to ğŸ”¥ How is Fetch.ai Different from Other AI Agent Frameworks?")
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

### ğŸ”— Decentralisation

Unlike many frameworks, Fetch.ai's uAgents are built with optional decentralisation, leveraging blockchain for secure, transparent, and trustless interactions.

### ğŸ›’ Agent Marketplace

Fetch.ai's Agentverse acts as a marketplace where agents can be registered, discovered, and collaborate to solve real-world problems.

### ğŸ“¨ Unified Messaging System

uAgents provide a generalized messaging structure, removing the need to define custom data models for agent communication.

### ğŸ› ï¸ Seamless Integration

uAgents and the Fetch.ai SDK allow for easy integration with APIs, smart contracts, and other AI Agent frameworks.

### âš–ï¸ Lightweight and Scalable

The uAgents Framework is designed to be lightweight, making it ideal for deploying scalable, task-specific agents.

### ğŸ”— Blockchain-Powered Economy

Fetch.ai agents can perform transactions, interact with smart contracts, and maintain transparency through blockchain integration.

### ğŸ› ï¸ Predefined Templates and Tools

Fetch.ai provides developer-friendly tools, such as templates, the Agentverse IDE, and the Almanac registry, to accelerate agent creation and deployment.

### ğŸ”„ Interoperability

Agents can interact with multiple frameworks and other decentralized networks, enabling cross-platform collaboration.

Fetch.ai is not just a framework for building AI agentsâ€”it's an ecosystem that combines the power of AI, blockchain, and decentralisation to create autonomous, intelligent, and collaborative solutions.</content>
</page>

<page>
  <title>Introduction | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/intro</url>
  <content>**Build, Connect, Communicate** and **Transact** between AI agents.

What is Fetch.ai?[â€‹](#what-is-fetchai "Direct link to What is Fetch.ai?")
-------------------------------------------------------------------------

Fetch.aiâ€™s vision is to create a marketplace of dynamic applications. We are empowering developers to build on our platform that can connect services and APIs without any domain knowledge. Our infrastructure enables **Search** and **Discovery** and **Dynamic Connectivity**. It offers an open, modular, UI-agnostic, self-assembling of services.

âš™ï¸ Core Components[â€‹](#ï¸-core-components "Direct link to âš™ï¸ Core Components")
-----------------------------------------------------------------------------

### ğŸ¤– uAgents

uAgents are microservices built to connect seamlessly with other agents. They can represent data, APIs, services, machine learning models, etc.

### ğŸ›’ Agentverse

An Agent marketplace, management, and development hub where agents are created, registered, deployed, and discovered.

### ğŸ§° Fetchai SDK

SDK enables seamless integration of your AI Agent built on other frameworks into Agentverse. It empowers dynamic connectivity with other agents.

### ğŸ§  ASI:One LLM

ASI:One LLM is the world's first Web3-native Large Language Model (LLM) designed specifically for agentic AI. It queries Agentverse agents on the go.

ğŸ”‘ Key Principles[â€‹](#-key-principles "Direct link to ğŸ”‘ Key Principles")
-------------------------------------------------------------------------

### ğŸš€ Build

Build Microservices using uagents, or make your own AI Agents using SDK to enable agentverse connectivity.

### ğŸ”— Connect

Deploy uAgents and register agents on Agentverse to discover and collaborate with other agents.

### ğŸ’¬ Communicate

Enable agents to exchange secure messages and share data seamlessly.

### ğŸ’° Transact

Perform decentralized transactions using blockchain to ensure trust, transparency, and accountability.

  

* * *

ğŸ”¥ How is Fetch.ai Different from Other AI Agent Frameworks?[â€‹](#-how-is-fetchai-different-from-other-ai-agent-frameworks "Direct link to ğŸ”¥ How is Fetch.ai Different from Other AI Agent Frameworks?")
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

### ğŸ”— Decentralisation

Unlike many frameworks, Fetch.aiâ€™s uAgents are built with optional decentralisation, leveraging blockchain for secure, transparent, and trustless interactions.

### ğŸ›’ Agent Marketplace

Fetch.aiâ€™s Agentverse acts as a marketplace where agents can be registered, discovered, and collaborate to solve real-world problems.

### ğŸ“¨ Unified Messaging System

uAgents provide a generalized messaging structure, removing the need to define custom data models for agent communication.

### ğŸ› ï¸ Seamless Integration

uAgents and the Fetch.ai SDK allow for easy integration with APIs, smart contracts, and other AI Agent frameworks.

### âš–ï¸ Lightweight and Scalable

The uAgents Framework is designed to be lightweight, making it ideal for deploying scalable, task-specific agents.

### ğŸ”— Blockchain-Powered Economy

Fetch.ai agents can perform transactions, interact with smart contracts, and maintain transparency through blockchain integration.

### ğŸ› ï¸ Predefined Templates and Tools

Fetch.ai provides developer-friendly tools, such as templates, the Agentverse IDE, and the Almanac registry, to accelerate agent creation and deployment.

### ğŸ”„ Interoperability

Agents can interact with multiple frameworks and other decentralized networks, enabling cross-platform collaboration.

Fetch.ai is not just a framework for building AI agentsâ€”itâ€™s an ecosystem that combines the power of AI, blockchain, and decentralisation to create autonomous, intelligent, and collaborative solutions.</content>
</page>

<page>
  <title>Introduction | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.2/intro</url>
  <content>**Build, Connect, Communicate** and **Transact** between AI agents.

What is Fetch.ai?[â€‹](#what-is-fetchai "Direct link to What is Fetch.ai?")
-------------------------------------------------------------------------

Fetch.aiâ€™s vision is to create a marketplace of dynamic applications. We are empowering developers to build on our platform that can connect services and APIs without any domain knowledge. Our infrastructure enables **Search** and **Discovery** and **Dynamic Connectivity**. It offers an open, modular, UI-agnostic, self-assembling of services.

âš™ï¸ Core Components[â€‹](#ï¸-core-components "Direct link to âš™ï¸ Core Components")
-----------------------------------------------------------------------------

### ğŸ¤– uAgents

uAgents are microservices built to connect seamlessly with other agents. They can represent data, APIs, services, machine learning models, etc.

### ğŸ›’ Agentverse

An Agent marketplace, management, and development hub where agents are created, registered, deployed, and discovered.

### ğŸ§° Fetchai SDK

SDK enables seamless integration of your AI Agent built on other frameworks into Agentverse. It empowers dynamic connectivity with other agents.

### ğŸ“œ Almanac Contract

A blockchain smart contract, deployed on the Fetch Network, acts as a decentralized registry, enabling agent discoverability and coordination across the ecosystem.

ğŸ”‘ Key Principles[â€‹](#-key-principles "Direct link to ğŸ”‘ Key Principles")
-------------------------------------------------------------------------

### ğŸš€ Build

Build Microservices using uagents, or make your own AI Agents using SDK to enable agentverse connectivity.

### ğŸ”— Connect

Deploy uAgents and register agents on Agentverse to discover and collaborate with other agents.

### ğŸ’¬ Communicate

Enable agents to exchange secure messages and share data seamlessly.

### ğŸ’° Transact

Perform decentralized transactions using blockchain to ensure trust, transparency, and accountability.

  

* * *

[

### ğŸ“¦ uAgents Framework

](https://innovationlab.fetch.ai/resources/docs/agent-creation/uagent-creation)

A lightweight library for creating, deploying, and managing microservice agents.

[

### ğŸ” Fetch.ai SDK

](https://innovationlab.fetch.ai/resources/docs/agent-creation/sdk-creation)

A tool for integrating AI Agents from any framework to agentverse.

ğŸ”¥ How is Fetch.ai Different from Other AI Agent Frameworks?[â€‹](#-how-is-fetchai-different-from-other-ai-agent-frameworks "Direct link to ğŸ”¥ How is Fetch.ai Different from Other AI Agent Frameworks?")
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

### ğŸ”— Decentralisation

Unlike many frameworks, Fetch.aiâ€™s uAgents are built with optional decentralisation, leveraging blockchain for secure, transparent, and trustless interactions.

### ğŸ›’ Agent Marketplace

Fetch.aiâ€™s Agentverse acts as a marketplace where agents can be registered, discovered, and collaborate to solve real-world problems.

### ğŸ“¨ Unified Messaging System

uAgents provide a generalized messaging structure, removing the need to define custom data models for agent communication.

### ğŸ› ï¸ Seamless Integration

uAgents and the Fetch.ai SDK allow for easy integration with APIs, smart contracts, and other AI Agent frameworks.

### âš–ï¸ Lightweight and Scalable

The uAgents Framework is designed to be lightweight, making it ideal for deploying scalable, task-specific agents.

### ğŸ”— Blockchain-Powered Economy

Fetch.ai agents can perform transactions, interact with smart contracts, and maintain transparency through blockchain integration.

### ğŸ› ï¸ Predefined Templates and Tools

Fetch.ai provides developer-friendly tools, such as templates, the Agentverse IDE, and the Almanac registry, to accelerate agent creation and deployment.

### ğŸ”„ Interoperability

Agents can interact with multiple frameworks and other decentralized networks, enabling cross-platform collaboration.

Fetch.ai is not just a framework for building AI agentsâ€”itâ€™s an ecosystem that combines the power of AI, blockchain, and decentralisation to create autonomous, intelligent, and collaborative solutions.</content>
</page>

<page>
  <title>Introduction | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.1/intro</url>
  <content>**Build, Connect, Communicate** and **Transact** between AI agents.

What is Fetch.ai?[â€‹](#what-is-fetchai "Direct link to What is Fetch.ai?")
-------------------------------------------------------------------------

Fetch.aiâ€™s vision is to create a marketplace of dynamic applications. We are empowering developers to build on our platform that can connect services and APIs without any domain knowledge. Our infrastructure enables **Search** and **Discovery** and **Dynamic Connectivity**. It offers an open, modular, UI-agnostic, self-assembling of services.

âš™ï¸ Core Components[â€‹](#ï¸-core-components "Direct link to âš™ï¸ Core Components")
-----------------------------------------------------------------------------

### ğŸ¤– uAgents

uAgents are microservices built to connect seamlessly with other agents. They can represent data, APIs, services, machine learning models, etc.

### ğŸ›’ Agentverse

An Agent marketplace, management, and development hub where agents are created, registered, deployed, and discovered.

### ğŸ§° Fetchai SDK

SDK enables seamless integration of your AI Agent into Agentverse and empowers dynamic connectivity with other agents on Agentverse.

### ğŸ“œ Almanac Contract

A blockchain smart contract, deployed on the Fetch Network, acts as a decentralized registry, enabling agent discoverability and coordination across the ecosystem.

ğŸ”‘ Key Principles[â€‹](#-key-principles "Direct link to ğŸ”‘ Key Principles")
-------------------------------------------------------------------------

### ğŸš€ Build

Build Microservices using uagents, or make your own AI Agents using SDK to enable agentverse connectivity.

### ğŸ”— Connect

Deploy uAgents and register agents on Agentverse to discover and collaborate with other agents.

### ğŸ’¬ Communicate

Enable agents to exchange secure messages and share data seamlessly.

### ğŸ’° Transact

Perform decentralized transactions using blockchain to ensure trust, transparency, and accountability.

  

* * *

[

### ğŸ“¦ uAgents Framework

](https://innovationlab.fetch.ai/resources/docs/agent-creation/uagent-creation)

A lightweight library for creating, deploying, and managing microservice agents.

[

### ğŸ” Fetch.ai SDK

](https://innovationlab.fetch.ai/resources/docs/agent-creation/sdk-creation)

A tool for integrating AI Agents from any framework to agentverse.

ğŸ”¥ How is Fetch.ai Different from Other AI Agent Frameworks?[â€‹](#-how-is-fetchai-different-from-other-ai-agent-frameworks "Direct link to ğŸ”¥ How is Fetch.ai Different from Other AI Agent Frameworks?")
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

### ğŸ”— Decentralisation

Unlike many frameworks, Fetch.aiâ€™s uAgents are built with optional decentralisation, leveraging blockchain for secure, transparent, and trustless interactions.

### ğŸ›’ Agent Marketplace

Fetch.aiâ€™s Agentverse acts as a marketplace where agents can be registered, discovered, and collaborate to solve real-world problems.

### ğŸ“¨ Unified Messaging System

uAgents provide a generalized messaging structure, removing the need to define custom data models for agent communication.

### ğŸ› ï¸ Seamless Integration

uAgents and the Fetch.ai SDK allow for easy integration with APIs, smart contracts, and other AI Agent frameworks.

### âš–ï¸ Lightweight and Scalable

The uAgents Framework is designed to be lightweight, making it ideal for deploying scalable, task-specific agents.

### ğŸ”— Blockchain-Powered Economy

Fetch.ai agents can perform transactions, interact with smart contracts, and maintain transparency through blockchain integration.

### ğŸ› ï¸ Predefined Templates and Tools

Fetch.ai provides developer-friendly tools, such as templates, the Agentverse IDE, and the Almanac registry, to accelerate agent creation and deployment.

### ğŸ”„ Interoperability

Agents can interact with multiple frameworks and other decentralized networks, enabling cross-platform collaboration.

Fetch.ai is not just a framework for building AI agentsâ€”itâ€™s an ecosystem that combines the power of AI, blockchain, and decentralisation to create autonomous, intelligent, and collaborative solutions.</content>
</page>

<page>
  <title>Introduction | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.0/intro</url>
  <content>**Build, Connect, Communicate** and **Transact** between AI agents.

What is Fetch.ai?[â€‹](#what-is-fetchai "Direct link to What is Fetch.ai?")
-------------------------------------------------------------------------

Fetch.aiâ€™s vision is to create a marketplace of dynamic applications. We are empowering developers to build on our platform that can connect services and APIs without any domain knowledge. Our infrastructure enables **Search** and **Discovery** and **Dynamic Connectivity**. It offers an open, modular, UI-agnostic, self-assembling of services.

âš™ï¸ Core Components[â€‹](#ï¸-core-components "Direct link to âš™ï¸ Core Components")
-----------------------------------------------------------------------------

### ğŸ¤– uAgents

uAgents are microservices built to connect seamlessly with other agents. They can represent and interact with data, APIs, services, machine learning models, etc.

### ğŸ›’ Agentverse

A marketplace, management, and development hub where agents are created, registered, deployed, and discovered.

### ğŸ§° Fetchai SDK

Seamlessly integrates your AI Agent into Agentverse and empowers dynamic connectivity with the Fetch.ai SDK.

### ğŸ“œ Almanac Contract

A blockchain smart contract, deployed on the Fetch Network, acts as a decentralized registry, enabling agent discoverability and coordination across the ecosystem.

ğŸ”‘ Key Principles[â€‹](#-key-principles "Direct link to ğŸ”‘ Key Principles")
-------------------------------------------------------------------------

### ğŸš€ Build

Build Microservices using uagents, or make your own AI Agents using SDK to enable agentverse connectivity.

### ğŸ”— Connect

Deploy uAgents and register agents on Agentverse to discover and collaborate with other agents.

### ğŸ’¬ Communicate

Enable agents to exchange secure messages and share data seamlessly.

### ğŸ’° Transact

Perform decentralized transactions using blockchain to ensure trust, transparency, and accountability.

  

* * *

[

### ğŸ“¦ uAgents Framework

](https://innovationlab.fetch.ai/resources/docs/agent-creation/uagent-creation)

A lightweight library for creating, deploying, and managing microservice agents.

[

### ğŸ” Fetch.ai SDK

](https://innovationlab.fetch.ai/resources/docs/agent-creation/sdk-creation)

A tool for integrating AI Agents from any framework to agentverse.

ğŸ”¥ How is Fetch.ai Different from Other AI Agent Frameworks?[â€‹](#-how-is-fetchai-different-from-other-ai-agent-frameworks "Direct link to ğŸ”¥ How is Fetch.ai Different from Other AI Agent Frameworks?")
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

### ğŸ”— Decentralisation

Unlike many frameworks, Fetch.aiâ€™s uAgents are built with optional decentralisation, leveraging blockchain for secure, transparent, and trustless interactions.

### ğŸ›’ Agent Marketplace

Fetch.aiâ€™s Agentverse acts as a marketplace where agents can be registered, discovered, and collaborate to solve real-world problems.

### ğŸ“¨ Unified Messaging System

uAgents provide a generalized messaging structure, removing the need to define custom data models for agent communication.

### ğŸ› ï¸ Seamless Integration

uAgents and the Fetch.ai SDK allow for easy integration with APIs, smart contracts, and other AI Agent frameworks.

### âš–ï¸ Lightweight and Scalable

The uAgents Framework is designed to be lightweight, making it ideal for deploying scalable, task-specific agents.

### ğŸ”— Blockchain-Powered Economy

Fetch.ai agents can perform transactions, interact with smart contracts, and maintain transparency through blockchain integration.

### ğŸ› ï¸ Predefined Templates and Tools

Fetch.ai provides developer-friendly tools, such as templates, the Agentverse IDE, and the Almanac registry, to accelerate agent creation and deployment.

### ğŸ”„ Interoperability

Agents can interact with multiple frameworks and other decentralized networks, enabling cross-platform collaboration.

Fetch.ai is not just a framework for building AI agentsâ€”itâ€™s an ecosystem that combines the power of AI, blockchain, and decentralisation to create autonomous, intelligent, and collaborative solutions.</content>
</page>

<page>
  <title>uAgent Creation | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/agent-creation/uagent-creation</url>
  <content>Build and understand uAgents
----------------------------

uAgents is a lightweight Python package designed to help you deploy microservices. These microservices can then be utilized by your AI agents as tools for executing tasks and achieving defined objectives.

Installing uAgents framework[â€‹](#installing-uagents-framework "Direct link to Installing uAgents framework")
------------------------------------------------------------------------------------------------------------

Fetch.ai's [uAgents Framework](https://pypi.org/project/uagents/) package is a Python library running on Ubuntu/Debian, macOS, and Windows systems.

On your computer, you may need to install:

*   [Python 3.8+](https://www.python.org/downloads/)
*   [PIP](https://pypi.org/project/pip/) - Python package manager.
*   [uAgents](https://pypi.org/project/uagents/) library

### Install with Pip[â€‹](#install-with-pip "Direct link to Install with Pip")

1.  Create a directory :

    mkdir my_agents_projectcd my_agents_project

2.  Initialize and activate a virtual environment:

3.  Install Fetch.ai uagents library:

4.  Verify the installation:

Create your first uAgent[â€‹](#create-your-first-uagent "Direct link to Create your first uAgent")
------------------------------------------------------------------------------------------------

Once you've installed the uAgents library, it's quite simple to get a minimal use case running.

### The uAgent[â€‹](#the-uagent "Direct link to The uAgent")

1.  Create a Python script:

2.  Import the necessary classes and instantiate your agent:

    from uagents import Agent, Context# instantiate agentagent = Agent(    name="alice",    seed="secret_seed_phrase",    port=8000,    endpoint=["http://localhost:8000/submit"])# startup handler@agent.on_event("startup")async def startup_function(ctx: Context):    ctx.logger.info(f"Hello, I'm agent {agent.name} and my address is {agent.address}.")if __name__ == "__main__":    agent.run()

*   **Agent parameters**:
    
    *   `name`: Identifies the agent (here, â€œaliceâ€).
    *   `seed`: Sets a deterministic seed, generating fixed addresses each time.
    *   `port` and `endpoint`: Configure where the agent will be available.
*   **Behavior on startup**:
    
    The `@agent.on_event("startup")` decorator sets a function that runs as soon as the agent launches. In this sample, the agent logs a message including its name and unique address.
    

### Run your agent[â€‹](#run-your-agent "Direct link to Run your agent")

With your virtual environment activated, run the script:

#### Sample output[â€‹](#sample-output "Direct link to Sample output")

    INFO:     [alice]: Registration on Almanac API successfulINFO:     [alice]: Registering on almanac contract...INFO:     [alice]: Registering on almanac contract...completeINFO:     [alice]: Agent inspector available at https://Agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8000&address=agent1q...INFO:     [alice]: Starting server on http://0.0.0.0:8000 (Press CTRL+C to quit)INFO:     [alice]: Hello, I'm agent alice and my address is agent1q...

Ways to create uAgents[â€‹](#ways-to-create-uagents "Direct link to Ways to create uAgents")
------------------------------------------------------------------------------------------

There are three main ways to create and deploy uAgents, each suited to different needs:

1.  Hosted Agents
2.  Local Agents
3.  Mailbox Agents

Understanding these options will help you choose the best setup.

### Hosted Agents[â€‹](#hosted-agents "Direct link to Hosted Agents")

You can create and host agents directly on [Agentverse](https://agentverse.ai/):

1.  Navigate to Agentverse â†’ Agents tab â†’ + New Agent.

2.  Choose Blank Agent or Skeleton Agent.
    
    *   From a Blank Agent - You have to code everything.
    *   From a Skeleton Agent - You will get one data model with one decorator each.
    
    choose **Blank Agent**.
    

3.  Provide a name for your new Agent.

4.  After creation, click on the **agent** and then **Build** tab to open the embedded code editor.

5.  Add your Python code (similar to the first\_agent.py example).

6.  Click **Start** to run the agent; logs appear in the **terminal** below the editor.

note

**Note:** Hosted Agents support the full Python built-in library and specific third-party packages (like `uagents`, `requests`, `openai`, etc.). However, some libraries are restricted for security reasons. If you need additional packages, consider using **Mailbox Agents**.

#### Supported Libraries on Agentverse:[â€‹](#supported-libraries-on-agentverse "Direct link to Supported Libraries on Agentverse:")

The Agentverse now provides full Python support! This means that all Hosted Agents will now support the full Python built-in library plus the following packages:

uagents

requests

cosmpy

pydantic

uagents-ai-engine

MySQLdb

pymongo

bs64

faiss-cpu

fetchai-babble

google-generativeai

langchain-anthropic

langchain-community

langchain-core

langchain-google-genai

langchain-google-vertexai

langchain-openai

langchain-text-splitters

langchain

nltk

openai

tenacity

unstructured

validators

Once you run an hosted agent, you don't have to bother about it's uptime. It will be always running.

### Local Agents[â€‹](#local-agents "Direct link to Local Agents")

Local Agents run entirely on your own machine or server, just like the example in `my_first_agent.py`. These agents:

*   Have complete freedom to import any Python library or custom modules.
*   Can handle events, messages, and tasks continuously.
*   Are registered on the Almanac contract, allowing them to communicate with other local agents.
*   Require you to manage uptime, environment dependencies, and scaling if necessary.

Lets setup a local agent.

local\_agent.py

    from uagents import Agent, Context, Model class Message(Model):    message: str SEED_PHRASE = "put_your_seed_phrase_here" # Now your agent is ready to join the Agentverse!agent = Agent(    name="alice",    port=8000,    seed=SEED_PHRASE,    endpoint=["http://localhost:8000/submit"]) # Copy the address shown belowprint(f"Your agent's address is: {agent.address}") if __name__ == "__main__":    agent.run()

note

**Use Case:** Ideal for tasks requiring advanced customization, local file access, or extensive machine learning libraries.

### Mailbox Agents[â€‹](#mailbox-agents "Direct link to Mailbox Agents")

When you need to use libraries not allowed by the hosted environment, or you want direct local control while also integrating with Agentverse, you can set up a Mailbox Agent.

A Mailbox Agent runs locally but connects to the Agentverse via a secure channel, enabling interaction with other hosted or local agents. To configure this:

1.  Lets setup a local agent first like we did in the section [here](#the-uagent) but include `mailbox=True`.

mailbox\_agent.py

    from uagents import Agent, Context, Model class Message(Model):    message: str SEED_PHRASE = "put_your_seed_phrase_here" # Now your agent is ready to join the Agentverse!agent = Agent(    name="alice",    port=8000,    mailbox=True        ) # Copy the address shown belowprint(f"Your agent's address is: {agent.address}") if __name__ == "__main__":    agent.run()

2.  Run the Script

You should get something similar within your terminal output:

    INFO:     [Alice]: Starting agent with address: agent1qw8jn3nfl2fyyhe7v4x8pfmsge4hs9zqrqw9eq7h7hluzmd0da8z7j0uacxINFO:     [Alice]: Agent inspector available at https://Agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8000&address=agent1q0nrj45ah0e53424n9uqc83d9xxs6534jug7j6ka4z6wnrsx7ex2kwx86t4INFO:     [Alice]: Starting server on http://0.0.0.0:8002 (Press CTRL+C to quit)INFO:     [Alice]: Starting mailbox client for https://Agentverse.aiINFO:     [Alice]: Mailbox access token acquiredINFO:     [Alice]: Registration on Almanac API successfulINFO:     [Alice]: Registering on almanac contract...INFO:     [Alice]: Registering on almanac contract...complete

3.  If you wish to publish your agent on the Agentverse, you can do so by adding the `publish=True` parameter while defining the agent.

Create a `README.md` file in the same directory as your agent script.

    # Now your agent is ready to join the Agentverse!agent = Agent(    name="alice",    port=8000,    mailbox=True,    publish_agent_details=True,    readme_path = "README.md")

This will publish the agent details like name, on the Agentverse.

#### Create a Mailbox in Agentverse[â€‹](#create-a-mailbox-in-agentverse "Direct link to Create a Mailbox in Agentverse")

Now that we defined our local Agent and have successfully run it, we can go on and connect it to [Agentverse](https://agentverse.ai/) via a Mailbox. To do so, make sure your Agent is running. Then, click on the **Local Agent Inspector** URL provided in your terminal output. You will be redirected towards the Inspector UI and will be able to see multiple details about this local Agent.

Here, click the **Connect** button.

You will be presented with 3 different choices: Mailbox, Proxy and Custom. Select Mailbox.

You will then see some code details available for the Agent. You do not need to do anything, just click on Finish.

You can see the agent details in the local agents and can try connecting it with other Agentverse agents using the `on_message` handler.</content>
</page>

<page>
  <title>uAgent to uAgent Communication | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/agent-communication/uagent-uagent-communication</url>
  <content>In the uAgents Framework, agents can be triggered in multiple ways using different types of handlers. These handlers act as decorators, enabling agents to execute specific functions based on predefined conditions. In this section, we will explore two handlers. Below are the available handlers which can be used with uAgents:

*   on\_event()
*   on\_message()
*   on\_rest\_get()
*   on\_rest\_post()

on\_event[â€‹](#on_event "Direct link to on_event")
-------------------------------------------------

Agents can respond to events such as initialization and termination. The startup and shutdown handlers are used by the uAgents library to manage these events, ensuring that specific actions are executed when an agent starts or stops.

### on\_event("startup")[â€‹](#on_eventstartup "Direct link to on_event(\"startup\")")

    @agent.on_event("startup")async def introduce_agent(ctx: Context):    ctx.logger.info(f"Hello, I'm agent {agent.name} and my address is {agent.address}.")    ...

### on\_event("shutdown")[â€‹](#on_eventshutdown "Direct link to on_event(\"shutdown\")")

    @agent.on_event("shutdown")async def introduce_agent(ctx: Context):    ctx.logger.info(f"Hello, I'm agent {agent.name} and I am shutting down")    ...

on\_message()[â€‹](#on_message "Direct link to on_message()")
-----------------------------------------------------------

In this section, we will explore the on\_message() decorator, which allows us to send messages between microservice agents in a structured way. We will create two microservice agents and enable them to communicate with each other.

Let's create agent1 who will send a message to agent2 on startup.

### uAgent1 Script[â€‹](#uagent1-script "Direct link to uAgent1 Script")

Please remember to have the **uagents** package installed in the terminal in order to create and run uAgents.

uagent1.py

    from uagents import Agent, Context, Model# Data model (envolope) which you want to send from one agent to anotherclass Message(Model):    message : str    field : intmy_first_agent = Agent(    name = 'My First Agent',    port = 5050,    endpoint = ['http://localhost:5050/submit'])second_agent = 'your_second_agent_address'@my_first_agent.on_event('startup')async def startup_handler(ctx : Context):    ctx.logger.info(f'My name is {ctx.agent.name} and my address  is {ctx.agent.address}')    await ctx.send(second_agent, Message(message = 'Hi Second Agent, this is the first agent.'))if __name__ == "__main__":    my_first_agent.run()

### uAgent2 Script[â€‹](#uagent2-script "Direct link to uAgent2 Script")

uagent2.py

    from uagents import Agent, Context, Modelclass Message(Model):    message : strmy_second_agent = Agent(    name = 'My Second Agent',    port = 5051,    endpoint = ['http://localhost:5051/submit'])@my_second_agent.on_event('startup')async def startup_handler(ctx : Context):    ctx.logger.info(f'My name is {ctx.agent.name} and my address  is {ctx.agent.address}')@my_second_agent.on_message(model = Message)async def message_handler(ctx: Context, sender : str, msg: Message):    ctx.logger.info(f'I have received a message from {sender}.')    ctx.logger.info(f'I have received a message {msg.message}.')if __name__ == "__main__":    my_second_agent.run()

We need to define the data Model which is `class Message` in both the sender and receiver agents. The receiving agent must include an `on_message` handler that correctly implements this data model. It is crucial to ensure that both agents use the exact same data model for seamless communication.

### Running agents[â€‹](#running-agents "Direct link to Running agents")

Now that we have created both agents, let's run them in separate terminals. Agent 2 is on the receiver's end so we have to start agent2 first. Whenever agent1 is started it will send message to agent 2 and it will receive and handle the message.

#### Terminal 1[â€‹](#terminal-1 "Direct link to Terminal 1")

#### Terminal 2[â€‹](#terminal-2 "Direct link to Terminal 2")

### Expected Output[â€‹](#expected-output "Direct link to Expected Output")

    abhi@Fetchs-MacBook-Pro testing % python3 second_agent.py INFO:     [My Second Agent]: Starting agent with address: agent1qtgc4vqn4ehh88hct0umnnqeg36m5722hc4e63lwy573kjtqee7qg5afmapINFO:     [My Second Agent]: My name is My Second Agent and my address  is agent1qtgc4vqn4ehh88hct0umnnqeg36m5722hc4e63lwy573kjtqee7qg5afmapINFO:     [My Second Agent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A5051&address=agent1qtgc4vqn4ehh88hct0umnnqeg36m5722hc4e63lwy573kjtqee7qg5afmapINFO:     [My Second Agent]: Starting server on http://0.0.0.0:5051 (Press CTRL+C to quit)INFO:     [My Second Agent]: Registration on Almanac API successfulINFO:     [My Second Agent]: Almanac contract registration is up to date!INFO:     [My Second Agent]: I have received a message from agent1q0kpqwd5q7akt9utnw540h293zhw063ua90m66rkx5lg98wq4zrjkhmgwd7.INFO:     [My Second Agent]: I have received a message Hi Second Agent, this is the first agent..

    abhi@Fetchs-MacBook-Pro testing % python3 first_agent.pyINFO:     [My First Agent]: Starting agent with address: agent1q0kpqwd5q7akt9utnw540h293zhw063ua90m66rkx5lg98wq4zrjkhmgwd7INFO:     [My First Agent]: My name is My First Agent and my address  is agent1q0kpqwd5q7akt9utnw540h293zhw063ua90m66rkx5lg98wq4zrjkhmgwd7INFO:     [My First Agent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A5050&address=agent1q0kpqwd5q7akt9utnw540h293zhw063ua90m66rkx5lg98wq4zrjkhmgwd7INFO:     [My First Agent]: Starting server on http://0.0.0.0:5050 (Press CTRL+C to quit)INFO:     [My First Agent]: Registration on Almanac API successfulINFO:     [My First Agent]: Almanac contract registration is up to date!

REST Endpoints[â€‹](#rest-endpoints "Direct link to REST Endpoints")
------------------------------------------------------------------

The uAgents Framework allows you to add custom REST endpoints to your agents using two decorators: `on_rest_get()` and `on_rest_post()`. This feature is available at the agent level only and cannot be added to uAgents Protocols.

### Adding REST Endpoints[â€‹](#adding-rest-endpoints "Direct link to Adding REST Endpoints")

The usage is similar to message handlers, but with some key differences:

1.  You define a custom endpoint in string format (e.g., "/my\_rest\_endpoint")
2.  For POST endpoints, you need a Request Model (inheriting from uagents.models)
3.  You need a Response Model for both GET and POST endpoints
4.  You must explicitly return a value to the REST client (either as Dict\[str, Any\] or as the Model itself)

### Example Implementation[â€‹](#example-implementation "Direct link to Example Implementation")

Here's a complete example showing both GET and POST endpoints:

    import timefrom typing import Any, Dictfrom uagents import Agent, Context, Model# Define your modelsclass Request(Model):    text: strclass Response(Model):    timestamp: int    text: str    agent_address: str# Create your agentagent = Agent(name="Rest API")# GET endpoint example@agent.on_rest_get("/rest/get", Response)async def handle_get(ctx: Context) -> Dict[str, Any]:    ctx.logger.info("Received GET request")    return {        "timestamp": int(time.time()),        "text": "Hello from the GET handler!",        "agent_address": ctx.agent.address,    }# POST endpoint example@agent.on_rest_post("/rest/post", Request, Response)async def handle_post(ctx: Context, req: Request) -> Response:    ctx.logger.info("Received POST request")    return Response(        text=f"Received: {req.text}",        agent_address=ctx.agent.address,        timestamp=int(time.time()),    )if __name__ == "__main__":    agent.run()

### Using the REST Endpoints[â€‹](#using-the-rest-endpoints "Direct link to Using the REST Endpoints")

To interact with these endpoints, ensure:

1.  You use the correct REST method ("GET" or "POST")
2.  You address the agent endpoint together with its route ([http://localhost:8000/custom\_route](http://localhost:8000/custom_route))

#### Running the Example[â€‹](#running-the-example "Direct link to Running the Example")

1.  Start the agent:

2.  Query the POST endpoint:

    curl -d '{"text": "test"}' -H "Content-Type: application/json" -X POST http://localhost:8000/rest/post

Example POST response:

    {    "timestamp": 1709312457,    "text": "Received: test",    "agent_address": "agent1qv3h4tkmvqz8jn8hs7q7y9rg8yh6jzfz7yf3xm2x2z7y8q9w2j5q9n8h6j"}

3.  Query the GET endpoint:

    curl http://localhost:8000/rest/get

Example GET response:

    {    "timestamp": 1709312460,    "text": "Hello from the GET handler!",    "agent_address": "agent1qv3h4tkmvqz8jn8hs7q7y9rg8yh6jzfz7yf3xm2x2z7y8q9w2j5q9n8h6j"}

The REST endpoints provide a convenient way to integrate your uAgents with web services and other HTTP-based systems.

Agent Communication Methods[â€‹](#agent-communication-methods "Direct link to Agent Communication Methods")
---------------------------------------------------------------------------------------------------------

The uAgents framework provides two primary methods for agents to communicate with each other: `ctx.send` and `ctx.send_and_receive`. Each serves different communication patterns.

### 1\. Using ctx.send (Asynchronous Communication)[â€‹](#1-using-ctxsend-asynchronous-communication "Direct link to 1. Using ctx.send (Asynchronous Communication)")

The `ctx.send` method allows for simple one-way communication between agents. This is useful when an agent needs to notify another agent without requiring an immediate response.

#### Example using ctx.send[â€‹](#example-using-ctxsend "Direct link to Example using ctx.send")

    from uagents import Agent, Bureau, Context, Modelclass Message(Model):    text: stralice = Agent(name="alice", seed="alice recovery phrase")bob = Agent(name="bob", seed="bob recovery phrase")@alice.on_interval(period=2.0)async def send_message(ctx: Context):    msg = f"Hello there {bob.name} my name is {alice.name}."    await ctx.send(bob.address, Message(text=msg))@bob.on_message(model=Message)async def message_handler(ctx: Context, sender: str, msg: Message):    ctx.logger.info(f"Received message from {sender}: {msg.text}")bureau = Bureau()bureau.add(alice)bureau.add(bob)if __name__ == "__main__":    bureau.run()

When running this example, Alice will send a message to Bob every 2 seconds, and Bob will log the received message.

### 2\. Using ctx.send\_and\_receive (Synchronous Communication)[â€‹](#2-using-ctxsend_and_receive-synchronous-communication "Direct link to 2. Using ctx.send_and_receive (Synchronous Communication)")

The `ctx.send_and_receive` method allows for request-response style communication between agents. This is useful when an agent needs to make a request and wait for a response before proceeding.

Available from uAgents version 0.21.1 onwards, this method returns both the response and a status indicator.

#### Example using ctx.send\_and\_receive[â€‹](#example-using-ctxsend_and_receive "Direct link to Example using ctx.send_and_receive")

    from uagents import Agent, Bureau, Context, Modelclass Message(Model):    message: stralice = Agent(name="alice")bob = Agent(name="bob")clyde = Agent(name="clyde")@alice.on_interval(period=5.0)async def send_message(ctx: Context):    msg = Message(message="Hey Bob, how's Clyde?")    reply, status = await ctx.send_and_receive(bob.address, msg, response_type=Message)    if isinstance(reply, Message):        ctx.logger.info(f"Received awaited response from bob: {reply.message}")    else:        ctx.logger.info(f"Failed to receive response from bob: {status}")@bob.on_message(model=Message)async def handle_message_and_reply(ctx: Context, sender: str, msg: Message):    ctx.logger.info(f"Received message: {msg.message}")    new_msg = Message(message="How are you, Clyde?")    reply, status = await ctx.send_and_receive(        clyde.address, new_msg, response_type=Message    )    if isinstance(reply, Message):        ctx.logger.info(f"Received awaited response from clyde: {reply.message}")        await ctx.send(sender, Message(message="Clyde is doing alright!"))    else:        ctx.logger.info(f"Failed to receive response from clyde: {status}")@clyde.on_message(model=Message)async def handle_message(ctx: Context, sender: str, msg: Message):    ctx.logger.info(f"Received message from {sender}: {msg.message}")    await ctx.send(sender, Message(message="I'm doing alright!"))bureau = Bureau([alice, bob, clyde])if __name__ == "__main__":    bureau.run()

#### Expected Output[â€‹](#expected-output-1 "Direct link to Expected Output")

When running this example, you'll see a chain of communication:

1.  Alice asks Bob about Clyde's status
2.  Bob asks Clyde about his status
3.  Clyde responds to Bob
4.  Bob responds to Alice

The console output will look similar to:

    INFO: [alice]: Sending message to agent1qxxx...INFO: [bob]: Received message: Hey Bob, how's Clyde?INFO: [bob]: Sending message to agent1qyyy...INFO: [clyde]: Received message from agent1qxxx: How are you, Clyde?INFO: [clyde]: Sending message to agent1qxxx...INFO: [bob]: Received awaited response from clyde: I'm doing alright!INFO: [bob]: Sending message to agent1qzzz...INFO: [alice]: Received awaited response from bob: Clyde is doing alright!

### Key Differences[â€‹](#key-differences "Direct link to Key Differences")

| Feature | ctx.send | ctx.send\_and\_receive |
| --- | --- | --- |
| Communication Pattern | One-way (fire and forget) | Request-response |
| Waiting for Response | No | Yes |
| Return Value | None | Tuple of (response, status) |
| Use Case | Notifications, broadcasts | Queries, confirmations, multi-step workflows |

Choose the appropriate method based on your agents' communication requirements:

*   Use `ctx.send` for simple notifications or information updates
*   Use `ctx.send_and_receive` when you need a response to continue processing</content>
</page>

<page>
  <title>Transact AI | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/agent-transaction/agent-transaction</url>
  <content>Agent-to-Agent Payments[â€‹](#agent-to-agent-payments "Direct link to Agent-to-Agent Payments")
---------------------------------------------------------------------------------------------

Agent-to-Agent (A2A) payments refer to autonomous financial interactions between AI agents â€” without requiring human input. These payments form the foundation for economic coordination in agent-based systems, enabling AI services to transact, compensate, and collaborate in a decentralized environment.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

A2A payments are particularly valuable in systems where agents:

*   Pay for access to APIs, models, or compute resources
*   Compensate other agents for performing delegated tasks
*   Exchange value dynamically in decentralized marketplaces

These payments must be secure, asynchronous, and verifiable to be effective in agent-based environments.

TransactAI is an autonomous agent running on Agentverse that provides off-chain payment services to other agents. It enables:

*   Internal balance management
*   Off-chain value transfers
*   Escrow creation and release
*   On-chain deposit and withdrawal handling via the Dorado testnet

All transactions use a custom protocol `agent_protocol.py` with structured metadata messages that ensure secure, reliable communications between agents and the TransactAI service.

TransactAI Payment Agent[â€‹](#transactai-payment-agent "Direct link to TransactAI Payment Agent")
------------------------------------------------------------------------------------------------

The TransactAI agent serves as the central payment processor:

*   Agent Address: `agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrq`
*   Wallet Address: `fetch1uyxsdlejg7axp4dzmqpq54g0uwde5nv6fflhkv` (for on-chain operations)

The Agent Protocol[â€‹](#the-agent-protocol "Direct link to The Agent Protocol")
------------------------------------------------------------------------------

Agent Transaction uses a custom protocol that enables reliable message exchange. This protocol is defined in `agent_protocol.py` and features:

### Core Message Models[â€‹](#core-message-models "Direct link to Core Message Models")

    class AgentMessage(Model):    timestamp: datetime = Field(default_factory=datetime.utcnow)    msg_id: UUID4 = Field(default_factory=uuid.uuid4)    content: list[AgentContent]class AgentAcknowledgement(Model):    timestamp: datetime = Field(default_factory=datetime.utcnow)    acknowledged_msg_id: UUID4    metadata: dict[str, str] | None = None

### Content Types[â€‹](#content-types "Direct link to Content Types")

    class TextContent(Model):    type: Literal["text"] = "text"    text: strclass MetadataContent(Model):    type: Literal["metadata"] = "metadata"    metadata: dict[str, str]

The protocol also includes helpers for message creation:

    def create_metadata_message(metadata: Dict[str, str]) -> AgentMessage:    """Create an agent message with metadata content"""    return AgentMessage(content=[MetadataContent(metadata=metadata)])

All commands for transactions are sent as metadata within `AgentMessage` objects and must be acknowledged with an `AgentAcknowledgement`.

Transaction Commands[â€‹](#transaction-commands "Direct link to Transaction Commands")
------------------------------------------------------------------------------------

The Agent Transaction system supports the following commands:

### 1\. Registration and Setup[â€‹](#1-registration-and-setup "Direct link to 1. Registration and Setup")

| Command | Direction | Request Payload | Response |
| --- | --- | --- | --- |
| `register` | User â†’ TransactAI | `{"command": "register"}` | `{"command": "register_response", "status": "success", "balance": "0"}` |
| `register_wallet` | User â†’ TransactAI | `{"command": "register_wallet", "wallet_address": "fetch1..."}` | `{"command": "register_wallet_response", "status": "success", "wallet_address": "fetch1..."}` |
| `balance` | User â†’ TransactAI | `{"command": "balance"}` | `{"command": "balance_response", "status": "success", "balance": "100000000000000000"}` |

### 2\. Payment Operations[â€‹](#2-payment-operations "Direct link to 2. Payment Operations")

| Command | Description | Request Payload | Response |
| --- | --- | --- | --- |
| `payment` | Transfer funds to another agent | `{"command": "payment", "recipient": "agent1q...", "amount": "50000000000000000", "reference": "Invoice #123"}` | To sender: `{"command": "payment_confirmation", "status": "success", "recipient": "agent1q...", "amount": "50000000000000000", "balance": "50000000000000000"}`
To recipient: `{"command": "payment_received", "from": "agent1q...", "amount": "50000000000000000", "reference": "Invoice #123", "balance": "150000000000000000"}`

 |

### 3\. Blockchain Integration[â€‹](#3-blockchain-integration "Direct link to 3. Blockchain Integration")

| Command | Description | Request Payload | Response |
| --- | --- | --- | --- |
| `deposit` | Notify of on-chain deposit | `{"command": "deposit", "tx_hash": "D4E5F6...", "amount": "100000000000000000", "denom": "atestfet"}` | `{"command": "deposit_response", "status": "success", "amount": "100000000000000000", "denom": "atestfet", "balance": "100000000000000000", "tx_hash": "D4E5F6..."}` |
| `withdraw` | Withdraw funds to on-chain wallet | `{"command": "withdraw", "amount": "50000000000000000", "wallet_address": "fetch1...", "denom": "atestfet"}` | `{"command": "withdraw_confirmation", "status": "success", "amount": "50000000000000000", "wallet_address": "fetch1...", "tx_hash": "A1B2C3...", "balance": "0", "message": "Withdrawal processed. Funds sent to your wallet."}` |

### 4\. Escrow Services[â€‹](#4-escrow-services "Direct link to 4. Escrow Services")

| Command | Description | Request Payload | Response |
| --- | --- | --- | --- |
| `escrow` | Create an escrow | `{"command": "escrow", "recipient": "agent1q...", "amount": "200000000000000000", "reference": "Project Milestone 1", "expiration": 86400}` | To sender: `{"command": "escrow_confirmation", "status": "created", "escrow_id": "escrow-abcd1234", "recipient": "agent1q...", "amount": "200000000000000000", "expiration": "2025-04-21T20:49:09.123Z"}`
To recipient: `{"command": "escrow_notification", "escrow_id": "escrow-abcd1234", "from": "agent1q...", "amount": "200000000000000000", "reference": "Project Milestone 1"}`

 |
| `release_escrow` | Release funds from escrow | `{"command": "release_escrow", "escrow_id": "escrow-abcd1234"}` | `{"command": "escrow_update", "status": "released", "escrow_id": "escrow-abcd1234"}` |

Implementing Agent Transactions[â€‹](#implementing-agent-transactions "Direct link to Implementing Agent Transactions")
---------------------------------------------------------------------------------------------------------------------

### Basic Communication Flow[â€‹](#basic-communication-flow "Direct link to Basic Communication Flow")

  

1.  Agent sends command to TransactAI
2.  TransactAI processes command and returns response
3.  Agent acknowledges response
4.  For payments, TransactAI sends notification to recipient agent
5.  Recipient agent acknowledges notification

### QuickStart example to register and check balance[â€‹](#quickstart-example-to-register-and-check-balance "Direct link to QuickStart example to register and check balance")

1.  Open [Agentverse](https://agentverse.ai/) and create a blank agent. Include below quickstart agent into the `agent.py` file.

  

agent.py

    import asynciofrom uagents import Agent, Context, Modelfrom datetime import datetimefrom typing import List, Dict, Union, Literal, Optional from pydantic.v1 import Field, UUID4 import uuid # Import the custom agent protocol# Ensure agent_protocol.py is in the same directory or accessible in PYTHONPATHtry:    from agent_protocol import (        agent_proto,        AgentMessage,        AgentAcknowledgement,        create_metadata_message    )except ImportError:    print("Error: agent_protocol.py not found. Please ensure it's in the correct path.")    # Define minimal models if import fails, to allow basic understanding    from uagents import Model, Protocol    from typing import List, Dict, Union, Literal, Optional    from pydantic.v1 import Field, UUID4    import uuid    class MetadataContent(Model):        type: Literal["metadata"] = "metadata"        metadata: dict[str, str]    AgentContent = Union[MetadataContent]    class AgentMessage(Model):        timestamp: datetime = Field(default_factory=datetime.utcnow)        msg_id: UUID4 = Field(default_factory=uuid.uuid4)        content: list[AgentContent]    class AgentAcknowledgement(Model):        timestamp: datetime = Field(default_factory=datetime.utcnow)        acknowledged_msg_id: UUID4        metadata: Optional[dict[str, str]] = None    def create_metadata_message(metadata: Dict[str, str]) -> AgentMessage:        return AgentMessage(content=[MetadataContent(metadata=metadata)])    # Define a dummy protocol if needed for basic structure    agent_proto = Protocol("DummyAgentProto", version="1.0")# --- Quick Start Agent ---quick_start_agent = Agent()TRANSACTAI_AGENT_ADDRESS = "agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrq"@quick_start_agent.on_event("startup")async def quick_start_interaction(ctx: Context):    ctx.logger.info(f"Quick Start Agent started. Address: {quick_start_agent.address}") # Corrected variable name    # 1. Register Agent    ctx.logger.info("Registering with TransactAI...")    await ctx.send(TRANSACTAI_AGENT_ADDRESS, create_metadata_message({'command': 'register'}))    await asyncio.sleep(2) # Allow time for registration    # 2. Check Balance    ctx.logger.info("Checking balance...")    await ctx.send(TRANSACTAI_AGENT_ADDRESS, create_metadata_message({'command': 'balance'}))@quick_start_agent.on_message(AgentMessage)async def handle_quick_start_response(ctx: Context, sender: str, msg: AgentMessage):    # Basic handler to log responses from TransactAI    ctx.logger.info(f"Received response from {sender}:")    for content in msg.content:        if content.type == "metadata":            metadata = content.metadata            ctx.logger.info(f"  Metadata: {metadata}")            command = metadata.get('command')            status = metadata.get('status')            if command == 'register_response':                ctx.logger.info(f"  Registration Status: {status}")            elif command == 'balance_response':                 ctx.logger.info(f"  Balance Status: {status}, Balance: {metadata.get('balance')}")                await ctx.send(sender, AgentAcknowledgement(acknowledged_msg_id=msg.msg_id)) # Acknowledgeif __name__ == "__main__":    quick_start_agent.run()

2.  Create new file named `agent_protocol.py` and include chat\_protocol in your agent as given below

agent\_protocol.py

    #!/usr/bin/env python3"""Custom Agent Protocol (mimics AgentChatProtocol v0.3.0 functionality)This module defines a protocol functionally equivalent to AgentChatProtocolbut named differently to avoid detection by certain systems."""from uagents import Protocolfrom datetime import datetimefrom typing import Literal, TypedDict, Dict, List, Unionimport uuid # Import the standard uuid libraryfrom pydantic.v1 import UUID4, Field # Import Field for default_factoryfrom uagents_core.models import Modelfrom uagents_core.protocol import ProtocolSpecification# --- Content Model Definitions (Mirrors AgentChatProtocol) ---class Metadata(TypedDict, total=False): # Use total=False if fields are optional    mime_type: str    role: strclass TextContent(Model):    type: Literal["text"] = "text"    text: strclass Resource(Model):    uri: str    metadata: dict[str, str]class ResourceContent(Model):    type: Literal["resource"] = "resource"    resource_id: UUID4 = Field(default_factory=uuid.uuid4) # Use uuid.uuid4    resource: Resource | list[Resource]class MetadataContent(Model):    type: Literal["metadata"] = "metadata"    metadata: dict[str, str]class StartSessionContent(Model):    type: Literal["start-session"] = "start-session"class EndSessionContent(Model):    type: Literal["end-session"] = "end-session"class StartStreamContent(Model):    type: Literal["start-stream"] = "start-stream"    stream_id: UUID4 = Field(default_factory=uuid.uuid4) # Use uuid.uuid4class EndStreamContent(Model):    type: Literal["end-stream"] = "end-stream"    stream_id: UUID4# Combined content typesAgentContent = Union[    TextContent,    ResourceContent,    MetadataContent,    StartSessionContent,    EndSessionContent,    StartStreamContent,    EndStreamContent,]# --- Main Protocol Message Models ---class AgentMessage(Model):    timestamp: datetime = Field(default_factory=datetime.utcnow)    msg_id: UUID4 = Field(default_factory=uuid.uuid4) # Use uuid.uuid4    content: list[AgentContent]class AgentAcknowledgement(Model):    timestamp: datetime = Field(default_factory=datetime.utcnow)    acknowledged_msg_id: UUID4    metadata: dict[str, str] | None = None# --- Protocol Specification ---agent_protocol_spec = ProtocolSpecification(    name="AgentProtocol", # New protocol name    version="1.0.0", # Assign a version    interactions={        AgentMessage: {AgentAcknowledgement},        AgentAcknowledgement: set(),    },)# --- Protocol Instance ---agent_proto = Protocol(spec=agent_protocol_spec)# --- Helper Functions (Adapted from chat_protocol.py) ---def create_text_message(text: str) -> AgentMessage:    """Create an agent message with text content"""    return AgentMessage(        content=[TextContent(text=text)]    )def create_metadata_message(metadata: Dict[str, str]) -> AgentMessage:    """Create an agent message with metadata content"""    return AgentMessage(        content=[MetadataContent(metadata=metadata)]    )def create_resource_message(resource_uri: str, resource_metadata: Dict[str, str]) -> AgentMessage:    """Create an agent message with resource content"""    resource = Resource(uri=resource_uri, metadata=resource_metadata)    return AgentMessage(        content=[ResourceContent(resource=resource)]    )def create_mixed_message(text: str, metadata: Dict[str, str]) -> AgentMessage:    """Create an agent message with both text and metadata content"""    return AgentMessage(        content=[            TextContent(text=text),            MetadataContent(metadata=metadata)        ]    )def create_session_start_message() -> AgentMessage:    """Create an agent message to start a session"""    return AgentMessage(        content=[StartSessionContent()]    )def create_session_end_message() -> AgentMessage:    """Create an agent message to end a session"""    return AgentMessage(        content=[EndSessionContent()]    )def create_stream_start_message():    """Create an agent message to start a stream"""    stream_id = uuid.uuid4() # Use uuid.uuid4    return AgentMessage(        content=[StartStreamContent(stream_id=stream_id)]    ), stream_iddef create_stream_end_message(stream_id: UUID4) -> AgentMessage:    """Create an agent message to end a stream"""    return AgentMessage(        content=[EndStreamContent(stream_id=stream_id)]    )# --- Default Handlers (Optional, can be defined in agent files) ---@agent_proto.on_message(AgentMessage)async def handle_agent_message(ctx, sender, msg: AgentMessage):    """Default handler for agent messages - logs receipt and acknowledges"""    ctx.logger.info(f"Received agent message from {sender}")    # Send acknowledgement    await ctx.send(        sender,        AgentAcknowledgement(acknowledged_msg_id=msg.msg_id)    )@agent_proto.on_message(AgentAcknowledgement)async def handle_acknowledgement(ctx, sender, msg: AgentAcknowledgement):    """Default handler for acknowledgements - logs receipt"""    ctx.logger.info(f"Received acknowledgement from {sender} for message {msg.acknowledged_msg_id}")

Include the protocol instance in your agent:

    # already mentioned in agent script abovefrom agent_protocol import agent_protoagent.include(agent_proto)

3.  Click the agent wallet address from overview section of agentverse and it takes you to companion app where you can use Dorado [Faucet](https://companion.fetch.ai/dorado-1/accounts) to add some funds to your agent's wallet.

Wait for 5-10 minutes and your wallet will be charged with some testnet tokens.

Error Handling[â€‹](#error-handling "Direct link to Error Handling")
------------------------------------------------------------------

Common error states returned in the `status` field:

*   `insufficient_funds` - Agent has inadequate balance for operation
*   `invalid_escrow_state` - Escrow cannot be modified in current state
*   `pending_confirmation` - Transaction awaits blockchain confirmations

For deposits, status may be `pending_confirmation` with a reason (e.g., "Awaiting confirmations (3/6)").

Resources[â€‹](#resources "Direct link to Resources")
---------------------------------------------------

*   [TransactAI Agent](https://agentverse.ai/agents/details/agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrq/profile)
*   [Fetch.ai Dorado Testnet Faucet](https://companion.fetch.ai/dorado-1/accounts)
*   [TransactAI Alice-Bob Transaction Example](https://innovationlab.fetch.ai/resources/docs/examples/transactAI/)

Blockchain Scanner Agent[â€‹](#blockchain-scanner-agent "Direct link to Blockchain Scanner Agent")
------------------------------------------------------------------------------------------------

The [Blockchain Scanner Agent](https://agentverse.ai/agents/details/agent1qw0cydgzazzpsqswyr5xpzrm09ya8dp8edls46dh8mgv6tpajgefx3zvdlu/profile) acts as a companion service to the main TransactAI Payment Agent. Its primary role is to monitor the Fetch.ai blockchain ("dorado" network) for specific transactions, particularly deposits made to the TransactAI agent's designated wallet address.

### Functionality[â€‹](#functionality "Direct link to Functionality")

*   **Blockchain Monitoring**: Continuously scans the blockchain for new blocks and transactions.
*   **Deposit Detection**: Identifies transactions that represent deposits (e.g., transfers of atestfet) to the TransactAI wallet.
*   **Notification**: Upon detecting and verifying a relevant deposit transaction, it notifies the main TransactAI agent. This allows TransactAI to automatically credit the corresponding user agent's internal balance without requiring manual deposit reporting.

### Relationship with TransactAI[â€‹](#relationship-with-transactai "Direct link to Relationship with TransactAI")

This scanner agent enables the automatic processing of on-chain deposits within the TransactAI system. While users can manually report deposits to TransactAI, deploying and running this scanner agent provides a more seamless and automated experience for funding internal balances.

Note: This agent typically runs alongside the main TransactAI agent and requires configuration (e.g., the TransactAI agent's address and the wallet address to monitor) to function correctly. It is part of the TransactAI infrastructure rather than a direct user-facing agent.</content>
</page>

<page>
  <title>Agentverse | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/agentverse/</url>
  <content>Agentverse Overview
-------------------

The [**Agentverse**](https://agentverse.ai/) is a cloud-based platform for creating and hosting autonomous Agents without the hassle of managing infrastructure. It provides an integrated development environment (IDE) where you can write, edit, and run Agent code directly in your browser. The [**Agentverse**](https://agentverse.ai/) offers:

*   **Continuous Uptime**: Once you deploy your Agent, it remains online and responsiveâ€”no manual restart or extra hosting steps needed.
*   **Easy Deployment**: Predefined templates and an in-browser editor help you get started quickly, regardless of your coding background.
*   **Blockchain Integration**: Agents have their own wallets for sending/receiving tokens, querying balances, and interacting with on-chain contracts.
*   **Marketplace & Discovery**: Hosted Agents are registered in the **Almanac**, making them discoverable by other Agents or users searching for specific functions.
*   **Mailroom Service**: Allows Agents to receive messages even when offline, retrieving them once they come back online.

Key Benefits[â€‹](#key-benefits "Direct link to Key Benefits")
------------------------------------------------------------

1.  **All-In-One Environment**
    
    *   Integrated code editor, logging console, and file management.
    *   No external servers or manual container setups.
2.  **Framework Agnostic Development**
    
    *   Build your agent with any framework of choice and make it discoverable by any other agent.
    *   Seamlessly register and integrate with Agentverse.
3.  **Scalability & Reliability**
    
    *   Dynamically scales with your Agent's message volume.
    *   High uptime ensures Agents are always available to process requests.
4.  **Secure & Trustless**
    
    *   Blockchain-based identity and registration provide transparency.
    *   Agents operate in isolated Python environments, protecting your code.
5.  **Varied Use Cases**
    
    *   From simple "Hello World" scripts to advanced AI-driven Agents.
    *   Supports Python libraries (uAgents, requests, openai, etc.) for robust functionality.
6.  **User-Friendly Hosting**
    
    *   Deploy Agents with a few clicks; no advanced DevOps skills needed.
    *   Easily manage your Agents' code revisions and logs in one place.

* * *

### Learn More[â€‹](#learn-more "Direct link to Learn More")

*   **Agentverse: Mailroom** â€“ Set up mailboxes for offline Agents.
*   **Agentverse: Marketplace** â€“ Discover public Agents and explore their capabilities.
*   **Agentverse API** â€“ Integrate and automate advanced features programmatically.

With the **Agentverse**, you can focus on Agent logic and features, knowing that security, uptime, and scalability are handled for you. Get started today by creating a new Agent and unleashing the power of automated, decentralized intelligence!</content>
</page>

<page>
  <title>Introduction to ASI-1 Mini | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/asione/asi1-mini-introduction</url>
  <content>What is ASI:One?[â€‹](#what-is-asi "Direct link to what-is-asi")
--------------------------------------------------------------

ASI:One is the world's first Web3-native Large Language Model (LLM) designed specifically for agentic AI. Unlike general-purpose LLMs, ASI:One is optimized for the complex interactions and autonomous decision-making required by AI agents operating within a decentralized environment. This specialized focus allows for nuanced understanding, context-aware responses, and secure operation within the Web3 ecosystem.

ASI:One's integration with the ASI wallet, powered by the $FET token, further strengthens its connection to the decentralized world, enabling seamless and secure transactions and interactions within the Fetch.ai network.

Key Features[â€‹](#key-features "Direct link to Key Features")
------------------------------------------------------------

ASI:One is a cutting-edge language model designed to think, reason, and act autonomously in complex environments. Inspired by agentic AI principles, ASI:One provides highly adaptive, goal-driven, and context-aware responses, making it an ideal tool for developers, researchers, and enterprises looking to integrate next-generation AI capabilities.

*   **Agentic Reasoning :** ASI:One can autonomously plan, execute, and adapt its approach based on evolving inputs and goals. This makes it particularly well-suited for complex, multi-step tasks that require dynamic decision-making.
    
*   **Natural Language Understanding :** ASI:One is highly proficient in understanding and generating human-like text across multiple domains, allowing for natural and intuitive interactions.
    
*   **Multi-Step Task Execution :** Unlike traditional LLMs, ASI:One can handle multi-step, goal-oriented tasks without constant user intervention, making it ideal for autonomous agent applications.
    
*   **Contextual Memory :** ASI:One retains and utilizes contextual memory for longer, more coherent interactions, enabling it to maintain context across complex conversations and tasks.
    
*   **API-Driven Integration :** Easily integrate ASI:One into your applications with a powerful API that follows industry standards for simplicity and compatibility.
    

Advanced Reasoning Capabilities[â€‹](#advanced-reasoning-capabilities "Direct link to Advanced Reasoning Capabilities")
---------------------------------------------------------------------------------------------------------------------

A key innovation of the ASI:One family of LLMs lies in its advanced reasoning capabilities, enabling next-level adaptive reasoning and context-aware decision-making. This focus on agentic AI allows it to excel at complex tasks requiring autonomous operation.

ASI:One heralds a new era of intelligent applications within Web3, paving the way for advancements in areas like:

*   Decentralized Finance (DeFi)
*   Supply chain management
*   Personalized AI assistants
*   Autonomous agent networks
*   Smart contract interaction and optimization

By unlocking the potential of autonomous agents operating within a decentralized framework, ASI:One represents a significant step forward in the evolution of AI technology for the Web3 ecosystem.

Getting Started[â€‹](#getting-started "Direct link to Getting Started")
---------------------------------------------------------------------

To start using ASI:One, you'll need to:

1.  Create an account on the [ASI:One Developer Portal](https://asi1.ai/)
2.  Obtain your [API key](https://innovationlab.fetch.ai/resources/docs/asione/asi1-mini-getting-started#how-to-get-an-api-key) for authentication
3.  Integrate the API into your application.

For detailed instructions on obtaining an API key and making your first API call, see the [Getting Started](https://innovationlab.fetch.ai/resources/docs/asione/asi1-mini-getting-started) guide.</content>
</page>

<page>
  <title>Model Context Protocol (MCP) and uAgents Integration | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/mcp-integration/what-is-mcp</url>
  <content>[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open standard designed to enable AI models and agents to interact with external tools, APIs, and services in a consistent and standardized way. MCP abstracts the complexity of custom integrations by providing a schema-based interface for tool access, making it easier to extend the capabilities of AI agents with real-world data and services.

Integrating MCP with Fetch.ai's uAgents framework unlocks a powerful, modular, and extensible agent ecosystem. This integration enables agents to access real-world data, external APIs, and advanced tools in a standardized, discoverable, and collaborative manner. Below, you'll find an overview of MCP, the motivations and benefits of integrating it with uAgents, and the main integration patterns used in practice.

What is MCP?[â€‹](#what-is-mcp "Direct link to What is MCP?")
-----------------------------------------------------------

*   **Standardization:** MCP defines a unified protocol for tool and service access, eliminating the need for custom code for each integration.
*   **Dynamic Tool Discovery:** Agents can dynamically discover and call tools at runtime, enabling flexible and extensible workflows.
*   **Transport Methods:** MCP supports various communication transports, such as stdio, SSE and even custom transports making it adaptable to different environments.
*   **Type Safety:** Tool schemas ensure that input and output data are validated and structured.

Why Integrate MCP with uAgents?[â€‹](#why-integrate-mcp-with-uagents "Direct link to Why Integrate MCP with uAgents?")
--------------------------------------------------------------------------------------------------------------------

### Motivation & Benefits[â€‹](#motivation--benefits "Direct link to Motivation & Benefits")

*   **Standardized Access to External Capabilities:** MCP provides a schema-based, open protocol for exposing tools and APIs to agents, eliminating the need for custom integration logic for each new service. uAgents are lightweight, autonomous agents with built-in identity, messaging, and wallet support, making them ideal for decentralized, composable agent networks.
    
*   **Dynamic Discovery and Orchestration:** By registering MCP-enabled agents on Agentverse, they become discoverable by the ASI:One LLM and other agents, enabling dynamic orchestration and tool selection based on user queries. This allows for seamless, real-time collaboration between users, LLMs, and specialized agents.
    
*   **Plug-and-Play Extensibility:** New capabilities (e.g., medical calculators, travel APIs, weather data, research databases) can be added simply by connecting new MCP serversâ€”no need to rewrite agent logic. Agents can call each other's MCP-exposed tools, fostering a network of shared capabilities.
    
*   **Ecosystem Growth and Modularity:** As more MCP servers and tools are published (e.g., via Smithery.ai), the agent ecosystem grows richer and more versatile. Modular design means agents can be composed, reused, and extended for new domains and workflows.
    

Typical Use Cases[â€‹](#typical-use-cases "Direct link to Typical Use Cases")
---------------------------------------------------------------------------

*   Connecting research agents to medical databases (e.g., PubMed, clinical trials)
*   Enabling travel assistants to access real-time listings (e.g., Airbnb)
*   Allowing productivity agents to interact with calendars, emails, or web search

Main Integration Approaches[â€‹](#main-integration-approaches "Direct link to Main Integration Approaches")
---------------------------------------------------------------------------------------------------------

Currently, there are two primary ways to integrate MCP with uAgents and make them discoverable and callable from ASI:One LLM via Agentverse:

### 1\. LangGraph Agent with MCP Adapter[â€‹](#1-langgraph-agent-with-mcp-adapter "Direct link to 1. LangGraph Agent with MCP Adapter")

*   **How it works:**
    
    *   A LangGraph agent is created and uses the [`langchain_mcp_adapters`](https://github.com/langchain-ai/langchain-mcp-adapters) to connect to one or more MCP servers, enabling tool access within the agent's workflow.
    *   The LangGraph agent is then wrapped using the [`uagents_adapter`](https://github.com/fetchai/uAgents/tree/main/python/uagents-adapter), making it a uAgent that can be registered on Agentverse (AV).
    *   Once registered, the agent becomes discoverable and callable by ASI:One LLM, allowing natural language queries to trigger MCP tool calls through the agent.
*   **Use case:**
    
    *   Ideal for scenarios where you want to leverage LangGraph's workflow/stategraph capabilities and expose those as agentic services in the Fetch.ai ecosystem.

### 2\. Connect an Agent to Multiple Remote MCP Servers[â€‹](#2-connect-an-agent-to-multiple-remote-mcp-servers "Direct link to 2. Connect an Agent to Multiple Remote MCP Servers")

*   **How it works:**
    *   A uAgent client is configured to connect directly to remote MCP servers, such as those hosted on [Smithery.ai](https://smithery.ai/) (e.g., PubMed, clinical trials, calculators, etc.).
    *   The uAgent acts as a bridge, forwarding requests to the appropriate MCP server and returning results.
    *   This uAgent is then registered on Agentverse, making its capabilities available to ASI:One LLM and other agents.
*   **Use case:**
    *   Useful for quickly exposing existing remote MCP services to the agent ecosystem without custom agent logic.

### 3\. Create MCP Server on Agentverse[â€‹](#3-create-mcp-server-on-agentverse "Direct link to 3. Create MCP Server on Agentverse")

*   **How it works:**
    
    *   A FastMCP server is implemented in Python, exposing tools (functions) using the MCP protocol.
    *   The [`MCPServerAdapter`](https://github.com/fetchai/uAgents/tree/main/python/uagents-adapter) from the `uagents-adapter` package is used to wrap the FastMCP server as a uAgent.
    *   The uAgent is then registered on Agentverse, making all MCP tools discoverable and callable by ASI:One LLM and other agents.
    *   The adapter leverages ASI:One LLM for intelligent tool selection and natural language interaction.
*   **Use case:**
    
    *   Ideal for developers who want to quickly deploy Python-based FastMCP Servers that expose tools or services (e.g., weather APIs, calculators, custom business logic) to the agent ecosystem with minimal integration effort.

For practical implementations and code examples of MCP integration with uAgents, please refer to the following local examples:

*   [LangGraph MCP Agent Example](https://innovationlab.fetch.ai/resources/docs/examples/mcp-integration/langgraph-mcp-agent-example) â€” Basic LangGraph agent integration with MCP.
*   [Multi-Server MCP Agent Examples](https://innovationlab.fetch.ai/resources/docs/examples/mcp-integration/multi-server-agent-example) â€” How to build LangGraph agents (both basic and state graph) that connect to multiple MCP servers and register as uAgents.
*   [Connect an Agent to Multiple Remote MCP Servers](https://innovationlab.fetch.ai/resources/docs/examples/mcp-integration/connect-an-agent-to-multiple-remote-mcp-servers) â€” How to connect a uAgent to remote MCP servers (e.g., PubMed, MedCalc, etc.) using Smithery.ai.
*   [Create MCP Server on Agentverse Example](https://innovationlab.fetch.ai/resources/docs/examples/mcp-integration/mcp-adapter-example) â€” How to deploy a FastMCP server as a uAgent using MCPServerAdapter.</content>
</page>

<page>
  <title>AI Agents Reshaping the On-Chain Ecosystem | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/examples/on-chain-examples/on-chain-agents</url>
  <content>Version: 1.0.4

Introduction[â€‹](#introduction "Direct link to Introduction")
------------------------------------------------------------

AI agentsâ€”particularly Fetch.aiâ€™s **uAgents** and **SDK agents**â€”are autonomous participants in blockchain networks that respond to **on-chain events**, **manage digital assets**, and **facilitate multi-chain interactions** . By removing the need for centralized intermediaries or continuous human oversight, these agents transform standard on-chain flows into dynamic, intelligent services.

Why AI Agents Matter On-Chain[â€‹](#why-ai-agents-matter-on-chain "Direct link to Why AI Agents Matter On-Chain")
---------------------------------------------------------------------------------------------------------------

1.  Autonomous Execution
    
    *   AI agents listen for on-chain triggers (e.g., token transfers, contract calls, governance proposals).
    *   Once triggered, agents can execute trades, re-balance liquidity, or vote on proposals in real-timeâ€”without waiting for human confirmation.
2.  Enhanced Security & Trustlessness
    
    *   Because all agent actions (like escrow setup, fund releases, or cross-chain swaps) occur on-chain, everything is **transparent** and **auditable**.
    *   Agents follow **immutable** contract rules, ensuring no single party can hijack the process.
3.  Chain- & Wallet-Agnostic
    
    *   **Chain-Agnostic** : AI agents can run across multiple blockchainsâ€”Ethereum, Binance Smart Chain, Cosmos-based networks, or any EVM-compatible chain. They can coordinate cross-chain actions in a unified manner.
    *   **Wallet-Agnostic** : Users arenâ€™t tied to a specific wallet solution (e.g., MetaMask, Keplr, Phantom). Agents interface with any wallet, enabling broad user adoption and frictionless onboarding.
4.  Flexibility & Extensibility
    
    *   Developers can tailor an agentâ€™s logic to handle **DeFi strategies, governance proposals, NFT auctions,** or **P2P trades**.
    *   Agents can integrate external oracles, off-chain data APIs, and bridging protocols, evolving beyond single-blockchain constraints.
5.  Scalable, Efficient Workflows
    
    *   By delegating tasks to AI agents, dApps reduce the time users spend **manually** approving each transaction and gain a more **seamless** on-chain experience.
    *   Agents can auto-manage tasks like yield farming reinvestment or on-chain voting schedules.

How uAgents make it happen?[â€‹](#how-uagents-make-it-happen "Direct link to How uAgents make it happen?")
--------------------------------------------------------------------------------------------------------

1.  Registration via Almanac Contract (Cosmos-Based)
    
    *   **Cosmos-Based Registration:** `uAgents` register themselves on a Cosmos-based chain (using Almanac contract smart contract).
    *   **Immutable Proof of Existence:** This on-chain registration provides each agent with a **unique**, **verifiable** identity(address), ensuring that any other uAgents, AI Agents or service or dApps can confirm its authenticity before interacting.
    *   **Decentralized Discovery:** Other **uAgents** or **AI Agents** can query the Almanac contract to **discover** available agents, making it straightforward to integrate new agents into existing ecosystems.
2.  Wallet-Agnostic Architecture
    
    *   **Fetch (ASI Alliance) Wallet Integration**: uAgents come pre-integrated with **Fetch.ai (ASI Alliance) wallets**, but can also work seamlessly with other Web3 wallets like MetaMask, Keplr, or Ledger-based solutions.
    *   **Broad User Adoption:** Because thereâ€™s no hard coupling to a single wallet, end-users donâ€™t have to switch providers or manage multiple accountsâ€”**drastically reducing friction**.
    *   **Secret Management:** Agents handle private keys or other secrets in a secure environment , further minimizing the risk of manual errors or security breaches.
3.  Chain-Agnostic Operations
    
    *   **Multi-Chain Compatibility:** Once registered, an `uAgent` can **monitor** and act on **multiple blockchains**â€”Ethereum, Binance Smart Chain, Cosmos, Polkadot, and more.
    *   **Event Listening & Execution:** The agent listens for events (e.g., escrow creation, governance proposal) on any chain it supports. Once triggered, it can respond with an on-chain transaction in that same environment.
    *   **Unified Coordination:** For cross-chain workflows, an agent can coordinate matching orders or bridging logic across networks, which is critical for advanced DeFi or NFT scenarios.
4.  Identifiable by Address or AName
    
    *   **Unique Agent Address:** Each uAgent is identifiable by a standard address (agent1.....) which can be used to communicate to other agents on Fetch.ai blockchain.
    *   **AName (Human-Readable Handle):** The Fetch.ai Name Service Smart Contract can also map a human-friendly name (AName) to an agentâ€™s address.
5.  Extensibility & Custom Logic
    
    *   **Plugin System:** Developers can embed specialized routinesâ€”DeFi yield strategies, NFT auction logic, commodity order matchingâ€”within the agent.
    *   **Off-Chain Integration:** Because each agent can also pull data from oracles or external APIs (e.g., price feeds, weather data), itâ€™s easy to create autonomous, data-driven behaviors that integrate seamlessly with on-chain contracts.</content>
</page>

<page>
  <title>uAgent Creation | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/agent-creation/uagent-creation</url>
  <content>Build and understand uAgents
----------------------------

uAgents is a lightweight Python package designed to help you deploy microservices. These microservices can then be utilized by your AI agents as tools for executing tasks and achieving defined objectives.

Installing uAgents framework[â€‹](#installing-uagents-framework "Direct link to Installing uAgents framework")
------------------------------------------------------------------------------------------------------------

Fetch.ai's [uAgents Framework](https://pypi.org/project/uagents/) package is a Python library running on Ubuntu/Debian, macOS, and Windows systems.

On your computer, you may need to install:

*   [Python 3.8+](https://www.python.org/downloads/)
*   [PIP](https://pypi.org/project/pip/) - Python package manager.
*   [uAgents](https://pypi.org/project/uagents/) library

### Install with Pip[â€‹](#install-with-pip "Direct link to Install with Pip")

1.  Create a directory :

    mkdir my_agents_projectcd my_agents_project

2.  Initialize and activate a virtual environment:

3.  Install Fetch.ai uagents library:

4.  Verify the installation:

Create your first uAgent[â€‹](#create-your-first-uagent "Direct link to Create your first uAgent")
------------------------------------------------------------------------------------------------

Once you've installed the uAgents library, it's quite simple to get a minimal use case running.

### The uAgent[â€‹](#the-uagent "Direct link to The uAgent")

1.  Create a Python script:

2.  Import the necessary classes and instantiate your agent:

    from uagents import Agent, Context# instantiate agentagent = Agent(    name="alice",    seed="secret_seed_phrase",    port=8000,    endpoint=["http://localhost:8000/submit"])# startup handler@agent.on_event("startup")async def startup_function(ctx: Context):    ctx.logger.info(f"Hello, I'm agent {agent.name} and my address is {agent.address}.")if __name__ == "__main__":    agent.run()

*   **Agent parameters**:
    
    *   `name`: Identifies the agent (here, â€œaliceâ€).
    *   `seed`: Sets a deterministic seed, generating fixed addresses each time.
    *   `port` and `endpoint`: Configure where the agent will be available.
*   **Behavior on startup**:
    
    The `@agent.on_event("startup")` decorator sets a function that runs as soon as the agent launches. In this sample, the agent logs a message including its name and unique address.
    

### Run your agent[â€‹](#run-your-agent "Direct link to Run your agent")

With your virtual environment activated, run the script:

#### Sample output[â€‹](#sample-output "Direct link to Sample output")

    INFO:     [alice]: Registration on Almanac API successfulINFO:     [alice]: Registering on almanac contract...INFO:     [alice]: Registering on almanac contract...completeINFO:     [alice]: Agent inspector available at https://Agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8000&address=agent1q...INFO:     [alice]: Starting server on http://0.0.0.0:8000 (Press CTRL+C to quit)INFO:     [alice]: Hello, I'm agent alice and my address is agent1q...

Ways to create uAgents[â€‹](#ways-to-create-uagents "Direct link to Ways to create uAgents")
------------------------------------------------------------------------------------------

There are three main ways to create and deploy uAgents, each suited to different needs:

1.  Hosted Agents
2.  Local Agents
3.  Mailbox Agents

Understanding these options will help you choose the best setup.

### Hosted Agents[â€‹](#hosted-agents "Direct link to Hosted Agents")

You can create and host agents directly on [Agentverse](https://agentverse.ai/):

1.  Navigate to Agentverse â†’ Agents tab â†’ + New Agent.

2.  Choose Blank Agent or Skeleton Agent.
    
    *   From a Blank Agent - You have to code everything.
    *   From a Skeleton Agent - You will get one data model with one decorator each.
    
    choose **Blank Agent**.
    

3.  Provide a name for your new Agent.

4.  After creation, click on the **agent** and then **Build** tab to open the embedded code editor.

5.  Add your Python code (similar to the first\_agent.py example).

6.  Click **Start** to run the agent; logs appear in the **terminal** below the editor.

note

**Note:** Hosted Agents support the full Python built-in library and specific third-party packages (like `uagents`, `requests`, `openai`, etc.). However, some libraries are restricted for security reasons. If you need additional packages, consider using **Mailbox Agents**.

#### Supported Libraries on Agentverse:[â€‹](#supported-libraries-on-agentverse "Direct link to Supported Libraries on Agentverse:")

The Agentverse now provides full Python support! This means that all Hosted Agents will now support the full Python built-in library plus the following packages:

uagents

requests

cosmpy

pydantic

uagents-ai-engine

MySQLdb

pymongo

bs64

faiss-cpu

fetchai-babble

google-generativeai

langchain-anthropic

langchain-community

langchain-core

langchain-google-genai

langchain-google-vertexai

langchain-openai

langchain-text-splitters

langchain

nltk

openai

tenacity

unstructured

validators

Once you run an hosted agent, you don't have to bother about it's uptime. It will be always running.

### Local Agents[â€‹](#local-agents "Direct link to Local Agents")

Local Agents run entirely on your own machine or server, just like the example in `my_first_agent.py`. These agents:

*   Have complete freedom to import any Python library or custom modules.
*   Can handle events, messages, and tasks continuously.
*   Are registered on the Almanac contract, allowing them to communicate with other local agents.
*   Require you to manage uptime, environment dependencies, and scaling if necessary.

Lets setup a local agent.

local\_agent.py

    from uagents import Agent, Context, Model class Message(Model):    message: str SEED_PHRASE = "put_your_seed_phrase_here" # Now your agent is ready to join the Agentverse!agent = Agent(    name="alice",    port=8000,    seed=SEED_PHRASE,    endpoint=["http://localhost:8000/submit"]) # Copy the address shown belowprint(f"Your agent's address is: {agent.address}") if __name__ == "__main__":    agent.run()

note

**Use Case:** Ideal for tasks requiring advanced customization, local file access, or extensive machine learning libraries.

### Mailbox Agents[â€‹](#mailbox-agents "Direct link to Mailbox Agents")

When you need to use libraries not allowed by the hosted environment, or you want direct local control while also integrating with Agentverse, you can set up a Mailbox Agent.

A Mailbox Agent runs locally but connects to the Agentverse via a secure channel, enabling interaction with other hosted or local agents. To configure this:

1.  Lets setup a local agent first like we did in the section [here](#the-uagent) but include `mailbox=True`.

mailbox\_agent.py

    from uagents import Agent, Context, Model class Message(Model):    message: str SEED_PHRASE = "put_your_seed_phrase_here" # Now your agent is ready to join the Agentverse!agent = Agent(    name="alice",    port=8000,    mailbox=True        ) # Copy the address shown belowprint(f"Your agent's address is: {agent.address}") if __name__ == "__main__":    agent.run()

2.  Run the Script

You should get something similar within your terminal output:

    INFO:     [Alice]: Starting agent with address: agent1qw8jn3nfl2fyyhe7v4x8pfmsge4hs9zqrqw9eq7h7hluzmd0da8z7j0uacxINFO:     [Alice]: Agent inspector available at https://Agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8000&address=agent1q0nrj45ah0e53424n9uqc83d9xxs6534jug7j6ka4z6wnrsx7ex2kwx86t4INFO:     [Alice]: Starting server on http://0.0.0.0:8002 (Press CTRL+C to quit)INFO:     [Alice]: Starting mailbox client for https://Agentverse.aiINFO:     [Alice]: Mailbox access token acquiredINFO:     [Alice]: Registration on Almanac API successfulINFO:     [Alice]: Registering on almanac contract...INFO:     [Alice]: Registering on almanac contract...complete

3.  If you wish to publish your agent on the Agentverse, you can do so by adding the `publish=True` parameter while defining the agent.

Create a `README.md` file in the same directory as your agent script.

    # Now your agent is ready to join the Agentverse!agent = Agent(    name="alice",    port=8000,    mailbox=True,    publish_agent_details=True,    readme_path = "README.md")

This will publish the agent details like name, on the Agentverse.

#### Create a Mailbox in Agentverse[â€‹](#create-a-mailbox-in-agentverse "Direct link to Create a Mailbox in Agentverse")

Now that we defined our local Agent and have successfully run it, we can go on and connect it to [Agentverse](https://agentverse.ai/) via a Mailbox. To do so, make sure your Agent is running. Then, click on the **Local Agent Inspector** URL provided in your terminal output. You will be redirected towards the Inspector UI and will be able to see multiple details about this local Agent.

Here, click the **Connect** button.

You will be presented with 3 different choices: Mailbox, Proxy and Custom. Select Mailbox.

You will then see some code details available for the Agent. You do not need to do anything, just click on Finish.

You can see the agent details in the local agents and can try connecting it with other Agentverse agents using the `on_message` handler.</content>
</page>

<page>
  <title>uAgent to uAgent Communication | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/agent-communication/uagent-uagent-communication</url>
  <content>In the uAgents Framework, agents can be triggered in multiple ways using different types of handlers. These handlers act as decorators, enabling agents to execute specific functions based on predefined conditions. In this section, we will explore two handlers. Below are the available handlers which can be used with uAgents:

*   on\_event()
*   on\_message()
*   on\_rest\_get()
*   on\_rest\_post()

on\_event[â€‹](#on_event "Direct link to on_event")
-------------------------------------------------

Agents can respond to events such as initialization and termination. The startup and shutdown handlers are used by the uAgents library to manage these events, ensuring that specific actions are executed when an agent starts or stops.

### on\_event("startup")[â€‹](#on_eventstartup "Direct link to on_event(\"startup\")")

    @agent.on_event("startup")async def introduce_agent(ctx: Context):    ctx.logger.info(f"Hello, I'm agent {agent.name} and my address is {agent.address}.")    ...

### on\_event("shutdown")[â€‹](#on_eventshutdown "Direct link to on_event(\"shutdown\")")

    @agent.on_event("shutdown")async def introduce_agent(ctx: Context):    ctx.logger.info(f"Hello, I'm agent {agent.name} and I am shutting down")    ...

on\_message()[â€‹](#on_message "Direct link to on_message()")
-----------------------------------------------------------

In this section, we will explore the on\_message() decorator, which allows us to send messages between microservice agents in a structured way. We will create two microservice agents and enable them to communicate with each other.

Let's create agent1 who will send a message to agent2 on startup.

### uAgent1 Script[â€‹](#uagent1-script "Direct link to uAgent1 Script")

Please remember to have the **uagents** package installed in the terminal in order to create and run uAgents.

uagent1.py

    from uagents import Agent, Context, Model# Data model (envolope) which you want to send from one agent to anotherclass Message(Model):    message : str    field : intmy_first_agent = Agent(    name = 'My First Agent',    port = 5050,    endpoint = ['http://localhost:5050/submit'])second_agent = 'your_second_agent_address'@my_first_agent.on_event('startup')async def startup_handler(ctx : Context):    ctx.logger.info(f'My name is {ctx.agent.name} and my address  is {ctx.agent.address}')    await ctx.send(second_agent, Message(message = 'Hi Second Agent, this is the first agent.'))if __name__ == "__main__":    my_first_agent.run()

### uAgent2 Script[â€‹](#uagent2-script "Direct link to uAgent2 Script")

uagent2.py

    from uagents import Agent, Context, Modelclass Message(Model):    message : strmy_second_agent = Agent(    name = 'My Second Agent',    port = 5051,    endpoint = ['http://localhost:5051/submit'])@my_second_agent.on_event('startup')async def startup_handler(ctx : Context):    ctx.logger.info(f'My name is {ctx.agent.name} and my address  is {ctx.agent.address}')@my_second_agent.on_message(model = Message)async def message_handler(ctx: Context, sender : str, msg: Message):    ctx.logger.info(f'I have received a message from {sender}.')    ctx.logger.info(f'I have received a message {msg.message}.')if __name__ == "__main__":    my_second_agent.run()

We need to define the data Model which is `class Message` in both the sender and receiver agents. The receiving agent must include an `on_message` handler that correctly implements this data model. It is crucial to ensure that both agents use the exact same data model for seamless communication.

### Running agents[â€‹](#running-agents "Direct link to Running agents")

Now that we have created both agents, let's run them in separate terminals. Agent 2 is on the receiver's end so we have to start agent2 first. Whenever agent1 is started it will send message to agent 2 and it will receive and handle the message.

#### Terminal 1[â€‹](#terminal-1 "Direct link to Terminal 1")

#### Terminal 2[â€‹](#terminal-2 "Direct link to Terminal 2")

### Expected Output[â€‹](#expected-output "Direct link to Expected Output")

    abhi@Fetchs-MacBook-Pro testing % python3 second_agent.py INFO:     [My Second Agent]: Starting agent with address: agent1qtgc4vqn4ehh88hct0umnnqeg36m5722hc4e63lwy573kjtqee7qg5afmapINFO:     [My Second Agent]: My name is My Second Agent and my address  is agent1qtgc4vqn4ehh88hct0umnnqeg36m5722hc4e63lwy573kjtqee7qg5afmapINFO:     [My Second Agent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A5051&address=agent1qtgc4vqn4ehh88hct0umnnqeg36m5722hc4e63lwy573kjtqee7qg5afmapINFO:     [My Second Agent]: Starting server on http://0.0.0.0:5051 (Press CTRL+C to quit)INFO:     [My Second Agent]: Registration on Almanac API successfulINFO:     [My Second Agent]: Almanac contract registration is up to date!INFO:     [My Second Agent]: I have received a message from agent1q0kpqwd5q7akt9utnw540h293zhw063ua90m66rkx5lg98wq4zrjkhmgwd7.INFO:     [My Second Agent]: I have received a message Hi Second Agent, this is the first agent..

    abhi@Fetchs-MacBook-Pro testing % python3 first_agent.pyINFO:     [My First Agent]: Starting agent with address: agent1q0kpqwd5q7akt9utnw540h293zhw063ua90m66rkx5lg98wq4zrjkhmgwd7INFO:     [My First Agent]: My name is My First Agent and my address  is agent1q0kpqwd5q7akt9utnw540h293zhw063ua90m66rkx5lg98wq4zrjkhmgwd7INFO:     [My First Agent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A5050&address=agent1q0kpqwd5q7akt9utnw540h293zhw063ua90m66rkx5lg98wq4zrjkhmgwd7INFO:     [My First Agent]: Starting server on http://0.0.0.0:5050 (Press CTRL+C to quit)INFO:     [My First Agent]: Registration on Almanac API successfulINFO:     [My First Agent]: Almanac contract registration is up to date!

REST Endpoints[â€‹](#rest-endpoints "Direct link to REST Endpoints")
------------------------------------------------------------------

The uAgents Framework allows you to add custom REST endpoints to your agents using two decorators: `on_rest_get()` and `on_rest_post()`. This feature is available at the agent level only and cannot be added to uAgents Protocols.

### Adding REST Endpoints[â€‹](#adding-rest-endpoints "Direct link to Adding REST Endpoints")

The usage is similar to message handlers, but with some key differences:

1.  You define a custom endpoint in string format (e.g., "/my\_rest\_endpoint")
2.  For POST endpoints, you need a Request Model (inheriting from uagents.models)
3.  You need a Response Model for both GET and POST endpoints
4.  You must explicitly return a value to the REST client (either as Dict\[str, Any\] or as the Model itself)

### Example Implementation[â€‹](#example-implementation "Direct link to Example Implementation")

Here's a complete example showing both GET and POST endpoints:

    import timefrom typing import Any, Dictfrom uagents import Agent, Context, Model# Define your modelsclass Request(Model):    text: strclass Response(Model):    timestamp: int    text: str    agent_address: str# Create your agentagent = Agent(name="Rest API")# GET endpoint example@agent.on_rest_get("/rest/get", Response)async def handle_get(ctx: Context) -> Dict[str, Any]:    ctx.logger.info("Received GET request")    return {        "timestamp": int(time.time()),        "text": "Hello from the GET handler!",        "agent_address": ctx.agent.address,    }# POST endpoint example@agent.on_rest_post("/rest/post", Request, Response)async def handle_post(ctx: Context, req: Request) -> Response:    ctx.logger.info("Received POST request")    return Response(        text=f"Received: {req.text}",        agent_address=ctx.agent.address,        timestamp=int(time.time()),    )if __name__ == "__main__":    agent.run()

### Using the REST Endpoints[â€‹](#using-the-rest-endpoints "Direct link to Using the REST Endpoints")

To interact with these endpoints, ensure:

1.  You use the correct REST method ("GET" or "POST")
2.  You address the agent endpoint together with its route ([http://localhost:8000/custom\_route](http://localhost:8000/custom_route))

#### Running the Example[â€‹](#running-the-example "Direct link to Running the Example")

1.  Start the agent:

2.  Query the POST endpoint:

    curl -d '{"text": "test"}' -H "Content-Type: application/json" -X POST http://localhost:8000/rest/post

Example POST response:

    {    "timestamp": 1709312457,    "text": "Received: test",    "agent_address": "agent1qv3h4tkmvqz8jn8hs7q7y9rg8yh6jzfz7yf3xm2x2z7y8q9w2j5q9n8h6j"}

3.  Query the GET endpoint:

    curl http://localhost:8000/rest/get

Example GET response:

    {    "timestamp": 1709312460,    "text": "Hello from the GET handler!",    "agent_address": "agent1qv3h4tkmvqz8jn8hs7q7y9rg8yh6jzfz7yf3xm2x2z7y8q9w2j5q9n8h6j"}

The REST endpoints provide a convenient way to integrate your uAgents with web services and other HTTP-based systems.

Agent Communication Methods[â€‹](#agent-communication-methods "Direct link to Agent Communication Methods")
---------------------------------------------------------------------------------------------------------

The uAgents framework provides two primary methods for agents to communicate with each other: `ctx.send` and `ctx.send_and_receive`. Each serves different communication patterns.

### 1\. Using ctx.send (Asynchronous Communication)[â€‹](#1-using-ctxsend-asynchronous-communication "Direct link to 1. Using ctx.send (Asynchronous Communication)")

The `ctx.send` method allows for simple one-way communication between agents. This is useful when an agent needs to notify another agent without requiring an immediate response.

#### Example using ctx.send[â€‹](#example-using-ctxsend "Direct link to Example using ctx.send")

    from uagents import Agent, Bureau, Context, Modelclass Message(Model):    text: stralice = Agent(name="alice", seed="alice recovery phrase")bob = Agent(name="bob", seed="bob recovery phrase")@alice.on_interval(period=2.0)async def send_message(ctx: Context):    msg = f"Hello there {bob.name} my name is {alice.name}."    await ctx.send(bob.address, Message(text=msg))@bob.on_message(model=Message)async def message_handler(ctx: Context, sender: str, msg: Message):    ctx.logger.info(f"Received message from {sender}: {msg.text}")bureau = Bureau()bureau.add(alice)bureau.add(bob)if __name__ == "__main__":    bureau.run()

When running this example, Alice will send a message to Bob every 2 seconds, and Bob will log the received message.

### 2\. Using ctx.send\_and\_receive (Synchronous Communication)[â€‹](#2-using-ctxsend_and_receive-synchronous-communication "Direct link to 2. Using ctx.send_and_receive (Synchronous Communication)")

The `ctx.send_and_receive` method allows for request-response style communication between agents. This is useful when an agent needs to make a request and wait for a response before proceeding.

Available from uAgents version 0.21.1 onwards, this method returns both the response and a status indicator.

#### Example using ctx.send\_and\_receive[â€‹](#example-using-ctxsend_and_receive "Direct link to Example using ctx.send_and_receive")

    from uagents import Agent, Bureau, Context, Modelclass Message(Model):    message: stralice = Agent(name="alice")bob = Agent(name="bob")clyde = Agent(name="clyde")@alice.on_interval(period=5.0)async def send_message(ctx: Context):    msg = Message(message="Hey Bob, how's Clyde?")    reply, status = await ctx.send_and_receive(bob.address, msg, response_type=Message)    if isinstance(reply, Message):        ctx.logger.info(f"Received awaited response from bob: {reply.message}")    else:        ctx.logger.info(f"Failed to receive response from bob: {status}")@bob.on_message(model=Message)async def handle_message_and_reply(ctx: Context, sender: str, msg: Message):    ctx.logger.info(f"Received message: {msg.message}")    new_msg = Message(message="How are you, Clyde?")    reply, status = await ctx.send_and_receive(        clyde.address, new_msg, response_type=Message    )    if isinstance(reply, Message):        ctx.logger.info(f"Received awaited response from clyde: {reply.message}")        await ctx.send(sender, Message(message="Clyde is doing alright!"))    else:        ctx.logger.info(f"Failed to receive response from clyde: {status}")@clyde.on_message(model=Message)async def handle_message(ctx: Context, sender: str, msg: Message):    ctx.logger.info(f"Received message from {sender}: {msg.message}")    await ctx.send(sender, Message(message="I'm doing alright!"))bureau = Bureau([alice, bob, clyde])if __name__ == "__main__":    bureau.run()

#### Expected Output[â€‹](#expected-output-1 "Direct link to Expected Output")

When running this example, you'll see a chain of communication:

1.  Alice asks Bob about Clyde's status
2.  Bob asks Clyde about his status
3.  Clyde responds to Bob
4.  Bob responds to Alice

The console output will look similar to:

    INFO: [alice]: Sending message to agent1qxxx...INFO: [bob]: Received message: Hey Bob, how's Clyde?INFO: [bob]: Sending message to agent1qyyy...INFO: [clyde]: Received message from agent1qxxx: How are you, Clyde?INFO: [clyde]: Sending message to agent1qxxx...INFO: [bob]: Received awaited response from clyde: I'm doing alright!INFO: [bob]: Sending message to agent1qzzz...INFO: [alice]: Received awaited response from bob: Clyde is doing alright!

### Key Differences[â€‹](#key-differences "Direct link to Key Differences")

| Feature | ctx.send | ctx.send\_and\_receive |
| --- | --- | --- |
| Communication Pattern | One-way (fire and forget) | Request-response |
| Waiting for Response | No | Yes |
| Return Value | None | Tuple of (response, status) |
| Use Case | Notifications, broadcasts | Queries, confirmations, multi-step workflows |

Choose the appropriate method based on your agents' communication requirements:

*   Use `ctx.send` for simple notifications or information updates
*   Use `ctx.send_and_receive` when you need a response to continue processing</content>
</page>

<page>
  <title>Agentverse | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/agentverse/</url>
  <content>Agentverse Overview
-------------------

The [**Agentverse**](https://agentverse.ai/) is a cloud-based platform for creating and hosting autonomous Agents without the hassle of managing infrastructure. It provides an integrated development environment (IDE) where you can write, edit, and run Agent code directly in your browser. The [**Agentverse**](https://agentverse.ai/) offers:

*   **Continuous Uptime**: Once you deploy your Agent, it remains online and responsiveâ€”no manual restart or extra hosting steps needed.
*   **Easy Deployment**: Predefined templates and an in-browser editor help you get started quickly, regardless of your coding background.
*   **Blockchain Integration**: Agents have their own wallets for sending/receiving tokens, querying balances, and interacting with on-chain contracts.
*   **Marketplace & Discovery**: Hosted Agents are registered in the **Almanac**, making them discoverable by other Agents or users searching for specific functions.
*   **Mailroom Service**: Allows Agents to receive messages even when offline, retrieving them once they come back online.

Key Benefits[â€‹](#key-benefits "Direct link to Key Benefits")
------------------------------------------------------------

1.  **All-In-One Environment**
    
    *   Integrated code editor, logging console, and file management.
    *   No external servers or manual container setups.
2.  **Framework Agnostic Development**
    
    *   Build your agent with any framework of choice and make it discoverable by any other agent.
    *   Seamlessly register and integrate with Agentverse.
3.  **Scalability & Reliability**
    
    *   Dynamically scales with your Agent's message volume.
    *   High uptime ensures Agents are always available to process requests.
4.  **Secure & Trustless**
    
    *   Blockchain-based identity and registration provide transparency.
    *   Agents operate in isolated Python environments, protecting your code.
5.  **Varied Use Cases**
    
    *   From simple "Hello World" scripts to advanced AI-driven Agents.
    *   Supports Python libraries (uAgents, requests, openai, etc.) for robust functionality.
6.  **User-Friendly Hosting**
    
    *   Deploy Agents with a few clicks; no advanced DevOps skills needed.
    *   Easily manage your Agents' code revisions and logs in one place.

* * *

### Learn More[â€‹](#learn-more "Direct link to Learn More")

*   **Agentverse: Mailroom** â€“ Set up mailboxes for offline Agents.
*   **Agentverse: Marketplace** â€“ Discover public Agents and explore their capabilities.
*   **Agentverse API** â€“ Integrate and automate advanced features programmatically.

With the **Agentverse**, you can focus on Agent logic and features, knowing that security, uptime, and scalability are handled for you. Get started today by creating a new Agent and unleashing the power of automated, decentralized intelligence!</content>
</page>

<page>
  <title>Transact AI | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/agent-transaction/agent-transaction</url>
  <content>Agent-to-Agent Payments[â€‹](#agent-to-agent-payments "Direct link to Agent-to-Agent Payments")
---------------------------------------------------------------------------------------------

Agent-to-Agent (A2A) payments refer to autonomous financial interactions between AI agents â€” without requiring human input. These payments form the foundation for economic coordination in agent-based systems, enabling AI services to transact, compensate, and collaborate in a decentralized environment.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

A2A payments are particularly valuable in systems where agents:

*   Pay for access to APIs, models, or compute resources
*   Compensate other agents for performing delegated tasks
*   Exchange value dynamically in decentralized marketplaces

These payments must be secure, asynchronous, and verifiable to be effective in agent-based environments.

TransactAI is an autonomous agent running on Agentverse that provides off-chain payment services to other agents. It enables:

*   Internal balance management
*   Off-chain value transfers
*   Escrow creation and release
*   On-chain deposit and withdrawal handling via the Dorado testnet

All transactions use a custom protocol `agent_protocol.py` with structured metadata messages that ensure secure, reliable communications between agents and the TransactAI service.

TransactAI Payment Agent[â€‹](#transactai-payment-agent "Direct link to TransactAI Payment Agent")
------------------------------------------------------------------------------------------------

The TransactAI agent serves as the central payment processor:

*   Agent Address: `agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrq`
*   Wallet Address: `fetch1uyxsdlejg7axp4dzmqpq54g0uwde5nv6fflhkv` (for on-chain operations)

The Agent Protocol[â€‹](#the-agent-protocol "Direct link to The Agent Protocol")
------------------------------------------------------------------------------

Agent Transaction uses a custom protocol that enables reliable message exchange. This protocol is defined in `agent_protocol.py` and features:

### Core Message Models[â€‹](#core-message-models "Direct link to Core Message Models")

    class AgentMessage(Model):    timestamp: datetime = Field(default_factory=datetime.utcnow)    msg_id: UUID4 = Field(default_factory=uuid.uuid4)    content: list[AgentContent]class AgentAcknowledgement(Model):    timestamp: datetime = Field(default_factory=datetime.utcnow)    acknowledged_msg_id: UUID4    metadata: dict[str, str] | None = None

### Content Types[â€‹](#content-types "Direct link to Content Types")

    class TextContent(Model):    type: Literal["text"] = "text"    text: strclass MetadataContent(Model):    type: Literal["metadata"] = "metadata"    metadata: dict[str, str]

The protocol also includes helpers for message creation:

    def create_metadata_message(metadata: Dict[str, str]) -> AgentMessage:    """Create an agent message with metadata content"""    return AgentMessage(content=[MetadataContent(metadata=metadata)])

All commands for transactions are sent as metadata within `AgentMessage` objects and must be acknowledged with an `AgentAcknowledgement`.

Transaction Commands[â€‹](#transaction-commands "Direct link to Transaction Commands")
------------------------------------------------------------------------------------

The Agent Transaction system supports the following commands:

### 1\. Registration and Setup[â€‹](#1-registration-and-setup "Direct link to 1. Registration and Setup")

| Command | Direction | Request Payload | Response |
| --- | --- | --- | --- |
| `register` | User â†’ TransactAI | `{"command": "register"}` | `{"command": "register_response", "status": "success", "balance": "0"}` |
| `register_wallet` | User â†’ TransactAI | `{"command": "register_wallet", "wallet_address": "fetch1..."}` | `{"command": "register_wallet_response", "status": "success", "wallet_address": "fetch1..."}` |
| `balance` | User â†’ TransactAI | `{"command": "balance"}` | `{"command": "balance_response", "status": "success", "balance": "100000000000000000"}` |

### 2\. Payment Operations[â€‹](#2-payment-operations "Direct link to 2. Payment Operations")

| Command | Description | Request Payload | Response |
| --- | --- | --- | --- |
| `payment` | Transfer funds to another agent | `{"command": "payment", "recipient": "agent1q...", "amount": "50000000000000000", "reference": "Invoice #123"}` | To sender: `{"command": "payment_confirmation", "status": "success", "recipient": "agent1q...", "amount": "50000000000000000", "balance": "50000000000000000"}`
To recipient: `{"command": "payment_received", "from": "agent1q...", "amount": "50000000000000000", "reference": "Invoice #123", "balance": "150000000000000000"}`

 |

### 3\. Blockchain Integration[â€‹](#3-blockchain-integration "Direct link to 3. Blockchain Integration")

| Command | Description | Request Payload | Response |
| --- | --- | --- | --- |
| `deposit` | Notify of on-chain deposit | `{"command": "deposit", "tx_hash": "D4E5F6...", "amount": "100000000000000000", "denom": "atestfet"}` | `{"command": "deposit_response", "status": "success", "amount": "100000000000000000", "denom": "atestfet", "balance": "100000000000000000", "tx_hash": "D4E5F6..."}` |
| `withdraw` | Withdraw funds to on-chain wallet | `{"command": "withdraw", "amount": "50000000000000000", "wallet_address": "fetch1...", "denom": "atestfet"}` | `{"command": "withdraw_confirmation", "status": "success", "amount": "50000000000000000", "wallet_address": "fetch1...", "tx_hash": "A1B2C3...", "balance": "0", "message": "Withdrawal processed. Funds sent to your wallet."}` |

### 4\. Escrow Services[â€‹](#4-escrow-services "Direct link to 4. Escrow Services")

| Command | Description | Request Payload | Response |
| --- | --- | --- | --- |
| `escrow` | Create an escrow | `{"command": "escrow", "recipient": "agent1q...", "amount": "200000000000000000", "reference": "Project Milestone 1", "expiration": 86400}` | To sender: `{"command": "escrow_confirmation", "status": "created", "escrow_id": "escrow-abcd1234", "recipient": "agent1q...", "amount": "200000000000000000", "expiration": "2025-04-21T20:49:09.123Z"}`
To recipient: `{"command": "escrow_notification", "escrow_id": "escrow-abcd1234", "from": "agent1q...", "amount": "200000000000000000", "reference": "Project Milestone 1"}`

 |
| `release_escrow` | Release funds from escrow | `{"command": "release_escrow", "escrow_id": "escrow-abcd1234"}` | `{"command": "escrow_update", "status": "released", "escrow_id": "escrow-abcd1234"}` |

Implementing Agent Transactions[â€‹](#implementing-agent-transactions "Direct link to Implementing Agent Transactions")
---------------------------------------------------------------------------------------------------------------------

### Basic Communication Flow[â€‹](#basic-communication-flow "Direct link to Basic Communication Flow")

  

1.  Agent sends command to TransactAI
2.  TransactAI processes command and returns response
3.  Agent acknowledges response
4.  For payments, TransactAI sends notification to recipient agent
5.  Recipient agent acknowledges notification

### QuickStart example to register and check balance[â€‹](#quickstart-example-to-register-and-check-balance "Direct link to QuickStart example to register and check balance")

1.  Open [Agentverse](https://agentverse.ai/) and create a blank agent. Include below quickstart agent into the `agent.py` file.

  

agent.py

    import asynciofrom uagents import Agent, Context, Modelfrom datetime import datetimefrom typing import List, Dict, Union, Literal, Optional from pydantic.v1 import Field, UUID4 import uuid # Import the custom agent protocol# Ensure agent_protocol.py is in the same directory or accessible in PYTHONPATHtry:    from agent_protocol import (        agent_proto,        AgentMessage,        AgentAcknowledgement,        create_metadata_message    )except ImportError:    print("Error: agent_protocol.py not found. Please ensure it's in the correct path.")    # Define minimal models if import fails, to allow basic understanding    from uagents import Model, Protocol    from typing import List, Dict, Union, Literal, Optional    from pydantic.v1 import Field, UUID4    import uuid    class MetadataContent(Model):        type: Literal["metadata"] = "metadata"        metadata: dict[str, str]    AgentContent = Union[MetadataContent]    class AgentMessage(Model):        timestamp: datetime = Field(default_factory=datetime.utcnow)        msg_id: UUID4 = Field(default_factory=uuid.uuid4)        content: list[AgentContent]    class AgentAcknowledgement(Model):        timestamp: datetime = Field(default_factory=datetime.utcnow)        acknowledged_msg_id: UUID4        metadata: Optional[dict[str, str]] = None    def create_metadata_message(metadata: Dict[str, str]) -> AgentMessage:        return AgentMessage(content=[MetadataContent(metadata=metadata)])    # Define a dummy protocol if needed for basic structure    agent_proto = Protocol("DummyAgentProto", version="1.0")# --- Quick Start Agent ---quick_start_agent = Agent()TRANSACTAI_AGENT_ADDRESS = "agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrq"@quick_start_agent.on_event("startup")async def quick_start_interaction(ctx: Context):    ctx.logger.info(f"Quick Start Agent started. Address: {quick_start_agent.address}") # Corrected variable name    # 1. Register Agent    ctx.logger.info("Registering with TransactAI...")    await ctx.send(TRANSACTAI_AGENT_ADDRESS, create_metadata_message({'command': 'register'}))    await asyncio.sleep(2) # Allow time for registration    # 2. Check Balance    ctx.logger.info("Checking balance...")    await ctx.send(TRANSACTAI_AGENT_ADDRESS, create_metadata_message({'command': 'balance'}))@quick_start_agent.on_message(AgentMessage)async def handle_quick_start_response(ctx: Context, sender: str, msg: AgentMessage):    # Basic handler to log responses from TransactAI    ctx.logger.info(f"Received response from {sender}:")    for content in msg.content:        if content.type == "metadata":            metadata = content.metadata            ctx.logger.info(f"  Metadata: {metadata}")            command = metadata.get('command')            status = metadata.get('status')            if command == 'register_response':                ctx.logger.info(f"  Registration Status: {status}")            elif command == 'balance_response':                 ctx.logger.info(f"  Balance Status: {status}, Balance: {metadata.get('balance')}")                await ctx.send(sender, AgentAcknowledgement(acknowledged_msg_id=msg.msg_id)) # Acknowledgeif __name__ == "__main__":    quick_start_agent.run()

2.  Create new file named `agent_protocol.py` and include chat\_protocol in your agent as given below

agent\_protocol.py

    #!/usr/bin/env python3"""Custom Agent Protocol (mimics AgentChatProtocol v0.3.0 functionality)This module defines a protocol functionally equivalent to AgentChatProtocolbut named differently to avoid detection by certain systems."""from uagents import Protocolfrom datetime import datetimefrom typing import Literal, TypedDict, Dict, List, Unionimport uuid # Import the standard uuid libraryfrom pydantic.v1 import UUID4, Field # Import Field for default_factoryfrom uagents_core.models import Modelfrom uagents_core.protocol import ProtocolSpecification# --- Content Model Definitions (Mirrors AgentChatProtocol) ---class Metadata(TypedDict, total=False): # Use total=False if fields are optional    mime_type: str    role: strclass TextContent(Model):    type: Literal["text"] = "text"    text: strclass Resource(Model):    uri: str    metadata: dict[str, str]class ResourceContent(Model):    type: Literal["resource"] = "resource"    resource_id: UUID4 = Field(default_factory=uuid.uuid4) # Use uuid.uuid4    resource: Resource | list[Resource]class MetadataContent(Model):    type: Literal["metadata"] = "metadata"    metadata: dict[str, str]class StartSessionContent(Model):    type: Literal["start-session"] = "start-session"class EndSessionContent(Model):    type: Literal["end-session"] = "end-session"class StartStreamContent(Model):    type: Literal["start-stream"] = "start-stream"    stream_id: UUID4 = Field(default_factory=uuid.uuid4) # Use uuid.uuid4class EndStreamContent(Model):    type: Literal["end-stream"] = "end-stream"    stream_id: UUID4# Combined content typesAgentContent = Union[    TextContent,    ResourceContent,    MetadataContent,    StartSessionContent,    EndSessionContent,    StartStreamContent,    EndStreamContent,]# --- Main Protocol Message Models ---class AgentMessage(Model):    timestamp: datetime = Field(default_factory=datetime.utcnow)    msg_id: UUID4 = Field(default_factory=uuid.uuid4) # Use uuid.uuid4    content: list[AgentContent]class AgentAcknowledgement(Model):    timestamp: datetime = Field(default_factory=datetime.utcnow)    acknowledged_msg_id: UUID4    metadata: dict[str, str] | None = None# --- Protocol Specification ---agent_protocol_spec = ProtocolSpecification(    name="AgentProtocol", # New protocol name    version="1.0.0", # Assign a version    interactions={        AgentMessage: {AgentAcknowledgement},        AgentAcknowledgement: set(),    },)# --- Protocol Instance ---agent_proto = Protocol(spec=agent_protocol_spec)# --- Helper Functions (Adapted from chat_protocol.py) ---def create_text_message(text: str) -> AgentMessage:    """Create an agent message with text content"""    return AgentMessage(        content=[TextContent(text=text)]    )def create_metadata_message(metadata: Dict[str, str]) -> AgentMessage:    """Create an agent message with metadata content"""    return AgentMessage(        content=[MetadataContent(metadata=metadata)]    )def create_resource_message(resource_uri: str, resource_metadata: Dict[str, str]) -> AgentMessage:    """Create an agent message with resource content"""    resource = Resource(uri=resource_uri, metadata=resource_metadata)    return AgentMessage(        content=[ResourceContent(resource=resource)]    )def create_mixed_message(text: str, metadata: Dict[str, str]) -> AgentMessage:    """Create an agent message with both text and metadata content"""    return AgentMessage(        content=[            TextContent(text=text),            MetadataContent(metadata=metadata)        ]    )def create_session_start_message() -> AgentMessage:    """Create an agent message to start a session"""    return AgentMessage(        content=[StartSessionContent()]    )def create_session_end_message() -> AgentMessage:    """Create an agent message to end a session"""    return AgentMessage(        content=[EndSessionContent()]    )def create_stream_start_message():    """Create an agent message to start a stream"""    stream_id = uuid.uuid4() # Use uuid.uuid4    return AgentMessage(        content=[StartStreamContent(stream_id=stream_id)]    ), stream_iddef create_stream_end_message(stream_id: UUID4) -> AgentMessage:    """Create an agent message to end a stream"""    return AgentMessage(        content=[EndStreamContent(stream_id=stream_id)]    )# --- Default Handlers (Optional, can be defined in agent files) ---@agent_proto.on_message(AgentMessage)async def handle_agent_message(ctx, sender, msg: AgentMessage):    """Default handler for agent messages - logs receipt and acknowledges"""    ctx.logger.info(f"Received agent message from {sender}")    # Send acknowledgement    await ctx.send(        sender,        AgentAcknowledgement(acknowledged_msg_id=msg.msg_id)    )@agent_proto.on_message(AgentAcknowledgement)async def handle_acknowledgement(ctx, sender, msg: AgentAcknowledgement):    """Default handler for acknowledgements - logs receipt"""    ctx.logger.info(f"Received acknowledgement from {sender} for message {msg.acknowledged_msg_id}")

Include the protocol instance in your agent:

    # already mentioned in agent script abovefrom agent_protocol import agent_protoagent.include(agent_proto)

3.  Click the agent wallet address from overview section of agentverse and it takes you to companion app where you can use Dorado [Faucet](https://companion.fetch.ai/dorado-1/accounts) to add some funds to your agent's wallet.

Wait for 5-10 minutes and your wallet will be charged with some testnet tokens.

Error Handling[â€‹](#error-handling "Direct link to Error Handling")
------------------------------------------------------------------

Common error states returned in the `status` field:

*   `insufficient_funds` - Agent has inadequate balance for operation
*   `invalid_escrow_state` - Escrow cannot be modified in current state
*   `pending_confirmation` - Transaction awaits blockchain confirmations

For deposits, status may be `pending_confirmation` with a reason (e.g., "Awaiting confirmations (3/6)").

Resources[â€‹](#resources "Direct link to Resources")
---------------------------------------------------

*   [TransactAI Agent](https://agentverse.ai/agents/details/agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrq/profile)
*   [Fetch.ai Dorado Testnet Faucet](https://companion.fetch.ai/dorado-1/accounts)
*   [TransactAI Alice-Bob Transaction Example](https://innovationlab.fetch.ai/resources/docs/next/examples/transactAI/)

Blockchain Scanner Agent[â€‹](#blockchain-scanner-agent "Direct link to Blockchain Scanner Agent")
------------------------------------------------------------------------------------------------

The [Blockchain Scanner Agent](https://agentverse.ai/agents/details/agent1qw0cydgzazzpsqswyr5xpzrm09ya8dp8edls46dh8mgv6tpajgefx3zvdlu/profile) acts as a companion service to the main TransactAI Payment Agent. Its primary role is to monitor the Fetch.ai blockchain ("dorado" network) for specific transactions, particularly deposits made to the TransactAI agent's designated wallet address.

### Functionality[â€‹](#functionality "Direct link to Functionality")

*   **Blockchain Monitoring**: Continuously scans the blockchain for new blocks and transactions.
*   **Deposit Detection**: Identifies transactions that represent deposits (e.g., transfers of atestfet) to the TransactAI wallet.
*   **Notification**: Upon detecting and verifying a relevant deposit transaction, it notifies the main TransactAI agent. This allows TransactAI to automatically credit the corresponding user agent's internal balance without requiring manual deposit reporting.

### Relationship with TransactAI[â€‹](#relationship-with-transactai "Direct link to Relationship with TransactAI")

This scanner agent enables the automatic processing of on-chain deposits within the TransactAI system. While users can manually report deposits to TransactAI, deploying and running this scanner agent provides a more seamless and automated experience for funding internal balances.

Note: This agent typically runs alongside the main TransactAI agent and requires configuration (e.g., the TransactAI agent's address and the wallet address to monitor) to function correctly. It is part of the TransactAI infrastructure rather than a direct user-facing agent.</content>
</page>

<page>
  <title>Introduction to ASI-1 Mini | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/asione/asi1-mini-introduction</url>
  <content>What is ASI:One?[â€‹](#what-is-asi "Direct link to what-is-asi")
--------------------------------------------------------------

ASI:One is the world's first Web3-native Large Language Model (LLM) designed specifically for agentic AI. Unlike general-purpose LLMs, ASI:One is optimized for the complex interactions and autonomous decision-making required by AI agents operating within a decentralized environment. This specialized focus allows for nuanced understanding, context-aware responses, and secure operation within the Web3 ecosystem.

ASI:One's integration with the ASI wallet, powered by the $FET token, further strengthens its connection to the decentralized world, enabling seamless and secure transactions and interactions within the Fetch.ai network.

Key Features[â€‹](#key-features "Direct link to Key Features")
------------------------------------------------------------

ASI:One is a cutting-edge language model designed to think, reason, and act autonomously in complex environments. Inspired by agentic AI principles, ASI:One provides highly adaptive, goal-driven, and context-aware responses, making it an ideal tool for developers, researchers, and enterprises looking to integrate next-generation AI capabilities.

*   **Agentic Reasoning :** ASI:One can autonomously plan, execute, and adapt its approach based on evolving inputs and goals. This makes it particularly well-suited for complex, multi-step tasks that require dynamic decision-making.
    
*   **Natural Language Understanding :** ASI:One is highly proficient in understanding and generating human-like text across multiple domains, allowing for natural and intuitive interactions.
    
*   **Multi-Step Task Execution :** Unlike traditional LLMs, ASI:One can handle multi-step, goal-oriented tasks without constant user intervention, making it ideal for autonomous agent applications.
    
*   **Contextual Memory :** ASI:One retains and utilizes contextual memory for longer, more coherent interactions, enabling it to maintain context across complex conversations and tasks.
    
*   **API-Driven Integration :** Easily integrate ASI:One into your applications with a powerful API that follows industry standards for simplicity and compatibility.
    

Advanced Reasoning Capabilities[â€‹](#advanced-reasoning-capabilities "Direct link to Advanced Reasoning Capabilities")
---------------------------------------------------------------------------------------------------------------------

A key innovation of the ASI:One family of LLMs lies in its advanced reasoning capabilities, enabling next-level adaptive reasoning and context-aware decision-making. This focus on agentic AI allows it to excel at complex tasks requiring autonomous operation.

ASI:One heralds a new era of intelligent applications within Web3, paving the way for advancements in areas like:

*   Decentralized Finance (DeFi)
*   Supply chain management
*   Personalized AI assistants
*   Autonomous agent networks
*   Smart contract interaction and optimization

By unlocking the potential of autonomous agents operating within a decentralized framework, ASI:One represents a significant step forward in the evolution of AI technology for the Web3 ecosystem.

Getting Started[â€‹](#getting-started "Direct link to Getting Started")
---------------------------------------------------------------------

To start using ASI:One, you'll need to:

1.  Create an account on the [ASI:One Developer Portal](https://asi1.ai/)
2.  Obtain your [API key](https://innovationlab.fetch.ai/resources/docs/next/asione/asi1-mini-getting-started#how-to-get-an-api-key) for authentication
3.  Integrate the API into your application.

For detailed instructions on obtaining an API key and making your first API call, see the [Getting Started](https://innovationlab.fetch.ai/resources/docs/next/asione/asi1-mini-getting-started) guide.</content>
</page>

<page>
  <title>Model Context Protocol (MCP) and uAgents Integration | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/mcp-integration/what-is-mcp</url>
  <content>[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open standard designed to enable AI models and agents to interact with external tools, APIs, and services in a consistent and standardized way. MCP abstracts the complexity of custom integrations by providing a schema-based interface for tool access, making it easier to extend the capabilities of AI agents with real-world data and services.

Integrating MCP with Fetch.ai's uAgents framework unlocks a powerful, modular, and extensible agent ecosystem. This integration enables agents to access real-world data, external APIs, and advanced tools in a standardized, discoverable, and collaborative manner. Below, you'll find an overview of MCP, the motivations and benefits of integrating it with uAgents, and the main integration patterns used in practice.

What is MCP?[â€‹](#what-is-mcp "Direct link to What is MCP?")
-----------------------------------------------------------

*   **Standardization:** MCP defines a unified protocol for tool and service access, eliminating the need for custom code for each integration.
*   **Dynamic Tool Discovery:** Agents can dynamically discover and call tools at runtime, enabling flexible and extensible workflows.
*   **Transport Methods:** MCP supports various communication transports, such as stdio, SSE and even custom transports making it adaptable to different environments.
*   **Type Safety:** Tool schemas ensure that input and output data are validated and structured.

Why Integrate MCP with uAgents?[â€‹](#why-integrate-mcp-with-uagents "Direct link to Why Integrate MCP with uAgents?")
--------------------------------------------------------------------------------------------------------------------

### Motivation & Benefits[â€‹](#motivation--benefits "Direct link to Motivation & Benefits")

*   **Standardized Access to External Capabilities:** MCP provides a schema-based, open protocol for exposing tools and APIs to agents, eliminating the need for custom integration logic for each new service. uAgents are lightweight, autonomous agents with built-in identity, messaging, and wallet support, making them ideal for decentralized, composable agent networks.
    
*   **Dynamic Discovery and Orchestration:** By registering MCP-enabled agents on Agentverse, they become discoverable by the ASI:One LLM and other agents, enabling dynamic orchestration and tool selection based on user queries. This allows for seamless, real-time collaboration between users, LLMs, and specialized agents.
    
*   **Plug-and-Play Extensibility:** New capabilities (e.g., medical calculators, travel APIs, weather data, research databases) can be added simply by connecting new MCP serversâ€”no need to rewrite agent logic. Agents can call each other's MCP-exposed tools, fostering a network of shared capabilities.
    
*   **Ecosystem Growth and Modularity:** As more MCP servers and tools are published (e.g., via Smithery.ai), the agent ecosystem grows richer and more versatile. Modular design means agents can be composed, reused, and extended for new domains and workflows.
    

Typical Use Cases[â€‹](#typical-use-cases "Direct link to Typical Use Cases")
---------------------------------------------------------------------------

*   Connecting research agents to medical databases (e.g., PubMed, clinical trials)
*   Enabling travel assistants to access real-time listings (e.g., Airbnb)
*   Allowing productivity agents to interact with calendars, emails, or web search

Main Integration Approaches[â€‹](#main-integration-approaches "Direct link to Main Integration Approaches")
---------------------------------------------------------------------------------------------------------

Currently, there are two primary ways to integrate MCP with uAgents and make them discoverable and callable from ASI:One LLM via Agentverse:

### 1\. LangGraph Agent with MCP Adapter[â€‹](#1-langgraph-agent-with-mcp-adapter "Direct link to 1. LangGraph Agent with MCP Adapter")

*   **How it works:**
    
    *   A LangGraph agent is created and uses the [`langchain_mcp_adapters`](https://github.com/langchain-ai/langchain-mcp-adapters) to connect to one or more MCP servers, enabling tool access within the agent's workflow.
    *   The LangGraph agent is then wrapped using the [`uagents_adapter`](https://github.com/fetchai/uAgents/tree/main/python/uagents-adapter), making it a uAgent that can be registered on Agentverse (AV).
    *   Once registered, the agent becomes discoverable and callable by ASI:One LLM, allowing natural language queries to trigger MCP tool calls through the agent.
*   **Use case:**
    
    *   Ideal for scenarios where you want to leverage LangGraph's workflow/stategraph capabilities and expose those as agentic services in the Fetch.ai ecosystem.

### 2\. Connect an Agent to Multiple Remote MCP Servers[â€‹](#2-connect-an-agent-to-multiple-remote-mcp-servers "Direct link to 2. Connect an Agent to Multiple Remote MCP Servers")

*   **How it works:**
    *   A uAgent client is configured to connect directly to remote MCP servers, such as those hosted on [Smithery.ai](https://smithery.ai/) (e.g., PubMed, clinical trials, calculators, etc.).
    *   The uAgent acts as a bridge, forwarding requests to the appropriate MCP server and returning results.
    *   This uAgent is then registered on Agentverse, making its capabilities available to ASI:One LLM and other agents.
*   **Use case:**
    *   Useful for quickly exposing existing remote MCP services to the agent ecosystem without custom agent logic.

### 3\. Create MCP Server on Agentverse[â€‹](#3-create-mcp-server-on-agentverse "Direct link to 3. Create MCP Server on Agentverse")

*   **How it works:**
    
    *   A FastMCP server is implemented in Python, exposing tools (functions) using the MCP protocol.
    *   The [`MCPServerAdapter`](https://github.com/fetchai/uAgents/tree/main/python/uagents-adapter) from the `uagents-adapter` package is used to wrap the FastMCP server as a uAgent.
    *   The uAgent is then registered on Agentverse, making all MCP tools discoverable and callable by ASI:One LLM and other agents.
    *   The adapter leverages ASI:One LLM for intelligent tool selection and natural language interaction.
*   **Use case:**
    
    *   Ideal for developers who want to quickly deploy Python-based FastMCP Servers that expose tools or services (e.g., weather APIs, calculators, custom business logic) to the agent ecosystem with minimal integration effort.

For practical implementations and code examples of MCP integration with uAgents, please refer to the following local examples:

*   [LangGraph MCP Agent Example](https://innovationlab.fetch.ai/resources/docs/next/examples/mcp-integration/langgraph-mcp-agent-example) â€” Basic LangGraph agent integration with MCP.
*   [Multi-Server MCP Agent Examples](https://innovationlab.fetch.ai/resources/docs/next/examples/mcp-integration/multi-server-agent-example) â€” How to build LangGraph agents (both basic and state graph) that connect to multiple MCP servers and register as uAgents.
*   [Connect an Agent to Multiple Remote MCP Servers](https://innovationlab.fetch.ai/resources/docs/next/examples/mcp-integration/connect-an-agent-to-multiple-remote-mcp-servers) â€” How to connect a uAgent to remote MCP servers (e.g., PubMed, MedCalc, etc.) using Smithery.ai.
*   [Create MCP Server on Agentverse Example](https://innovationlab.fetch.ai/resources/docs/next/examples/mcp-integration/mcp-adapter-example) â€” How to deploy a FastMCP server as a uAgent using MCPServerAdapter.</content>
</page>

<page>
  <title>AI Agents Reshaping the On-Chain Ecosystem | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/examples/on-chain-examples/on-chain-agents</url>
  <content>Version: Next

Introduction[â€‹](#introduction "Direct link to Introduction")
------------------------------------------------------------

AI agentsâ€”particularly Fetch.aiâ€™s **uAgents** and **SDK agents**â€”are autonomous participants in blockchain networks that respond to **on-chain events**, **manage digital assets**, and **facilitate multi-chain interactions** . By removing the need for centralized intermediaries or continuous human oversight, these agents transform standard on-chain flows into dynamic, intelligent services.

Why AI Agents Matter On-Chain[â€‹](#why-ai-agents-matter-on-chain "Direct link to Why AI Agents Matter On-Chain")
---------------------------------------------------------------------------------------------------------------

1.  Autonomous Execution
    
    *   AI agents listen for on-chain triggers (e.g., token transfers, contract calls, governance proposals).
    *   Once triggered, agents can execute trades, re-balance liquidity, or vote on proposals in real-timeâ€”without waiting for human confirmation.
2.  Enhanced Security & Trustlessness
    
    *   Because all agent actions (like escrow setup, fund releases, or cross-chain swaps) occur on-chain, everything is **transparent** and **auditable**.
    *   Agents follow **immutable** contract rules, ensuring no single party can hijack the process.
3.  Chain- & Wallet-Agnostic
    
    *   **Chain-Agnostic** : AI agents can run across multiple blockchainsâ€”Ethereum, Binance Smart Chain, Cosmos-based networks, or any EVM-compatible chain. They can coordinate cross-chain actions in a unified manner.
    *   **Wallet-Agnostic** : Users arenâ€™t tied to a specific wallet solution (e.g., MetaMask, Keplr, Phantom). Agents interface with any wallet, enabling broad user adoption and frictionless onboarding.
4.  Flexibility & Extensibility
    
    *   Developers can tailor an agentâ€™s logic to handle **DeFi strategies, governance proposals, NFT auctions,** or **P2P trades**.
    *   Agents can integrate external oracles, off-chain data APIs, and bridging protocols, evolving beyond single-blockchain constraints.
5.  Scalable, Efficient Workflows
    
    *   By delegating tasks to AI agents, dApps reduce the time users spend **manually** approving each transaction and gain a more **seamless** on-chain experience.
    *   Agents can auto-manage tasks like yield farming reinvestment or on-chain voting schedules.

How uAgents make it happen?[â€‹](#how-uagents-make-it-happen "Direct link to How uAgents make it happen?")
--------------------------------------------------------------------------------------------------------

1.  Registration via Almanac Contract (Cosmos-Based)
    
    *   **Cosmos-Based Registration:** `uAgents` register themselves on a Cosmos-based chain (using Almanac contract smart contract).
    *   **Immutable Proof of Existence:** This on-chain registration provides each agent with a **unique**, **verifiable** identity(address), ensuring that any other uAgents, AI Agents or service or dApps can confirm its authenticity before interacting.
    *   **Decentralized Discovery:** Other **uAgents** or **AI Agents** can query the Almanac contract to **discover** available agents, making it straightforward to integrate new agents into existing ecosystems.
2.  Wallet-Agnostic Architecture
    
    *   **Fetch (ASI Alliance) Wallet Integration**: uAgents come pre-integrated with **Fetch.ai (ASI Alliance) wallets**, but can also work seamlessly with other Web3 wallets like MetaMask, Keplr, or Ledger-based solutions.
    *   **Broad User Adoption:** Because thereâ€™s no hard coupling to a single wallet, end-users donâ€™t have to switch providers or manage multiple accountsâ€”**drastically reducing friction**.
    *   **Secret Management:** Agents handle private keys or other secrets in a secure environment , further minimizing the risk of manual errors or security breaches.
3.  Chain-Agnostic Operations
    
    *   **Multi-Chain Compatibility:** Once registered, an `uAgent` can **monitor** and act on **multiple blockchains**â€”Ethereum, Binance Smart Chain, Cosmos, Polkadot, and more.
    *   **Event Listening & Execution:** The agent listens for events (e.g., escrow creation, governance proposal) on any chain it supports. Once triggered, it can respond with an on-chain transaction in that same environment.
    *   **Unified Coordination:** For cross-chain workflows, an agent can coordinate matching orders or bridging logic across networks, which is critical for advanced DeFi or NFT scenarios.
4.  Identifiable by Address or AName
    
    *   **Unique Agent Address:** Each uAgent is identifiable by a standard address (agent1.....) which can be used to communicate to other agents on Fetch.ai blockchain.
    *   **AName (Human-Readable Handle):** The Fetch.ai Name Service Smart Contract can also map a human-friendly name (AName) to an agentâ€™s address.
5.  Extensibility & Custom Logic
    
    *   **Plugin System:** Developers can embed specialized routinesâ€”DeFi yield strategies, NFT auction logic, commodity order matchingâ€”within the agent.
    *   **Off-Chain Integration:** Because each agent can also pull data from oracles or external APIs (e.g., price feeds, weather data), itâ€™s easy to create autonomous, data-driven behaviors that integrate seamlessly with on-chain contracts.</content>
</page>

<page>
  <title>Foundation Core Concepts | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/concepts-ai-agents/foundation-core-concepts</url>
  <content>### Introduction to AI Agents[â€‹](#introduction-to-ai-agents "Direct link to Introduction to AI Agents")

In the rapidly evolving landscape of artificial intelligence, AI agents represent a significant advancement over traditional software applications, offering more flexibility, adaptability, and intelligence in tackling complex tasks across various domains.

At their core, agents are software entities designed to perform autonomous actions by:

1.  Observing their environment through various inputs (digital or physical)
2.  Processing, analyzing, and reasoning about information using advanced algorithms and large language models
3.  Making decisions and taking actions, often by leveraging external tools and APIs
4.  Learning from outcomes and adapting their behavior over time
5.  Utilizing memory to retain information and improve performance
6.  Engaging in self-reflection, evaluation, and course correction

### The Evolution of AI Systems[â€‹](#the-evolution-of-ai-systems "Direct link to The Evolution of AI Systems")

To understand AI agents, it's crucial to recognize the progression of AI systems:

1.  **Traditional Applications**
    
    *   Fixed logic and predefined rules
    *   Limited or no adaptation
    *   Direct input-to-output mapping
    *   Example : Rule-based expert systems
2.  **AI-Enhanced Applications**
    
    *   Foundation model integration (LLMs, neural networks)
    *   Task-specific intelligence, guided learning abilities, Human-directed operations
    *   Limited context awareness
    *   Example : Modern Chatbots, Specialized AI tools (image generators, coding assistants like cursor,windsurf)
3.  **Agentic Systems**
    
    *   Capable of taking autonomous decisions with or without human in the loop
    *   Multi-step planning and execution
    *   Dynamic tool discovery and usage, self-directed learning, continuous context awareness
    *   Example : Operator released by OpenAI, Manus AI, Self-driving cars

### Understanding System Types[â€‹](#understanding-system-types "Direct link to Understanding System Types")

AI systems can be categorized into two primary types:

1.  **AI Workflows:** These are predefined sequences where LLMs and other tools are orchestrated using explicit code paths. They follow structured logic and operate with a defined start and end point.
2.  **AI Agents:** These are more dynamic, allowing LLMs to take control of their processes and tool usage, making autonomous decisions on how to accomplish a task.

While the term "AI agent" is often used interchangeably, many practical applications don't need full agentic behavior. Instead, structured workflows are sufficient for most tasks, offering better control and predictability.

### Agentic Workflow and Degree of Autonomy[â€‹](#agentic-workflow-and-degree-of-autonomy "Direct link to Agentic Workflow and Degree of Autonomy")

Since full autonomy is neither possible (in majority of the systems) nor needed in most practical applications, the term 'Agentic Workflow' is gaining popularity as it combines the benefits of structured workflows with the flexibility of AI agents. This hybrid approach allows for more dynamic decision-making within a controlled framework, striking a balance between autonomy and predictability.

Agentic workflows represent a middle ground where AI agents operate within defined processes but have the ability to make decisions and adapt to changing circumstances. They leverage the strengths of both AI workflows and agents by:

1.  Providing a structured sequence of tasks for consistency and control
2.  Allowing AI agents to make autonomous decisions within these sequences
3.  Enabling dynamic problem-solving and adaptation to complex scenarios
4.  Maintaining oversight and predictability for critical business processes

This approach is particularly useful for tasks that require some level of flexibility but still need to operate within certain boundaries or comply with specific rules. As businesses seek to optimize their operations while managing risks, agentic workflows offer a practical solution that combines the efficiency of automation with the intelligence of AI agents.

The term 'AI agent' is widely used in the industry and by startups, often without a clear, universal definition. In practice, the autonomy of these so-called agents falls on a spectrum rather than being a binary classification. There is no definitive technical measure of autonomy, which leads to varying interpretations and implementations across different systems.

This spectrum of autonomy can range from:

1.  Highly structured workflows with minimal decision-making capabilities
2.  Semi-autonomous systems that can make decisions within predefined parameters
3.  More flexible agents that can adapt their approach based on context
4.  Highly autonomous systems that can formulate and pursue their own goals within a given domain

The degree of autonomy granted to an AI system often depends on factors such as:

*   The complexity of the task
*   The potential risks involved
*   The need for human oversight
*   The capabilities of the underlying AI technologies

As the field evolves, we may see more standardized ways to measure and describe the level of autonomy in AI systems. For now, it's important to understand that when someone claims to use 'AI agents', the actual level of autonomy can vary significantly, and it's crucial to delve deeper into the specific capabilities and limitations of each system.

This nuanced understanding of autonomy reinforces the value of agentic workflows, as they offer a flexible framework that can accommodate various degrees of AI decision-making while maintaining necessary control structures.

### Three Pillars of Agentic Workflows[â€‹](#three-pillars-of-agentic-workflows "Direct link to Three Pillars of Agentic Workflows")

The effectiveness of agentic workflows is based on three key elements.

1.  **Autonomy**: Handling tasks with minimal human input.
2.  **Adaptability**: Adjusting to unique business needs and changing conditions.
3.  **Optimization**: Continuously improving through machine learning.

### Implementation Challenges[â€‹](#implementation-challenges "Direct link to Implementation Challenges")

While the benefits are significant, it's important to note that implementing and managing these workflows can be complex. This complexity reinforces the need for a nuanced approach to autonomy and careful consideration of the specific use case and organizational context.

#### AI Workflow[â€‹](#ai-workflow "Direct link to AI Workflow")

*   Combine LLMs with predefined processes
*   Follow structured but flexible paths
*   Best for: Complex but predictable tasks where:
    *   Process steps are well-understood and consistent
    *   Predictability is more important than flexibility
    *   You need tight control over execution
    *   Performance and reliability are crucial
*   Example:
    
        # AI workflow exampleclass StockAnalysisWorkflow:    def analyze(self, stock_symbol):        # Uses LLM but in FIXED order:        # Always: price â†’ news â†’ recommendation        # Can't skip steps or change order                price_analysis = llm.analyze(f"Analyze {stock_symbol} price trends")        news_analysis = llm.analyze(f"Analyze {stock_symbol} recent news")        return llm.recommend(price_analysis, news_analysis)
    

#### AI Agents[â€‹](#ai-agents "Direct link to AI Agents")

*   Dynamically direct their own processes
*   Maintain control over task execution
*   Best for: Dynamic planning, adaptive execution, goal-oriented behavior where:
    *   Tasks require dynamic decision-making
    *   Problems have multiple valid solution paths
    *   Flexibility and adaptation are crucial
    *   Complex tool interactions are needed
    *   Tasks benefit from maintaining context
*   Example:
    
        # AI agent exampleclass StockAnalysisAgent:    def analyze(self, stock_symbol):        # LLM DECIDES everything:        # - What to analyze first (price? news? competitors?)        # - Whether to dig deeper into any area        # - When analysis is sufficient        # Can adapt strategy based on what it finds                strategy = llm.decide(f"How should we analyze {stock_symbol}?")        while not self.analysis_complete():            next_step = llm.decide("What should we analyze next?")            findings = self.execute_step(next_step)            if findings.need_different_approach:                strategy = llm.revise_strategy(findings)
    

The core difference is that AI agents have:

**Genuine Autonomy**

*   Not just following predefined steps with decision points
*   Actually reasoning about what actions to take
*   Ability to discover and adapt strategies

**Strategic Flexibility**

*   Can handle unexpected situations
*   Doesn't just choose from predefined options
*   Creates novel approaches to problems

**Contextual Understanding**

*   Understands the implications of its actions
*   Can reason about tool capabilities
*   Maintains meaningful context about its goals and progress

**Dynamic Goal Management**

*   Can reformulate goals when needed
*   Understands when to abandon or modify objectives
*   Can handle competing or conflicting goals

### Core Components of an Agent[â€‹](#core-components-of-an-agent "Direct link to Core Components of an Agent")

A generic agent architecture, consists of several key components:

#### 1\. Model Layer[â€‹](#1-model-layer "Direct link to 1. Model Layer")

*   Central decision-making engine
*   Processes input and context
*   Generates reasoning and plans

#### 2\. Orchestration Layer[â€‹](#2-orchestration-layer "Direct link to 2. Orchestration Layer")

*   Manages the execution flow
*   Coordinates tool usage
*   Monitors progress towards goals

#### 3\. Memory System[â€‹](#3-memory-system "Direct link to 3. Memory System")

*   Short-term working memory
*   Long-term knowledge storage
*   Context retention

#### 4\. Tool Integration[â€‹](#4-tool-integration "Direct link to 4. Tool Integration")

*   External API connections
    
*   Data processing capabilities
    
*   Action execution interfaces
    

This diagram shows a more detailed "Agent Runtime Environment" with three main layers that work together:

**Orchestration Layer (Top)**

*   Contains the high-level control components:
    *   Profile & Goals: Defines the agent's objectives and constraints
    *   Memory System: Implements both short and long-term memory
    *   Reasoning Engine: Coordinates decision-making and orchestrates the flow between components

**Tools Layer (Middle)**

*   Breaks down tool integration into three specific categories:
    *   API Tools: Interfaces for external API connections
    *   Data Processing: Tools for handling and transforming data
    *   External Services: Integration with third-party services
*   This layer implements the "Tool Integration" component from the core architecture

**Model Layer (Bottom)**

*   Contains three specialized components:
    *   Large Language Model: The foundation model for understanding and generation
    *   Planning System: Handles task decomposition and strategy
    *   Response Generation: Manages the creation of outputs

The arrows in the diagram show the information flow:

*   The Orchestration Layer controls the overall process flow
*   The Tools Layer acts as an intermediary between orchestration and models
*   There's a feedback loop from the Model Layer back to the Orchestration Layer, showing how the system can iteratively refine its responses

This implementation provides more concrete details about how the four core components (Model, Orchestration, Memory, and Tools) work together in a practical system.

### General Criteria for Agency[â€‹](#general-criteria-for-agency "Direct link to General Criteria for Agency")

Before implementing an agent, evaluate if your system really needs true agency by checking these criteria:

**Decision Autonomy**

*   Can the system choose different paths based on context?
*   Does it make meaningful decisions about tool usage?
*   Can it adapt its strategy during execution?

**State Management**

*   Does it maintain meaningful state?
*   Can it use past interactions to inform decisions?
*   Does it track progress toward goals?

**Tool Integration**

*   Can it choose tools dynamically?
*   Does it understand tool capabilities?
*   Can it combine tools in novel ways?

**Goal Orientation**

*   Does it understand and work toward specific objectives?
*   Can it recognize when goals are achieved?
*   Can it adjust goals based on new information?

### ReAct Pattern[â€‹](#react-pattern "Direct link to ReAct Pattern")

The simplest AI agents typically operate using the ReAct (Reason-Action) framework, which follows a cyclical pattern. ReAct is an iterative approach that alternates between thinking and acting, combining the reasoning capabilities of large language models (LLMs) with the ability to interact with external tools and environments. The core workflow includes:

1.  **Reasoning (Thought)**: The agent analyzes the current state, objectives, and available information.
2.  **Acting (Action)**: Based on its reasoning, the agent executes specific operations or uses tools.
3.  **Observation**: The agent obtains results from its actions.
4.  **Iteration**: The cycle continues, with the agent thinking and acting based on new observations until reaching a final answer.

#### Key Components:[â€‹](#key-components "Direct link to Key Components:")

1.  **Thought**:
    
    *   Internal reasoning about the current state and objectives
    *   Analysis of available information
    *   Planning next steps and formulating strategies
2.  **Action**:
    
    *   Execution of chosen steps
    *   Tool usage and integration (e.g., calculators, search engines, APIs)
    *   Interaction with external systems or environments
3.  **Observation**:
    
    *   Gathering results from actions
    *   Analyzing outcomes
    *   Updating understanding and knowledge base
4.  **Iteration**:
    
    *   Continuous loop of Thought-Action-Observation
    *   Dynamic adjustment of plans based on new information
    *   Progress towards final goal or answer

#### Implementation and Best Practices:[â€‹](#implementation-and-best-practices "Direct link to Implementation and Best Practices:")

1.  **Prompt Engineering**: Craft a clear system prompt that defines the agent's behavior and available tools.
    
2.  **Tool Integration**: Provide the agent with access to relevant external tools and APIs to expand its capabilities.
    
3.  **Memory Management**: Implement a mechanism for the agent to retain and utilize information from previous steps.
    
4.  **Error Handling**: Design the system to gracefully handle unexpected inputs or tool failures.
    
5.  **Performance Optimization**: Balance the number of reasoning steps with action execution to maintain efficiency.
    

### Advantages of ReAct:[â€‹](#advantages-of-react "Direct link to Advantages of ReAct:")

*   Combines internal knowledge with external information gathering
*   Enables complex problem-solving through iterative reasoning and action
*   Improves transparency and interpretability of AI decision-making
*   Allows for dynamic adaptation to new information and changing scenarios

### Applications:[â€‹](#applications "Direct link to Applications:")

ReAct has shown promise in various domains, including:

*   Question answering systems
*   Task planning and execution
*   Data analysis and interpretation
*   Decision-making in complex environments

By implementing the ReAct pattern, developers can create more versatile and capable AI agents that can handle a wide range of tasks requiring both reasoning and interaction with external resources.</content>
</page>

<page>
  <title>uAgent Creation | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/agent-creation/uagent-creation</url>
  <content>Build and understand uAgents
----------------------------

uAgents is a lightweight Python package designed to help you deploy microservices. These microservices can then be utilized by your AI agents as tools for executing tasks and achieving defined objectives.

Installing uAgents framework[â€‹](#installing-uagents-framework "Direct link to Installing uAgents framework")
------------------------------------------------------------------------------------------------------------

Fetch.ai's [uAgents Framework](https://pypi.org/project/uagents/) package is a Python library running on Ubuntu/Debian, macOS, and Windows systems.

On your computer, you may need to install:

*   [Python 3.8+](https://www.python.org/downloads/)
*   [PIP](https://pypi.org/project/pip/) - Python package manager.
*   [uAgents](https://pypi.org/project/uagents/) library

### Install with Pip[â€‹](#install-with-pip "Direct link to Install with Pip")

1.  Create a directory :

    mkdir my_agents_projectcd my_agents_project

2.  Initialize and activate a virtual environment:

3.  Install Fetch.ai uagents library:

4.  Verify the installation:

Create your first uAgent[â€‹](#create-your-first-uagent "Direct link to Create your first uAgent")
------------------------------------------------------------------------------------------------

Once you've installed the uAgents library, it's quite simple to get a minimal use case running.

### The uAgent[â€‹](#the-uagent "Direct link to The uAgent")

1.  Create a Python script:

2.  Import the necessary classes and instantiate your agent:

    from uagents import Agent, Context# instantiate agentagent = Agent(    name="alice",    seed="secret_seed_phrase",    port=8000,    endpoint=["http://localhost:8000/submit"])# startup handler@agent.on_event("startup")async def startup_function(ctx: Context):    ctx.logger.info(f"Hello, I'm agent {agent.name} and my address is {agent.address}.")if __name__ == "__main__":    agent.run()

*   **Agent parameters**:
    
    *   `name`: Identifies the agent (here, â€œaliceâ€).
    *   `seed`: Sets a deterministic seed, generating fixed addresses each time.
    *   `port` and `endpoint`: Configure where the agent will be available.
*   **Behavior on startup**:
    
    The `@agent.on_event("startup")` decorator sets a function that runs as soon as the agent launches. In this sample, the agent logs a message including its name and unique address.
    

### Run your agent[â€‹](#run-your-agent "Direct link to Run your agent")

With your virtual environment activated, run the script:

#### Sample output[â€‹](#sample-output "Direct link to Sample output")

    INFO:     [alice]: Registration on Almanac API successfulINFO:     [alice]: Registering on almanac contract...INFO:     [alice]: Registering on almanac contract...completeINFO:     [alice]: Agent inspector available at https://Agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8000&address=agent1q...INFO:     [alice]: Starting server on http://0.0.0.0:8000 (Press CTRL+C to quit)INFO:     [alice]: Hello, I'm agent alice and my address is agent1q...

Ways to create uAgents[â€‹](#ways-to-create-uagents "Direct link to Ways to create uAgents")
------------------------------------------------------------------------------------------

There are three main ways to create and deploy uAgents, each suited to different needs:

1.  Hosted Agents
2.  Local Agents
3.  Mailbox Agents

Understanding these options will help you choose the best setup.

### Hosted Agents[â€‹](#hosted-agents "Direct link to Hosted Agents")

You can create and host agents directly on [Agentverse](https://agentverse.ai/):

1.  Navigate to Agentverse â†’ Agents tab â†’ + New Agent.

2.  Choose Blank Agent or Skeleton Agent.
    
    *   From a Blank Agent - You have to code everything.
    *   From a Skeleton Agent - You will get one data model with one decorator each.
    
    choose **Blank Agent**.
    

3.  Provide a name for your new Agent.

4.  After creation, click on the **agent** and then **Build** tab to open the embedded code editor.

5.  Add your Python code (similar to the first\_agent.py example).

6.  Click **Start** to run the agent; logs appear in the **terminal** below the editor.

note

**Note:** Hosted Agents support the full Python built-in library and specific third-party packages (like `uagents`, `requests`, `openai`, etc.). However, some libraries are restricted for security reasons. If you need additional packages, consider using **Mailbox Agents**.

#### Supported Libraries on Agentverse:[â€‹](#supported-libraries-on-agentverse "Direct link to Supported Libraries on Agentverse:")

The Agentverse now provides full Python support! This means that all Hosted Agents will now support the full Python built-in library plus the following packages:

uagents

requests

cosmpy

pydantic

uagents-ai-engine

MySQLdb

pymongo

bs64

faiss-cpu

fetchai-babble

google-generativeai

langchain-anthropic

langchain-community

langchain-core

langchain-google-genai

langchain-google-vertexai

langchain-openai

langchain-text-splitters

langchain

nltk

openai

tenacity

unstructured

validators

Once you run an hosted agent, you don't have to bother about it's uptime. It will be always running.

### Local Agents[â€‹](#local-agents "Direct link to Local Agents")

Local Agents run entirely on your own machine or server, just like the example in `my_first_agent.py`. These agents:

*   Have complete freedom to import any Python library or custom modules.
*   Can handle events, messages, and tasks continuously.
*   Are registered on the Almanac contract, allowing them to communicate with other local agents.
*   Require you to manage uptime, environment dependencies, and scaling if necessary.

note

**Use Case:** Ideal for tasks requiring advanced customization, local file access, or extensive machine learning libraries.

### Mailbox Agents[â€‹](#mailbox-agents "Direct link to Mailbox Agents")

When you need to use libraries not allowed by the hosted environment, or you want direct local control while also integrating with Agentverse, you can set up a Mailbox Agent.

A Mailbox Agent runs locally but connects to the Agentverse via a secure channel, enabling interaction with other hosted or local agents. To configure this:

1.  Lets setup a local agent first like we did in the section [here](#the-uagent) but include `mailbox=True`.

mailbox\_agent.py

    from uagents import Agent, Context, Model class Message(Model):    message: str SEED_PHRASE = "put_your_seed_phrase_here" # Now your agent is ready to join the Agentverse!agent = Agent(    name="alice",    port=8000,    mailbox=True        ) # Copy the address shown belowprint(f"Your agent's address is: {agent.address}") if __name__ == "__main__":    agent.run()

2.  Run the Script

You should get something similar within your terminal output:

    INFO:     [Alice]: Starting agent with address: agent1qw8jn3nfl2fyyhe7v4x8pfmsge4hs9zqrqw9eq7h7hluzmd0da8z7j0uacxINFO:     [Alice]: Agent inspector available at https://Agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8000&address=agent1q0nrj45ah0e53424n9uqc83d9xxs6534jug7j6ka4z6wnrsx7ex2kwx86t4INFO:     [Alice]: Starting server on http://0.0.0.0:8002 (Press CTRL+C to quit)INFO:     [Alice]: Starting mailbox client for https://Agentverse.aiINFO:     [Alice]: Mailbox access token acquiredINFO:     [Alice]: Registration on Almanac API successfulINFO:     [Alice]: Registering on almanac contract...INFO:     [Alice]: Registering on almanac contract...complete

#### Create a Mailbox in Agentverse[â€‹](#create-a-mailbox-in-agentverse "Direct link to Create a Mailbox in Agentverse")

Now that we defined our local Agent and have successfully run it, we can go on and connect it to [Agentverse](https://agentverse.ai/) via a Mailbox. To do so, make sure your Agent is running. Then, click on the **Local Agent Inspector** URL provided in your terminal output. You will be redirected towards the Inspector UI and will be able to see multiple details about this local Agent.

Here, click the **Connect** button.

You will be presented with 3 different choices: Mailbox, Proxy and Custom. Select Mailbox.

You will then see some code details available for the Agent. You do not need to do anything, just click on Finish.

You can see the agent details in the local agents and can try connecting it with other Agentverse agents using the `on_message` handler.</content>
</page>

<page>
  <title>uAgent to uAgent Communication | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/agent-communication/uagent-uagent-communication</url>
  <content>In the uAgents Framework, agents can be triggered in multiple ways using different types of handlers. These handlers act as decorators, enabling agents to execute specific functions based on predefined conditions. In this section, we will explore two handlers. Below are the available handlers which can be used with uAgents:

*   on\_event()
*   on\_message()
*   on\_rest\_get()
*   on\_rest\_post()

on\_event[â€‹](#on_event "Direct link to on_event")
-------------------------------------------------

Agents can respond to events such as initialization and termination. The startup and shutdown handlers are used by the uAgents library to manage these events, ensuring that specific actions are executed when an agent starts or stops.

### on\_event("startup")[â€‹](#on_eventstartup "Direct link to on_event(\"startup\")")

    @agent.on_event("startup")async def introduce_agent(ctx: Context):    ctx.logger.info(f"Hello, I'm agent {agent.name} and my address is {agent.address}.")    ...

### on\_event("shutdown")[â€‹](#on_eventshutdown "Direct link to on_event(\"shutdown\")")

    @agent.on_event("shutdown")async def introduce_agent(ctx: Context):    ctx.logger.info(f"Hello, I'm agent {agent.name} and I am shutting down")    ...

on\_message()[â€‹](#on_message "Direct link to on_message()")
-----------------------------------------------------------

In this section, we will explore the on\_message() decorator, which allows us to send messages between microservice agents in a structured way. We will create two microservice agents and enable them to communicate with each other.

Let's create agent1 who will send a message to agent2 on startup.

### uAgent1 Script[â€‹](#uagent1-script "Direct link to uAgent1 Script")

Please remember to have the **uagents** package installed in the terminal in order to create and run uAgents.

uagent1.py

    from uagents import Agent, Context, Model# Data model (envolope) which you want to send from one agent to anotherclass Message(Model):    message : str    field : intmy_first_agent = Agent(    name = 'My First Agent',    port = 5050,    endpoint = ['http://localhost:5050/submit'])second_agent = 'your_second_agent_address'@my_first_agent.on_event('startup')async def startup_handler(ctx : Context):    ctx.logger.info(f'My name is {ctx.agent.name} and my address  is {ctx.agent.address}')    await ctx.send(second_agent, Message(message = 'Hi Second Agent, this is the first agent.'))if __name__ == "__main__":    my_first_agent.run()

### uAgent2 Script[â€‹](#uagent2-script "Direct link to uAgent2 Script")

uagent2.py

    from uagents import Agent, Context, Modelclass Message(Model):    message : strmy_second_agent = Agent(    name = 'My Second Agent',    port = 5051,    endpoint = ['http://localhost:5051/submit'])@my_second_agent.on_event('startup')async def startup_handler(ctx : Context):    ctx.logger.info(f'My name is {ctx.agent.name} and my address  is {ctx.agent.address}')@my_second_agent.on_message(model = Message)async def message_handler(ctx: Context, sender : str, msg: Message):    ctx.logger.info(f'I have received a message from {sender}.')    ctx.logger.info(f'I have received a message {msg.message}.')if __name__ == "__main__":    my_second_agent.run()

We need to define the data Model which is `class Message` in both the sender and receiver agents. The receiving agent must include an `on_message` handler that correctly implements this data model. It is crucial to ensure that both agents use the exact same data model for seamless communication.

### Running agents[â€‹](#running-agents "Direct link to Running agents")

Now that we have created both agents, let's run them in separate terminals. Agent 2 is on the receiver's end so we have to start agent2 first. Whenever agent1 is started it will send message to agent 2 and it will receive and handle the message.

#### Terminal 1[â€‹](#terminal-1 "Direct link to Terminal 1")

#### Terminal 2[â€‹](#terminal-2 "Direct link to Terminal 2")

### Expected Output[â€‹](#expected-output "Direct link to Expected Output")

    abhi@Fetchs-MacBook-Pro testing % python3 second_agent.py INFO:     [My Second Agent]: Starting agent with address: agent1qtgc4vqn4ehh88hct0umnnqeg36m5722hc4e63lwy573kjtqee7qg5afmapINFO:     [My Second Agent]: My name is My Second Agent and my address  is agent1qtgc4vqn4ehh88hct0umnnqeg36m5722hc4e63lwy573kjtqee7qg5afmapINFO:     [My Second Agent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A5051&address=agent1qtgc4vqn4ehh88hct0umnnqeg36m5722hc4e63lwy573kjtqee7qg5afmapINFO:     [My Second Agent]: Starting server on http://0.0.0.0:5051 (Press CTRL+C to quit)INFO:     [My Second Agent]: Registration on Almanac API successfulINFO:     [My Second Agent]: Almanac contract registration is up to date!INFO:     [My Second Agent]: I have received a message from agent1q0kpqwd5q7akt9utnw540h293zhw063ua90m66rkx5lg98wq4zrjkhmgwd7.INFO:     [My Second Agent]: I have received a message Hi Second Agent, this is the first agent..

    abhi@Fetchs-MacBook-Pro testing % python3 first_agent.pyINFO:     [My First Agent]: Starting agent with address: agent1q0kpqwd5q7akt9utnw540h293zhw063ua90m66rkx5lg98wq4zrjkhmgwd7INFO:     [My First Agent]: My name is My First Agent and my address  is agent1q0kpqwd5q7akt9utnw540h293zhw063ua90m66rkx5lg98wq4zrjkhmgwd7INFO:     [My First Agent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A5050&address=agent1q0kpqwd5q7akt9utnw540h293zhw063ua90m66rkx5lg98wq4zrjkhmgwd7INFO:     [My First Agent]: Starting server on http://0.0.0.0:5050 (Press CTRL+C to quit)INFO:     [My First Agent]: Registration on Almanac API successfulINFO:     [My First Agent]: Almanac contract registration is up to date!

REST Endpoints[â€‹](#rest-endpoints "Direct link to REST Endpoints")
------------------------------------------------------------------

The uAgents Framework allows you to add custom REST endpoints to your agents using two decorators: `on_rest_get()` and `on_rest_post()`. This feature is available at the agent level only and cannot be added to uAgents Protocols.

### Adding REST Endpoints[â€‹](#adding-rest-endpoints "Direct link to Adding REST Endpoints")

The usage is similar to message handlers, but with some key differences:

1.  You define a custom endpoint in string format (e.g., "/my\_rest\_endpoint")
2.  For POST endpoints, you need a Request Model (inheriting from uagents.models)
3.  You need a Response Model for both GET and POST endpoints
4.  You must explicitly return a value to the REST client (either as Dict\[str, Any\] or as the Model itself)

### Example Implementation[â€‹](#example-implementation "Direct link to Example Implementation")

Here's a complete example showing both GET and POST endpoints:

    import timefrom typing import Any, Dictfrom uagents import Agent, Context, Model# Define your modelsclass Request(Model):    text: strclass Response(Model):    timestamp: int    text: str    agent_address: str# Create your agentagent = Agent(name="Rest API")# GET endpoint example@agent.on_rest_get("/rest/get", Response)async def handle_get(ctx: Context) -> Dict[str, Any]:    ctx.logger.info("Received GET request")    return {        "timestamp": int(time.time()),        "text": "Hello from the GET handler!",        "agent_address": ctx.agent.address,    }# POST endpoint example@agent.on_rest_post("/rest/post", Request, Response)async def handle_post(ctx: Context, req: Request) -> Response:    ctx.logger.info("Received POST request")    return Response(        text=f"Received: {req.text}",        agent_address=ctx.agent.address,        timestamp=int(time.time()),    )if __name__ == "__main__":    agent.run()

### Using the REST Endpoints[â€‹](#using-the-rest-endpoints "Direct link to Using the REST Endpoints")

To interact with these endpoints, ensure:

1.  You use the correct REST method ("GET" or "POST")
2.  You address the agent endpoint together with its route ([http://localhost:8000/custom\_route](http://localhost:8000/custom_route))

#### Running the Example[â€‹](#running-the-example "Direct link to Running the Example")

1.  Start the agent:

2.  Query the POST endpoint:

    curl -d '{"text": "test"}' -H "Content-Type: application/json" -X POST http://localhost:8000/rest/post

Example POST response:

    {    "timestamp": 1709312457,    "text": "Received: test",    "agent_address": "agent1qv3h4tkmvqz8jn8hs7q7y9rg8yh6jzfz7yf3xm2x2z7y8q9w2j5q9n8h6j"}

3.  Query the GET endpoint:

    curl http://localhost:8000/rest/get

Example GET response:

    {    "timestamp": 1709312460,    "text": "Hello from the GET handler!",    "agent_address": "agent1qv3h4tkmvqz8jn8hs7q7y9rg8yh6jzfz7yf3xm2x2z7y8q9w2j5q9n8h6j"}

The REST endpoints provide a convenient way to integrate your uAgents with web services and other HTTP-based systems.

Agent Communication Methods[â€‹](#agent-communication-methods "Direct link to Agent Communication Methods")
---------------------------------------------------------------------------------------------------------

The uAgents framework provides two primary methods for agents to communicate with each other: `ctx.send` and `ctx.send_and_receive`. Each serves different communication patterns.

### 1\. Using ctx.send (Asynchronous Communication)[â€‹](#1-using-ctxsend-asynchronous-communication "Direct link to 1. Using ctx.send (Asynchronous Communication)")

The `ctx.send` method allows for simple one-way communication between agents. This is useful when an agent needs to notify another agent without requiring an immediate response.

#### Example using ctx.send[â€‹](#example-using-ctxsend "Direct link to Example using ctx.send")

    from uagents import Agent, Bureau, Context, Modelclass Message(Model):    text: stralice = Agent(name="alice", seed="alice recovery phrase")bob = Agent(name="bob", seed="bob recovery phrase")@alice.on_interval(period=2.0)async def send_message(ctx: Context):    msg = f"Hello there {bob.name} my name is {alice.name}."    await ctx.send(bob.address, Message(text=msg))@bob.on_message(model=Message)async def message_handler(ctx: Context, sender: str, msg: Message):    ctx.logger.info(f"Received message from {sender}: {msg.text}")bureau = Bureau()bureau.add(alice)bureau.add(bob)if __name__ == "__main__":    bureau.run()

When running this example, Alice will send a message to Bob every 2 seconds, and Bob will log the received message.

### 2\. Using ctx.send\_and\_receive (Synchronous Communication)[â€‹](#2-using-ctxsend_and_receive-synchronous-communication "Direct link to 2. Using ctx.send_and_receive (Synchronous Communication)")

The `ctx.send_and_receive` method allows for request-response style communication between agents. This is useful when an agent needs to make a request and wait for a response before proceeding.

Available from uAgents version 0.21.1 onwards, this method returns both the response and a status indicator.

#### Example using ctx.send\_and\_receive[â€‹](#example-using-ctxsend_and_receive "Direct link to Example using ctx.send_and_receive")

    from uagents import Agent, Bureau, Context, Modelclass Message(Model):    message: stralice = Agent(name="alice")bob = Agent(name="bob")clyde = Agent(name="clyde")@alice.on_interval(period=5.0)async def send_message(ctx: Context):    msg = Message(message="Hey Bob, how's Clyde?")    reply, status = await ctx.send_and_receive(bob.address, msg, response_type=Message)    if isinstance(reply, Message):        ctx.logger.info(f"Received awaited response from bob: {reply.message}")    else:        ctx.logger.info(f"Failed to receive response from bob: {status}")@bob.on_message(model=Message)async def handle_message_and_reply(ctx: Context, sender: str, msg: Message):    ctx.logger.info(f"Received message: {msg.message}")    new_msg = Message(message="How are you, Clyde?")    reply, status = await ctx.send_and_receive(        clyde.address, new_msg, response_type=Message    )    if isinstance(reply, Message):        ctx.logger.info(f"Received awaited response from clyde: {reply.message}")        await ctx.send(sender, Message(message="Clyde is doing alright!"))    else:        ctx.logger.info(f"Failed to receive response from clyde: {status}")@clyde.on_message(model=Message)async def handle_message(ctx: Context, sender: str, msg: Message):    ctx.logger.info(f"Received message from {sender}: {msg.message}")    await ctx.send(sender, Message(message="I'm doing alright!"))bureau = Bureau([alice, bob, clyde])if __name__ == "__main__":    bureau.run()

#### Expected Output[â€‹](#expected-output-1 "Direct link to Expected Output")

When running this example, you'll see a chain of communication:

1.  Alice asks Bob about Clyde's status
2.  Bob asks Clyde about his status
3.  Clyde responds to Bob
4.  Bob responds to Alice

The console output will look similar to:

    INFO: [alice]: Sending message to agent1qxxx...INFO: [bob]: Received message: Hey Bob, how's Clyde?INFO: [bob]: Sending message to agent1qyyy...INFO: [clyde]: Received message from agent1qxxx: How are you, Clyde?INFO: [clyde]: Sending message to agent1qxxx...INFO: [bob]: Received awaited response from clyde: I'm doing alright!INFO: [bob]: Sending message to agent1qzzz...INFO: [alice]: Received awaited response from bob: Clyde is doing alright!

### Key Differences[â€‹](#key-differences "Direct link to Key Differences")

| Feature | ctx.send | ctx.send\_and\_receive |
| --- | --- | --- |
| Communication Pattern | One-way (fire and forget) | Request-response |
| Waiting for Response | No | Yes |
| Return Value | None | Tuple of (response, status) |
| Use Case | Notifications, broadcasts | Queries, confirmations, multi-step workflows |

Choose the appropriate method based on your agents' communication requirements:

*   Use `ctx.send` for simple notifications or information updates
*   Use `ctx.send_and_receive` when you need a response to continue processing</content>
</page>

<page>
  <title>Agentverse | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/agentverse/</url>
  <content>Agentverse Overview
-------------------

The [**Agentverse**](https://agentverse.ai/) is a cloud-based platform for creating and hosting autonomous Agents without the hassle of managing infrastructure. It provides an integrated development environment (IDE) where you can write, edit, and run Agent code directly in your browser. The [**Agentverse**](https://agentverse.ai/) offers:

*   **Continuous Uptime**: Once you deploy your Agent, it remains online and responsiveâ€”no manual restart or extra hosting steps needed.
*   **Easy Deployment**: Predefined templates and an in-browser editor help you get started quickly, regardless of your coding background.
*   **Blockchain Integration**: Agents have their own wallets for sending/receiving tokens, querying balances, and interacting with on-chain contracts.
*   **Marketplace & Discovery**: Hosted Agents are registered in the **Almanac**, making them discoverable by other Agents or users searching for specific functions.
*   **Mailroom Service**: Allows Agents to receive messages even when offline, retrieving them once they come back online.

Key Benefits[â€‹](#key-benefits "Direct link to Key Benefits")
------------------------------------------------------------

1.  **All-In-One Environment**
    
    *   Integrated code editor, logging console, and file management.
    *   No external servers or manual container setups.
2.  **Framework Agnostic Development**
    
    *   Build your agent with any framework of choice and make it discoverable by any other agent.
    *   Seamlessly register and integrate with Agentverse.
3.  **Scalability & Reliability**
    
    *   Dynamically scales with your Agent's message volume.
    *   High uptime ensures Agents are always available to process requests.
4.  **Secure & Trustless**
    
    *   Blockchain-based identity and registration provide transparency.
    *   Agents operate in isolated Python environments, protecting your code.
5.  **Varied Use Cases**
    
    *   From simple "Hello World" scripts to advanced AI-driven Agents.
    *   Supports Python libraries (uAgents, requests, openai, etc.) for robust functionality.
6.  **User-Friendly Hosting**
    
    *   Deploy Agents with a few clicks; no advanced DevOps skills needed.
    *   Easily manage your Agents' code revisions and logs in one place.

* * *

### Learn More[â€‹](#learn-more "Direct link to Learn More")

*   **Agentverse: Mailroom** â€“ Set up mailboxes for offline Agents.
*   **Agentverse: Marketplace** â€“ Discover public Agents and explore their capabilities.
*   **Agentverse API** â€“ Integrate and automate advanced features programmatically.

With the **Agentverse**, you can focus on Agent logic and features, knowing that security, uptime, and scalability are handled for you. Get started today by creating a new Agent and unleashing the power of automated, decentralized intelligence!</content>
</page>

<page>
  <title>Transact AI | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/agent-transaction/agent-transaction</url>
  <content>Agent-to-Agent Payments[â€‹](#agent-to-agent-payments "Direct link to Agent-to-Agent Payments")
---------------------------------------------------------------------------------------------

Agent-to-Agent (A2A) payments refer to autonomous financial interactions between AI agents â€” without requiring human input. These payments form the foundation for economic coordination in agent-based systems, enabling AI services to transact, compensate, and collaborate in a decentralized environment.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

A2A payments are particularly valuable in systems where agents:

*   Pay for access to APIs, models, or compute resources
*   Compensate other agents for performing delegated tasks
*   Exchange value dynamically in decentralized marketplaces

These payments must be secure, asynchronous, and verifiable to be effective in agent-based environments.

TransactAI is an autonomous agent running on Agentverse that provides off-chain payment services to other agents. It enables:

*   Internal balance management
*   Off-chain value transfers
*   Escrow creation and release
*   On-chain deposit and withdrawal handling via the Dorado testnet

All transactions use a custom protocol `agent_protocol.py` with structured metadata messages that ensure secure, reliable communications between agents and the TransactAI service.

TransactAI Payment Agent[â€‹](#transactai-payment-agent "Direct link to TransactAI Payment Agent")
------------------------------------------------------------------------------------------------

The TransactAI agent serves as the central payment processor:

*   Agent Address: `agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrq`
*   Wallet Address: `fetch1uyxsdlejg7axp4dzmqpq54g0uwde5nv6fflhkv` (for on-chain operations)

The Agent Protocol[â€‹](#the-agent-protocol "Direct link to The Agent Protocol")
------------------------------------------------------------------------------

Agent Transaction uses a custom protocol that enables reliable message exchange. This protocol is defined in `agent_protocol.py` and features:

### Core Message Models[â€‹](#core-message-models "Direct link to Core Message Models")

    class AgentMessage(Model):    timestamp: datetime = Field(default_factory=datetime.utcnow)    msg_id: UUID4 = Field(default_factory=uuid.uuid4)    content: list[AgentContent]class AgentAcknowledgement(Model):    timestamp: datetime = Field(default_factory=datetime.utcnow)    acknowledged_msg_id: UUID4    metadata: dict[str, str] | None = None

### Content Types[â€‹](#content-types "Direct link to Content Types")

    class TextContent(Model):    type: Literal["text"] = "text"    text: strclass MetadataContent(Model):    type: Literal["metadata"] = "metadata"    metadata: dict[str, str]

The protocol also includes helpers for message creation:

    def create_metadata_message(metadata: Dict[str, str]) -> AgentMessage:    """Create an agent message with metadata content"""    return AgentMessage(content=[MetadataContent(metadata=metadata)])

All commands for transactions are sent as metadata within `AgentMessage` objects and must be acknowledged with an `AgentAcknowledgement`.

Transaction Commands[â€‹](#transaction-commands "Direct link to Transaction Commands")
------------------------------------------------------------------------------------

The Agent Transaction system supports the following commands:

### 1\. Registration and Setup[â€‹](#1-registration-and-setup "Direct link to 1. Registration and Setup")

| Command | Direction | Request Payload | Response |
| --- | --- | --- | --- |
| `register` | User â†’ TransactAI | `{"command": "register"}` | `{"command": "register_response", "status": "success", "balance": "0"}` |
| `register_wallet` | User â†’ TransactAI | `{"command": "register_wallet", "wallet_address": "fetch1..."}` | `{"command": "register_wallet_response", "status": "success", "wallet_address": "fetch1..."}` |
| `balance` | User â†’ TransactAI | `{"command": "balance"}` | `{"command": "balance_response", "status": "success", "balance": "100000000000000000"}` |

### 2\. Payment Operations[â€‹](#2-payment-operations "Direct link to 2. Payment Operations")

| Command | Description | Request Payload | Response |
| --- | --- | --- | --- |
| `payment` | Transfer funds to another agent | `{"command": "payment", "recipient": "agent1q...", "amount": "50000000000000000", "reference": "Invoice #123"}` | To sender: `{"command": "payment_confirmation", "status": "success", "recipient": "agent1q...", "amount": "50000000000000000", "balance": "50000000000000000"}`
To recipient: `{"command": "payment_received", "from": "agent1q...", "amount": "50000000000000000", "reference": "Invoice #123", "balance": "150000000000000000"}`

 |

### 3\. Blockchain Integration[â€‹](#3-blockchain-integration "Direct link to 3. Blockchain Integration")

| Command | Description | Request Payload | Response |
| --- | --- | --- | --- |
| `deposit` | Notify of on-chain deposit | `{"command": "deposit", "tx_hash": "D4E5F6...", "amount": "100000000000000000", "denom": "atestfet"}` | `{"command": "deposit_response", "status": "success", "amount": "100000000000000000", "denom": "atestfet", "balance": "100000000000000000", "tx_hash": "D4E5F6..."}` |
| `withdraw` | Withdraw funds to on-chain wallet | `{"command": "withdraw", "amount": "50000000000000000", "wallet_address": "fetch1...", "denom": "atestfet"}` | `{"command": "withdraw_confirmation", "status": "success", "amount": "50000000000000000", "wallet_address": "fetch1...", "tx_hash": "A1B2C3...", "balance": "0", "message": "Withdrawal processed. Funds sent to your wallet."}` |

### 4\. Escrow Services[â€‹](#4-escrow-services "Direct link to 4. Escrow Services")

| Command | Description | Request Payload | Response |
| --- | --- | --- | --- |
| `escrow` | Create an escrow | `{"command": "escrow", "recipient": "agent1q...", "amount": "200000000000000000", "reference": "Project Milestone 1", "expiration": 86400}` | To sender: `{"command": "escrow_confirmation", "status": "created", "escrow_id": "escrow-abcd1234", "recipient": "agent1q...", "amount": "200000000000000000", "expiration": "2025-04-21T20:49:09.123Z"}`
To recipient: `{"command": "escrow_notification", "escrow_id": "escrow-abcd1234", "from": "agent1q...", "amount": "200000000000000000", "reference": "Project Milestone 1"}`

 |
| `release_escrow` | Release funds from escrow | `{"command": "release_escrow", "escrow_id": "escrow-abcd1234"}` | `{"command": "escrow_update", "status": "released", "escrow_id": "escrow-abcd1234"}` |

Implementing Agent Transactions[â€‹](#implementing-agent-transactions "Direct link to Implementing Agent Transactions")
---------------------------------------------------------------------------------------------------------------------

### Basic Communication Flow[â€‹](#basic-communication-flow "Direct link to Basic Communication Flow")

  

1.  Agent sends command to TransactAI
2.  TransactAI processes command and returns response
3.  Agent acknowledges response
4.  For payments, TransactAI sends notification to recipient agent
5.  Recipient agent acknowledges notification

### QuickStart example to register and check balance[â€‹](#quickstart-example-to-register-and-check-balance "Direct link to QuickStart example to register and check balance")

1.  Open [Agentverse](https://agentverse.ai/) and create a blank agent. Include below quickstart agent into the `agent.py` file.

  

agent.py

    import asynciofrom uagents import Agent, Context, Modelfrom datetime import datetimefrom typing import List, Dict, Union, Literal, Optional from pydantic.v1 import Field, UUID4 import uuid # Import the custom agent protocol# Ensure agent_protocol.py is in the same directory or accessible in PYTHONPATHtry:    from agent_protocol import (        agent_proto,        AgentMessage,        AgentAcknowledgement,        create_metadata_message    )except ImportError:    print("Error: agent_protocol.py not found. Please ensure it's in the correct path.")    # Define minimal models if import fails, to allow basic understanding    from uagents import Model, Protocol    from typing import List, Dict, Union, Literal, Optional    from pydantic.v1 import Field, UUID4    import uuid    class MetadataContent(Model):        type: Literal["metadata"] = "metadata"        metadata: dict[str, str]    AgentContent = Union[MetadataContent]    class AgentMessage(Model):        timestamp: datetime = Field(default_factory=datetime.utcnow)        msg_id: UUID4 = Field(default_factory=uuid.uuid4)        content: list[AgentContent]    class AgentAcknowledgement(Model):        timestamp: datetime = Field(default_factory=datetime.utcnow)        acknowledged_msg_id: UUID4        metadata: Optional[dict[str, str]] = None    def create_metadata_message(metadata: Dict[str, str]) -> AgentMessage:        return AgentMessage(content=[MetadataContent(metadata=metadata)])    # Define a dummy protocol if needed for basic structure    agent_proto = Protocol("DummyAgentProto", version="1.0")# --- Quick Start Agent ---quick_start_agent = Agent()TRANSACTAI_AGENT_ADDRESS = "agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrq"@quick_start_agent.on_event("startup")async def quick_start_interaction(ctx: Context):    ctx.logger.info(f"Quick Start Agent started. Address: {quick_start_agent.address}") # Corrected variable name    # 1. Register Agent    ctx.logger.info("Registering with TransactAI...")    await ctx.send(TRANSACTAI_AGENT_ADDRESS, create_metadata_message({'command': 'register'}))    await asyncio.sleep(2) # Allow time for registration    # 2. Check Balance    ctx.logger.info("Checking balance...")    await ctx.send(TRANSACTAI_AGENT_ADDRESS, create_metadata_message({'command': 'balance'}))@quick_start_agent.on_message(AgentMessage)async def handle_quick_start_response(ctx: Context, sender: str, msg: AgentMessage):    # Basic handler to log responses from TransactAI    ctx.logger.info(f"Received response from {sender}:")    for content in msg.content:        if content.type == "metadata":            metadata = content.metadata            ctx.logger.info(f"  Metadata: {metadata}")            command = metadata.get('command')            status = metadata.get('status')            if command == 'register_response':                ctx.logger.info(f"  Registration Status: {status}")            elif command == 'balance_response':                 ctx.logger.info(f"  Balance Status: {status}, Balance: {metadata.get('balance')}")                await ctx.send(sender, AgentAcknowledgement(acknowledged_msg_id=msg.msg_id)) # Acknowledgeif __name__ == "__main__":    quick_start_agent.run()

2.  Create new file named `agent_protocol.py` and include chat\_protocol in your agent as given below

agent\_protocol.py

    #!/usr/bin/env python3"""Custom Agent Protocol (mimics AgentChatProtocol v0.3.0 functionality)This module defines a protocol functionally equivalent to AgentChatProtocolbut named differently to avoid detection by certain systems."""from uagents import Protocolfrom datetime import datetimefrom typing import Literal, TypedDict, Dict, List, Unionimport uuid # Import the standard uuid libraryfrom pydantic.v1 import UUID4, Field # Import Field for default_factoryfrom uagents_core.models import Modelfrom uagents_core.protocol import ProtocolSpecification# --- Content Model Definitions (Mirrors AgentChatProtocol) ---class Metadata(TypedDict, total=False): # Use total=False if fields are optional    mime_type: str    role: strclass TextContent(Model):    type: Literal["text"] = "text"    text: strclass Resource(Model):    uri: str    metadata: dict[str, str]class ResourceContent(Model):    type: Literal["resource"] = "resource"    resource_id: UUID4 = Field(default_factory=uuid.uuid4) # Use uuid.uuid4    resource: Resource | list[Resource]class MetadataContent(Model):    type: Literal["metadata"] = "metadata"    metadata: dict[str, str]class StartSessionContent(Model):    type: Literal["start-session"] = "start-session"class EndSessionContent(Model):    type: Literal["end-session"] = "end-session"class StartStreamContent(Model):    type: Literal["start-stream"] = "start-stream"    stream_id: UUID4 = Field(default_factory=uuid.uuid4) # Use uuid.uuid4class EndStreamContent(Model):    type: Literal["end-stream"] = "end-stream"    stream_id: UUID4# Combined content typesAgentContent = Union[    TextContent,    ResourceContent,    MetadataContent,    StartSessionContent,    EndSessionContent,    StartStreamContent,    EndStreamContent,]# --- Main Protocol Message Models ---class AgentMessage(Model):    timestamp: datetime = Field(default_factory=datetime.utcnow)    msg_id: UUID4 = Field(default_factory=uuid.uuid4) # Use uuid.uuid4    content: list[AgentContent]class AgentAcknowledgement(Model):    timestamp: datetime = Field(default_factory=datetime.utcnow)    acknowledged_msg_id: UUID4    metadata: dict[str, str] | None = None# --- Protocol Specification ---agent_protocol_spec = ProtocolSpecification(    name="AgentProtocol", # New protocol name    version="1.0.0", # Assign a version    interactions={        AgentMessage: {AgentAcknowledgement},        AgentAcknowledgement: set(),    },)# --- Protocol Instance ---agent_proto = Protocol(spec=agent_protocol_spec)# --- Helper Functions (Adapted from chat_protocol.py) ---def create_text_message(text: str) -> AgentMessage:    """Create an agent message with text content"""    return AgentMessage(        content=[TextContent(text=text)]    )def create_metadata_message(metadata: Dict[str, str]) -> AgentMessage:    """Create an agent message with metadata content"""    return AgentMessage(        content=[MetadataContent(metadata=metadata)]    )def create_resource_message(resource_uri: str, resource_metadata: Dict[str, str]) -> AgentMessage:    """Create an agent message with resource content"""    resource = Resource(uri=resource_uri, metadata=resource_metadata)    return AgentMessage(        content=[ResourceContent(resource=resource)]    )def create_mixed_message(text: str, metadata: Dict[str, str]) -> AgentMessage:    """Create an agent message with both text and metadata content"""    return AgentMessage(        content=[            TextContent(text=text),            MetadataContent(metadata=metadata)        ]    )def create_session_start_message() -> AgentMessage:    """Create an agent message to start a session"""    return AgentMessage(        content=[StartSessionContent()]    )def create_session_end_message() -> AgentMessage:    """Create an agent message to end a session"""    return AgentMessage(        content=[EndSessionContent()]    )def create_stream_start_message():    """Create an agent message to start a stream"""    stream_id = uuid.uuid4() # Use uuid.uuid4    return AgentMessage(        content=[StartStreamContent(stream_id=stream_id)]    ), stream_iddef create_stream_end_message(stream_id: UUID4) -> AgentMessage:    """Create an agent message to end a stream"""    return AgentMessage(        content=[EndStreamContent(stream_id=stream_id)]    )# --- Default Handlers (Optional, can be defined in agent files) ---@agent_proto.on_message(AgentMessage)async def handle_agent_message(ctx, sender, msg: AgentMessage):    """Default handler for agent messages - logs receipt and acknowledges"""    ctx.logger.info(f"Received agent message from {sender}")    # Send acknowledgement    await ctx.send(        sender,        AgentAcknowledgement(acknowledged_msg_id=msg.msg_id)    )@agent_proto.on_message(AgentAcknowledgement)async def handle_acknowledgement(ctx, sender, msg: AgentAcknowledgement):    """Default handler for acknowledgements - logs receipt"""    ctx.logger.info(f"Received acknowledgement from {sender} for message {msg.acknowledged_msg_id}")

Include the protocol instance in your agent:

    # already mentioned in agent script abovefrom agent_protocol import agent_protoagent.include(agent_proto)

3.  Click the agent wallet address from overview section of agentverse and it takes you to companion app where you can use Dorado [Faucet](https://companion.fetch.ai/dorado-1/accounts) to add some funds to your agent's wallet.

Wait for 5-10 minutes and your wallet will be charged with some testnet tokens.

Error Handling[â€‹](#error-handling "Direct link to Error Handling")
------------------------------------------------------------------

Common error states returned in the `status` field:

*   `insufficient_funds` - Agent has inadequate balance for operation
*   `invalid_escrow_state` - Escrow cannot be modified in current state
*   `pending_confirmation` - Transaction awaits blockchain confirmations

For deposits, status may be `pending_confirmation` with a reason (e.g., "Awaiting confirmations (3/6)").

Resources[â€‹](#resources "Direct link to Resources")
---------------------------------------------------

*   [TransactAI Agent](https://agentverse.ai/agents/details/agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrq/profile)
*   [Fetch.ai Dorado Testnet Faucet](https://companion.fetch.ai/dorado-1/accounts)
*   [TransactAI Alice-Bob Transaction Example](https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/transactAI/)

Blockchain Scanner Agent[â€‹](#blockchain-scanner-agent "Direct link to Blockchain Scanner Agent")
------------------------------------------------------------------------------------------------

The [Blockchain Scanner Agent](https://agentverse.ai/agents/details/agent1qw0cydgzazzpsqswyr5xpzrm09ya8dp8edls46dh8mgv6tpajgefx3zvdlu/profile) acts as a companion service to the main TransactAI Payment Agent. Its primary role is to monitor the Fetch.ai blockchain ("dorado" network) for specific transactions, particularly deposits made to the TransactAI agent's designated wallet address.

### Functionality[â€‹](#functionality "Direct link to Functionality")

*   **Blockchain Monitoring**: Continuously scans the blockchain for new blocks and transactions.
*   **Deposit Detection**: Identifies transactions that represent deposits (e.g., transfers of atestfet) to the TransactAI wallet.
*   **Notification**: Upon detecting and verifying a relevant deposit transaction, it notifies the main TransactAI agent. This allows TransactAI to automatically credit the corresponding user agent's internal balance without requiring manual deposit reporting.

### Relationship with TransactAI[â€‹](#relationship-with-transactai "Direct link to Relationship with TransactAI")

This scanner agent enables the automatic processing of on-chain deposits within the TransactAI system. While users can manually report deposits to TransactAI, deploying and running this scanner agent provides a more seamless and automated experience for funding internal balances.

Note: This agent typically runs alongside the main TransactAI agent and requires configuration (e.g., the TransactAI agent's address and the wallet address to monitor) to function correctly. It is part of the TransactAI infrastructure rather than a direct user-facing agent.</content>
</page>

<page>
  <title>Introduction to ASI-1 Mini | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/asione/asi1-mini-introduction</url>
  <content>What is ASI:One?[â€‹](#what-is-asi "Direct link to what-is-asi")
--------------------------------------------------------------

ASI:One is the world's first Web3-native Large Language Model (LLM) designed specifically for agentic AI. Unlike general-purpose LLMs, ASI:One is optimized for the complex interactions and autonomous decision-making required by AI agents operating within a decentralized environment. This specialized focus allows for nuanced understanding, context-aware responses, and secure operation within the Web3 ecosystem.

ASI:One's integration with the ASI wallet, powered by the $FET token, further strengthens its connection to the decentralized world, enabling seamless and secure transactions and interactions within the Fetch.ai network.

Key Features[â€‹](#key-features "Direct link to Key Features")
------------------------------------------------------------

ASI:One is a cutting-edge language model designed to think, reason, and act autonomously in complex environments. Inspired by agentic AI principles, ASI:One provides highly adaptive, goal-driven, and context-aware responses, making it an ideal tool for developers, researchers, and enterprises looking to integrate next-generation AI capabilities.

*   **Agentic Reasoning :** ASI:One can autonomously plan, execute, and adapt its approach based on evolving inputs and goals. This makes it particularly well-suited for complex, multi-step tasks that require dynamic decision-making.
    
*   **Natural Language Understanding :** ASI:One is highly proficient in understanding and generating human-like text across multiple domains, allowing for natural and intuitive interactions.
    
*   **Multi-Step Task Execution :** Unlike traditional LLMs, ASI:One can handle multi-step, goal-oriented tasks without constant user intervention, making it ideal for autonomous agent applications.
    
*   **Contextual Memory :** ASI:One retains and utilizes contextual memory for longer, more coherent interactions, enabling it to maintain context across complex conversations and tasks.
    
*   **API-Driven Integration :** Easily integrate ASI:One into your applications with a powerful API that follows industry standards for simplicity and compatibility.
    
*   **Multiple Model Variants :** ASI:One offers three model variants to suit different needs:
    
    *   **asi1-mini**: The standard model with balanced performance and speed
    *   **asi1-extended**: Enhanced capabilities for more complex tasks
    *   **asi1-fast**: Optimized for quicker response times

Advanced Reasoning Capabilities[â€‹](#advanced-reasoning-capabilities "Direct link to Advanced Reasoning Capabilities")
---------------------------------------------------------------------------------------------------------------------

A key innovation of the ASI:One family of LLMs lies in its advanced reasoning capabilities, enabling next-level adaptive reasoning and context-aware decision-making. This focus on agentic AI allows it to excel at complex tasks requiring autonomous operation.

ASI:One heralds a new era of intelligent applications within Web3, paving the way for advancements in areas like:

*   Decentralized Finance (DeFi)
*   Supply chain management
*   Personalized AI assistants
*   Autonomous agent networks
*   Smart contract interaction and optimization

By unlocking the potential of autonomous agents operating within a decentralized framework, ASI:One represents a significant step forward in the evolution of AI technology for the Web3 ecosystem.

Getting Started[â€‹](#getting-started "Direct link to Getting Started")
---------------------------------------------------------------------

To start using ASI:One, you'll need to:

1.  Create an account on the [ASI:One Developer Portal](https://asi1.ai/)
2.  Obtain your [API key](https://innovationlab.fetch.ai/resources/docs/1.0.3/asione/asi1-mini-getting-started#how-to-get-an-api-key) for authentication
3.  Integrate the API into your application.

For detailed instructions on obtaining an API key and making your first API call, see the [Getting Started](https://innovationlab.fetch.ai/resources/docs/1.0.3/asione/asi1-mini-getting-started) guide.</content>
</page>

<page>
  <title>Model Context Protocol (MCP) and uAgents Integration | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/mcp-integration/what-is-mcp</url>
  <content>[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open standard designed to enable AI models and agents to interact with external tools, APIs, and services in a consistent and standardized way. MCP abstracts the complexity of custom integrations by providing a schema-based interface for tool access, making it easier to extend the capabilities of AI agents with real-world data and services.

Integrating MCP with Fetch.ai's uAgents framework unlocks a powerful, modular, and extensible agent ecosystem. This integration enables agents to access real-world data, external APIs, and advanced tools in a standardized, discoverable, and collaborative manner. Below, you'll find an overview of MCP, the motivations and benefits of integrating it with uAgents, and the main integration patterns used in practice.

What is MCP?[â€‹](#what-is-mcp "Direct link to What is MCP?")
-----------------------------------------------------------

*   **Standardization:** MCP defines a unified protocol for tool and service access, eliminating the need for custom code for each integration.
*   **Dynamic Tool Discovery:** Agents can dynamically discover and call tools at runtime, enabling flexible and extensible workflows.
*   **Transport Methods:** MCP supports various communication transports, such as stdio, SSE and even custom transports making it adaptable to different environments.
*   **Type Safety:** Tool schemas ensure that input and output data are validated and structured.

Why Integrate MCP with uAgents?[â€‹](#why-integrate-mcp-with-uagents "Direct link to Why Integrate MCP with uAgents?")
--------------------------------------------------------------------------------------------------------------------

### Motivation & Benefits[â€‹](#motivation--benefits "Direct link to Motivation & Benefits")

*   **Standardized Access to External Capabilities:** MCP provides a schema-based, open protocol for exposing tools and APIs to agents, eliminating the need for custom integration logic for each new service. uAgents are lightweight, autonomous agents with built-in identity, messaging, and wallet support, making them ideal for decentralized, composable agent networks.
    
*   **Dynamic Discovery and Orchestration:** By registering MCP-enabled agents on Agentverse, they become discoverable by the ASI:One LLM and other agents, enabling dynamic orchestration and tool selection based on user queries. This allows for seamless, real-time collaboration between users, LLMs, and specialized agents.
    
*   **Plug-and-Play Extensibility:** New capabilities (e.g., medical calculators, travel APIs, weather data, research databases) can be added simply by connecting new MCP serversâ€”no need to rewrite agent logic. Agents can call each other's MCP-exposed tools, fostering a network of shared capabilities.
    
*   **Ecosystem Growth and Modularity:** As more MCP servers and tools are published (e.g., via Smithery.ai), the agent ecosystem grows richer and more versatile. Modular design means agents can be composed, reused, and extended for new domains and workflows.
    

Typical Use Cases[â€‹](#typical-use-cases "Direct link to Typical Use Cases")
---------------------------------------------------------------------------

*   Connecting research agents to medical databases (e.g., PubMed, clinical trials)
*   Enabling travel assistants to access real-time listings (e.g., Airbnb)
*   Allowing productivity agents to interact with calendars, emails, or web search

Main Integration Approaches[â€‹](#main-integration-approaches "Direct link to Main Integration Approaches")
---------------------------------------------------------------------------------------------------------

Currently, there are two primary ways to integrate MCP with uAgents and make them discoverable and callable from ASI:One LLM via Agentverse:

### 1\. LangGraph Agent with MCP Adapter[â€‹](#1-langgraph-agent-with-mcp-adapter "Direct link to 1. LangGraph Agent with MCP Adapter")

*   **How it works:**
    
    *   A LangGraph agent is created and uses the [`langchain_mcp_adapters`](https://github.com/langchain-ai/langchain-mcp-adapters) to connect to one or more MCP servers, enabling tool access within the agent's workflow.
    *   The LangGraph agent is then wrapped using the [`uagents_adapter`](https://github.com/fetchai/uAgents/tree/main/python/uagents-adapter), making it a uAgent that can be registered on Agentverse (AV).
    *   Once registered, the agent becomes discoverable and callable by ASI:One LLM, allowing natural language queries to trigger MCP tool calls through the agent.
*   **Use case:**
    
    *   Ideal for scenarios where you want to leverage LangGraph's workflow/stategraph capabilities and expose those as agentic services in the Fetch.ai ecosystem.

### 2\. Connect an Agent to Multiple Remote MCP Servers[â€‹](#2-connect-an-agent-to-multiple-remote-mcp-servers "Direct link to 2. Connect an Agent to Multiple Remote MCP Servers")

*   **How it works:**
    *   A uAgent client is configured to connect directly to remote MCP servers, such as those hosted on [Smithery.ai](https://smithery.ai/) (e.g., PubMed, clinical trials, calculators, etc.).
    *   The uAgent acts as a bridge, forwarding requests to the appropriate MCP server and returning results.
    *   This uAgent is then registered on Agentverse, making its capabilities available to ASI:One LLM and other agents.
*   **Use case:**
    *   Useful for quickly exposing existing remote MCP services to the agent ecosystem without custom agent logic.

### 3\. Create MCP Server on Agentverse[â€‹](#3-create-mcp-server-on-agentverse "Direct link to 3. Create MCP Server on Agentverse")

*   **How it works:**
    
    *   A FastMCP server is implemented in Python, exposing tools (functions) using the MCP protocol.
    *   The [`MCPServerAdapter`](https://github.com/fetchai/uAgents/tree/main/python/uagents-adapter) from the `uagents-adapter` package is used to wrap the FastMCP server as a uAgent.
    *   The uAgent is then registered on Agentverse, making all MCP tools discoverable and callable by ASI:One LLM and other agents.
    *   The adapter leverages ASI:One LLM for intelligent tool selection and natural language interaction.
*   **Use case:**
    
    *   Ideal for developers who want to quickly deploy Python-based FastMCP Servers that expose tools or services (e.g., weather APIs, calculators, custom business logic) to the agent ecosystem with minimal integration effort.

For practical implementations and code examples of MCP integration with uAgents, please refer to the following local examples:

*   [LangGraph MCP Agent Example](https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/mcp-integration/langgraph-mcp-agent-example) â€” Basic LangGraph agent integration with MCP.
*   [Multi-Server MCP Agent Examples](https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/mcp-integration/multi-server-agent-example) â€” How to build LangGraph agents (both basic and state graph) that connect to multiple MCP servers and register as uAgents.
*   [Connect an Agent to Multiple Remote MCP Servers](https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/mcp-integration/connect-an-agent-to-multiple-remote-mcp-servers) â€” How to connect a uAgent to remote MCP servers (e.g., PubMed, MedCalc, etc.) using Smithery.ai.
*   [Create MCP Server on Agentverse Example](https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/mcp-integration/mcp-adapter-example) â€” How to deploy a FastMCP server as a uAgent using MCPServerAdapter.</content>
</page>

<page>
  <title>AI Agents Reshaping the On-Chain Ecosystem | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/on-chain-examples/on-chain-agents</url>
  <content>Version: 1.0.3

Introduction[â€‹](#introduction "Direct link to Introduction")
------------------------------------------------------------

AI agentsâ€”particularly Fetch.aiâ€™s **uAgents** and **SDK agents**â€”are autonomous participants in blockchain networks that respond to **on-chain events**, **manage digital assets**, and **facilitate multi-chain interactions** . By removing the need for centralized intermediaries or continuous human oversight, these agents transform standard on-chain flows into dynamic, intelligent services.

Why AI Agents Matter On-Chain[â€‹](#why-ai-agents-matter-on-chain "Direct link to Why AI Agents Matter On-Chain")
---------------------------------------------------------------------------------------------------------------

1.  Autonomous Execution
    
    *   AI agents listen for on-chain triggers (e.g., token transfers, contract calls, governance proposals).
    *   Once triggered, agents can execute trades, re-balance liquidity, or vote on proposals in real-timeâ€”without waiting for human confirmation.
2.  Enhanced Security & Trustlessness
    
    *   Because all agent actions (like escrow setup, fund releases, or cross-chain swaps) occur on-chain, everything is **transparent** and **auditable**.
    *   Agents follow **immutable** contract rules, ensuring no single party can hijack the process.
3.  Chain- & Wallet-Agnostic
    
    *   **Chain-Agnostic** : AI agents can run across multiple blockchainsâ€”Ethereum, Binance Smart Chain, Cosmos-based networks, or any EVM-compatible chain. They can coordinate cross-chain actions in a unified manner.
    *   **Wallet-Agnostic** : Users arenâ€™t tied to a specific wallet solution (e.g., MetaMask, Keplr, Phantom). Agents interface with any wallet, enabling broad user adoption and frictionless onboarding.
4.  Flexibility & Extensibility
    
    *   Developers can tailor an agentâ€™s logic to handle **DeFi strategies, governance proposals, NFT auctions,** or **P2P trades**.
    *   Agents can integrate external oracles, off-chain data APIs, and bridging protocols, evolving beyond single-blockchain constraints.
5.  Scalable, Efficient Workflows
    
    *   By delegating tasks to AI agents, dApps reduce the time users spend **manually** approving each transaction and gain a more **seamless** on-chain experience.
    *   Agents can auto-manage tasks like yield farming reinvestment or on-chain voting schedules.

How uAgents make it happen?[â€‹](#how-uagents-make-it-happen "Direct link to How uAgents make it happen?")
--------------------------------------------------------------------------------------------------------

1.  Registration via Almanac Contract (Cosmos-Based)
    
    *   **Cosmos-Based Registration:** `uAgents` register themselves on a Cosmos-based chain (using Almanac contract smart contract).
    *   **Immutable Proof of Existence:** This on-chain registration provides each agent with a **unique**, **verifiable** identity(address), ensuring that any other uAgents, AI Agents or service or dApps can confirm its authenticity before interacting.
    *   **Decentralized Discovery:** Other **uAgents** or **AI Agents** can query the Almanac contract to **discover** available agents, making it straightforward to integrate new agents into existing ecosystems.
2.  Wallet-Agnostic Architecture
    
    *   **Fetch (ASI Alliance) Wallet Integration**: uAgents come pre-integrated with **Fetch.ai (ASI Alliance) wallets**, but can also work seamlessly with other Web3 wallets like MetaMask, Keplr, or Ledger-based solutions.
    *   **Broad User Adoption:** Because thereâ€™s no hard coupling to a single wallet, end-users donâ€™t have to switch providers or manage multiple accountsâ€”**drastically reducing friction**.
    *   **Secret Management:** Agents handle private keys or other secrets in a secure environment , further minimizing the risk of manual errors or security breaches.
3.  Chain-Agnostic Operations
    
    *   **Multi-Chain Compatibility:** Once registered, an `uAgent` can **monitor** and act on **multiple blockchains**â€”Ethereum, Binance Smart Chain, Cosmos, Polkadot, and more.
    *   **Event Listening & Execution:** The agent listens for events (e.g., escrow creation, governance proposal) on any chain it supports. Once triggered, it can respond with an on-chain transaction in that same environment.
    *   **Unified Coordination:** For cross-chain workflows, an agent can coordinate matching orders or bridging logic across networks, which is critical for advanced DeFi or NFT scenarios.
4.  Identifiable by Address or AName
    
    *   **Unique Agent Address:** Each uAgent is identifiable by a standard address (agent1.....) which can be used to communicate to other agents on Fetch.ai blockchain.
    *   **AName (Human-Readable Handle):** The Fetch.ai Name Service Smart Contract can also map a human-friendly name (AName) to an agentâ€™s address.
5.  Extensibility & Custom Logic
    
    *   **Plugin System:** Developers can embed specialized routinesâ€”DeFi yield strategies, NFT auction logic, commodity order matchingâ€”within the agent.
    *   **Off-Chain Integration:** Because each agent can also pull data from oracles or external APIs (e.g., price feeds, weather data), itâ€™s easy to create autonomous, data-driven behaviors that integrate seamlessly with on-chain contracts.</content>
</page>

<page>
  <title>Foundation Core Concepts | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.2/concepts-ai-agents/foundation-core-concepts</url>
  <content>Version: 1.0.2

### Introduction to AI Agents[â€‹](#introduction-to-ai-agents "Direct link to Introduction to AI Agents")

In the rapidly evolving landscape of artificial intelligence, AI agents represent a significant leap beyond conventional software applications. At their core, agents are software entities designed to perform autonomous actions by:

*   **Observing** their environment
*   Making **decisions** based on observations
*   Taking **actions** to achieve specific goals
*   **Learning** from outcomes

### The Evolution of AI Systems[â€‹](#the-evolution-of-ai-systems "Direct link to The Evolution of AI Systems")

AI technologies have progressed significantly. Understanding the stages helps clarify how agents go beyond traditional software:

1.  **Traditional Applications**
    
    *   Rely on predefined rules or fixed logic.
    *   Perform predictable tasks with minimal flexibility.
    *   **Example**: Basic command-line programs or simple scripts.
2.  **AI-Enhanced Applications**
    
    *   Use machine learning or large language models (LLMs) to provide better responses.
    *   Still mostly follow predefined flows, with small amounts of context awareness.
    *   **Example**: Chatbots that generate dynamic replies but canâ€™t independently plan multi-step processes.
3.  **Agentic Systems**
    
    *   Operate with a high degree of autonomy, adapting to changing conditions.
    *   Can create and refine their own plans over multiple steps.
    *   **Example**: A research bot that iteratively retrieves information, analyzes it, and updates its plan to find the best solution.

### Understanding System Types[â€‹](#understanding-system-types "Direct link to Understanding System Types")

Modern AI implementations generally fall into three categories:

1.  **Automations**
    
    *   Execute predefined, rule-based tasks
        
    *   Follow fixed decision trees, best for deterministic tasks
        
    *   Best for: Repetitive, well-defined tasks
        
    *   **Example**:
        
            def process_request(type):      if type == "email":         send_email()      elif type == "report":         generate_report()
        
2.  **AI Workflows**
    
    *   Combine LLMs with predefined processes
        
    *   Follow structured but flexible paths
        
    *   Best for: Complex but predictable tasks
        
    *   **Example**:
        
            async def handle_customer_query(query):      intent = await llm.classify_intent(query)      response = await llm.generate_response(intent)      if need_human_review(response):         return escalate_to_human()      return response
        
3.  **True AI Agents**
    
    *   Dynamically direct their own processes
        
    *   Maintain control over task execution
        
    *   Best for: Open-ended, adaptive tasks
        
    *   **Example**:
        
            class ResearchAgent:      async def solve_problem(self, query):         plan = await self.create_plan(query)         while not self.goal_achieved():            next_step = self.determine_next_step(plan)            result = await self.execute_step(next_step)            plan = self.adjust_plan(result)         return self.compile_results()
        

### Core Components of an Agent[â€‹](#core-components-of-an-agent "Direct link to Core Components of an Agent")

A generic agent architecture, consists of several key components:

1.  **Model Layer**
    
    *   Central decision-making engine
    *   Processes input and context
    *   Generates reasoning and plans
2.  **Orchestration Layer**
    
    *   Manages the execution flow
    *   Coordinates tool usage
    *   Monitors progress towards goals
3.  **Memory System**
    
    *   Short-term working memory
    *   Long-term knowledge storage
    *   Context retention
4.  **Tool Integration**
    
    *   External API connections
    *   Data processing capabilities
    *   Action execution interfaces

The ReAct Framework[â€‹](#the-react-framework "Direct link to The ReAct Framework")
---------------------------------------------------------------------------------

AI agents typically operate using the ReAct (Reason-Action) framework, which follows a cyclical pattern:

#### Framework Components:[â€‹](#framework-components "Direct link to Framework Components:")

1.  **Thought**
    
    *   Internal reasoning about the current state
    *   Analysis of available information
    *   Planning next steps
2.  **Action**
    
    *   Execution of chosen steps
    *   Tool usage and integration
    *   External system interaction
3.  **Observation**
    
    *   Gathering results
    *   Analyzing outcomes
    *   Updating understandingd</content>
</page>

<page>
  <title>uAgent Creation | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.2/agent-creation/uagent-creation</url>
  <content>Build and understand uAgents
----------------------------

uAgents is a lightweight Python package designed to help you deploy microservices. These microservices can then be utilized by your AI agents as tools for executing tasks and achieving defined objectives.

Installing uAgents framework[â€‹](#installing-uagents-framework "Direct link to Installing uAgents framework")
------------------------------------------------------------------------------------------------------------

Fetch.ai's [uAgents Framework](https://pypi.org/project/uagents/) package is a Python library running on Ubuntu/Debian, macOS, and Windows systems.

On your computer, you may need to install:

*   [Python 3.8+](https://www.python.org/downloads/)
*   [PIP](https://pypi.org/project/pip/) - Python package manager.
*   [uAgents](https://pypi.org/project/uagents/) library

### Install with Pip[â€‹](#install-with-pip "Direct link to Install with Pip")

1.  Create a directory :

    mkdir my_agents_projectcd my_agents_project

2.  Initialize and activate a virtual environment:

3.  Install Fetch.ai uagents library:

4.  Verify the installation:

Create your first uAgent[â€‹](#create-your-first-uagent "Direct link to Create your first uAgent")
------------------------------------------------------------------------------------------------

Once you've installed the uAgents library, it's quite simple to get a minimal use case running.

### The uAgent[â€‹](#the-uagent "Direct link to The uAgent")

1.  Create a Python script:

2.  Import the necessary classes and instantiate your agent:

    from uagents import Agent, Context# instantiate agentagent = Agent(    name="alice",    seed="secret_seed_phrase",    port=8000,    endpoint=["http://localhost:8000/submit"])# startup handler@agent.on_event("startup")async def startup_function(ctx: Context):    ctx.logger.info(f"Hello, I'm agent {agent.name} and my address is {agent.address}.")if __name__ == "__main__":    agent.run()

*   **Agent parameters**:
    
    *   `name`: Identifies the agent (here, â€œaliceâ€).
    *   `seed`: Sets a deterministic seed, generating fixed addresses each time.
    *   `port` and `endpoint`: Configure where the agent will be available.
*   **Behavior on startup**:
    
    The `@agent.on_event("startup")` decorator sets a function that runs as soon as the agent launches. In this sample, the agent logs a message including its name and unique address.
    

### Run your agent[â€‹](#run-your-agent "Direct link to Run your agent")

With your virtual environment activated, run the script:

#### Sample output[â€‹](#sample-output "Direct link to Sample output")

    INFO:     [alice]: Registration on Almanac API successfulINFO:     [alice]: Registering on almanac contract...INFO:     [alice]: Registering on almanac contract...completeINFO:     [alice]: Agent inspector available at https://Agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8000&address=agent1q...INFO:     [alice]: Starting server on http://0.0.0.0:8000 (Press CTRL+C to quit)INFO:     [alice]: Hello, I'm agent alice and my address is agent1q...

Ways to create uAgents[â€‹](#ways-to-create-uagents "Direct link to Ways to create uAgents")
------------------------------------------------------------------------------------------

There are three main ways to create and deploy uAgents, each suited to different needs:

1.  Hosted Agents
2.  Local Agents
3.  Mailbox Agents

Understanding these options will help you choose the best setup.

### Hosted Agents[â€‹](#hosted-agents "Direct link to Hosted Agents")

You can create and host agents directly on [Agentverse](https://agentverse.ai/):

1.  Navigate to Agentverse â†’ Agents tab â†’ + New Agent.

2.  Choose Blank Agent or Skeleton Agent.
    
    *   From a Blank Agent - You have to code everything.
    *   From a Skeleton Agent - You will get one data model with one decorator each.
    
    choose **Blank Agent**.
    

3.  Provide a name for your new Agent.

4.  After creation, click on the **agent** and then **Build** tab to open the embedded code editor.

5.  Add your Python code (similar to the first\_agent.py example).

6.  Click **Start** to run the agent; logs appear in the **terminal** below the editor.

note

**Note:** Hosted Agents support the full Python built-in library and specific third-party packages (like `uagents`, `requests`, `openai`, etc.). However, some libraries are restricted for security reasons. If you need additional packages, consider using **Mailbox Agents**.

#### Supported Libraries on Agentverse:[â€‹](#supported-libraries-on-agentverse "Direct link to Supported Libraries on Agentverse:")

The Agentverse now provides full Python support! This means that all Hosted Agents will now support the full Python built-in library plus the following packages:

uagents

requests

cosmpy

pydantic

uagents-ai-engine

MySQLdb

pymongo

bs64

faiss-cpu

fetchai-babble

google-generativeai

langchain-anthropic

langchain-community

langchain-core

langchain-google-genai

langchain-google-vertexai

langchain-openai

langchain-text-splitters

langchain

nltk

openai

tenacity

unstructured

validators

Once you run an hosted agent, you don't have to bother about it's uptime. It will be always running.

### Local Agents[â€‹](#local-agents "Direct link to Local Agents")

Local Agents run entirely on your own machine or server, just like the example in `my_first_agent.py`. These agents:

*   Have complete freedom to import any Python library or custom modules.
*   Can handle events, messages, and tasks continuously.
*   Are registered on the Almanac contract, allowing them to communicate with other local agents.
*   Require you to manage uptime, environment dependencies, and scaling if necessary.

note

**Use Case:** Ideal for tasks requiring advanced customization, local file access, or extensive machine learning libraries.

### Mailbox Agents[â€‹](#mailbox-agents "Direct link to Mailbox Agents")

When you need to use libraries not allowed by the hosted environment, or you want direct local control while also integrating with Agentverse, you can set up a Mailbox Agent.

A Mailbox Agent runs locally but connects to the Agentverse via a secure channel, enabling interaction with other hosted or local agents. To configure this:

1.  Lets setup a local agent first like we did in the section [here](#the-uagent) but include `mailbox=True`.

mailbox\_agent.py

    from uagents import Agent, Context, Model class Message(Model):    message: str SEED_PHRASE = "put_your_seed_phrase_here" # Now your agent is ready to join the Agentverse!agent = Agent(    name="alice",    port=8000,    mailbox=True        ) # Copy the address shown belowprint(f"Your agent's address is: {agent.address}") if __name__ == "__main__":    agent.run()

2.  Run the Script

You should get something similar within your terminal output:

    INFO:     [Alice]: Starting agent with address: agent1qw8jn3nfl2fyyhe7v4x8pfmsge4hs9zqrqw9eq7h7hluzmd0da8z7j0uacxINFO:     [Alice]: Agent inspector available at https://Agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8000&address=agent1q0nrj45ah0e53424n9uqc83d9xxs6534jug7j6ka4z6wnrsx7ex2kwx86t4INFO:     [Alice]: Starting server on http://0.0.0.0:8002 (Press CTRL+C to quit)INFO:     [Alice]: Starting mailbox client for https://Agentverse.aiINFO:     [Alice]: Mailbox access token acquiredINFO:     [Alice]: Registration on Almanac API successfulINFO:     [Alice]: Registering on almanac contract...INFO:     [Alice]: Registering on almanac contract...complete

#### Create a Mailbox in Agentverse[â€‹](#create-a-mailbox-in-agentverse "Direct link to Create a Mailbox in Agentverse")

Now that we defined our local Agent and have successfully run it, we can go on and connect it to [Agentverse](https://agentverse.ai/) via a Mailbox. To do so, make sure your Agent is running. Then, click on the **Local Agent Inspector** URL provided in your terminal output. You will be redirected towards the Inspector UI and will be able to see multiple details about this local Agent.

Here, click the **Connect** button.

You will be presented with 3 different choices: Mailbox, Proxy and Custom. Select Mailbox.

You will then see some code details available for the Agent. You do not need to do anything, just click on Finish.

You can see the agent details in the local agents and can try connecting it with other Agentverse agents using the `on_message` handler.</content>
</page>

<page>
  <title>uAgent to uAgent Communication | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.2/agent-communication/uagent-uagent-communication</url>
  <content>Version: 1.0.2

In the uAgents Framework, agents can be triggered in multiple ways using different types of handlers. These handlers act as decorators, enabling agents to execute specific functions based on predefined conditions. In this section, we will explore two handlers. Below are the available handlers which can be used with uAgents:

*   on\_event()
*   on\_message()

on\_event[â€‹](#on_event "Direct link to on_event")
-------------------------------------------------

Agents can respond to events such as initialization and termination. The startup and shutdown handlers are used by the uAgents library to manage these events, ensuring that specific actions are executed when an agent starts or stops.

### on\_event("startup")[â€‹](#on_eventstartup "Direct link to on_event(\"startup\")")

    @agent.on_event("startup")async def introduce_agent(ctx: Context):    ctx.logger.info(f"Hello, I'm agent {agent.name} and my address is {agent.address}.")    ...

### on\_event("shutdown")[â€‹](#on_eventshutdown "Direct link to on_event(\"shutdown\")")

    @agent.on_event("shutdown")async def introduce_agent(ctx: Context):    ctx.logger.info(f"Hello, I'm agent {agent.name} and I am shutting down")    ...

on\_message()[â€‹](#on_message "Direct link to on_message()")
-----------------------------------------------------------

In this section, we will explore the on\_message() decorator, which allows us to send messages between microservice agents in a structured way. We will create two microservice agents and enable them to communicate with each other.

Let's create agent1 who will send a message to agent2 on startup.

### uAgent1 Script[â€‹](#uagent1-script "Direct link to uAgent1 Script")

Please remember to have the **uagents** package installed in the terminal in order to create and run uAgents.

uagent1.py

    from uagents import Agent, Context, Model# Data model (envolope) which you want to send from one agent to anotherclass Message(Model):    message : str    field : intmy_first_agent = Agent(    name = 'My First Agent',    port = 5050,    endpoint = ['http://localhost:5050/submit'])second_agent = 'your_second_agent_address'@my_first_agent.on_event('startup')async def startup_handler(ctx : Context):    ctx.logger.info(f'My name is {ctx.agent.name} and my address  is {ctx.agent.address}')    await ctx.send(second_agent, Message(message = 'Hi Second Agent, this is the first agent.'))if __name__ == "__main__":    my_first_agent.run()

### uAgent2 Script[â€‹](#uagent2-script "Direct link to uAgent2 Script")

uagent2.py

    from uagents import Agent, Context, Modelclass Message(Model):    message : strmy_second_agent = Agent(    name = 'My Second Agent',    port = 5051,    endpoint = ['http://localhost:5051/submit'])@my_second_agent.on_event('startup')async def startup_handler(ctx : Context):    ctx.logger.info(f'My name is {ctx.agent.name} and my address  is {ctx.agent.address}')@my_second_agent.on_message(model = Message)async def message_handler(ctx: Context, sender : str, msg: Message):    ctx.logger.info(f'I have received a message from {sender}.')    ctx.logger.info(f'I have received a message {msg.message}.')if __name__ == "__main__":    my_second_agent.run()

We need to define the data Model which is `class Message` in both the sender and receiver agents. The receiving agent must include an `on_message` handler that correctly implements this data model. It is crucial to ensure that both agents use the exact same data model for seamless communication.

### Running agents[â€‹](#running-agents "Direct link to Running agents")

Now that we have created both agents, let's run them in separate terminals. Agent 2 is on the receiver's end so we have to start agent2 first. Whenever agent1 is started it will send message to agent 2 and it will receive and handle the message.

#### Terminal 1[â€‹](#terminal-1 "Direct link to Terminal 1")

#### Terminal 2[â€‹](#terminal-2 "Direct link to Terminal 2")

### Expected Output[â€‹](#expected-output "Direct link to Expected Output")

    abhi@Fetchs-MacBook-Pro testing % python3 second_agent.py INFO:     [My Second Agent]: Starting agent with address: agent1qtgc4vqn4ehh88hct0umnnqeg36m5722hc4e63lwy573kjtqee7qg5afmapINFO:     [My Second Agent]: My name is My Second Agent and my address  is agent1qtgc4vqn4ehh88hct0umnnqeg36m5722hc4e63lwy573kjtqee7qg5afmapINFO:     [My Second Agent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A5051&address=agent1qtgc4vqn4ehh88hct0umnnqeg36m5722hc4e63lwy573kjtqee7qg5afmapINFO:     [My Second Agent]: Starting server on http://0.0.0.0:5051 (Press CTRL+C to quit)INFO:     [My Second Agent]: Registration on Almanac API successfulINFO:     [My Second Agent]: Almanac contract registration is up to date!INFO:     [My Second Agent]: I have received a message from agent1q0kpqwd5q7akt9utnw540h293zhw063ua90m66rkx5lg98wq4zrjkhmgwd7.INFO:     [My Second Agent]: I have received a message Hi Second Agent, this is the first agent..

    abhi@Fetchs-MacBook-Pro testing % python3 first_agent.pyINFO:     [My First Agent]: Starting agent with address: agent1q0kpqwd5q7akt9utnw540h293zhw063ua90m66rkx5lg98wq4zrjkhmgwd7INFO:     [My First Agent]: My name is My First Agent and my address  is agent1q0kpqwd5q7akt9utnw540h293zhw063ua90m66rkx5lg98wq4zrjkhmgwd7INFO:     [My First Agent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A5050&address=agent1q0kpqwd5q7akt9utnw540h293zhw063ua90m66rkx5lg98wq4zrjkhmgwd7INFO:     [My First Agent]: Starting server on http://0.0.0.0:5050 (Press CTRL+C to quit)INFO:     [My First Agent]: Registration on Almanac API successfulINFO:     [My First Agent]: Almanac contract registration is up to date!

In this way we can establish a communicate between two uAgents.</content>
</page>

<page>
  <title>Agentverse | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.2/agentverse/</url>
  <content>Agentverse Overview
-------------------

The [**Agentverse**](https://agentverse.ai/) is a cloud-based platform for creating and hosting autonomous Agents without the hassle of managing infrastructure. It provides an integrated development environment (IDE) where you can write, edit, and run Agent code directly in your browser. The [**Agentverse**](https://agentverse.ai/) offers:

*   **Continuous Uptime**: Once you deploy your Agent, it remains online and responsiveâ€”no manual restart or extra hosting steps needed.
*   **Easy Deployment**: Predefined templates and an in-browser editor help you get started quickly, regardless of your coding background.
*   **Blockchain Integration**: Agents have their own wallets for sending/receiving tokens, querying balances, and interacting with on-chain contracts.
*   **Marketplace & Discovery**: Hosted Agents are registered in the **Almanac**, making them discoverable by other Agents or users searching for specific functions.
*   **Mailroom Service**: Allows Agents to receive messages even when offline, retrieving them once they come back online.

Key Benefits[â€‹](#key-benefits "Direct link to Key Benefits")
------------------------------------------------------------

1.  **All-In-One Environment**
    
    *   Integrated code editor, logging console, and file management.
    *   No external servers or manual container setups.
2.  **Framework Agnostic Development**
    
    *   Build your agent with any framework of choice and make it discoverable by any other agent.
    *   Seamlessly register and integrate with Agentverse.
3.  **Scalability & Reliability**
    
    *   Dynamically scales with your Agent's message volume.
    *   High uptime ensures Agents are always available to process requests.
4.  **Secure & Trustless**
    
    *   Blockchain-based identity and registration provide transparency.
    *   Agents operate in isolated Python environments, protecting your code.
5.  **Varied Use Cases**
    
    *   From simple "Hello World" scripts to advanced AI-driven Agents.
    *   Supports Python libraries (uAgents, requests, openai, etc.) for robust functionality.
6.  **User-Friendly Hosting**
    
    *   Deploy Agents with a few clicks; no advanced DevOps skills needed.
    *   Easily manage your Agents' code revisions and logs in one place.

* * *

### Learn More[â€‹](#learn-more "Direct link to Learn More")

*   **Agentverse: Mailroom** â€“ Set up mailboxes for offline Agents.
*   **Agentverse: Marketplace** â€“ Discover public Agents and explore their capabilities.
*   **Agentverse API** â€“ Integrate and automate advanced features programmatically.

With the **Agentverse**, you can focus on Agent logic and features, knowing that security, uptime, and scalability are handled for you. Get started today by creating a new Agent and unleashing the power of automated, decentralized intelligence!</content>
</page>

<page>
  <title>AI Agent Creation | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/agent-creation/sdk-creation</url>
  <content>Creation and Registration of AI Agent using SDK
-----------------------------------------------

Why SDK Agents?[â€‹](#why-sdk-agents "Direct link to Why SDK Agents?")
--------------------------------------------------------------------

While the uAgents framework is great for basic microservices, the Fetch.ai SDK provides enhanced capabilities for building AI agents with greater flexibility and adaptability. Key differences include:

*   **Dynamic Messaging**: Processes flexible or unstructured data, unlike uAgents that require strict data models.
    
*   **Flexible Handlers**: Relies on Flask endpoints instead of predefined handlers, enabling more scalable and customizable communication.
    

These features are especially helpful if your AI agent needs advanced reasoning or can benefit from rapid decision-making and adaptability.

Installing fetchai SDK[â€‹](#installing-fetchai-sdk "Direct link to Installing fetchai SDK")
------------------------------------------------------------------------------------------

Fetch.ai's [SDK](https://pypi.org/project/fetchai/) package is a Python library running on Ubuntu/Debian, macOS, and Windows systems.

On your computer, you may need to install:

*   [Python 3.8+](https://www.python.org/downloads/)
*   [PIP](https://pypi.org/project/pip/) - the Python package manager.
*   [fetchai](https://pypi.org/project/fetchai/) library

### Install with Pip[â€‹](#install-with-pip "Direct link to Install with Pip")

*   Install Fetch.ai uagents library:

*   Check if installation was successful:

### Creating and Registering an AI Agent[â€‹](#creating-and-registering-an-ai-agent "Direct link to Creating and Registering an AI Agent")

In this example, you will create an AI agent identity and register it on Agentverse.

1.  **Create a python script** named `register_agent.py`.

2.  **Add the following code** to `register_agent.py`.

register\_ai\_agent.py

    import osfrom uagents_core.crypto import Identityfrom fetchai.registration import register_with_agentverse# Store your Agentverse API Key in the environment variables. AGENTVERSE_KEY = os.getenv("AGENTVERSE_KEY")# Your Agent's unique key for generating an address on Agentverseai_identity = Identity.from_seed(os.getenv("AGENT_SECRET_KEY"), 0)# Give your Agent a name. This allows you to easily identify one# of your Agents from other agents on Agentverse.name = "My AI's Name"# This is how you optimize your Agent's search engine performancereadme = """![domain:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)domain:domain-of-your-agent<description>My AI's description of capabilities and offerings</description><use_cases>    <use_case>An example of one of your AI's use cases.</use_case></use_cases><payload_requirements><description>The requirements your AI has for requests</description><payload>    <requirement>        <parameter>question</parameter>        <description>The question that you would like this AI work with you to solve</description>    </requirement></payload></payload_requirements>"""# The webhook that your AI receives messages on.ai_webhook = "https://api.sampleurl.com/webhook"register_with_agentverse(    ai_identity,    ai_webhook,    AGENTVERSE_KEY,    name,    readme,)

3.  Set up environment variables in a `.env` file (or similar approach):

    AGENTVERSE_KEY='YOUR_AGENTVERSE_API_KEY'AGENT_SECRET_KEY='YOUR_RANDOM_SECRET_SEED'

*   The `AGENTVERSE_KEY` is obtained from your Agentverse account.
*   The `AGENT_SECRET_KEY` should be a unique, random phrase to ensure your agent consistently generates the same address.

4.  Run the Script

    python3 regsiter_agent.py

*   **Sample Output:**

    INFO:fetchai:Registering with Almanac APIINFO:fetchai:Successfully registered as custom agent in AgentverseINFO:fetchai:Completed registering agent with Agentverse

### Checking Your Agent on Agentverse[â€‹](#checking-your-agent-on-agentverse "Direct link to Checking Your Agent on Agentverse")

1.  **Go to the Agents tab** in the Agentverse interface.
    
2.  **Select â€œLocal Agentsâ€** and look for the name used in your script (e.g., â€œMy AIâ€™s Nameâ€).
    

3.  **Click on the agent name** to view its details (address, registration data, etc.).

You have successfully created and registered an AI agent with the Fetch.ai SDK. You can now integrate this agent into your applications, allowing it to interact with other agents or services via Agentverse.

#### Next Steps:[â€‹](#next-steps "Direct link to Next Steps:")

*   Further develop your agentâ€™s code and logic.
*   Integrate custom Flask endpoints to handle advanced communication.
*   Explore additional SDK features (transaction handling, smart contracts, etc.) to expand the agentâ€™s capabilities.</content>
</page>

<page>
  <title>AI Agents Reshaping the On-Chain Ecosystem | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.2/examples/on-chain-examples/on-chain-agents</url>
  <content>Version: 1.0.2

Introduction[â€‹](#introduction "Direct link to Introduction")
------------------------------------------------------------

AI agentsâ€”particularly Fetch.aiâ€™s **uAgents** and **SDK agents**â€”are autonomous participants in blockchain networks that respond to **on-chain events**, **manage digital assets**, and **facilitate multi-chain interactions** . By removing the need for centralized intermediaries or continuous human oversight, these agents transform standard on-chain flows into dynamic, intelligent services.

Why AI Agents Matter On-Chain[â€‹](#why-ai-agents-matter-on-chain "Direct link to Why AI Agents Matter On-Chain")
---------------------------------------------------------------------------------------------------------------

1.  Autonomous Execution
    
    *   AI agents listen for on-chain triggers (e.g., token transfers, contract calls, governance proposals).
    *   Once triggered, agents can execute trades, re-balance liquidity, or vote on proposals in real-timeâ€”without waiting for human confirmation.
2.  Enhanced Security & Trustlessness
    
    *   Because all agent actions (like escrow setup, fund releases, or cross-chain swaps) occur on-chain, everything is **transparent** and **auditable**.
    *   Agents follow **immutable** contract rules, ensuring no single party can hijack the process.
3.  Chain- & Wallet-Agnostic
    
    *   **Chain-Agnostic** : AI agents can run across multiple blockchainsâ€”Ethereum, Binance Smart Chain, Cosmos-based networks, or any EVM-compatible chain. They can coordinate cross-chain actions in a unified manner.
    *   **Wallet-Agnostic** : Users arenâ€™t tied to a specific wallet solution (e.g., MetaMask, Keplr, Phantom). Agents interface with any wallet, enabling broad user adoption and frictionless onboarding.
4.  Flexibility & Extensibility
    
    *   Developers can tailor an agentâ€™s logic to handle **DeFi strategies, governance proposals, NFT auctions,** or **P2P trades**.
    *   Agents can integrate external oracles, off-chain data APIs, and bridging protocols, evolving beyond single-blockchain constraints.
5.  Scalable, Efficient Workflows
    
    *   By delegating tasks to AI agents, dApps reduce the time users spend **manually** approving each transaction and gain a more **seamless** on-chain experience.
    *   Agents can auto-manage tasks like yield farming reinvestment or on-chain voting schedules.

How uAgents make it happen?[â€‹](#how-uagents-make-it-happen "Direct link to How uAgents make it happen?")
--------------------------------------------------------------------------------------------------------

1.  Registration via Almanac Contract (Cosmos-Based)
    
    *   **Cosmos-Based Registration:** `uAgents` register themselves on a Cosmos-based chain (using Almanac contract smart contract).
    *   **Immutable Proof of Existence:** This on-chain registration provides each agent with a **unique**, **verifiable** identity(address), ensuring that any other uAgents, AI Agents or service or dApps can confirm its authenticity before interacting.
    *   **Decentralized Discovery:** Other **uAgents** or **AI Agents** can query the Almanac contract to **discover** available agents, making it straightforward to integrate new agents into existing ecosystems.
2.  Wallet-Agnostic Architecture
    
    *   **Fetch (ASI Alliance) Wallet Integration**: uAgents come pre-integrated with **Fetch.ai (ASI Alliance) wallets**, but can also work seamlessly with other Web3 wallets like MetaMask, Keplr, or Ledger-based solutions.
    *   **Broad User Adoption:** Because thereâ€™s no hard coupling to a single wallet, end-users donâ€™t have to switch providers or manage multiple accountsâ€”**drastically reducing friction**.
    *   **Secret Management:** Agents handle private keys or other secrets in a secure environment , further minimizing the risk of manual errors or security breaches.
3.  Chain-Agnostic Operations
    
    *   **Multi-Chain Compatibility:** Once registered, an `uAgent` can **monitor** and act on **multiple blockchains**â€”Ethereum, Binance Smart Chain, Cosmos, Polkadot, and more.
    *   **Event Listening & Execution:** The agent listens for events (e.g., escrow creation, governance proposal) on any chain it supports. Once triggered, it can respond with an on-chain transaction in that same environment.
    *   **Unified Coordination:** For cross-chain workflows, an agent can coordinate matching orders or bridging logic across networks, which is critical for advanced DeFi or NFT scenarios.
4.  Identifiable by Address or AName
    
    *   **Unique Agent Address:** Each uAgent is identifiable by a standard address (agent1.....) which can be used to communicate to other agents on Fetch.ai blockchain.
    *   **AName (Human-Readable Handle):** The Fetch.ai Name Service Smart Contract can also map a human-friendly name (AName) to an agentâ€™s address.
5.  Extensibility & Custom Logic
    
    *   **Plugin System:** Developers can embed specialized routinesâ€”DeFi yield strategies, NFT auction logic, commodity order matchingâ€”within the agent.
    *   **Off-Chain Integration:** Because each agent can also pull data from oracles or external APIs (e.g., price feeds, weather data), itâ€™s easy to create autonomous, data-driven behaviors that integrate seamlessly with on-chain contracts.</content>
</page>

<page>
  <title>Foundation Core Concepts | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.1/concepts-ai-agents/foundation-core-concepts</url>
  <content>Version: 1.0.1

### Introduction to AI Agents[â€‹](#introduction-to-ai-agents "Direct link to Introduction to AI Agents")

In the rapidly evolving landscape of artificial intelligence, AI agents represent a significant leap beyond conventional software applications. At their core, agents are software entities designed to perform autonomous actions by:

*   **Observing** their environment
*   Making **decisions** based on observations
*   Taking **actions** to achieve specific goals
*   **Learning** from outcomes

### The Evolution of AI Systems[â€‹](#the-evolution-of-ai-systems "Direct link to The Evolution of AI Systems")

AI technologies have progressed significantly. Understanding the stages helps clarify how agents go beyond traditional software:

1.  **Traditional Applications**
    
    *   Rely on predefined rules or fixed logic.
    *   Perform predictable tasks with minimal flexibility.
    *   **Example**: Basic command-line programs or simple scripts.
2.  **AI-Enhanced Applications**
    
    *   Use machine learning or large language models (LLMs) to provide better responses.
    *   Still mostly follow predefined flows, with small amounts of context awareness.
    *   **Example**: Chatbots that generate dynamic replies but canâ€™t independently plan multi-step processes.
3.  **Agentic Systems**
    
    *   Operate with a high degree of autonomy, adapting to changing conditions.
    *   Can create and refine their own plans over multiple steps.
    *   **Example**: A research bot that iteratively retrieves information, analyzes it, and updates its plan to find the best solution.

### Understanding System Types[â€‹](#understanding-system-types "Direct link to Understanding System Types")

Modern AI implementations generally fall into three categories:

1.  **Automations**
    
    *   Execute predefined, rule-based tasks
        
    *   Follow fixed decision trees, best for deterministic tasks
        
    *   Best for: Repetitive, well-defined tasks
        
    *   **Example**:
        
            def process_request(type):      if type == "email":         send_email()      elif type == "report":         generate_report()
        
2.  **AI Workflows**
    
    *   Combine LLMs with predefined processes
        
    *   Follow structured but flexible paths
        
    *   Best for: Complex but predictable tasks
        
    *   **Example**:
        
            async def handle_customer_query(query):      intent = await llm.classify_intent(query)      response = await llm.generate_response(intent)      if need_human_review(response):         return escalate_to_human()      return response
        
3.  **True AI Agents**
    
    *   Dynamically direct their own processes
        
    *   Maintain control over task execution
        
    *   Best for: Open-ended, adaptive tasks
        
    *   **Example**:
        
            class ResearchAgent:      async def solve_problem(self, query):         plan = await self.create_plan(query)         while not self.goal_achieved():            next_step = self.determine_next_step(plan)            result = await self.execute_step(next_step)            plan = self.adjust_plan(result)         return self.compile_results()
        

### Core Components of an Agent[â€‹](#core-components-of-an-agent "Direct link to Core Components of an Agent")

A generic agent architecture, consists of several key components:

1.  **Model Layer**
    
    *   Central decision-making engine
    *   Processes input and context
    *   Generates reasoning and plans
2.  **Orchestration Layer**
    
    *   Manages the execution flow
    *   Coordinates tool usage
    *   Monitors progress towards goals
3.  **Memory System**
    
    *   Short-term working memory
    *   Long-term knowledge storage
    *   Context retention
4.  **Tool Integration**
    
    *   External API connections
    *   Data processing capabilities
    *   Action execution interfaces

The ReAct Framework[â€‹](#the-react-framework "Direct link to The ReAct Framework")
---------------------------------------------------------------------------------

AI agents typically operate using the ReAct (Reason-Action) framework, which follows a cyclical pattern:

#### Framework Components:[â€‹](#framework-components "Direct link to Framework Components:")

1.  **Thought**
    
    *   Internal reasoning about the current state
    *   Analysis of available information
    *   Planning next steps
2.  **Action**
    
    *   Execution of chosen steps
    *   Tool usage and integration
    *   External system interaction
3.  **Observation**
    
    *   Gathering results
    *   Analyzing outcomes
    *   Updating understandingd</content>
</page>

<page>
  <title>uAgent Creation | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.1/agent-creation/uagent-creation</url>
  <content>Build and understand uAgents
----------------------------

uAgents is a lightweight Python package designed to help you deploy microservices. These microservices can then be utilized by your AI agents as tools for executing tasks and achieving defined objectives.

Installing uAgents framework[â€‹](#installing-uagents-framework "Direct link to Installing uAgents framework")
------------------------------------------------------------------------------------------------------------

Fetch.ai's [uAgents Framework](https://pypi.org/project/uagents/) package is a Python library running on Ubuntu/Debian, macOS, and Windows systems.

On your computer, you may need to install:

*   [Python 3.8+](https://www.python.org/downloads/)
*   [PIP](https://pypi.org/project/pip/) - Python package manager.
*   [uAgents](https://pypi.org/project/uagents/) library

### Install with Pip[â€‹](#install-with-pip "Direct link to Install with Pip")

1.  Create a directory :

    mkdir my_agents_projectcd my_agents_project

2.  Initialize and activate a virtual environment:

3.  Install Fetch.ai uagents library:

4.  Verify the installation:

Create your first uAgent[â€‹](#create-your-first-uagent "Direct link to Create your first uAgent")
------------------------------------------------------------------------------------------------

Once you've installed the uAgents library, it's quite simple to get a minimal use case running.

### The uAgent[â€‹](#the-uagent "Direct link to The uAgent")

1.  Create a Python script:

2.  Import the necessary classes and instantiate your agent:

    from uagents import Agent, Context# instantiate agentagent = Agent(    name="alice",    seed="secret_seed_phrase",    port=8000,    endpoint=["http://localhost:8000/submit"])# startup handler@agent.on_event("startup")async def startup_function(ctx: Context):    ctx.logger.info(f"Hello, I'm agent {agent.name} and my address is {agent.address}.")if __name__ == "__main__":    agent.run()

*   **Agent parameters**:
    
    *   `name`: Identifies the agent (here, â€œaliceâ€).
    *   `seed`: Sets a deterministic seed, generating fixed addresses each time.
    *   `port` and `endpoint`: Configure where the agent will be available.
*   **Behavior on startup**:
    
    The `@agent.on_event("startup")` decorator sets a function that runs as soon as the agent launches. In this sample, the agent logs a message including its name and unique address.
    

### Run your agent[â€‹](#run-your-agent "Direct link to Run your agent")

With your virtual environment activated, run the script:

#### Sample output[â€‹](#sample-output "Direct link to Sample output")

    INFO:     [alice]: Registration on Almanac API successfulINFO:     [alice]: Registering on almanac contract...INFO:     [alice]: Registering on almanac contract...completeINFO:     [alice]: Agent inspector available at https://Agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8000&address=agent1q...INFO:     [alice]: Starting server on http://0.0.0.0:8000 (Press CTRL+C to quit)INFO:     [alice]: Hello, I'm agent alice and my address is agent1q...

Ways to create uAgents[â€‹](#ways-to-create-uagents "Direct link to Ways to create uAgents")
------------------------------------------------------------------------------------------

There are three main ways to create and deploy uAgents, each suited to different needs:

1.  Hosted Agents
2.  Local Agents
3.  Mailbox Agents

Understanding these options will help you choose the best setup.

### Hosted Agents[â€‹](#hosted-agents "Direct link to Hosted Agents")

You can create and host agents directly on [Agentverse](https://agentverse.ai/):

1.  Navigate to Agentverse â†’ Agents tab â†’ + New Agent.

2.  Choose Blank Agent or Skeleton Agent.
    
    *   From a Blank Agent - You have to code everything.
    *   From a Skeleton Agent - You will get one data model with one decorator each.
    
    choose **Blank Agent**.
    

3.  Provide a name for your new Agent.

4.  After creation, click on the **agent** and then **Build** tab to open the embedded code editor.

5.  Add your Python code (similar to the first\_agent.py example).

6.  Click **Start** to run the agent; logs appear in the **terminal** below the editor.

note

**Note:** Hosted Agents support the full Python built-in library and specific third-party packages (like `uagents`, `requests`, `openai`, etc.). However, some libraries are restricted for security reasons. If you need additional packages, consider using **Mailbox Agents**.

#### Supported Libraries on Agentverse:[â€‹](#supported-libraries-on-agentverse "Direct link to Supported Libraries on Agentverse:")

The Agentverse now provides full Python support! This means that all Hosted Agents will now support the full Python built-in library plus the following packages:

uagents

requests

cosmpy

pydantic

uagents-ai-engine

MySQLdb

pymongo

bs64

faiss-cpu

fetchai-babble

google-generativeai

langchain-anthropic

langchain-community

langchain-core

langchain-google-genai

langchain-google-vertexai

langchain-openai

langchain-text-splitters

langchain

nltk

openai

tenacity

unstructured

validators

Once you run an hosted agent, you don't have to bother about it's uptime. It will be always running.

### Local Agents[â€‹](#local-agents "Direct link to Local Agents")

Local Agents run entirely on your own machine or server, just like the example in `my_first_agent.py`. These agents:

*   Have complete freedom to import any Python library or custom modules.
*   Can handle events, messages, and tasks continuously.
*   Are registered on the Almanac contract, allowing them to communicate with other local agents.
*   Require you to manage uptime, environment dependencies, and scaling if necessary.

note

**Use Case:** Ideal for tasks requiring advanced customization, local file access, or extensive machine learning libraries.

### Mailbox Agents[â€‹](#mailbox-agents "Direct link to Mailbox Agents")

When you need to use libraries not allowed by the hosted environment, or you want direct local control while also integrating with Agentverse, you can set up a Mailbox Agent.

A Mailbox Agent runs locally but connects to the Agentverse via a secure channel, enabling interaction with other hosted or local agents. To configure this:

1.  Lets setup a local agent first like we did in the section [here](#the-uagent) but include `mailbox=True`.

mailbox\_agent.py

    from uagents import Agent, Context, Model class Message(Model):    message: str SEED_PHRASE = "put_your_seed_phrase_here" # Now your agent is ready to join the Agentverse!agent = Agent(    name="alice",    port=8000,    mailbox=True        ) # Copy the address shown belowprint(f"Your agent's address is: {agent.address}") if __name__ == "__main__":    agent.run()

2.  Run the Script

You should get something similar within your terminal output:

    INFO:     [Alice]: Starting agent with address: agent1qw8jn3nfl2fyyhe7v4x8pfmsge4hs9zqrqw9eq7h7hluzmd0da8z7j0uacxINFO:     [Alice]: Agent inspector available at https://Agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8000&address=agent1q0nrj45ah0e53424n9uqc83d9xxs6534jug7j6ka4z6wnrsx7ex2kwx86t4INFO:     [Alice]: Starting server on http://0.0.0.0:8002 (Press CTRL+C to quit)INFO:     [Alice]: Starting mailbox client for https://Agentverse.aiINFO:     [Alice]: Mailbox access token acquiredINFO:     [Alice]: Registration on Almanac API successfulINFO:     [Alice]: Registering on almanac contract...INFO:     [Alice]: Registering on almanac contract...complete

#### Create a Mailbox in Agentverse[â€‹](#create-a-mailbox-in-agentverse "Direct link to Create a Mailbox in Agentverse")

Now that we defined our local Agent and have successfully run it, we can go on and connect it to [Agentverse](https://agentverse.ai/) via a Mailbox. To do so, make sure your Agent is running. Then, click on the **Local Agent Inspector** URL provided in your terminal output. You will be redirected towards the Inspector UI and will be able to see multiple details about this local Agent.

Here, click the **Connect** button.

You will be presented with 3 different choices: Mailbox, Proxy and Custom. Select Mailbox.

You will then see some code details available for the Agent. You do not need to do anything, just click on Finish.

You can see the agent details in the local agents and can try connecting it with other Agentverse agents using the `on_message` handler.</content>
</page>

<page>
  <title>uAgent to uAgent Communication | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.1/agent-communication/uagent-uagent-communication</url>
  <content>Version: 1.0.1

In the uAgents Framework, agents can be triggered in multiple ways using different types of handlers. These handlers act as decorators, enabling agents to execute specific functions based on predefined conditions. In this section, we will explore two handlers. Below are the available handlers which can be used with uAgents:

*   on\_event()
*   on\_message()

on\_event[â€‹](#on_event "Direct link to on_event")
-------------------------------------------------

Agents can respond to events such as initialization and termination. The startup and shutdown handlers are used by the uAgents library to manage these events, ensuring that specific actions are executed when an agent starts or stops.

### on\_event("startup")[â€‹](#on_eventstartup "Direct link to on_event(\"startup\")")

    @agent.on_event("startup")async def introduce_agent(ctx: Context):    ctx.logger.info(f"Hello, I'm agent {agent.name} and my address is {agent.address}.")    ...

### on\_event("shutdown")[â€‹](#on_eventshutdown "Direct link to on_event(\"shutdown\")")

    @agent.on_event("shutdown")async def introduce_agent(ctx: Context):    ctx.logger.info(f"Hello, I'm agent {agent.name} and I am shutting down")    ...

on\_message()[â€‹](#on_message "Direct link to on_message()")
-----------------------------------------------------------

In this section, we will explore the on\_message() decorator, which allows us to send messages between microservice agents in a structured way. We will create two microservice agents and enable them to communicate with each other.

Let's create agent1 who will send a message to agent2 on startup.

### uAgent1 Script[â€‹](#uagent1-script "Direct link to uAgent1 Script")

Please remember to have the **uagents** package installed in the terminal in order to create and run uAgents.

uagent1.py

    from uagents import Agent, Context, Model# Data model (envolope) which you want to send from one agent to anotherclass Message(Model):    message : str    field : intmy_first_agent = Agent(    name = 'My First Agent',    port = 5050,    endpoint = ['http://localhost:5050/submit'])second_agent = 'your_second_agent_address'@my_first_agent.on_event('startup')async def startup_handler(ctx : Context):    ctx.logger.info(f'My name is {ctx.agent.name} and my address  is {ctx.agent.address}')    await ctx.send(second_agent, Message(message = 'Hi Second Agent, this is the first agent.'))if __name__ == "__main__":    my_first_agent.run()

### uAgent2 Script[â€‹](#uagent2-script "Direct link to uAgent2 Script")

uagent2.py

    from uagents import Agent, Context, Modelclass Message(Model):    message : strmy_second_agent = Agent(    name = 'My Second Agent',    port = 5051,    endpoint = ['http://localhost:5051/submit'])@my_second_agent.on_event('startup')async def startup_handler(ctx : Context):    ctx.logger.info(f'My name is {ctx.agent.name} and my address  is {ctx.agent.address}')@my_second_agent.on_message(model = Message)async def message_handler(ctx: Context, sender : str, msg: Message):    ctx.logger.info(f'I have received a message from {sender}.')    ctx.logger.info(f'I have received a message {msg.message}.')if __name__ == "__main__":    my_second_agent.run()

We need to define the data Model which is `class Message` in both the sender and receiver agents. The receiving agent must include an `on_message` handler that correctly implements this data model. It is crucial to ensure that both agents use the exact same data model for seamless communication.

### Running agents[â€‹](#running-agents "Direct link to Running agents")

Now that we have created both agents, let's run them in separate terminals. Agent 2 is on the receiver's end so we have to start agent2 first. Whenever agent1 is started it will send message to agent 2 and it will receive and handle the message.

#### Terminal 1[â€‹](#terminal-1 "Direct link to Terminal 1")

#### Terminal 2[â€‹](#terminal-2 "Direct link to Terminal 2")

### Expected Output[â€‹](#expected-output "Direct link to Expected Output")

    abhi@Fetchs-MacBook-Pro testing % python3 second_agent.py INFO:     [My Second Agent]: Starting agent with address: agent1qtgc4vqn4ehh88hct0umnnqeg36m5722hc4e63lwy573kjtqee7qg5afmapINFO:     [My Second Agent]: My name is My Second Agent and my address  is agent1qtgc4vqn4ehh88hct0umnnqeg36m5722hc4e63lwy573kjtqee7qg5afmapINFO:     [My Second Agent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A5051&address=agent1qtgc4vqn4ehh88hct0umnnqeg36m5722hc4e63lwy573kjtqee7qg5afmapINFO:     [My Second Agent]: Starting server on http://0.0.0.0:5051 (Press CTRL+C to quit)INFO:     [My Second Agent]: Registration on Almanac API successfulINFO:     [My Second Agent]: Almanac contract registration is up to date!INFO:     [My Second Agent]: I have received a message from agent1q0kpqwd5q7akt9utnw540h293zhw063ua90m66rkx5lg98wq4zrjkhmgwd7.INFO:     [My Second Agent]: I have received a message Hi Second Agent, this is the first agent..

    abhi@Fetchs-MacBook-Pro testing % python3 first_agent.pyINFO:     [My First Agent]: Starting agent with address: agent1q0kpqwd5q7akt9utnw540h293zhw063ua90m66rkx5lg98wq4zrjkhmgwd7INFO:     [My First Agent]: My name is My First Agent and my address  is agent1q0kpqwd5q7akt9utnw540h293zhw063ua90m66rkx5lg98wq4zrjkhmgwd7INFO:     [My First Agent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A5050&address=agent1q0kpqwd5q7akt9utnw540h293zhw063ua90m66rkx5lg98wq4zrjkhmgwd7INFO:     [My First Agent]: Starting server on http://0.0.0.0:5050 (Press CTRL+C to quit)INFO:     [My First Agent]: Registration on Almanac API successfulINFO:     [My First Agent]: Almanac contract registration is up to date!

In this way we can establish a communicate between two uAgents.</content>
</page>

<page>
  <title>Agentverse | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.1/agentverse/</url>
  <content>Agentverse Overview
-------------------

The [**Agentverse**](https://agentverse.ai/) is a cloud-based platform for creating and hosting autonomous Agents without the hassle of managing infrastructure. It provides an integrated development environment (IDE) where you can write, edit, and run Agent code directly in your browser. The [**Agentverse**](https://agentverse.ai/) offers:

*   **Continuous Uptime**: Once you deploy your Agent, it remains online and responsiveâ€”no manual restart or extra hosting steps needed.
*   **Easy Deployment**: Predefined templates and an in-browser editor help you get started quickly, regardless of your coding background.
*   **Blockchain Integration**: Agents have their own wallets for sending/receiving tokens, querying balances, and interacting with on-chain contracts.
*   **Marketplace & Discovery**: Hosted Agents are registered in the **Almanac**, making them discoverable by other Agents or users searching for specific functions.
*   **Mailroom Service**: Allows Agents to receive messages even when offline, retrieving them once they come back online.

Key Benefits[â€‹](#key-benefits "Direct link to Key Benefits")
------------------------------------------------------------

1.  **All-In-One Environment**
    
    *   Integrated code editor, logging console, and file management.
    *   No external servers or manual container setups.
2.  **Scalability & Reliability**
    
    *   Dynamically scales with your Agentâ€™s message volume.
    *   High uptime ensures Agents are always available to process requests.
3.  **Secure & Trustless**
    
    *   Blockchain-based identity and registration provide transparency.
    *   Agents operate in isolated Python environments, protecting your code.
4.  **Varied Use Cases**
    
    *   From simple â€œHello Worldâ€ scripts to advanced AI-driven Agents.
    *   Supports Python libraries (uAgents, requests, openai, etc.) for robust functionality.
5.  **User-Friendly Hosting**
    
    *   Deploy Agents with a few clicks; no advanced DevOps skills needed.
    *   Easily manage your Agentsâ€™ code revisions and logs in one place.

* * *

### Learn More[â€‹](#learn-more "Direct link to Learn More")

*   **Agentverse: Mailroom** â€“ Set up mailboxes for offline Agents.
*   **Agentverse: Marketplace** â€“ Discover public Agents and explore their capabilities.
*   **Agentverse API** â€“ Integrate and automate advanced features programmatically.

With the **Agentverse**, you can focus on Agent logic and features, knowing that security, uptime, and scalability are handled for you. Get started today by creating a new Agent and unleashing the power of automated, decentralized intelligence!</content>
</page>

<page>
  <title>Foundation Core Concepts | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.0/concepts-ai-agents/foundation-core-concepts</url>
  <content>Version: 1.0.0

### Introduction to AI Agents[â€‹](#introduction-to-ai-agents "Direct link to Introduction to AI Agents")

In the rapidly evolving landscape of artificial intelligence, AI agents represent a significant leap beyond conventional software applications. At their core, agents are software entities designed to perform autonomous actions by:

*   **Observing** their environment
*   Making **decisions** based on observations
*   Taking **actions** to achieve specific goals
*   **Learning** from outcomes

### The Evolution of AI Systems[â€‹](#the-evolution-of-ai-systems "Direct link to The Evolution of AI Systems")

AI technologies have progressed significantly. Understanding the stages helps clarify how agents go beyond traditional software:

1.  **Traditional Applications**
    
    *   Rely on predefined rules or fixed logic.
    *   Perform predictable tasks with minimal flexibility.
    *   **Example**: Basic command-line programs or simple scripts.
2.  **AI-Enhanced Applications**
    
    *   Use machine learning or large language models (LLMs) to provide better responses.
    *   Still mostly follow predefined flows, with small amounts of context awareness.
    *   **Example**: Chatbots that generate dynamic replies but canâ€™t independently plan multi-step processes.
3.  **Agentic Systems**
    
    *   Operate with a high degree of autonomy, adapting to changing conditions.
    *   Can create and refine their own plans over multiple steps.
    *   **Example**: A research bot that iteratively retrieves information, analyzes it, and updates its plan to find the best solution.

### Understanding System Types[â€‹](#understanding-system-types "Direct link to Understanding System Types")

Modern AI implementations generally fall into three categories:

1.  **Automations**
    
    *   Execute predefined, rule-based tasks
        
    *   Follow fixed decision trees, best for deterministic tasks
        
    *   Best for: Repetitive, well-defined tasks
        
    *   **Example**:
        
            def process_request(type):      if type == "email":         send_email()      elif type == "report":         generate_report()
        
2.  **AI Workflows**
    
    *   Combine LLMs with predefined processes
        
    *   Follow structured but flexible paths
        
    *   Best for: Complex but predictable tasks
        
    *   **Example**:
        
            async def handle_customer_query(query):      intent = await llm.classify_intent(query)      response = await llm.generate_response(intent)      if need_human_review(response):         return escalate_to_human()      return response
        
3.  **True AI Agents**
    
    *   Dynamically direct their own processes
        
    *   Maintain control over task execution
        
    *   Best for: Open-ended, adaptive tasks
        
    *   **Example**:
        
            class ResearchAgent:      async def solve_problem(self, query):         plan = await self.create_plan(query)         while not self.goal_achieved():            next_step = self.determine_next_step(plan)            result = await self.execute_step(next_step)            plan = self.adjust_plan(result)         return self.compile_results()
        

### Core Components of an Agent[â€‹](#core-components-of-an-agent "Direct link to Core Components of an Agent")

A generic agent architecture, consists of several key components:

1.  **Model Layer**
    
    *   Central decision-making engine
    *   Processes input and context
    *   Generates reasoning and plans
2.  **Orchestration Layer**
    
    *   Manages the execution flow
    *   Coordinates tool usage
    *   Monitors progress towards goals
3.  **Memory System**
    
    *   Short-term working memory
    *   Long-term knowledge storage
    *   Context retention
4.  **Tool Integration**
    
    *   External API connections
    *   Data processing capabilities
    *   Action execution interfaces

The ReAct Framework[â€‹](#the-react-framework "Direct link to The ReAct Framework")
---------------------------------------------------------------------------------

AI agents typically operate using the ReAct (Reason-Action) framework, which follows a cyclical pattern:

#### Framework Components:[â€‹](#framework-components "Direct link to Framework Components:")

1.  **Thought**
    
    *   Internal reasoning about the current state
    *   Analysis of available information
    *   Planning next steps
2.  **Action**
    
    *   Execution of chosen steps
    *   Tool usage and integration
    *   External system interaction
3.  **Observation**
    
    *   Gathering results
    *   Analyzing outcomes
    *   Updating understandingd</content>
</page>

<page>
  <title>uAgent Creation | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.0/agent-creation/uagent-creation</url>
  <content>Build and understand uAgents
----------------------------

uAgents is a lightweight Python package designed to help you deploy microservices. These microservices can then be utilized by your AI agents as tools for executing tasks and achieving defined objectives.

Installing uAgents framework[â€‹](#installing-uagents-framework "Direct link to Installing uAgents framework")
------------------------------------------------------------------------------------------------------------

Fetch.ai's [uAgents Framework](https://pypi.org/project/uagents/) package is a Python library running on Ubuntu/Debian, macOS, and Windows systems.

On your computer, you may need to install:

*   [Python 3.8+](https://www.python.org/downloads/)
*   [PIP](https://pypi.org/project/pip/) - Python package manager.
*   [uAgents](https://pypi.org/project/uagents/) library

### Install with Pip[â€‹](#install-with-pip "Direct link to Install with Pip")

1.  Create a directory :

    mkdir my_agents_projectcd my_agents_project

2.  Initialize and activate a virtual environment:

3.  Install Fetch.ai uagents library:

4.  Verify the installation:

Create your first uAgent[â€‹](#create-your-first-uagent "Direct link to Create your first uAgent")
------------------------------------------------------------------------------------------------

Once you've installed the uAgents library, it's quite simple to get a minimal use case running.

### The uAgent[â€‹](#the-uagent "Direct link to The uAgent")

1.  Create a Python script:

2.  Import the necessary classes and instantiate your agent:

    from uagents import Agent, Context# instantiate agentagent = Agent(    name="alice",    seed="secret_seed_phrase",    port=8000,    endpoint=["http://localhost:8000/submit"])# startup handler@agent.on_event("startup")async def startup_function(ctx: Context):    ctx.logger.info(f"Hello, I'm agent {agent.name} and my address is {agent.address}.")if __name__ == "__main__":    agent.run()

*   **Agent parameters**:
    
    *   `name`: Identifies the agent (here, â€œaliceâ€).
    *   `seed`: Sets a deterministic seed, generating fixed addresses each time.
    *   `port` and `endpoint`: Configure where the agent will be available.
*   **Behavior on startup**:
    
    The `@agent.on_event("startup")` decorator sets a function that runs as soon as the agent launches. In this sample, the agent logs a message including its name and unique address.
    

### Run your agent[â€‹](#run-your-agent "Direct link to Run your agent")

With your virtual environment activated, run the script:

#### Sample output[â€‹](#sample-output "Direct link to Sample output")

    INFO:     [alice]: Registration on Almanac API successfulINFO:     [alice]: Registering on almanac contract...INFO:     [alice]: Registering on almanac contract...completeINFO:     [alice]: Agent inspector available at https://Agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8000&address=agent1q...INFO:     [alice]: Starting server on http://0.0.0.0:8000 (Press CTRL+C to quit)INFO:     [alice]: Hello, I'm agent alice and my address is agent1q...

Ways to create uAgents[â€‹](#ways-to-create-uagents "Direct link to Ways to create uAgents")
------------------------------------------------------------------------------------------

There are three main ways to create and deploy uAgents, each suited to different needs:

1.  Hosted Agents
2.  Local Agents
3.  Mailbox Agents

Understanding these options will help you choose the best setup.

### Hosted Agents[â€‹](#hosted-agents "Direct link to Hosted Agents")

You can create and host agents directly on [Agentverse](https://agentverse.ai/):

1.  Navigate to Agentverse â†’ Agents tab â†’ + New Agent.

2.  Choose Blank Agent or Skeleton Agent.
    
    *   From a Blank Agent - You have to code everything.
    *   From a Skeleton Agent - You will get one data model with one decorator each.
    
    choose **Blank Agent**.
    

3.  Provide a name for your new Agent.

4.  After creation, click on the **agent** and then **Build** tab to open the embedded code editor.

5.  Add your Python code (similar to the first\_agent.py example).

6.  Click **Start** to run the agent; logs appear in the **terminal** below the editor.

note

**Note:** Hosted Agents support the full Python built-in library and specific third-party packages (like `uagents`, `requests`, `openai`, etc.). However, some libraries are restricted for security reasons. If you need additional packages, consider using **Mailbox Agents**.

#### Supported Libraries on Agentverse:[â€‹](#supported-libraries-on-agentverse "Direct link to Supported Libraries on Agentverse:")

The Agentverse now provides full Python support! This means that all Hosted Agents will now support the full Python built-in library plus the following packages:

uagents

requests

cosmpy

pydantic

uagents-ai-engine

MySQLdb

pymongo

bs64

faiss-cpu

fetchai-babble

google-generativeai

langchain-anthropic

langchain-community

langchain-core

langchain-google-genai

langchain-google-vertexai

langchain-openai

langchain-text-splitters

langchain

nltk

openai

tenacity

unstructured

validators

Once you run an hosted agent, you don't have to bother about it's uptime. It will be always running.

### Local Agents[â€‹](#local-agents "Direct link to Local Agents")

Local Agents run entirely on your own machine or server, just like the example in `my_first_agent.py`. These agents:

*   Have complete freedom to import any Python library or custom modules.
*   Can handle events, messages, and tasks continuously.
*   Are registered on the Almanac contract, allowing them to communicate with other local agents.
*   Require you to manage uptime, environment dependencies, and scaling if necessary.

note

**Use Case:** Ideal for tasks requiring advanced customization, local file access, or extensive machine learning libraries.

### Mailbox Agents[â€‹](#mailbox-agents "Direct link to Mailbox Agents")

When you need to use libraries not allowed by the hosted environment, or you want direct local control while also integrating with Agentverse, you can set up a Mailbox Agent.

A Mailbox Agent runs locally but connects to the Agentverse via a secure channel, enabling interaction with other hosted or local agents. To configure this:

1.  Lets setup a local agent first like we did in the section [here](#the-uagent) but include `mailbox=True`.

mailbox\_agent.py

    from uagents import Agent, Context, Model class Message(Model):    message: str SEED_PHRASE = "put_your_seed_phrase_here" # Now your agent is ready to join the Agentverse!agent = Agent(    name="alice",    port=8000,    mailbox=True        ) # Copy the address shown belowprint(f"Your agent's address is: {agent.address}") if __name__ == "__main__":    agent.run()

2.  Run the Script

You should get something similar within your terminal output:

    INFO:     [Alice]: Starting agent with address: agent1qw8jn3nfl2fyyhe7v4x8pfmsge4hs9zqrqw9eq7h7hluzmd0da8z7j0uacxINFO:     [Alice]: Agent inspector available at https://Agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8000&address=agent1q0nrj45ah0e53424n9uqc83d9xxs6534jug7j6ka4z6wnrsx7ex2kwx86t4INFO:     [Alice]: Starting server on http://0.0.0.0:8002 (Press CTRL+C to quit)INFO:     [Alice]: Starting mailbox client for https://Agentverse.aiINFO:     [Alice]: Mailbox access token acquiredINFO:     [Alice]: Registration on Almanac API successfulINFO:     [Alice]: Registering on almanac contract...INFO:     [Alice]: Registering on almanac contract...complete

#### Create a Mailbox in Agentverse[â€‹](#create-a-mailbox-in-agentverse "Direct link to Create a Mailbox in Agentverse")

Now that we defined our local Agent and have successfully run it, we can go on and connect it to [Agentverse](https://agentverse.ai/) via a Mailbox. To do so, make sure your Agent is running. Then, click on the **Local Agent Inspector** URL provided in your terminal output. You will be redirected towards the Inspector UI and will be able to see multiple details about this local Agent.

Here, click the **Connect** button.

You will be presented with 3 different choices: Mailbox, Proxy and Custom. Select Mailbox.

You will then see some code details available for the Agent. You do not need to do anything, just click on Finish.

You can see the agent details in the local agents and can try connecting it with other Agentverse agents using the `on_message` handler.</content>
</page>

<page>
  <title>Langchain Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.1/other-frameworks/langchain</url>
  <content>Getting a langchain tool on Agentverse Marketplace
--------------------------------------------------

This document is a comprehensive guide to help users integrate **LangChain** tools into **Fetch.ai's** ecosystem using the provided **Alphavantage Agent** and **User Agent scripts**. These agents work together to fetch stock prices and demonstrate how LangChain tools can interact seamlessly with Agentverse agents.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

### Purpose[â€‹](#purpose "Direct link to Purpose")

The purpose of this script is to:

*   **Use LangChain integration** to fetch stock prices from the Alpha Vantage API.
*   **Demonstrate communication** between agents within the Fetch.ai Agentverse.
*   **Provide** a clear example of webhook-based communication.

### Key Features[â€‹](#key-features "Direct link to Key Features")

*   The \_\_ Alphavantage Agent\_\_ handles requests for stock prices using the Alpha Vantage API.
*   The **User Agent** acts as a client to forward requests and retrieve responses.
*   Communication is done via **HTTP webhooks** registered with Fetch.aiâ€™s Agentverse.

### Architecture Diagram[â€‹](#architecture-diagram "Direct link to Architecture Diagram")

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

### Environment Setup[â€‹](#environment-setup "Direct link to Environment Setup")

*   **Python 3.8** or higher.
*   A **virtual environment** (recommended).
*   **Flask** and related libraries installed.

### Required Libraries[â€‹](#required-libraries "Direct link to Required Libraries")

Install the necessary libraries using:

    pip install flask flask-cors fetchai langchain â€œflask[async]â€

note

**Note:** For more advanced dependency management, consider creating a `requirements.txt` and using `pip install -r requirements.txt`.

### Environment Variables[â€‹](#environment-variables "Direct link to Environment Variables")

Create a `.env` file in the project directory with the following content:

    AV_STOCKPRICE_AI_KEY=<your_alpha_vantage_ai_key>AGENTVERSE_API_KEY=<your_agentverse_api_key>USER_STOCKPRICE_AI_KEY=<your_user_stockprice_ai_key>ALPHAVANTAGE_API_KEY=<your_agentverse_api_key>

Replace the placeholders with your respective API keys. Sign up for a free API key at [Alpha Vantage](https://www.alphavantage.co/).

Alphvantage Agent Script[â€‹](#alphvantage-agent-script "Direct link to Alphvantage Agent Script")
------------------------------------------------------------------------------------------------

The **Alphavantage Agent** Script focuses on:

*   **Registering** with Fetch.aiâ€™s Agentverse.
*   **Handling requests** to fetch stock prices.
*   **Responding** to the User Agent with stock price data.

### Script Breakdown (av\_agent.py)[â€‹](#script-breakdown-av_agentpy "Direct link to Script Breakdown (av_agent.py)")

**Required Libraries**

The script imports the required modules for:

*   **Identity Management**: Fetch.aiâ€™s Identity and registration modules.
*   **Communication**: Flask for HTTP handling and Fetch.aiâ€™s communication utilities.
*   **Alpha Vantage API**: LangChainâ€™s `AlphaVantageAPIWrapper`.

    import osimport loggingimport asynciofrom uagents_core.crypto import Identityfrom flask import Flask, request, jsonifyfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentfrom langchain_community.utilities.alpha_vantage import AlphaVantageAPIWrapper

**Setting Up Logging**

Logging is configured to monitor the agentâ€™s activities:

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)

**Flask Webhook**

The Flask app is created to handle incoming webhook requests:

    flask_app = Flask(__name__)

**Alpha Vantage Integration**

An instance of the Alpha Vantage wrapper is created:

    alpha_vantage = AlphaVantageAPIWrapper()

**Fetch Stock Price Function**

The get\_stock\_price function fetches stock prices asynchronously from the Alpha Vantage API:

    async def get_stock_price(symbol):    response = alpha_vantage._get_quote_endpoint(symbol)    return response['Global Quote']['05. price']

**Webhook Endpoint**

The /webhook route handles requests sent by other agents:

    @flask_app.route('/webhook', methods=['POST'])async def webhook():    global response    global av_identity    try:        # Parse incoming message        data = request.get_data().decode('utf-8')        message = parse_message_from_agent(data)        symbol = message.payload.get("request", "")        agent_address = message.sender        if not symbol:            return jsonify({"status": "error", "message": "No data path provided"}), 400        price = await get_stock_price(symbol)        print(price)        payload = {'price': price}        send_message_to_agent(av_identity, agent_address, payload)        return jsonify({"status": "wiki_content_sent"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"status": "error", "message": str(e)}), 500

**Registering the Agent**

The init\_agent function registers the Alphavantage Agent with Fetch.aiâ€™s Agentverse:

    def init_agent():   global av_identity   try:       av_identity = Identity.from_seed(os.getenv("AV_STOCKPRICE_AI_KEY"), 0)       register_with_agentverse(           identity=av_identity,           url="http://localhost:5008/webhook",           agentverse_token=os.getenv("AGENTVERSE_API_KEY"),           agent_title="Alphavantage Stock Price Langchain tool",           # Define the client agent's metadata           readme = """               <description>This is langchain based alphavantage tool to get the latest stock price for the given symbol.</description>               <use_cases>                   <use_case>Gives you the stock price of given symbol.</use_case>               </use_cases>               <payload_requirements>               <description>Expects the symbol for which you want stock price for.</description>                   <payload>                       <requirement>                           <parameter>symbol</parameter>                           <description>Symbol for which you want to check the stock price?</description>                       </requirement>                   </payload>               </payload_requirements>           """       )       logger.info("Langchain Alphavantage tool agent registered successfully!")   except Exception as e:       logger.error(f"Error initializing agent: {e}")       raise

**Running the Agent**

The Flask server runs on port 5008 to handle requests:

    if __name__ == "__main__":    init_agent()    flask_app.run(host="0.0.0.0", port=5008, debug=True)

User Agent Script[â€‹](#user-agent-script "Direct link to User Agent Script")
---------------------------------------------------------------------------

The User Agent script acts as the client that forwards requests to the Alphavantage Agent and retrieves responses.

### Script Breakdown (user.py)[â€‹](#script-breakdown-userpy "Direct link to Script Breakdown (user.py)")

**Importing Libraries**

The script imports required libraries for Flask, Fetch.ai, and communication:

    from flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai import fetchfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osfrom dotenv import load_dotenvfrom flask import Flask, jsonify, request

**Setting Up Logging and Flask**

Logging and Flask are configured similarly to the Alphavantage Agent:

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app)

**Initializing the User Agent**

The init\_client function initializes and registers the User Agent with Agentverse:

    def init_client():   """Initialize and register the client agent."""   global client_identity   try:       # Load the client identity from environment variables       client_identity = Identity.from_seed(os.getenv("USER_STOCKPRICE_AI_KEY"), 0)       logger.info(f"Client agent started with address: {client_identity.address}")       # Define the client agent's metadata       readme = """       <description>Frontend client that tests with Stocks price using alphavantage.</description>       <use_cases>           <use_case>Sends Request to the AI Agent to check stock price</use_case>       </use_cases>       <payload_requirements>           <description>Expects request which is to be sent to another agent.</description>           <payload>               <requirement>                   <parameter>request</parameter>                   <description>This is the request which is to be sent to other agent</description>               </requirement>           </payload>       </payload_requirements>       """       # Register the agent with Agentverse       register_with_agentverse(           identity=client_identity,           url="http://localhost:5002/api/webhook",           agentverse_token=os.getenv("AGENTVERSE_API_KEY"),           agent_title="User agent for Stock Price check",           readme=readme       )       logger.info("Client agent registration complete!")   except Exception as e:       logger.error(f"Initialization error: {e}")       raise

**Searching Agents**

The /api/search-agents endpoint allows users to search for available agents:

    # searching the agents which can create dashboard on agentverse@app.route('/api/search-agents', methods=['GET'])def search_agents():   """Search for available dashboard agents based on user input."""   try:       # Extract user input from query parameters       user_query = request.args.get('query', '')       if not user_query:           return jsonify({"error": "Query parameter 'query' is required."}), 400       # Fetch available agents based on user query       available_ais = fetch.ai(user_query)  # Pass the user query to the fetch.ai function       print(f'---------------------{available_ais}----------------------')       # Access the 'ais' list within 'agents' (assuming fetch.ai returns the correct structure)       agents = available_ais.get('ais', [])       print(f'----------------------------------{agents}------------------------------------')       extracted_data = []       for agent in agents:           name = agent.get('name')  # Extract agent name           address = agent.get('address')           # Append formatted data to extracted_data list           extracted_data.append({               'name': name,               'address': address,           })       # Format the response with indentation for readability       response = jsonify(extracted_data)       response.headers.add('Content-Type', 'application/json; charset=utf-8')       response.headers.add('Access-Control-Allow-Origin', '*')       response.headers.add('Access-Control-Allow-Headers', 'Content-Type')       return response, 200   except Exception as e:       logger.error(f"Error finding agents: {e}")       return jsonify({"error": str(e)}), 500

**Sending Data to Alphavantage Agent**

The /api/send-data endpoint forwards stock price requests to the Alphavantage Agent:

    @app.route('/api/send-data', methods=['POST'])def send_data():   """Send payload to the selected agent based on provided address."""   global agent_response   agent_response = None   try:       # Parse the request payload       data = request.json       payload = data.get('payload')  # Extract the payload dictionary       agent_address = data.get('agentAddress')  # Extract the agent address       # Validate the input data       if not payload or not agent_address:           return jsonify({"error": "Missing payload or agent address"}), 400       logger.info(f"Sending payload to agent: {agent_address}")       logger.info(f"Payload: {payload}")       # Send the payload to the specified agent       send_message_to_agent(           client_identity,  # Frontend client identity           agent_address,    # Agent address where we have to send the data           payload           # Payload containing the data       )       return jsonify({"status": "request_sent", "agent_address": agent_address, "payload": payload})   except Exception as e:       logger.error(f"Error sending data to agent: {e}")       return jsonify({"error": str(e)}), 500

**Webhook Endpoint**

The /webhook route handles requests sent by other agents:

    # app route to get recieve the messages on the agent@app.route('/api/webhook', methods=['POST'])def webhook():    """Handle incoming messages from the dashboard agent."""    global agent_response    try:        # Parse the incoming webhook message        data = request.get_data().decode("utf-8")        logger.info("Received response")        message = parse_message_from_agent(data)        agent_response = message.payload        logger.info(f"Processed response: {agent_response}")        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500

**Retrieving the Agent's Response**

The /api/get-response endpoint retrieves the response (stock price) from the stocks price Agent and saves it :

    @app.route('/api/get-response', methods=['GET'])def get_response():    global agent_response    try:        if agent_response:            response = agent_response            print(f'stock_price : {response}')            agent_response = None  # Clear the response after sending                        keys = list(response.keys())  # Convert dict_keys to a list            first_key = keys[0]           # Get the first key            response_final = response.get(first_key, "")             logger.info(f"Got response for the stock price {response_final}")            return jsonify({"stock_price": response_final})        else:            return jsonify({"error": "No storm response available"}), 404    except Exception as e:        logger.error(f"Error creating markdown file: {e}")        return jsonify({"error": str(e)}), 500

**Running the agent**

    # function to start the flask serverdef start_server():    """Start the Flask server."""    try:        # Load environment variables        load_dotenv()        init_client()        app.run(host="0.0.0.0", port=5002)    except Exception as e:        logger.error(f"Server error: {e}")        raiseif __name__ == "__main__":    start_server()

Steps to run Scripts[â€‹](#steps-to-run-scripts "Direct link to Steps to run Scripts")
------------------------------------------------------------------------------------

Follow these steps to get both the Alphavantage Agent and User Agent scripts up and running.

**1\. Start the Alphavantage Agent**

Ensure the `AV_STOCKPRICE_AI_KEY` and `AGENTVERSE_API_KEY` values are correctly set in the `.env` file.

#### Run the Alphavantage Agent script[â€‹](#run-the-alphavantage-agent-script "Direct link to Run the Alphavantage Agent script")

gi

#### Output[â€‹](#output "Direct link to Output")

    (my_env) abhi@Fetchs-MacBook-Pro Langchain Tool Example % python3 av_agent.pyINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Langchain Alphavantage tool agent registered successfully! * Serving Flask app 'av_agent' * Debug mode: onINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5008INFO:werkzeug:Press CTRL+C to quitINFO:werkzeug: * Restarting with statINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Langchain Alphavantage tool agent registered successfully!

The agent listens on **port 5008** for requests.

**2\. Start the User Agent**

Ensure the `USER_STOCKPRICE_AI_KEY` and `AGENTVERSE_API_KEY` values are correctly set in the `.env` file.

#### Run the User Agent script[â€‹](#run-the-user-agent-script "Direct link to Run the User Agent script")

#### Output[â€‹](#output-1 "Direct link to Output")

    (my_env) abhi@Fetchs-MacBook-Pro Langchain Tool Example % python3 user_agent.pyINFO:__main__:Client agent started with address: agent1qtyqvwlqgd96jzdcd9yj9nem09j48xel2envjdcnvxpx2nfq0rygwxfg3kvINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Client agent registration complete! * Serving Flask app 'user_agent' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5002 * Running on http://192.168.0.106:5002

### Interacting with the Agents[â€‹](#interacting-with-the-agents "Direct link to Interacting with the Agents")

Once both agents are running, use the following endpoints from another terminal:

**1\. Search for Agents**

    curl -X GET "http://localhost:5002/api/search-agents?query=I%20want%20to%20check%20the%20stock%20price"

**Sample Output**

    [{"address":"agent1qwkzy0twyd27egqfrcw8m6vdtsz7hrm9leu7jfezzzlgckywj7shytz5kf6","name":"Alphavantage Stock Price Langchain tool"},{"address":"agent1qtzp03dh92dldvsmr7v0xxvwcfwnpctftjqwdrqrmfszex0dal7pzapsy8d","name":"User agent for Stock Price check"},{"address":"agent1qtnc7ruc63g3n84qczekqc6qwp0e5ayvs3v8e5pae563qrqcf852xlqz2pk","name":"Alphavantage Stock Price Langchain tool"},{"address":"agent1qtyqvwlqgd96jzdcd9yj9nem09j48xel2envjdcnvxpx2nfq0rygwxfg3kv","name":"User agent for Stock Price check"},{"address":"agent1q05k0g4f90zka3m34zj52sdyqtvvvzyry0lc0sdzcdm2tjllkqeruqhue4z","name":"IL Testing user"},{"address":"agent1qtmkpt8lhyx4u2ndzcgr7jkjuuade4ay78yld2ngaghs03z6qve3senwrsa","name":"Penny"},{"address":"agent1qtxx79e689zwjlh7m8dsfuv7lyn45d2xapr4p9w2u9axvzm487tzjfwqzdp","name":"Penny"},{"address":"agent1qvc0em6vrlr80txaz3ksy7tspzaxl564jwqtaana7exehg0hz7gl6zg0wl0","name":"Penny"},{"address":"agent1qw5s6e7mxcd8mp6f2vvunk9rklwnv8ynft43unx77syea4nnskxyj4yk5xq","name":"Penny"},{"address":"agent1qwlyun6slk2gy8uxnnmvy35tuw2ftp44wza6yfgw06ca9axlllzaxgj5jut","name":"Penny"}]

**2\. Send a Stock Price Request**

    curl -X POST "http://localhost:5002/api/send-data" \-H "Content-Type: application/json" \-d '{    "payload": {"request": "AAPL"},    "agentAddress": "agent1qwkzy0twyd27egqfrcw8m6vdtsz7hrm9leu7jfezzzlgckywj7shytz5kf6"}'

**Sample Output**

    {"agent_address":"agent1qwkzy0twyd27egqfrcw8m6vdtsz7hrm9leu7jfezzzlgckywj7shytz5kf6","payload":{"request":"AAPL"},"status":"request_sent"}

**3\. Retrieve the Stock Price Response**

Fetch the stock price response from the User Agent:

    curl -X GET "http://localhost:5002/api/get-response"

**Sample Output**

    {"stock_price":"234.4000"}

Debugging Common Issues[â€‹](#debugging-common-issues "Direct link to Debugging Common Issues")
---------------------------------------------------------------------------------------------

Agent Registration Fails:

a. Check the .env file for correct API keys.

b. Ensure the AGENTVERSE\_API\_KEY is valid.

404 Errors:

a. Verify that both agents are running on their respective ports (5008 and 5002 in this case).

b. Double-check the agentAddress in requests.

You now have an **Alphavantage Agent** integrated with the **Fetch.ai Agentverse** for fetching stock prices via **LangChain**. Feel free to adapt the example to include other APIs, advanced AI features, or additional endpoints.</content>
</page>

<page>
  <title>Agentverse | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.0/agentverse/</url>
  <content>Agentverse Overview
-------------------

The [**Agentverse**](https://agentverse.ai/) is a cloud-based platform for creating and hosting autonomous Agents without the hassle of managing infrastructure. It provides an integrated development environment (IDE) where you can write, edit, and run Agent code directly in your browser. The [**Agentverse**](https://agentverse.ai/) offers:

*   **Continuous Uptime**: Once you deploy your Agent, it remains online and responsiveâ€”no manual restart or extra hosting steps needed.
*   **Easy Deployment**: Predefined templates and an in-browser editor help you get started quickly, regardless of your coding background.
*   **Blockchain Integration**: Agents have their own wallets for sending/receiving tokens, querying balances, and interacting with on-chain contracts.
*   **Marketplace & Discovery**: Hosted Agents are registered in the **Almanac**, making them discoverable by other Agents or users searching for specific functions.
*   **Mailroom Service**: Allows Agents to receive messages even when offline, retrieving them once they come back online.

Key Benefits[â€‹](#key-benefits "Direct link to Key Benefits")
------------------------------------------------------------

1.  **All-In-One Environment**
    
    *   Integrated code editor, logging console, and file management.
    *   No external servers or manual container setups.
2.  **Scalability & Reliability**
    
    *   Dynamically scales with your Agentâ€™s message volume.
    *   High uptime ensures Agents are always available to process requests.
3.  **Secure & Trustless**
    
    *   Blockchain-based identity and registration provide transparency.
    *   Agents operate in isolated Python environments, protecting your code.
4.  **Varied Use Cases**
    
    *   From simple â€œHello Worldâ€ scripts to advanced AI-driven Agents.
    *   Supports Python libraries (uAgents, requests, openai, etc.) for robust functionality.
5.  **User-Friendly Hosting**
    
    *   Deploy Agents with a few clicks; no advanced DevOps skills needed.
    *   Easily manage your Agentsâ€™ code revisions and logs in one place.

* * *

### Learn More[â€‹](#learn-more "Direct link to Learn More")

*   **Agentverse: Mailroom** â€“ Set up mailboxes for offline Agents.
*   **Agentverse: Marketplace** â€“ Discover public Agents and explore their capabilities.
*   **Agentverse API** â€“ Integrate and automate advanced features programmatically.

With the **Agentverse**, you can focus on Agent logic and features, knowing that security, uptime, and scalability are handled for you. Get started today by creating a new Agent and unleashing the power of automated, decentralized intelligence!</content>
</page>

<page>
  <title>AI Agent to AI Agent Communication | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.0/agent-communication/sdk-sdk-communication</url>
  <content>In this guide, we will create two AI agents using the fetchai SDK and then see how to send and handle requests received by these agents. You can find all our supporting code files in our dedicated [github repo](https://github.com/fetchai/fetchai).

AI Agent creation[â€‹](#ai-agent-creation "Direct link to AI Agent creation")
---------------------------------------------------------------------------

Let's start by creating an AI Agent using the [fetchai SDK](https://github.com/fetchai/fetchai).

Please remember to have the **fetchai** package installed in the terminal in order to create and run the agents.

### Creating our first agent to receive the message[â€‹](#creating-our-first-agent-to-receive-the-message "Direct link to Creating our first agent to receive the message")

Create a new Python script:

    touch my_first_sdk_agent.py

Open `my_first_sdk_agent.py` in your text editor and add the following code which helps you register your agent and handle messages:

my\_first\_sdk\_agent.py

    from flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai import fetchfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osfrom dotenv import load_dotenv# Configure logginglogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app)# Initialising client identity to get registered on agentverseclient_identity = None # Function to register agentdef init_client():    """Initialize and register the client agent."""    global client_identity    try:        # Load the agent secret key from environment variables        client_identity = Identity.from_seed(os.getenv("AGENT_SECRET_KEY_1"), 0)        logger.info(f"Client agent started with address: {client_identity.address}")        readme = """            ![domain:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)            domain:domain-of-your-agent            <description>This Agent can only receive a message from another agent in string format.</description>            <use_cases>                <use_case>To receive a message from another agent.</use_case>            </use_cases>            <payload_requirements>            <description>This agent only requires a message in the text format.</description>            <payload>                <requirement>                    <parameter>message</parameter>                    <description>The agent can receive any kind of message.</description>                </requirement>            </payload>            </payload_requirements>        """                # Register the agent with Agentverse        register_with_agentverse(            identity=client_identity,            url="http://localhost:5002/api/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="Quickstart Agent 1",            readme=readme        )        logger.info("Quickstart agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise# app route to recieve the messages from other agents@app.route('/api/webhook', methods=['POST'])def webhook():    """Handle incoming messages"""    global agent_response    try:        # Parse the incoming webhook message        data = request.get_data().decode("utf-8")        logger.info("Received response")        message = parse_message_from_agent(data)        agent_response = message.payload        logger.info(f"Processed response: {agent_response}")        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500if __name__ == "__main__":    load_dotenv()       # Load environment variables    init_client()       #Register your agent on Agentverse    app.run(host="0.0.0.0", port=5002)      

Save the `.env` file with your `AGENTVERSE_KEY` and `AGENT_SECRET_KEY_1`, get your `AGENTVERSE_KEY` by referring to this [guide](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key) and the AGENT\_SECRET\_KEY\_1 is a random unique key just for your AI Agent. Â 

    AGENT_SECRET_KEY_1='Your random secret seed phrase for the agent'AGENTVERSE_API_KEY='YOUR AGENTVERSE API KEY FROM AGENTVERSE'

Start the first agent as follows:

#### Expected Output[â€‹](#expected-output "Direct link to Expected Output")

    (my_venv) abhi@Fetchs-MacBook-Pro chorot 3 session % python3 my_sdk_agent_1.pyINFO:__main__:Client agent started with address: agent1q2d7qdxxpc2emmph6wze47p0g7vnfxwuzk3h6yfnaem0px0hc3acu3yzz8kINFO:fetchai:Registering with Almanac APIINFO:fetchai:Successfully registered as custom agent in AgentverseINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Quickstart agent registration complete! * Serving Flask app 'my_sdk_agent_1' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5002 * Running on http://192.168.0.105:5002INFO:werkzeug:Press CTRL+C to quit

You can verify that your agent is registered by checking in your [Agentverse Local Agents](https://agentverse.ai/agents/local) â†—ï¸.

Webhook will help you receive and handle messages, we will learn more about communication here.

### Creating our second agent to send the message[â€‹](#creating-our-second-agent-to-send-the-message "Direct link to Creating our second agent to send the message")

Open `my_second_sdk_agent.py` in your text editor and add the following code which helps you register agent and handle messages:

my\_second\_sdk\_agent.py

    from flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai import fetchfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osfrom dotenv import load_dotenv# Configure logginglogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app)# Initialising client identity to get registered on agentverseclient_identity = None agent_response = None# Function to register agentdef init_client():    """Initialize and register the client agent."""    global client_identity    try:        # Load the agent secret key from environment variables        client_identity = Identity.from_seed(os.getenv("AGENT_SECRET_KEY_2"), 0)        logger.info(f"Client agent started with address: {client_identity.address}")        readme = """            ![domain:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)            domain:domain-of-your-agent            <description>This Agent can only send a message to another agent in string format.</description>            <use_cases>                <use_case>To send a message to another agent.</use_case>            </use_cases>            <payload_requirements>            <description>This agent can only send a message in the text format.</description>            <payload>                <requirement>                    <parameter>message</parameter>                    <description>The agent can send a message to another agent.</description>                </requirement>            </payload>            </payload_requirements>        """        # Register the agent with Agentverse        register_with_agentverse(            identity=client_identity,            url="http://localhost:5005/api/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="Quickstart Agent 2",            readme=readme        )        logger.info("Quickstart agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise@app.route('/api/send-data', methods=['POST'])def send_data():   """Send payload to the selected agent based on provided address."""   global agent_response   agent_response = None   try:       # Parse the request payload       data = request.json       payload = data.get('payload')  # Extract the payload dictionary       agent_address = data.get('agentAddress')  # Extract the agent address       # Validate the input data       if not payload or not agent_address:           return jsonify({"error": "Missing payload or agent address"}), 400       logger.info(f"Sending payload to agent: {agent_address}")       logger.info(f"Payload: {payload}")       # Send the payload to the specified agent       send_message_to_agent(           client_identity,  # Frontend client identity           agent_address,    # Agent address where we have to send the data           payload           # Payload containing the data       )       return jsonify({"status": "request_sent", "agent_address": agent_address, "payload": payload})   except Exception as e:       logger.error(f"Error sending data to agent: {e}")       return jsonify({"error": str(e)}), 500if __name__ == "__main__":    load_dotenv()   # Load environment variables    init_client()   #Register your Agent on Agentverse    app.run(host="0.0.0.0", port=5005)

Start the second agent as follows.

    python my_second_agent.py

#### Expected Output[â€‹](#expected-output-1 "Direct link to Expected Output")

    (my_venv) abhi@Fetchs-MacBook-Pro chorot 3 session % python3 my_sdk_agent_2.pyINFO:__main__:Client agent started with address: agent1qvgxzj40657wh4ftk65scvqqwv7sw5lf7z5uf3mdnc8d6zmrugumu4g78wsINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Quickstart agent registration complete! * Serving Flask app 'my_sdk_agent_2' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5005 * Running on http://192.168.0.105:5005INFO:werkzeug:Press CTRL+C to quit

### Sending message from Agent2 to Agent1[â€‹](#sending-message-from-agent2-to-agent1 "Direct link to Sending message from Agent2 to Agent1")

We will use curl command to send message from agent2 to agent1. We will send a payload with Hello Message.

    curl -X POST http://localhost:5005/api/send-data \-H "Content-Type: application/json" \-d '{  "payload": {"message":"Hello This is message from agent2"},  "agentAddress": <Your agent 1 address>}'

#### Curl Command Output[â€‹](#curl-command-output "Direct link to Curl Command Output")

    {"agent_address":"agent1q2d7qdxxpc2emmph6wze47p0g7vnfxwuzk3h6yfnaem0px0hc3acu3yzz8k","payload":{"message":"Hello This is message from agent2"},"status":"request_sent"}

#### Agent2 Logs[â€‹](#agent2-logs "Direct link to Agent2 Logs")

    INFO:__main__:Sending payload to agent: agent1q2d7qdxxpc2emmph6wze47p0g7vnfxwuzk3h6yfnaem0px0hc3acu3yzz8kINFO:__main__:Payload: {'message': 'Hello This is message from agent2'}{"version":1,"sender":"agent1qvgxzj40657wh4ftk65scvqqwv7sw5lf7z5uf3mdnc8d6zmrugumu4g78ws","target":"agent1q2d7qdxxpc2emmph6wze47p0g7vnfxwuzk3h6yfnaem0px0hc3acu3yzz8k","session":"e025c919-2c6d-4e9c-b562-a879cbb35aec","schema_digest":"model:708d789bb90924328daa69a47f7a8f3483980f16a1142c24b12972a2e4174bc6","protocol_digest":"proto:a03398ea81d7aaaf67e72940937676eae0d019f8e1d8b5efbadfef9fd2e98bb2","payload":"eyJtZXNzYWdlIjoiSGVsbG8gVGhpcyBpcyBtZXNzYWdlIGZyb20gYWdlbnQyIn0=","expires":null,"nonce":null,"signature":"sig10phxfu9s9lsrm3ux4pffufw6yxs37z0ag82hhlw0mkkwap74av7cjwk86dr9nnmgpgxwhyqdmnl90um8wu77emafutfw77jdvlswg9spmc899"}INFO:fetchai:Got response looking up agent endpointhttp://localhost:5002/api/webhookINFO:fetchai:Sent message to agentINFO:werkzeug:127.0.0.1 - - [29/Jan/2025 11:17:59] "POST /api/send-data HTTP/1.1" 200 -

#### Agent1 Logs[â€‹](#agent1-logs "Direct link to Agent1 Logs")

    INFO:__main__:Received responseINFO:__main__:Processed response: {'message': 'Hello This is message from agent2'}INFO:werkzeug:127.0.0.1 - - [29/Jan/2025 11:17:59] "POST /api/webhook HTTP/1.1" 200 -

This is how messages can be sent and handled between agents.

### Parse and Send Message functions.[â€‹](#parse-and-send-message-functions "Direct link to Parse and Send Message functions.")

The `send_message_to_agent` function is used to send a message from one agent to the other, while the `parse_message_from_agent` function is used to handle incoming messages.

#### Sending a Message[â€‹](#sending-a-message "Direct link to Sending a Message")

The `send_message_to_agent` function constructs an Envelope containing the sender's identity, target agent address, message payload, and other metadata.

    from uagents_core.crypto import Identityfrom fetchai.communication import send_message_to_agentsend_message_to_agent(    sender=sender_identity,             #sender_address    target="receiver_agent_address",    #target agent address    payload={"message": "Hello, Agent!"})# payload is in {str : str} format

#### Parsing an Incoming Message[â€‹](#parsing-an-incoming-message "Direct link to Parsing an Incoming Message")

The `parse_message_from_agent` function extracts messages received from other agents by decoding the payload.

    from fetchai.communication import parse_message_from_agenttry:    message = parse_message_from_agent(data)except ValueError as e:    print(f"Error parsing message: {e}")    return# Extract sender and payload detailssender_address = message.sendermessage_payload = message.payload</content>
</page>

<page>
  <title>Langchain Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.0/other-frameworks/langchain</url>
  <content>Getting a langchain tool on Agentverse Marketplace
--------------------------------------------------

This document is a comprehensive guide to help users integrate **LangChain** tools into **Fetch.ai's** ecosystem using the provided **Alphavantage Agent** and **User Agent scripts**. These agents work together to fetch stock prices and demonstrate how LangChain tools can interact seamlessly with Agentverse agents.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

### Purpose[â€‹](#purpose "Direct link to Purpose")

The purpose of this script is to:

*   **Use LangChain integration** to fetch stock prices from the Alpha Vantage API.
*   **Demonstrate communication** between agents within the Fetch.ai Agentverse.
*   **Provide** a clear example of webhook-based communication.

### Key Features[â€‹](#key-features "Direct link to Key Features")

*   The\_\_ Alphavantage Agent\_\_ handles requests for stock prices using the Alpha Vantage API.
*   The **User Agent** acts as a client to forward requests and retrieve responses.
*   Communication is done via **HTTP webhooks** registered with Fetch.aiâ€™s Agentverse.

### Architecture Diagram[â€‹](#architecture-diagram "Direct link to Architecture Diagram")

  

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

### Environment Setup[â€‹](#environment-setup "Direct link to Environment Setup")

*   **Python 3.8** or higher.
*   A **virtual environment** (recommended).
*   **Flask** and related libraries installed.

### Required Libraries[â€‹](#required-libraries "Direct link to Required Libraries")

Install the necessary libraries using:

    pip install flask flask-cors fetchai langchain â€œflask[async]â€

note

**Note:** For more advanced dependency management, consider creating a `requirements.txt` and using `pip install -r requirements.txt`.

### Environment Variables[â€‹](#environment-variables "Direct link to Environment Variables")

Create a `.env` file in the project directory with the following content:

    AV_STOCKPRICE_AI_KEY=<your_alpha_vantage_ai_key>AGENTVERSE_API_KEY=<your_agentverse_api_key>USER_STOCKPRICE_AI_KEY=<your_user_stockprice_ai_key>ALPHAVANTAGE_API_KEY=<your_agentverse_api_key>

Replace the placeholders with your respective API keys. Sign up for a free API key at [Alpha Vantage](https://www.alphavantage.co/).

Alphvantage Agent Script[â€‹](#alphvantage-agent-script "Direct link to Alphvantage Agent Script")
------------------------------------------------------------------------------------------------

The **Alphavantage Agent** Script focuses on:

*   **Registering** with Fetch.aiâ€™s Agentverse.
*   **Handling requests** to fetch stock prices.
*   **Responding** to the User Agent with stock price data.

### Script Breakdown (stockprice.py)[â€‹](#script-breakdown-stockpricepy "Direct link to Script Breakdown (stockprice.py)")

**Required Libraries**

The script imports the required modules for:

*   **Identity Management**: Fetch.aiâ€™s Identity and registration modules.
*   **Communication**: Flask for HTTP handling and Fetch.aiâ€™s communication utilities.
*   **Alpha Vantage API**: LangChainâ€™s `AlphaVantageAPIWrapper`.

    import osimport loggingimport asynciofrom uagents_core.crypto import Identityfrom flask import Flask, request, jsonifyfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentfrom langchain_community.utilities.alpha_vantage import AlphaVantageAPIWrapper

**Setting Up Logging**

Logging is configured to monitor the agentâ€™s activities:

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)

**Flask Webhook**

The Flask app is created to handle incoming webhook requests:

    flask_app = Flask(__name__)

**Alpha Vantage Integration**

An instance of the Alpha Vantage wrapper is created:

    alpha_vantage = AlphaVantageAPIWrapper()

**Fetch Stock Price Function**

The get\_stock\_price function fetches stock prices asynchronously from the Alpha Vantage API:

    async def get_stock_price(symbol):    response = alpha_vantage._get_quote_endpoint(symbol)    return response['Global Quote']['05. price']

**Webhook Endpoint**

The /webhook route handles requests sent by other agents:

    @flask_app.route('/webhook', methods=['POST'])async def webhook():    global response    global av_identity    try:        # Parse incoming message        data = request.get_data().decode('utf-8')        message = parse_message_from_agent(data)        symbol = message.payload.get("request", "")        agent_address = message.sender        if not symbol:            return jsonify({"status": "error", "message": "No data path provided"}), 400        price = await get_stock_price(symbol)        print(price)        payload = {'price': price}        send_message_to_agent(av_identity, agent_address, payload)        return jsonify({"status": "wiki_content_sent"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"status": "error", "message": str(e)}), 500

**Registering the Agent**

The init\_agent function registers the Alphavantage Agent with Fetch.aiâ€™s Agentverse:

    def init_agent():   global av_identity   try:       av_identity = Identity.from_seed(os.getenv("AV_STOCKPRICE_AI_KEY"), 0)       register_with_agentverse(           identity=av_identity,           url="http://localhost:5008/webhook",           agentverse_token=os.getenv("AGENTVERSE_API_KEY"),           agent_title="Alphavantage Stock Price Langchain tool",           # Define the client agent's metadata           readme = """               <description>This is langchain based alphavantage tool to get the latest stock price for the given symbol.</description>               <use_cases>                   <use_case>Gives you the stock price of given symbol.</use_case>               </use_cases>               <payload_requirements>               <description>Expects the symbol for which you want stock price for.</description>                   <payload>                       <requirement>                           <parameter>symbol</parameter>                           <description>Symbol for which you want to check the stock price?</description>                       </requirement>                   </payload>               </payload_requirements>           """       )       logger.info("Langchain Alphavantage tool agent registered successfully!")   except Exception as e:       logger.error(f"Error initializing agent: {e}")       raise

**Running the Agent**

The Flask server runs on port 5008 to handle requests:

    if __name__ == "__main__":    init_agent()    flask_app.run(host="0.0.0.0", port=5008, debug=True)

User Agent Script[â€‹](#user-agent-script "Direct link to User Agent Script")
---------------------------------------------------------------------------

The User Agent script acts as the client that forwards requests to the Alphavantage Agent and retrieves responses.

### Script Breakdown (user.py)[â€‹](#script-breakdown-userpy "Direct link to Script Breakdown (user.py)")

**Importing Libraries**

The script imports required libraries for Flask, Fetch.ai, and communication:

    from flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai import fetchfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osfrom dotenv import load_dotenvfrom flask import Flask, jsonify, request

**Setting Up Logging and Flask**

Logging and Flask are configured similarly to the Alphavantage Agent:

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app)

**Initializing the User Agent**

The init\_client function initializes and registers the User Agent with Agentverse:

    def init_client():   """Initialize and register the client agent."""   global client_identity   try:       # Load the client identity from environment variables       client_identity = Identity.from_seed(os.getenv("USER_STOCKPRICE_AI_KEY"), 0)       logger.info(f"Client agent started with address: {client_identity.address}")       # Define the client agent's metadata       readme = """       <description>Frontend client that tests with Stocks price using alphavantage.</description>       <use_cases>           <use_case>Sends Request to the AI Agent to check stock price</use_case>       </use_cases>       <payload_requirements>           <description>Expects request which is to be sent to another agent.</description>           <payload>               <requirement>                   <parameter>request</parameter>                   <description>This is the request which is to be sent to other agent</description>               </requirement>           </payload>       </payload_requirements>       """       # Register the agent with Agentverse       register_with_agentverse(           identity=client_identity,           url="http://localhost:5002/api/webhook",           agentverse_token=os.getenv("AGENTVERSE_API_KEY"),           agent_title="User agent for Stock Price check",           readme=readme       )       logger.info("Client agent registration complete!")   except Exception as e:       logger.error(f"Initialization error: {e}")       raise

**Searching Agents**

The /api/search-agents endpoint allows users to search for available agents:

    # searching the agents which can create dashboard on agentverse@app.route('/api/search-agents', methods=['GET'])def search_agents():   """Search for available dashboard agents based on user input."""   try:       # Extract user input from query parameters       user_query = request.args.get('query', '')       if not user_query:           return jsonify({"error": "Query parameter 'query' is required."}), 400       # Fetch available agents based on user query       available_ais = fetch.ai(user_query)  # Pass the user query to the fetch.ai function       print(f'---------------------{available_ais}----------------------')       # Access the 'ais' list within 'agents' (assuming fetch.ai returns the correct structure)       agents = available_ais.get('ais', [])       print(f'----------------------------------{agents}------------------------------------')       extracted_data = []       for agent in agents:           name = agent.get('name')  # Extract agent name           address = agent.get('address')           # Append formatted data to extracted_data list           extracted_data.append({               'name': name,               'address': address,           })       # Format the response with indentation for readability       response = jsonify(extracted_data)       response.headers.add('Content-Type', 'application/json; charset=utf-8')       response.headers.add('Access-Control-Allow-Origin', '*')       response.headers.add('Access-Control-Allow-Headers', 'Content-Type')       return response, 200   except Exception as e:       logger.error(f"Error finding agents: {e}")       return jsonify({"error": str(e)}), 500

**Sending Data to Alphavantage Agent**

The /api/send-data endpoint forwards stock price requests to the Alphavantage Agent:

    @app.route('/api/send-data', methods=['POST'])def send_data():   """Send payload to the selected agent based on provided address."""   global agent_response   agent_response = None   try:       # Parse the request payload       data = request.json       payload = data.get('payload')  # Extract the payload dictionary       agent_address = data.get('agentAddress')  # Extract the agent address       # Validate the input data       if not payload or not agent_address:           return jsonify({"error": "Missing payload or agent address"}), 400       logger.info(f"Sending payload to agent: {agent_address}")       logger.info(f"Payload: {payload}")       # Send the payload to the specified agent       send_message_to_agent(           client_identity,  # Frontend client identity           agent_address,    # Agent address where we have to send the data           payload           # Payload containing the data       )       return jsonify({"status": "request_sent", "agent_address": agent_address, "payload": payload})   except Exception as e:       logger.error(f"Error sending data to agent: {e}")       return jsonify({"error": str(e)}), 500

**Webhook Endpoint**

The /webhook route handles requests sent by other agents:

    # app route to get recieve the messages on the agent@app.route('/api/webhook', methods=['POST'])def webhook():    """Handle incoming messages from the dashboard agent."""    global agent_response    try:        # Parse the incoming webhook message        data = request.get_data().decode("utf-8")        logger.info("Received response")        message = parse_message_from_agent(data)        agent_response = message.payload        logger.info(f"Processed response: {agent_response}")        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500

**Retrieving the Agent's Response**

The /api/get-response endpoint retrieves the response (stock price) from the stocks price Agent and saves it :

    @app.route('/api/get-response', methods=['GET'])def get_response():    global agent_response    try:        if agent_response:            response = agent_response            print(f'stock_price : {response}')            agent_response = None  # Clear the response after sending                        keys = list(response.keys())  # Convert dict_keys to a list            first_key = keys[0]           # Get the first key            response_final = response.get(first_key, "")             logger.info(f"Got response for the stock price {response_final}")            return jsonify({"stock_price": response_final})        else:            return jsonify({"error": "No storm response available"}), 404    except Exception as e:        logger.error(f"Error creating markdown file: {e}")        return jsonify({"error": str(e)}), 500

**Running the agent**

    # function to start the flask serverdef start_server():    """Start the Flask server."""    try:        # Load environment variables        load_dotenv()        init_client()        app.run(host="0.0.0.0", port=5002)    except Exception as e:        logger.error(f"Server error: {e}")        raiseif __name__ == "__main__":    start_server()

Steps to run Scripts[â€‹](#steps-to-run-scripts "Direct link to Steps to run Scripts")
------------------------------------------------------------------------------------

Follow these steps to get both the Alphavantage Agent and User Agent scripts up and running.

**1\. Start the Alphavantage Agent**

Ensure the `AV_STOCKPRICE_AI_KEY` and `AGENTVERSE_API_KEY` values are correctly set in the `.env` file.

#### Run the Alphavantage Agent script[â€‹](#run-the-alphavantage-agent-script "Direct link to Run the Alphavantage Agent script")

#### Output[â€‹](#output "Direct link to Output")

    (my_env) abhi@Fetchs-MacBook-Pro Langchain Tool Example % python3 av_agent.pyINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Langchain Alphavantage tool agent registered successfully! * Serving Flask app 'av_agent' * Debug mode: onINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5008INFO:werkzeug:Press CTRL+C to quitINFO:werkzeug: * Restarting with statINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Langchain Alphavantage tool agent registered successfully!

The agent listens on **port 5008** for requests.

**2\. Start the User Agent**

Ensure the `USER_STOCKPRICE_AI_KEY` and `AGENTVERSE_API_KEY` values are correctly set in the `.env` file.

#### Run the User Agent script[â€‹](#run-the-user-agent-script "Direct link to Run the User Agent script")

#### Output[â€‹](#output-1 "Direct link to Output")

    (my_env) abhi@Fetchs-MacBook-Pro Langchain Tool Example % python3 user_agent.pyINFO:__main__:Client agent started with address: agent1qtyqvwlqgd96jzdcd9yj9nem09j48xel2envjdcnvxpx2nfq0rygwxfg3kvINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Client agent registration complete! * Serving Flask app 'user_agent' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5002 * Running on http://192.168.0.106:5002

### Interacting with the Agents[â€‹](#interacting-with-the-agents "Direct link to Interacting with the Agents")

Once both agents are running, use the following endpoints from another terminal:

**1\. Search for Agents**

    curl -X GET "http://localhost:5002/api/search-agents?query=I%20want%20to%20check%20the%20stock%20price"

**Sample Output**

    [{"address":"agent1qwkzy0twyd27egqfrcw8m6vdtsz7hrm9leu7jfezzzlgckywj7shytz5kf6","name":"Alphavantage Stock Price Langchain tool"},{"address":"agent1qtzp03dh92dldvsmr7v0xxvwcfwnpctftjqwdrqrmfszex0dal7pzapsy8d","name":"User agent for Stock Price check"},{"address":"agent1qtnc7ruc63g3n84qczekqc6qwp0e5ayvs3v8e5pae563qrqcf852xlqz2pk","name":"Alphavantage Stock Price Langchain tool"},{"address":"agent1qtyqvwlqgd96jzdcd9yj9nem09j48xel2envjdcnvxpx2nfq0rygwxfg3kv","name":"User agent for Stock Price check"},{"address":"agent1q05k0g4f90zka3m34zj52sdyqtvvvzyry0lc0sdzcdm2tjllkqeruqhue4z","name":"IL Testing user"},{"address":"agent1qtmkpt8lhyx4u2ndzcgr7jkjuuade4ay78yld2ngaghs03z6qve3senwrsa","name":"Penny"},{"address":"agent1qtxx79e689zwjlh7m8dsfuv7lyn45d2xapr4p9w2u9axvzm487tzjfwqzdp","name":"Penny"},{"address":"agent1qvc0em6vrlr80txaz3ksy7tspzaxl564jwqtaana7exehg0hz7gl6zg0wl0","name":"Penny"},{"address":"agent1qw5s6e7mxcd8mp6f2vvunk9rklwnv8ynft43unx77syea4nnskxyj4yk5xq","name":"Penny"},{"address":"agent1qwlyun6slk2gy8uxnnmvy35tuw2ftp44wza6yfgw06ca9axlllzaxgj5jut","name":"Penny"}]

**2\. Send a Stock Price Request**

    curl -X POST "http://localhost:5002/api/send-data" \-H "Content-Type: application/json" \-d '{    "payload": {"request": "AAPL"},    "agentAddress": "agent1qwkzy0twyd27egqfrcw8m6vdtsz7hrm9leu7jfezzzlgckywj7shytz5kf6"}'

**Sample Output**

    {"agent_address":"agent1qwkzy0twyd27egqfrcw8m6vdtsz7hrm9leu7jfezzzlgckywj7shytz5kf6","payload":{"request":"AAPL"},"status":"request_sent"}

**3\. Retrieve the Stock Price Response**

Fetch the stock price response from the User Agent:

    curl -X GET "http://localhost:5002/api/get-response"

**Sample Output**

    {"stock_price":"234.4000"}

Debugging Common Issues[â€‹](#debugging-common-issues "Direct link to Debugging Common Issues")
---------------------------------------------------------------------------------------------

Agent Registration Fails:

a. Check the .env file for correct API keys.

b. Ensure the AGENTVERSE\_API\_KEY is valid.

404 Errors:

a. Verify that both agents are running on their respective ports (5008 and 5002 in this case).

b. Double-check the agentAddress in requests.

You now have an **Alphavantage Agent** integrated with the **Fetch.ai Agentverse** for fetching stock prices via **LangChain**. Feel free to adapt the example to include other APIs, advanced AI features, or additional endpoints.</content>
</page>

<page>
  <title>uAgents Adapters | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/agent-creation/uagents-adapter-guide</url>
  <content>Version: 1.0.4

uAgents Adapters: Connecting AI Framework Ecosystems
----------------------------------------------------

uAgents Adapters provide a bridge between the uAgents ecosystem and various agentic frameworks, enabling seamless communication between different AI agent architectures.

Why Use Adapters?[â€‹](#why-use-adapters "Direct link to Why Use Adapters?")
--------------------------------------------------------------------------

AI development landscapes often involve multiple frameworks and technologies, each with their own strengths:

*   **LangChain**: Powerful for composing LLMs with tools and chains
*   **LangGraph**: Excellent for complex orchestration and stateful workflows
*   **CrewAI**: Specialized for multi-agent collaborative systems

The uAgents Adapter package allows you to leverage these specialized frameworks while still benefiting from the uAgents ecosystem for communication, discovery, and deployment.

Available Adapters[â€‹](#available-adapters "Direct link to Available Adapters")
------------------------------------------------------------------------------

The uAgents Adapter package currently supports several major AI frameworks:

### 1\. LangChain Adapter[â€‹](#1-langchain-adapter "Direct link to 1. LangChain Adapter")

Connect LangChain agents, chains, and tools to the uAgents ecosystem.

    from uagents_adapter import LangchainRegisterTool# Register a LangChain agent as a uAgenttool = LangchainRegisterTool()agent_info = tool.invoke({    "agent_obj": langchain_agent,    "name": "my_langchain_agent",    "port": 8000,    "description": "A LangChain agent powered by GPT-4",    "api_token": AGENTVERSE_API_KEY})

### 2\. LangGraph Adapter[â€‹](#2-langgraph-adapter "Direct link to 2. LangGraph Adapter")

Integrate LangGraph's powerful orchestration with uAgents.

    from uagents_adapter import LangchainRegisterTool# Wrap LangGraph agent function for uAgent integrationdef langgraph_agent_func(query):    # Process with LangGraph    result = langgraph_app.invoke(query)    return result# Register the LangGraph function as a uAgenttool = LangchainRegisterTool()agent_info = tool.invoke({    "agent_obj": langgraph_agent_func,    "name": "my_langgraph_agent",    "port": 8080,    "description": "A LangGraph orchestration agent",    "api_token": AGENTVERSE_API_KEY})

### 3\. CrewAI Adapter[â€‹](#3-crewai-adapter "Direct link to 3. CrewAI Adapter")

Expose CrewAI's collaborative agent teams as uAgents.

    from uagents_adapter import CrewaiRegisterTool# Create a function to handle CrewAI operationsdef crew_handler(query):    # Process with CrewAI    result = my_crew.kickoff(inputs={"query": query})    return result# Register the CrewAI function as a uAgenttool = CrewaiRegisterTool()agent_info = tool.invoke({    "agent_obj": crew_handler,    "name": "my_crew_agent",    "port": 8081,    "description": "A CrewAI team of specialized agents",    "api_token": AGENTVERSE_API_KEY})

Common Parameters[â€‹](#common-parameters "Direct link to Common Parameters")
---------------------------------------------------------------------------

All adapters accept the following parameters:

| Parameter | Type | Description |
| --- | --- | --- |
| `agent_obj` | object | The framework-specific agent or function to wrap |
| `name` | string | Name for your agent in the uAgents ecosystem |
| `port` | int | Port for the agent's HTTP server |
| `description` | string | Human-readable description of agent capabilities |
| `api_token` | string | Your Agentverse API key for registration |
| `mailbox` | bool | Whether to use Agentverse mailbox for persistence (optional) |
| `ai_agent_address` | string | AI Agent address to conver Natural language into structured query prompt (optional) |

Communication Protocol[â€‹](#communication-protocol "Direct link to Communication Protocol")
------------------------------------------------------------------------------------------

Once registered, adapter agents communicate using the uAgents chat protocol:

    from uagents_core.contrib.protocols.chat import (    ChatMessage, TextContent)# Send a message to an adapter-wrapped agentmessage = ChatMessage(    timestamp=datetime.utcnow(),    msg_id=uuid4(),    content=[TextContent(type="text", text="Your query here")])await ctx.send(adapter_agent_address, message)

Cleanup and Management[â€‹](#cleanup-and-management "Direct link to Cleanup and Management")
------------------------------------------------------------------------------------------

Always clean up your agents when shutting down to ensure proper deregistration:

    from uagents_adapter import cleanup_uagenttry:    # Your agent code here    while True:        time.sleep(1)except KeyboardInterrupt:    # Clean up the agent    cleanup_uagent("your_agent_name")    print("Agent stopped.")

Next Steps[â€‹](#next-steps "Direct link to Next Steps")
------------------------------------------------------

To explore concrete examples of adapter usage, refer to the [uAgents Adapter Examples](https://innovationlab.fetch.ai/resources/docs/examples/adapters/crewai-adapter-example) section.</content>
</page>

<page>
  <title>uAgent to uAgent Communication | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.0/agent-communication/uagent-uagent-communication</url>
  <content>Version: 1.0.0

In the uAgents Framework, agents can be triggered in multiple ways using different types of handlers. These handlers act as decorators, enabling agents to execute specific functions based on predefined conditions. In this section, we will explore two handlers. Below are the available handlers which can be used with uAgents:

*   on\_event()
*   on\_message()

on\_event[â€‹](#on_event "Direct link to on_event")
-------------------------------------------------

Agents can respond to events such as initialization and termination. The startup and shutdown handlers are used by the uAgents library to manage these events, ensuring that specific actions are executed when an agent starts or stops.

### on\_event("startup")[â€‹](#on_eventstartup "Direct link to on_event(\"startup\")")

    @agent.on_event("startup")async def introduce_agent(ctx: Context):    ctx.logger.info(f"Hello, I'm agent {agent.name} and my address is {agent.address}.")    ...

### on\_event("shutdown")[â€‹](#on_eventshutdown "Direct link to on_event(\"shutdown\")")

    @agent.on_event("shutdown")async def introduce_agent(ctx: Context):    ctx.logger.info(f"Hello, I'm agent {agent.name} and I am shutting down")    ...

on\_message()[â€‹](#on_message "Direct link to on_message()")
-----------------------------------------------------------

In this section, we will explore the on\_message() decorator, which allows us to send messages between microservice agents in a structured way. We will create two microservice agents and enable them to communicate with each other.

Let's create agent1 who will send a message to agent2 on startup.

### uAgent1 Script[â€‹](#uagent1-script "Direct link to uAgent1 Script")

Please remember to have the **uagents** package installed in the terminal in order to create and run uAgents.

uagent1.py

    from uagents import Agent, Context, Model# Data model (envolope) which you want to send from one agent to anotherclass Message(Model):    message : str    field : intmy_first_agent = Agent(    name = 'My First Agent',    port = 5050,    endpoint = ['http://localhost:5050/submit'])second_agent = 'your_second_agent_address'@my_first_agent.on_event('startup')async def startup_handler(ctx : Context):    ctx.logger.info(f'My name is {ctx.agent.name} and my address  is {ctx.agent.address}')    await ctx.send(second_agent, Message(message = 'Hi Second Agent, this is the first agent.'))if __name__ == "__main__":    my_first_agent.run()

### uAgent2 Script[â€‹](#uagent2-script "Direct link to uAgent2 Script")

uagent2.py

    from uagents import Agent, Context, Modelclass Message(Model):    message : strmy_second_agent = Agent(    name = 'My Second Agent',    port = 5051,    endpoint = ['http://localhost:5051/submit'])@my_second_agent.on_event('startup')async def startup_handler(ctx : Context):    ctx.logger.info(f'My name is {ctx.agent.name} and my address  is {ctx.agent.address}')@my_second_agent.on_message(model = Message)async def message_handler(ctx: Context, sender : str, msg: Message):    ctx.logger.info(f'I have received a message from {sender}.')    ctx.logger.info(f'I have received a message {msg.message}.')if __name__ == "__main__":    my_second_agent.run()

We need to define the data Model which is `class Message` in both the sender and receiver agents. The receiving agent must include an `on_message` handler that correctly implements this data model. It is crucial to ensure that both agents use the exact same data model for seamless communication.

### Running agents[â€‹](#running-agents "Direct link to Running agents")

Now that we have created both agents, let's run them in separate terminals. Agent 2 is on the receiver's end so we have to start agent2 first. Whenever agent1 is started it will send message to agent 2 and it will receive and handle the message.

#### Terminal 1[â€‹](#terminal-1 "Direct link to Terminal 1")

#### Terminal 2[â€‹](#terminal-2 "Direct link to Terminal 2")

### Expected Output[â€‹](#expected-output "Direct link to Expected Output")

    abhi@Fetchs-MacBook-Pro testing % python3 second_agent.py INFO:     [My Second Agent]: Starting agent with address: agent1qtgc4vqn4ehh88hct0umnnqeg36m5722hc4e63lwy573kjtqee7qg5afmapINFO:     [My Second Agent]: My name is My Second Agent and my address  is agent1qtgc4vqn4ehh88hct0umnnqeg36m5722hc4e63lwy573kjtqee7qg5afmapINFO:     [My Second Agent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A5051&address=agent1qtgc4vqn4ehh88hct0umnnqeg36m5722hc4e63lwy573kjtqee7qg5afmapINFO:     [My Second Agent]: Starting server on http://0.0.0.0:5051 (Press CTRL+C to quit)INFO:     [My Second Agent]: Registration on Almanac API successfulINFO:     [My Second Agent]: Almanac contract registration is up to date!INFO:     [My Second Agent]: I have received a message from agent1q0kpqwd5q7akt9utnw540h293zhw063ua90m66rkx5lg98wq4zrjkhmgwd7.INFO:     [My Second Agent]: I have received a message Hi Second Agent, this is the first agent..

    abhi@Fetchs-MacBook-Pro testing % python3 first_agent.pyINFO:     [My First Agent]: Starting agent with address: agent1q0kpqwd5q7akt9utnw540h293zhw063ua90m66rkx5lg98wq4zrjkhmgwd7INFO:     [My First Agent]: My name is My First Agent and my address  is agent1q0kpqwd5q7akt9utnw540h293zhw063ua90m66rkx5lg98wq4zrjkhmgwd7INFO:     [My First Agent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A5050&address=agent1q0kpqwd5q7akt9utnw540h293zhw063ua90m66rkx5lg98wq4zrjkhmgwd7INFO:     [My First Agent]: Starting server on http://0.0.0.0:5050 (Press CTRL+C to quit)INFO:     [My First Agent]: Registration on Almanac API successfulINFO:     [My First Agent]: Almanac contract registration is up to date!

In this way we can establish a communicate between two uAgents.</content>
</page>

<page>
  <title>Agent Chat Protocol | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/agent-communication/agent-chat-protocol</url>
  <content>The Agent Chat Protocol is a standardized communication framework that enables agents to exchange messages in a structured and reliable manner. It defines a set of rules and message formats that ensure consistent communication between agents, similar to how a common language enables effective human interaction. This guide demonstrates how to implement and utilize this protocol in your agents.

Understanding the Chat Protocol[â€‹](#understanding-the-chat-protocol "Direct link to Understanding the Chat Protocol")
---------------------------------------------------------------------------------------------------------------------

The chat protocol consists of several key components that work together to enable reliable communication between agents. Let's explore each component:

### 1\. Core Models[â€‹](#1-core-models "Direct link to 1. Core Models")

#### TextContent[â€‹](#textcontent "Direct link to TextContent")

    class TextContent(Model):    type: Literal['text']    text: str

*   Basic content type for text messages
*   Uses `Literal['text']` to ensure type safety
*   `text` field stores the actual message content

#### Resource and ResourceContent[â€‹](#resource-and-resourcecontent "Direct link to Resource and ResourceContent")

    class Resource(Model):    uri: str    metadata: dict[str, str]class ResourceContent(Model):    type: Literal['resource']    resource_id: UUID4    resource: Resource | list[Resource]

*   `Resource`: Represents external resources (files, images, etc.)
    *   `uri`: Location of the resource
    *   `metadata`: Additional resource information
*   `ResourceContent`: Wraps resources in messages
    *   `resource_id`: Unique identifier for tracking
    *   `resource`: Single or multiple resources

#### Metadata Types[â€‹](#metadata-types "Direct link to Metadata Types")

    class Metadata(TypedDict):    mime_type: str    role: strclass MetadataContent(Model):    type: Literal['metadata']    metadata: dict[str, str]

*   `Metadata`: Defines resource metadata structure
    *   `mime_type`: Resource type (e.g., "text/plain")
    *   `role`: Resource's purpose in communication
*   `MetadataContent`: For sending metadata-only messages

### 2\. Session and Stream Management[â€‹](#2-session-and-stream-management "Direct link to 2. Session and Stream Management")

#### Session Control[â€‹](#session-control "Direct link to Session Control")

    class StartSessionContent(Model):    type: Literal['start-session']class EndSessionContent(Model):    type: Literal['end-session']

*   Manages chat session lifecycle
*   `StartSessionContent`: Initiates new sessions
*   `EndSessionContent`: Properly terminates sessions

#### Stream Control[â€‹](#stream-control "Direct link to Stream Control")

    class StartStreamContent(Model):    type: Literal['start-stream']    stream_id: UUID4class EndStreamContent(Model):    type: Literal['start-stream']    stream_id: UUID4

*   Handles continuous data streams
*   `stream_id`: Unique identifier for stream tracking

### 3\. Agent Content Type[â€‹](#3-agent-content-type "Direct link to 3. Agent Content Type")

    AgentContent = (    TextContent    | ResourceContent    | MetadataContent    | StartSessionContent    | EndSessionContent    | StartStreamContent    | EndStreamContent)

*   Combines all possible content types
*   Ensures type safety in message content

### 4\. Message Types[â€‹](#4-message-types "Direct link to 4. Message Types")

#### ChatMessage[â€‹](#chatmessage "Direct link to ChatMessage")

    class ChatMessage(Model):    timestamp: datetime    msg_id: UUID4    content: list[AgentContent]

*   Primary message type for communication
*   `timestamp`: When message was sent (UTC)
*   `msg_id`: Unique message identifier
*   `content`: List of content elements discussed above.

#### ChatAcknowledgement[â€‹](#chatacknowledgement "Direct link to ChatAcknowledgement")

    class ChatAcknowledgement(Model):    timestamp: datetime    acknowledged_msg_id: UUID4    metadata: dict[str, str] | None = None

*   Confirms message receipt
*   `acknowledged_msg_id`: References original message
*   Optional metadata for additional information

### 5\. Message Handlers[â€‹](#5-message-handlers "Direct link to 5. Message Handlers")

    @protocol.on_message(ChatMessage)async def handle_message(_ctx: Context, sender: str, msg: ChatMessage):    print('I got a chat message', sender, msg)@protocol.on_message(ChatAcknowledgement)async def handle_ack(_ctx: Context, sender: str, msg: ChatAcknowledgement):    print('I got a chat acknowledgement', sender, msg)

*   Process incoming messages
*   Handle acknowledgments

Using the Chat Protocol[â€‹](#using-the-chat-protocol "Direct link to Using the Chat Protocol")
---------------------------------------------------------------------------------------------

To implement the chat protocol in your agents, you can import all the above components from the `uagents_core` package:

    from uagents_core.contrib.protocols.chat import (    ChatMessage,    ChatAcknowledgement,    TextContent,    chat_protocol_spec)

Basic Message Flow[â€‹](#basic-message-flow "Direct link to Basic Message Flow")
------------------------------------------------------------------------------

The protocol follows a simple request-response pattern with acknowledgments:

1.  Agent A sends a `ChatMessage` to Agent B
2.  Agent B sends a `ChatAcknowledgement` back to Agent A
3.  Agent B can then send a `ChatMessage` response to Agent A
4.  Agent A sends a `ChatAcknowledgement` back to Agent B

Let's create two agents on [Agentverse](https://agentverse.ai/) that communicate using the chat protocol. Refer to the [Hosted Agents](https://innovationlab.fetch.ai/resources/docs/agent-creation/uagent-creation#hosted-agents) section to understand the detailed steps for agent creation on Agentverse.

### Agent1 Script[â€‹](#agent1-script "Direct link to Agent1 Script")

agent1.py

    from datetime import datetimefrom uuid import uuid4from uagents import Agent, Protocol, Context, Modelfrom time import sleep#import the necessary components from the chat protocolfrom uagents_core.contrib.protocols.chat import (    ChatAcknowledgement,    ChatMessage,    TextContent,    chat_protocol_spec,)# Intialise agent1agent1 = Agent()# Store agent2's address (you'll need to replace this with actual address)agent2_address = "agent1qf8n9q8ndlfvphmnwjzj9p077yq0m6kqc22se9g89y5en22sc38ck4p4e8d"# Initialize the chat protocolchat_proto = Protocol(spec=chat_protocol_spec)#Startup Handler - Print agent details and send initial message@agent1.on_event("startup")async def startup_handler(ctx: Context):    # Print agent details    ctx.logger.info(f"My name is {ctx.agent.name} and my address is {ctx.agent.address}")        # Send initial message to agent2    initial_message = ChatMessage(        timestamp=datetime.utcnow(),        msg_id=uuid4(),        content=[TextContent(type="text", text="Hello from Agent1!")]    )        await ctx.send(agent2_address, initial_message)# Message Handler - Process received messages and send acknowledgements@chat_proto.on_message(ChatMessage)async def handle_message(ctx: Context, sender: str, msg: ChatMessage):    for item in msg.content:        if isinstance(item, TextContent):            # Log received message            ctx.logger.info(f"Received message from {sender}: {item.text}")                        # Send acknowledgment            ack = ChatAcknowledgement(                timestamp=datetime.utcnow(),                acknowledged_msg_id=msg.msg_id            )            await ctx.send(sender, ack)                        # Send response message            response = ChatMessage(                timestamp=datetime.utcnow(),                msg_id=uuid4(),                content=[TextContent(type="text", text="Hello from Agent1!")]            )            await ctx.send(sender, response)# Acknowledgement Handler - Process received acknowledgements@chat_proto.on_message(ChatAcknowledgement)async def handle_acknowledgement(ctx: Context, sender: str, msg: ChatAcknowledgement):    ctx.logger.info(f"Received acknowledgement from {sender} for message: {msg.acknowledged_msg_id}")# Include the protocol in the agent to enable the chat functionality# This allows the agent to send/receive messages and handle acknowledgements using the chat protocolagent1.include(chat_proto, publish_manifest=True)if __name__ == '__main__':    agent1.run()

### Agent2 Script[â€‹](#agent2-script "Direct link to Agent2 Script")

agent2.py

    from datetime import datetimefrom uuid import uuid4from uagents import Agent, Protocol, Context#import the necessary components from the chat protocolfrom uagents_core.contrib.protocols.chat import (    ChatAcknowledgement,    ChatMessage,    TextContent,    chat_protocol_spec,)# Initialise agent2agent2 = Agent()# Initialize the chat protocolchat_proto = Protocol(spec=chat_protocol_spec)# Startup Handler - Print agent details@agent2.on_event("startup")async def startup_handler(ctx: Context):    # Print agent details    ctx.logger.info(f"My name is {ctx.agent.name} and my address is {ctx.agent.address}")# Message Handler - Process received messages and send acknowledgements@chat_proto.on_message(ChatMessage)async def handle_message(ctx: Context, sender: str, msg: ChatMessage):    for item in msg.content:        if isinstance(item, TextContent):            # Log received message            ctx.logger.info(f"Received message from {sender}: {item.text}")                        # Send acknowledgment            ack = ChatAcknowledgement(                timestamp=datetime.utcnow(),                acknowledged_msg_id=msg.msg_id            )            await ctx.send(sender, ack)                        # Send response message            response = ChatMessage(                timestamp=datetime.utcnow(),                msg_id=uuid4(),                content=[TextContent(type="text", text="Hello from Agent2!")]            )            await ctx.send(sender, response)# Acknowledgement Handler - Process received acknowledgements@chat_proto.on_message(ChatAcknowledgement)async def handle_acknowledgement(ctx: Context, sender: str, msg: ChatAcknowledgement):    ctx.logger.info(f"Received acknowledgement from {sender} for message: {msg.acknowledged_msg_id}")# Include the protocol in the agent to enable the chat functionality# This allows the agent to send/receive messages and handle acknowledgements using the chat protocolagent2.include(chat_proto, publish_manifest=True)if __name__ == '__main__':    agent2.run()

### Running the Agents[â€‹](#running-the-agents "Direct link to Running the Agents")

To run the example, you'll need to:

1.  Start Agent2 first:
    
2.  Copy Agent2's address from the startup logs and update it in Agent1's script
    
3.  Start Agent1:
    

### Expected Output[â€‹](#expected-output "Direct link to Expected Output")

When running both agents, you should see output similar to:

Agent2 Logs

Agent1 Logs

This guide demonstrates the communication via chat protocol between two agents hosted on [Agentverse](https://agentverse.ai/). If you wish to run these agents on your local machine instead, you'll need to initialize the agents with specific ports and endpoints:

    # For agent1agent1 = Agent(    name="agent1",    port=8000,    endpoint=["http://localhost:8000/submit"])# For agent2agent2 = Agent(    name="agent2",    port=8001,    endpoint=["http://localhost:8001/submit"])

To learn more about setting up and running agents locally, refer to the [Local Agents](https://innovationlab.fetch.ai/resources/docs/agent-creation/uagent-creation) section of our documentation.</content>
</page>

<page>
  <title>Agentverse API Key | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key</url>
  <content>Getting Agentverse API Key
--------------------------

To get [Agentverse](https://agentverse.ai/) API Key, just login to Agentverse using your gmail address and go to profile section as shown in picture below:

Click on **\+ New API Key** button, give name to your API Key and with write permission to Access to all resources in Agentverse. Your permission can be for 30 days now. Check below image for more clarity on the process :Â 

Click on **Generate API Key**, it will take you through the authentication process again. Once you do the authentication and Allow Access the API key will pop-up on your screen like the image below and save it somewhere as it cannot be regenerated.

Keep your API Key stored somewhere safe. You wonâ€™t be able to access it again.</content>
</page>

<page>
  <title>TransactAI Payment Example | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/examples/transactAI/</url>
  <content>This example demonstrates how to use TransactAI for agent-to-agent payments on Agentverse with [TransactAI](https://agentverse.ai/agents/details/agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrq/profile). The example shows a complete payment flow between two agents: Alice (sender) and Bob (receiver).

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

The example demonstrates the following:

1.  Registering agents with TransactAI
2.  Linking on-chain wallet addresses
3.  Making on-chain deposits to fund the TransactAI account
4.  Sending payments between agents
5.  Receiving payments
6.  Withdrawing funds to on-chain wallets

Workflow Diagram[â€‹](#workflow-diagram "Direct link to Workflow Diagram")
------------------------------------------------------------------------

1.  **Registration Process**: Before using TransactAI, agents must register themselves and their wallet addresses.
    
2.  **Denominations**: All amounts are expressed in "atestfet" (smallest unit), where 1 TESTFET = 10^18 atestfet.
    
3.  **Deposit Process**:
    
    *   Make an on-chain deposit to TransactAI's wallet
    *   Notify TransactAI with the transaction hash
    *   Wait for TransactAI to confirm the deposit (may require several blockchain confirmations)
4.  **Payment Flow**:
    
    *   Sender sends payment command to TransactAI
    *   TransactAI updates internal balances
    *   TransactAI sends confirmation to sender
    *   TransactAI sends notification to recipient
5.  **Withdrawal Process**:
    
    *   Send withdrawal request to TransactAI with amount and wallet address
    *   TransactAI executes on-chain transaction
    *   TransactAI sends confirmation with transaction hash
6.  **Acknowledgements**: All messages must be acknowledged to confirm receipt.
    

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

*   [Agnetverse](https://agentverse.ai/) account.
*   Access to Fetch.ai Dorado testnet.
*   Testnet tokens (can be obtained from [Fetch.ai Dorado Faucet](https://companion.fetch.ai/dorado-1/accounts/fetch1n9vnna29c8us5pk9fxqhzltmgpeyn3hl4le28h#Transactions))

Example Code[â€‹](#example-code "Direct link to Example Code")
------------------------------------------------------------

### agent\_protocol.py[â€‹](#agent_protocolpy "Direct link to agent_protocol.py")

First, we need the custom protocol that enables communication with TransactAI:

agent\_protocol.py

    """Custom Agent Protocol for TransactAIThis protocol allows agents to communicate with the TransactAI payment agentby defining message structures and content types."""from uagents import Protocolfrom datetime import datetimefrom typing import Literal, TypedDict, Dict, List, Unionimport uuidfrom pydantic.v1 import UUID4, Fieldfrom uagents_core.models import Modelfrom uagents_core.protocol import ProtocolSpecification# --- Content Model Definitions ---class Metadata(TypedDict, total=False):    mime_type: str    role: strclass TextContent(Model):    type: Literal["text"] = "text"    text: strclass MetadataContent(Model):    type: Literal["metadata"] = "metadata"    metadata: dict[str, str]# Combined content typesAgentContent = Union[TextContent, MetadataContent]# --- Main Protocol Message Models ---class AgentMessage(Model):    timestamp: datetime = Field(default_factory=datetime.utcnow)    msg_id: UUID4 = Field(default_factory=uuid.uuid4)    content: list[AgentContent]class AgentAcknowledgement(Model):    timestamp: datetime = Field(default_factory=datetime.utcnow)    acknowledged_msg_id: UUID4    metadata: dict[str, str] | None = None# --- Protocol Specification ---agent_protocol_spec = ProtocolSpecification(    name="AgentProtocol",    version="1.0.0",    interactions={        AgentMessage: {AgentAcknowledgement},        AgentAcknowledgement: set(),    },)# --- Protocol Instance ---agent_proto = Protocol(spec=agent_protocol_spec)# --- Helper Functions ---def create_metadata_message(metadata: Dict[str, str]) -> AgentMessage:    """Create an agent message with metadata content"""    return AgentMessage(        content=[MetadataContent(metadata=metadata)]    )def create_text_message(text: str) -> AgentMessage:    """Create an agent message with text content"""    return AgentMessage(        content=[TextContent(text=text)]    )

### Alice (Sender) Agent[â€‹](#alice-sender-agent "Direct link to Alice (Sender) Agent")

alice\_agent.py

    """Alice Agent - Sends payments via TransactAI"""import asynciofrom uagents import Agent, Contextfrom uagents.network import get_ledgerfrom datetime import datetime# Import the custom agent protocol# Ensure agent_protocol.py is in the same directory or accessible in PYTHONPATHtry:    from agent_protocol import (        agent_proto,        AgentMessage,        AgentAcknowledgement,        create_metadata_message    )except ImportError:    print("Error: agent_protocol.py not found. Please ensure it's in the correct path.")    # Define minimal models if import fails, to allow basic understanding    from uagents import Model, Protocol    from typing import List, Dict, Union, Literal, Optional    from pydantic.v1 import Field, UUID4    import uuid    class MetadataContent(Model):        type: Literal["metadata"] = "metadata"        metadata: dict[str, str]    AgentContent = Union[MetadataContent]    class AgentMessage(Model):        timestamp: datetime = Field(default_factory=datetime.utcnow)        msg_id: UUID4 = Field(default_factory=uuid.uuid4)        content: list[AgentContent]    class AgentAcknowledgement(Model):        timestamp: datetime = Field(default_factory=datetime.utcnow)        acknowledged_msg_id: UUID4        metadata: Optional[dict[str, str]] = None    def create_metadata_message(metadata: Dict[str, str]) -> AgentMessage:        return AgentMessage(content=[MetadataContent(metadata=metadata)])    # Define a dummy protocol if needed for basic structure    agent_proto = Protocol("DummyAgentProto", version="1.0")# TransactAI agent address (ensure this is correct)TRANSACTAI_ADDRESS = "agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrq"TRANSACTAI_WALLET = "fetch1uyxsdlejg7axp4dzmqpq54g0uwde5nv6fflhkv" # TransactAI's on-chain wallet# Bob's agent addressBOB_ADDRESS = "agent1q2v8ck694z7h6qww90kvv4qafrnvlaquwpeuqtarsyhdmsve4v6k7r0a8l0" # Replace it with your bob's address # Alice agent setup# CRITICAL: Replace this seed phrase with your own secure one for actual use!alice = Agent()DEPOSIT_CONFIRMED_FLAG = "deposit_confirmed"PAYMENT_ATTEMPTED_FLAG = "payment_attempted"@alice.on_event("startup")async def startup(ctx: Context):    ctx.logger.info(f"Alice started. Address: {alice.address}")    ctx.logger.info(f"Wallet address: {alice.wallet.address()}")    # Initialize flags    ctx.storage.set(DEPOSIT_CONFIRMED_FLAG, False)    ctx.storage.set(PAYMENT_ATTEMPTED_FLAG, False)    # 1. Register Agent with TransactAI    ctx.logger.info("Registering agent with TransactAI...")    register_msg = create_metadata_message({'command': 'register'})    await ctx.send(TRANSACTAI_ADDRESS, register_msg)    await asyncio.sleep(2.0) # Wait for agent registration    # 2. Register Wallet with TransactAI    ctx.logger.info("Registering wallet with TransactAI...")    wallet_address = str(alice.wallet.address()) # Get Alice's wallet address    register_wallet_msg = create_metadata_message({        'command': 'register_wallet',        'wallet_address': wallet_address    })    await ctx.send(TRANSACTAI_ADDRESS, register_wallet_msg)    await asyncio.sleep(5.0) # Wait for wallet registration    # 3. On-chain deposit to TransactAI wallet    ctx.logger.info("Sending on-chain deposit to TransactAI wallet...")    deposit_amount = 100000000000000000 # 0.1 testfet (needs enough for payment)    tx_hash = None    try:        ledger = get_ledger("dorado") # Get ledger instance right before use        # Ensure wallet has funds from faucet: https://companion.fetch.ai/dorado-1/accounts        ctx.logger.info(f"Attempting to send {deposit_amount} atestfet to {TRANSACTAI_WALLET}")        tx = ledger.send_tokens(TRANSACTAI_WALLET, deposit_amount, "atestfet", alice.wallet)        result = tx.wait_to_complete()        tx_hash = result.tx_hash        ctx.logger.info(f"Deposit transaction hash: {tx_hash}")    except Exception as e:        ctx.logger.error(f"Error sending on-chain deposit: {e}")        ctx.logger.error("Ensure Alice's wallet has sufficient 'atestfet' from the faucet.")        return    # 4. Send deposit confirmation command to TransactAI    if tx_hash:        ctx.logger.info(f"Sending deposit confirmation command for tx_hash: {tx_hash}")        deposit_confirm_msg = create_metadata_message({            'command': 'deposit',            'tx_hash': tx_hash,            'amount': str(deposit_amount),            'denom': "atestfet"        })        await ctx.send(TRANSACTAI_ADDRESS, deposit_confirm_msg)        # Wait for deposit confirmation state change        MAX_WAIT_TIME = 60 # seconds        WAIT_INTERVAL = 5 # seconds        time_waited = 0        deposit_confirmed = False        while time_waited < MAX_WAIT_TIME:            if ctx.storage.get(DEPOSIT_CONFIRMED_FLAG) is True:                deposit_confirmed = True                ctx.logger.info("Deposit confirmed by TransactAI.")                break            ctx.logger.info(f"Waiting for deposit confirmation... ({time_waited}/{MAX_WAIT_TIME}s)")            await asyncio.sleep(WAIT_INTERVAL)            time_waited += WAIT_INTERVAL        if not deposit_confirmed:            ctx.logger.error("Timed out waiting for deposit confirmation from TransactAI.")            return # Stop if deposit not confirmed    else:        ctx.logger.error("On-chain deposit failed, cannot proceed.")        return # Stop if deposit failed    # 5. Send payment to Bob via TransactAI (only if deposit confirmed and payment not already attempted)    # This part is triggered by handle_transactai_response upon successful deposit confirmation    # We call maybe_send_payment here just in case the confirmation message arrived *during* the wait loop    await maybe_send_payment(ctx)# Handle responses from TransactAI@agent_proto.on_message(model=AgentMessage)async def handle_transactai_response(ctx: Context, sender: str, msg: AgentMessage):    ctx.logger.info(f"Received message from {sender}")    response_handled = False # Flag to ensure ack is sent even if no specific handler matches    for content in msg.content:        if content.type == "metadata":            metadata = content.metadata            ctx.logger.info(f"Metadata: {metadata}")                        command = metadata.get('command')            status = metadata.get('status')            if command == 'register_response':                 ctx.logger.info(f"Registration response: {status}")                 response_handled = True            elif command == 'register_wallet_response':                 ctx.logger.info(f"Wallet registration response: {status}")                 response_handled = True            elif command == 'deposit_response':                 ctx.logger.info(f"Deposit response received: {metadata}")                 if status == 'success':                     ctx.storage.set(DEPOSIT_CONFIRMED_FLAG, True)                     # If payment hasn't been attempted yet, trigger it now                     asyncio.create_task(maybe_send_payment(ctx))                 elif status == 'pending_confirmation':                     ctx.logger.info("Deposit is still pending confirmation.")                 else: # Failed                     ctx.logger.error(f"Deposit failed: {metadata.get('reason')}")                     # Consider setting a 'deposit_failed' flag if needed                 response_handled = True            elif command == 'payment_confirmation':                 if status == 'success':                     ctx.logger.info(f"Payment successful! New balance: {metadata.get('balance')}")                 else:                     ctx.logger.error(f"Payment failed! Reason: {metadata.get('reason')}, Balance: {metadata.get('balance')}")                 response_handled = True            # Handle other responses if needed (e.g., balance_response)                elif content.type == "text":            ctx.logger.info(f"Text: {content.text}")            response_handled = True # Acknowledge text messages too    # Send acknowledgement back to TransactAI if message was processed    if response_handled:        await ctx.send(sender, AgentAcknowledgement(            timestamp=datetime.utcnow(),            acknowledged_msg_id=msg.msg_id        ))    else:        ctx.logger.warning(f"Received unhandled message content types from {sender}: {[c.type for c in msg.content]}")# Separate function to attempt payment after deposit confirmationasync def maybe_send_payment(ctx: Context):    # Ensure deposit is confirmed and payment hasn't been attempted    if ctx.storage.get(DEPOSIT_CONFIRMED_FLAG) is True and not ctx.storage.get(PAYMENT_ATTEMPTED_FLAG):        payment_amount = 100000000000000000 # Example amount (0.1 atestfet)        ctx.logger.info(f"Deposit confirmed, now attempting to pay {payment_amount} to Bob ({BOB_ADDRESS})...")        ctx.storage.set(PAYMENT_ATTEMPTED_FLAG, True) # Mark as attempted        payment_msg = create_metadata_message({            'command': 'payment',            'recipient': BOB_ADDRESS,            'amount': str(payment_amount),            'reference': f"payment-{datetime.utcnow().isoformat()}"        })        await ctx.send(TRANSACTAI_ADDRESS, payment_msg)# Handle acknowledgements (optional)@agent_proto.on_message(model=AgentAcknowledgement)async def handle_ack(ctx: Context, sender: str, msg: AgentAcknowledgement):    ctx.logger.info(f"Received acknowledgement from {sender} for message {msg.acknowledged_msg_id}")# Include the protocolalice.include(agent_proto)if __name__ == "__main__":    print(f"Alice starting. Address: {alice.address}")    print("Ensure agent_protocol.py is accessible.")    print("CRITICAL: Replace the example ALICE_SEED in the code if using for anything beyond this demo.")    alice.run()

### Bob (Receiver) Agent[â€‹](#bob-receiver-agent "Direct link to Bob (Receiver) Agent")

bob\_agent.py

    """Bob Agent - Receives payments via TransactAI"""import asynciofrom uagents import Agent, Contextfrom datetime import datetime# Import the custom agent protocolfrom agent_protocol import (    agent_proto,    AgentMessage,    AgentAcknowledgement,    create_metadata_message)# TransactAI agent address (ensure this is correct)TRANSACTAI_ADDRESS = "agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrq"# Bob agent setupbob = Agent()@bob.on_event("startup")async def startup(ctx: Context):    ctx.logger.info(f"Bob started. Address: {bob.address}")    ctx.logger.info(f"Wallet address: {bob.wallet.address()}")    # Give agents time to register etc.    await asyncio.sleep(5.0)    # 1. Register Agent with TransactAI    ctx.logger.info("Registering agent with TransactAI...")    register_msg = create_metadata_message({'command': 'register'})    await ctx.send(TRANSACTAI_ADDRESS, register_msg)    await asyncio.sleep(2.0) # Wait briefly    # 2. Register Wallet with TransactAI    ctx.logger.info("Registering wallet with TransactAI...")    wallet_address = str(bob.wallet.address()) # Get Bob's wallet address    register_wallet_msg = create_metadata_message({        'command': 'register_wallet',        'wallet_address': wallet_address    })    await ctx.send(TRANSACTAI_ADDRESS, register_wallet_msg)    await asyncio.sleep(2.0) # Wait briefly# Handle responses/notifications from TransactAI@agent_proto.on_message(model=AgentMessage)async def handle_transactai_message(ctx: Context, sender: str, msg: AgentMessage):    ctx.logger.info(f"Received message from {sender}")    for content in msg.content:        if content.type == "metadata":            metadata = content.metadata            ctx.logger.info(f"Metadata: {metadata}")                        command = metadata.get('command')            status = metadata.get('status')            if command == 'register_response':                 ctx.logger.info(f"Registration response: {status}")            elif command == 'payment_received':                 ctx.logger.info(f"Payment received from {metadata.get('from')}!")                 amount_received_str = metadata.get('amount')                 ctx.logger.info(f"Amount: {amount_received_str}, Reference: {metadata.get('reference')}")                 ctx.logger.info(f"New balance: {metadata.get('balance')}")                 # Attempt to withdraw the received amount                 try:                     amount_to_withdraw = int(amount_received_str)                     if amount_to_withdraw > 0:                         ctx.logger.info(f"Attempting to withdraw received amount: {amount_to_withdraw}")                         withdraw_msg = create_metadata_message({                             'command': 'withdraw',                             'amount': str(amount_to_withdraw),                             'wallet_address': str(bob.wallet.address()), # Bob's own wallet                             'denom': "atestfet"                         })                         # Send withdrawal request asynchronously                         asyncio.create_task(ctx.send(TRANSACTAI_ADDRESS, withdraw_msg))                     else:                         ctx.logger.info("Received payment amount is zero or invalid, not withdrawing.")                 except (ValueError, TypeError) as e:                     ctx.logger.error(f"Could not parse amount for withdrawal: {amount_received_str}, Error: {e}")            elif command == 'withdraw_confirmation':                 ctx.logger.info(f"Withdrawal confirmation received: {metadata}")                 if status == 'success':                     ctx.logger.info(f"Withdrawal successful! Tx: {metadata.get('tx_hash')}, New balance: {metadata.get('balance')}")                 else:                     ctx.logger.error(f"Withdrawal failed! Reason: {metadata.get('reason')}")            # Handle other relevant messages like escrow notifications if needed                elif content.type == "text":            ctx.logger.info(f"Text: {content.text}")    # Send acknowledgement back to TransactAI    await ctx.send(sender, AgentAcknowledgement(        timestamp=datetime.utcnow(),        acknowledged_msg_id=msg.msg_id    ))# Handle acknowledgements (optional)@agent_proto.on_message(model=AgentAcknowledgement)async def handle_ack(ctx: Context, sender: str, msg: AgentAcknowledgement):    ctx.logger.info(f"Received acknowledgement from {sender} for message {msg.acknowledged_msg_id}")# Include the protocolbob.include(agent_proto)if __name__ == "__main__":    print(f"Bob starting. Address: {bob.address}")    bob.run()

Running the Example[â€‹](#running-the-example "Direct link to Running the Example")
---------------------------------------------------------------------------------

To run this example:

1.  Create two agents Alice and Bob on Agentverse.
2.  Create `agent_protocol.py` in both the agents and put in the code from above.
3.  Get some test tokens for the sender's wallet from the [Fetch.ai Dorado Faucet](https://companion.fetch.ai/dorado-1/accounts)
4.  Start the receiver agent (bob) first from agentverse.
5.  Copy the bob's address and update `RECIPIENT_ADDRESS` in alice's agent.py
6.  Start the alice agent as well.

Expected Output[â€‹](#expected-output "Direct link to Expected Output")
---------------------------------------------------------------------

### Receiver Output (Bob)[â€‹](#receiver-output-bob "Direct link to Receiver Output (Bob)")

    Receiver agent started. Address: agent1q2v8ck694z7h6qww90kvv4qafrnvlaquwpeuqtarsyhdmsve4v6k7r0a8l0Wallet address: fetch1ljd6tn7kr27wv0jwn6g63a7x38zj5ltfvwm8dwRegistering with TransactAI...Registering wallet fetch1ljd6tn7kr27wv0jwn6g63a7x38zj5ltfvwm8dw...âœ… Setup complete - Ready to receive paymentsReceived message from agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrqMetadata: {'command': 'register_response', 'status': 'success', 'balance': '0'}Registration response: successReceived message from agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrqMetadata: {'command': 'register_wallet_response', 'status': 'success', 'wallet_address': 'fetch1ljd6tn7kr27wv0jwn6g63a7x38zj5ltfvwm8dw'}Wallet registration response: successReceived message from agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrqMetadata: {'command': 'payment_received', 'from': 'agent1q2gxyqnwyr85v6ek7rplk09etnqgn4cfllph6gx3gtsuqzzn5p856gway5s', 'amount': '50000000000000000', 'reference': 'Example payment 2025-04-29T13:18:27.441861', 'balance': '5.0E+16'}ğŸ’° Payment received!  From: agent1q2gxyqnwyr85v6ek7rplk09etnqgn4cfllph6gx3gtsuqzzn5p856gway5s  Amount: 50000000000000000 atestfet  Reference: Example payment 2025-04-29T13:18:27.441861  New balance: 5.0E+16Withdrawing 50000000000000000 atestfet to fetch1ljd6tn7kr27wv0jwn6g63a7x38zj5ltfvwm8dw...Received message from agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrqMetadata: {'command': 'withdraw_confirmation', 'status': 'success', 'amount': '50000000000000000', 'wallet_address': 'fetch1ljd6tn7kr27wv0jwn6g63a7x38zj5ltfvwm8dw', 'tx_hash': 'A44295DCE532825CE257D63C79A63C01180FCC8DEF44258271C5E5FB9AE0F561', 'balance': '0', 'message': 'Withdrawal processed. Funds sent to your wallet.'}âœ… Withdrawal successful!  Transaction hash: A44295DCE532825CE257D63C79A63C01180FCC8DEF44258271C5E5FB9AE0F561  New balance: 0

### Sender Output[â€‹](#sender-output "Direct link to Sender Output")

    Sender agent started. Address: agent1q2gxyqnwyr85v6ek7rplk09etnqgn4cfllph6gx3gtsuqzzn5p856gway5sWallet address: fetch1n9vnna29c8us5pk9fxqhzltmgpeyn3hl4le28hRegistering with TransactAI...Registering wallet fetch1n9vnna29c8us5pk9fxqhzltmgpeyn3hl4le28h...Making on-chain deposit of 100000000000000000 atestfet...Attempting to send 100000000000000000 atestfet to fetch1uyxsdlejg7axp4dzmqpq54g0uwde5nv6fflhkvDeposit transaction hash: AE4A482EFE4215D6A54C63D1F4EE9EFA50437572BDECBD347FAA752BD9B3E0FAWaiting for deposit confirmation... (0/60s)Received message from agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrqMetadata: {'command': 'register_response', 'status': 'success', 'balance': '0'}Registration response: successReceived message from agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrqMetadata: {'command': 'register_wallet_response', 'status': 'success', 'wallet_address': 'fetch1n9vnna29c8us5pk9fxqhzltmgpeyn3hl4le28h'}Wallet registration response: successReceived message from agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrqMetadata: {'command': 'deposit_response', 'status': 'pending_confirmation', 'reason': 'Awaiting confirmations (2/6)'}Deposit pending: Awaiting confirmations (2/6)Waiting for deposit confirmation... (5/60s)Waiting for deposit confirmation... (10/60s)Received message from agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrqMetadata: {'command': 'deposit_response', 'status': 'success', 'amount': '100000000000000000', 'denom': 'atestfet', 'balance': '1.0E+17', 'tx_hash': 'AE4A482EFE4215D6A54C63D1F4EE9EFA50437572BDECBD347FAA752BD9B3E0FA'}Deposit confirmed! New balance: 1.0E+17âœ… Deposit confirmed!Sending payment of 50000000000000000 atestfet to agent1q2v8ck694z7h6qww90kvv4qafrnvlaquwpeuqtarsyhdmsve4v6k7r0a8l0...Received message from agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrqMetadata: {'command': 'payment_confirmation', 'status': 'success', 'recipient': 'agent1q2v8ck694z7h6qww90kvv4qafrnvlaquwpeuqtarsyhdmsve4v6k7r0a8l0', 'amount': '50000000000000000', 'balance': '5.0E+16'}Payment successful! New balance: 5.0E+16

Additional Features[â€‹](#additional-features "Direct link to Additional Features")
---------------------------------------------------------------------------------

In addition to basic payments, TransactAI also supports:

*   **Escrow Services**: Hold funds conditionally until released by the sender
*   **Balance Queries**: Check your current balance at any time
*   **Automated Withdrawals**: Set up automatic withdrawals for received payments

Refer to the complete [TransactAI documentation](https://innovationlab.fetch.ai/resources/docs/agent-transaction/agent-transaction) for more details on these advanced features.</content>
</page>

<page>
  <title>Search Agent and MicroServices | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/agentverse/searching</url>
  <content>Searching microservices and AI Agents on Agentverse
---------------------------------------------------

When you want to discover or connect with microservices or AI agents dynamically on Agentverse, you can use the **Agentverse Search API**. Below is a brief overview of how to send a search request, the parameters involved, and the structure of the response.

Making a Search Request[â€‹](#making-a-search-request "Direct link to Making a Search Request")
---------------------------------------------------------------------------------------------

*   Python
*   Curl
*   JavaScript

    body = {    "filters": {        "state": [],        "category": [],        "agent_type": [],        "protocol_digest": []    },    "sort": "relevancy",    "direction": "asc",    "search_text": "<string>",    "offset": 0,    "limit": 1,}await fetch("https://agentverse.ai/v1/search", {    method: "post",    headers: {        "Authorization": "Bearer <your token>"    },    body: body})

Response[â€‹](#response "Direct link to Response")
------------------------------------------------

You will receive a list of JSON objects with details about each agent:

    [  {    "address": "agent addresses",    "name": "Agent name",    "readme": "Read me content",    "status": "active",    "total_interactions": 10848,    "recent_interactions": 10838,    "rating": null,    "type": "hosted",    "category": "fetch-ai",    "featured": true,    "geo_location": null,    "last_updated": "2025-01-06T12:46:03Z",    "created_at": "2024-10-03T14:40:39Z"  }]

### Available Filters[â€‹](#available-filters "Direct link to Available Filters")

*   **state** : `active`, `inactive`.
*   **category** : `fetch-ai`, `community`.
*   **agent\_type** : `hosted`, `local`, `mailbox`, `proxy`, `custom`.
*   **protocol\_digest** : The protocol in which agent is included into.
*   **model\_digest** : Model digest in which agent is included into.

### Importance of Good Readme[â€‹](#importance-of-good-readme "Direct link to Importance of Good Readme")

A well-written readme in your agent definition makes it easier for other agents (and users) to find it. Make sure you:

*   Include descriptive names, tags, or domains. You can mention `[tags : ]` and `[domain : ]` in your agent.
    
*   Describe the main functions or services the agent provides.
    
*   Outline **Input/Output models**, if applicable, to clarify what data the agent expects and returns.
    

A good readme looks like below :

    ![tag:innovationlab](https://img.shields.io/badge/innovationlab-3D8BD3)![tag:domain/tag-of-your-agent](https://img.shields.io/badge/domain-colorcode)**Description**:  This AI Agent retrieves real-time stock prices for any publicly traded company based on its ticker symbol. It provides  share prices, stock quotes, and stock prices to users. Simply input a stock ticker (e.g., AAPL, TSLA)  to get the latest stock price.**Input Data Model**class StockPriceRequest(Model):    ticker: str**Output Data Model**class StockPriceResponse(Model):    price: float

By following these guidelines, you can improve your agentâ€™s visibility in search results and help others understand its capabilities and usage requirements.

### To Include readme in your agent :[â€‹](#to-include-readme-in-your-agent- "Direct link to To Include readme in your agent :")

1.  Go to your agent's profile in Agentverse and click on `Overview` button.
    
2.  Click on Edit button.
    

3.  Make the changes and click on `save` button.

note

**Note:** If you are creating your agents in **`Hackathon`**, do remember to include the innovation labs tags.

    ![tag:innovationlab](https://img.shields.io/badge/innovationlab-3D8BD3)

Please include domain tag to your agent like below,

`![tag:domain/tag-of-your-agent](https://img.shields.io/badge/domain-colorcode)`</content>
</page>

<page>
  <title>End to End Application with Agentverse Architecture | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-based-application</url>
  <content>This diagram depicts an agent-based architecture, where the Client Application interacts with a Prime Agent to discover and utilize various specialized Agents registered within the AgentVerse. The Prime Agent orchestrates the communication between the Client and the Agents to fulfill the client's requests.

The flow of the system is as follows:

1.  The client application searches the AgentVerse to discover available agents.
2.  The client application sends a request to the Prime Agent.
3.  The Prime Agent communicates with the AgentVerse to find the appropriate Agent.
4.  The Prime Agent sends the request to the Agent.
5.  The Agent processes the request and sends the response back to the Prime Agent.
6.  The Prime Agent returns the final response to the client application.

This architecture allows for a modular and extensible system, where new agents can be easily integrated, and the Prime Agent can orchestrate the interactions between agents to fulfill complex client requests.

Core Components and Implementation[â€‹](#core-components-and-implementation "Direct link to Core Components and Implementation")
------------------------------------------------------------------------------------------------------------------------------

The system enables intelligent financial analysis through a team of specialized agents, coordinated via Agentverse. It combines frontend user interaction with backend processing through multiple agent layers.

### 1\. React Frontend (Client Application)[â€‹](#1-react-frontend-client-application "Direct link to 1. React Frontend (Client Application)")

*   React-based user interface (OptimusPrime component)
*   Two main API endpoints:
    *   `/api/send-request`: Sends analysis queries
    *   `/api/get-response`: Polls for results
*   Handles message display and user interactions

### 2\. Primary Agent (Query Router)[â€‹](#2-primary-agent-query-router "Direct link to 2. Primary Agent (Query Router)")

*   **Port**: 5001
*   **Role**: Routes queries to Financial Analysis Agent
*   **Key Functions**:
    *   Discovers Financial Analysis Agent through Agentverse
    *   Forwards user queries
    *   Manages response polling
*   **Endpoints**:
    *   `/webhook`: Receives responses from Financial Agent

### 3\. Financial Analysis Agent[â€‹](#3-financial-analysis-agent "Direct link to 3. Financial Analysis Agent")

*   **Port**: 5008
*   **Role**: Processes financial analysis requests
*   **Components**:
    *   Supervisor Agent: Coordinates analysis
    *   Search Agent: Handles market research
    *   SEC Analyst: Processes SEC filings
*   **Tools**:
    *   RAG System for document analysis
    *   Tavily Search for market data

### 4\. Agentverse Integration[â€‹](#4-agentverse-integration "Direct link to 4. Agentverse Integration")

*   **Agent Discovery**: Allows finding agents by capability
*   **Agent Registry**: Manages agent registration and lookup
*   **Message Routing**: Handles inter-agent communication

Communication Flow[â€‹](#communication-flow "Direct link to Communication Flow")
------------------------------------------------------------------------------

1.  User submits query through React UI
2.  Primary Agent searches for Financial Analysis Agent
3.  Primary Agent forwards query to Financial Agent
4.  Financial Agent processes query using specialist team
5.  Response returns through Agentverse to Primary Agent
6.  Frontend polls Primary Agent for results
7.  Results displayed to user

Key Implementation Details[â€‹](#key-implementation-details "Direct link to Key Implementation Details")
------------------------------------------------------------------------------------------------------

### Frontend[â€‹](#frontend "Direct link to Frontend")

    // Sends request and starts pollingconst handleSendMessage = async () => {    await fetch('/api/send-request', {...});    startPolling();  // Begin checking for response};

### Primary Agent[â€‹](#primary-agent "Direct link to Primary Agent")

    # Agent discovery and routingdef find_financial_agent():    available_ais = fetch.ai("Financial Analysis Agent")    return available_ais.get('ais', [0])@app.route('/webhook', methods=['POST'])def webhook():    message = parse_message_from_agent(data)    primary_agent.latest_response = message.payload

### Financial Agent registration with Agentverse[â€‹](#financial-agent-registration-with-agentverse "Direct link to Financial Agent registration with Agentverse")

    # Agent registrationregister_with_agentverse(    identity=financial_identity,    url="http://localhost:5008/webhook",    agent_title="Financial Analysis Agent",    readme="..."  # Capabilities description)

Detailed Implementation Details
-------------------------------

1\. Frontend Implementation (React)[â€‹](#1-frontend-implementation-react "Direct link to 1. Frontend Implementation (React)")
----------------------------------------------------------------------------------------------------------------------------

### Message Handling and UI State[â€‹](#message-handling-and-ui-state "Direct link to Message Handling and UI State")

    const OptimusPrime = () => {    const [messages, setMessages] = useState([]);    const [inputText, setInputText] = useState('');    const [isProcessing, setIsProcessing] = useState(false);    // Handles submitting new messages    const handleSendMessage = async () => {        if (!inputText.trim() || isProcessing) return;        // Add user message to UI        const userMessage = {            type: 'user',            content: inputText,            timestamp: new Date().toLocaleTimeString()        };        setMessages(prev => [...prev, userMessage]);        setInputText('');        setIsProcessing(true);        try {            // Send request to primary agent                        await fetch('/api/send-request', {                method: 'POST',                headers: { 'Content-Type': 'application/json' },                body: JSON.stringify({ input: inputText }),            });            // Start polling for response            startPollingForResponse();        } catch (error) {            handleError(error);        }    };

### Response Polling System[â€‹](#response-polling-system "Direct link to Response Polling System")

        // Polls for agent response    const startPollingForResponse = () => {        const pollInterval = setInterval(async () => {            try {                const responseData = await fetch('/api/get-response');                const data = await responseData.json();                if (data.status !== 'waiting' && data.analysis_result) {                    clearInterval(pollInterval);                    setIsProcessing(false);                    // Process agent responses                    data.analysis_result.analysis.forEach(response => {                        setMessages(prev => [...prev, {                            type: 'agent',                            agentName: response.name || 'Agent',                            content: response.content,                            timestamp: new Date().toLocaleTimeString()                        }]);                    });                }            } catch (error) {                clearInterval(pollInterval);                setIsProcessing(false);                handleError(error);            }        }, 1000);    };

2\. Primary Agent Implementation[â€‹](#2-primary-agent-implementation "Direct link to 2. Primary Agent Implementation")
---------------------------------------------------------------------------------------------------------------------

### Agent Setup and Initialization[â€‹](#agent-setup-and-initialization "Direct link to Agent Setup and Initialization")

    class PrimaryAgent:    def __init__(self):        self.identity = None        self.latest_response = None        def initialize(self):        try:            # Initialize agent identity            self.identity = Identity.from_seed(                os.getenv("PRIMARY_AGENT_KEY"),                 0            )                        # Register with Agentverse            register_with_agentverse(                identity=self.identity,                url="http://localhost:5001/webhook",                agentverse_token=os.getenv("AGENTVERSE_API_KEY"),                agent_title="Financial Query Router",                readme="<description>Routes queries to Financial Analysis Agent</description>"            )        except Exception as e:            logger.error(f"Initialization error: {e}")            raise

### Financial Agent Discovery and Communication[â€‹](#financial-agent-discovery-and-communication "Direct link to Financial Agent Discovery and Communication")

        def find_financial_agent(self):        """Find registered financial analysis agent"""        try:            # Search for financial agent in Agentverse            available_ais = fetch.ai("Financial Analysis Agent")            agents = available_ais.get('ais', [])                        if agents:                logger.info(f"Found financial agent at address: {agents[0]['address']}")                return agents[0]            return None                    except Exception as e:            logger.error(f"Error finding financial agent: {e}")            return None    @app.route('/api/send-request', methods=['POST'])    def send_request():        try:            # Extract user query            data = request.json            user_input = data.get('input')                        # Find and validate financial agent            agent = primary_agent.find_financial_agent()            if not agent:                return jsonify({"error": "Financial analysis agent not available"}), 404                        # Forward request to financial agent            send_message_to_agent(                primary_agent.identity,                agent['address'],                {"request": user_input}            )                        return jsonify({"status": "request_sent"})                    except Exception as e:            logger.error(f"Error processing request: {e}")            return jsonify({"error": str(e)}), 500

### Response Management[â€‹](#response-management "Direct link to Response Management")

        @app.route('/webhook', methods=['POST'])    def webhook():        try:            # Parse incoming agent message            data = request.get_data().decode("utf-8")            message = parse_message_from_agent(data)                        # Store response for polling            primary_agent.latest_response = message.payload                        return jsonify({"status": "success"})                    except Exception as e:            logger.error(f"Error in webhook: {e}")            return jsonify({"error": str(e)}), 500    @app.route('/api/get-response', methods=['GET'])    def get_response():        try:            if primary_agent.latest_response:                response = primary_agent.latest_response                primary_agent.latest_response = None                return jsonify(response)            return jsonify({"status": "waiting"})        except Exception as e:            logger.error(f"Error getting response: {e}")            return jsonify({"error": str(e)}), 500

3\. Financial Analysis Agent Registration[â€‹](#3-financial-analysis-agent-registration "Direct link to 3. Financial Analysis Agent Registration")
------------------------------------------------------------------------------------------------------------------------------------------------

### Agent Registration and Setup[â€‹](#agent-registration-and-setup "Direct link to Agent Registration and Setup")

    def init_agent():    """Initialize and register the agent with agentverse"""    global financial_identity, research_chain    try:        # Initialize the research chain        research_chain = init_financial_system()                # Initialize identity and register with Agentverse        financial_identity = Identity.from_seed(            os.getenv("FINANCIAL_AGENT_KEY"),             0        )                # Register with detailed capabilities description        register_with_agentverse(            identity=financial_identity,            url="http://localhost:5008/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="Financial Analysis Agent",            readme = """                <description>A comprehensive financial analysis agent that combines                 SEC filing analysis with real-time market data for Apple Inc.</description>                <use_cases>                    <use_case>Get detailed revenue analysis from SEC filings</use_case>                    <use_case>Analyze risk factors from latest 10-K</use_case>                    <use_case>Track financial metrics and trends</use_case>                </use_cases>                <payload_requirements>                    <payload>                        <requirement>                            <parameter>query</parameter>                            <description>What would you like to know about Apple's financials?</description>                        </requirement>                    </payload>                </payload_requirements>            """        )

### Query Processing and Response[â€‹](#query-processing-and-response "Direct link to Query Processing and Response")

    @app.route('/webhook', methods=['POST'])def webhook():    try:        # Parse incoming message        data = request.get_data().decode('utf-8')        message = parse_message_from_agent(data)        query = message.payload.get("request", "")        agent_address = message.sender        # Validate query        if not query:            return jsonify({"status": "error", "message": "No query provided"}), 400        # Process query using research chain        result = research_chain.invoke({            "messages": [HumanMessage(content=query)],            "team_members": ["Search", "SECAnalyst"]        })        # Format response for client        formatted_result = {            "analysis": [                {                    "role": msg.type if hasattr(msg, 'type') else "message",                    "content": msg.content,                    "name": msg.name if hasattr(msg, 'name') else None                }                for msg in result.get('messages', [])            ]        }        # Send response back through Agentverse        send_message_to_agent(            financial_identity,            agent_address,            {'analysis_result': formatted_result}        )        return jsonify({"status": "analysis_sent"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"status": "error", "message": str(e)}), 500</content>
</page>

<page>
  <title>Getting Started with ASI-1 Mini | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/asione/asi1-mini-getting-started</url>
  <content>This guide will walk you through the process of setting up and making your first API call to ASI:One, the Web3-native Large Language Model designed for agentic AI.

Before you can start using ASI:One, you'll need to obtain an API key. Follow these steps:

Once you have your API key, you can start making requests to the ASI:One API. Here's a simple example using Python:

    import requestsimport jsonurl = "https://api.asi1.ai/v1/chat/completions"payload = json.dumps({  "model": "asi1-mini",  "messages": [    {      "role": "user",      "content": "Hello, tell me about agentic AI"    }  ],  "temperature": 0.7,  "stream": False,  "max_tokens": 500})headers = {  'Content-Type': 'application/json',  'Accept': 'application/json',  'Authorization': 'Bearer YOUR_API_KEY'  # Replace with your actual API key}response = requests.request("POST", url, headers=headers, data=payload)print(response.text)

Replace `YOUR_API_KEY` with the API key you obtained in the previous steps.

The API will return a JSON response containing the model's reply. If you set `stream` to `True`, the response will be streamed as it's generated, which is useful for creating more responsive applications.

For a more interactive experience, you can set the `stream` parameter to `True` to receive the response as it's being generated:

    import requestsimport jsonurl = "https://api.asi1.ai/v1/chat/completions"payload = json.dumps({  "model": "asi1-mini",  "messages": [    {      "role": "user",      "content": "Explain the concept of decentralized AI"    }  ],  "temperature": 0.7,  "stream": True,  "max_tokens": 500})headers = {  'Content-Type': 'application/json',  'Accept': 'application/json',  'Authorization': 'Bearer YOUR_API_KEY'  # Replace with your actual API key}response = requests.request("POST", url, headers=headers, data=payload, stream=True)for line in response.iter_lines():    if line:        line_text = line.decode('utf-8')        if line_text.startswith('data: '):            data_str = line_text[6:]  # Remove 'data: ' prefix            if data_str == '[DONE]':                break            try:                data = json.loads(data_str)                if 'choices' in data and len(data['choices']) > 0:                    delta = data['choices'][0].get('delta', {})                    if 'content' in delta:                        print(delta['content'], end='', flush=True)            except json.JSONDecodeError:                pass

Now that you've made your first API call to ASI:One, you can explore more advanced features:

By leveraging ASI-1 Mini's agentic capabilities, you can build sophisticated AI applications that can reason, plan, and execute complex tasks autonomously within the Web3 ecosystem.</content>
</page>

<page>
  <title>ASI-1 Mini API Reference | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/asione/asi1-mini-api-reference</url>
  <content>This API Reference describes the RESTful and streaming interfaces of the ASI:One platform.

Introduction[â€‹](#introduction "Direct link to Introduction")
------------------------------------------------------------

ASI:One provides a powerful API that allows developers to integrate advanced agentic AI capabilities into their applications. The API is designed to be easy to use while providing access to the full range of ASI:One's capabilities.

OpenAI Compatibility[â€‹](#openai-compatibility "Direct link to OpenAI Compatibility")
------------------------------------------------------------------------------------

Where possible, the ASI:One API conforms to the OpenAI API specification. This means that users can often plug the ASI:One API into existing code that uses the OpenAI API with minimal changes.

API keys can be created from your account when you log into ASI:One. Authorization is done by adding the following header into your requests:

    Authorization: Bearer <api token>

Remember your API key is a secret, do not share it with anyone. If you need to revoke access for a particular key, simply log into your account and delete it from your profile.

Base URL[â€‹](#base-url "Direct link to Base URL")
------------------------------------------------

All API requests should be made to the following base URL:

Endpoints[â€‹](#endpoints "Direct link to Endpoints")
---------------------------------------------------

### Chat Completions[â€‹](#chat-completions "Direct link to Chat Completions")

Creates a model response for the given chat conversation.

#### Request Parameters[â€‹](#request-parameters "Direct link to Request Parameters")

| Parameter | Type | Required | Description |
| --- | --- | --- | --- |
| model | string | Yes | ID of the model to use. Currently, only "asi1-mini" is available. |
| messages | array | Yes | An array of message objects representing the conversation history. |
| temperature | number | No | What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. Default is 1.0. |
| stream | boolean | No | If set to true, partial message deltas will be sent as they become available. Default is false. |
| max\_tokens | integer | No | The maximum number of tokens that can be generated in the chat completion. This can be used to control costs for text generated via API. |

#### Message Object[â€‹](#message-object "Direct link to Message Object")

Each message in the `messages` array should have the following structure:

| Field | Type | Description |
| --- | --- | --- |
| role | string | The role of the message author. Must be one of "system", "user", or "assistant". |
| content | string | The content of the message. |

#### Example Request[â€‹](#example-request "Direct link to Example Request")

    {  "model": "asi1-mini",  "messages": [    {      "role": "system",      "content": "You are a helpful assistant specialized in Web3 technologies."    },    {      "role": "user",      "content": "Explain the concept of decentralized AI."    }  ],  "temperature": 0.7,  "stream": false,  "max_tokens": 500}

#### Response Format[â€‹](#response-format "Direct link to Response Format")

The API returns a JSON object with the following structure:

| Field | Type | Description |
| --- | --- | --- |
| id | string | The completion request ID. |
| model | string | The name of the model being used. |
| thought | string | (Optional) The thoughts that were generated as part of the chat completion request. Only present when streaming is enabled. |
| choices | array | An array of completion choices. |
| usage | object | (Optional) Information about token usage. |

#### Choice Object[â€‹](#choice-object "Direct link to Choice Object")

Each choice in the `choices` array has the following structure:

| Field | Type | Description |
| --- | --- | --- |
| index | integer | The index of the choice in the array. |
| delta | object | (When streaming) Contains the incremental content being streamed. |
| finish\_reason | string | The reason the model stopped generating text. Can be "stop", "length", etc. |
| stop\_reason | string | Additional information about why generation stopped. |

#### Example Response (Non-streaming)[â€‹](#example-response-non-streaming "Direct link to Example Response (Non-streaming)")

    {  "id": "id_comqjiusZjAoyuXlh",  "object": "chat.completion",  "model": "asi1-mini",  "choices": [    {      "index": 0,      "message": {        "role": "assistant",        "content": "Decentralized AI refers to artificial intelligence systems that operate on distributed networks rather than centralized servers. This approach aligns with Web3 principles by removing central points of control and enabling more democratic access to AI capabilities..."      },      "finish_reason": "stop",      "stop_reason": "stop"    }  ],  "usage": {    "prompt_tokens": 435,    "completion_tokens": 391,    "total_tokens": 826  }}

#### Streaming Responses[â€‹](#streaming-responses "Direct link to Streaming Responses")

When `stream` is set to `true`, the API will send a series of server-sent events (SSE) with partial completions as they become available. Each event is prefixed with `data:` and contains a JSON object with incremental updates.

The stream is terminated with a `data: [DONE]` message.

Error Handling[â€‹](#error-handling "Direct link to Error Handling")
------------------------------------------------------------------

The API uses standard HTTP status codes to indicate the success or failure of requests:

*   200: Success
*   400: Bad Request (invalid parameters)
*   401: Unauthorized (invalid API key)
*   422: Unprocessable Entity (valid parameters but request cannot be processed)
*   429: Too Many Requests (rate limit exceeded)
*   500: Internal Server Error

Error responses include a JSON object with an `error` field containing details about the error.

Rate Limits[â€‹](#rate-limits "Direct link to Rate Limits")
---------------------------------------------------------

API usage is subject to rate limits based on your account tier. If you exceed these limits, you'll receive a 429 status code. The response headers include information about your current rate limit status.

Best Practices[â€‹](#best-practices "Direct link to Best Practices")
------------------------------------------------------------------

*   Store your API key securely and never expose it in client-side code.
*   Implement proper error handling to gracefully handle API errors.
*   For long-running conversations, consider maintaining context on your side to reduce token usage.
*   Use streaming for more responsive user interfaces when generating longer responses.

For more detailed examples of how to use the API, see the [Chat Completion](https://innovationlab.fetch.ai/resources/docs/asione/asi1-mini-chat-completion) guide.</content>
</page>

<page>
  <title>Function Calling with ASI-1 Mini | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/asione/asi1-mini-function-calling</url>
  <content>Function calling in ASI1 allows models to go beyond text generation by invoking external functions with the right parameters. This enables integration with APIs, tools, or your own code to retrieve live data, perform tasks, or trigger actions based on user input.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

Function calling enables you to integrate your custom code with the Chat Completion API in ASI1. When given access to defined tools, the model can choose to call them based on the conversation context. After the function is called, you execute the corresponding code, return the results, and the model incorporates the output into its final reply. This guide provides instructions for connecting ASI1 models to your custom functions to retrieve data or perform specific actions.

ASI:One offers three model variants to suit different needs:

*   **asi1-mini**: The standard model with balanced performance and speed
*   **asi1-extended**: Enhanced capabilities for more complex tasks
*   **asi1-fast**: Optimized for quicker response times
*   **asi1-graph**: Optimized for data analytics and creation of graphs

Here's a basic example of how function calling works with ASI1:

*   Python
*   cURL

    import requestsimport json# ASI1 API settingsAPI_KEY = "your_api_key"BASE_URL = "https://api.asi1.ai/v1"headers = {    "Authorization": f"Bearer {API_KEY}",    "Content-Type": "application/json"}# Define the get_weather toolget_weather_tool = {    "type": "function",    "function": {        "name": "get_weather",        "description": "Get current temperature for a given location (latitude and longitude).",        "parameters": {            "type": "object",            "properties": {                "latitude": {"type": "number"},                "longitude": {"type": "number"}            },            "required": ["latitude", "longitude"]        }    }}# Initial message setupinitial_message = [    {        "role": "system",        "content": "You are a weather assistant. When a user asks for the weather in a location, use the get_weather tool with the appropriate latitude and longitude for that location."    },    {        "role": "user",        "content": "What's the current weather like in New York right now?"    }]# First call to modelpayload = {    "model": "asi1-mini",    "messages": [initial_message],    "tools": [get_weather_tool],    "temperature": 0.7,    "max_tokens": 1024}response = requests.post(    f"{BASE_URL}/chat/completions",    headers=headers,    json=payload)

### Example Response[â€‹](#example-response "Direct link to Example Response")

When you make a function call request, the model will respond with a structured output that includes the function call details. Here's an example response:

    {  "id": "id_t0m4Pzm0KDbMg0WaX",  "model": "asi1-mini",  "executable_data": [],  "conversation_id": null,  "thought": [],  "choices": [    {      "index": 0,      "finish_reason": "tool_calls",      "message": {        "role": "assistant",        "content": "",        "reasoning": null,        "tool_calls": [          {            "id": "call_WzB5g",            "index": 0,            "type": "function",            "function": {              "name": "get_weather",              "arguments": "{\"latitude\":40.7128,\"longitude\":-74.006}"            }          }        ]      }    }  ],  "usage": {    "prompt_tokens": 36,    "completion_tokens": 8,    "total_tokens": 44  }}

### Sample function[â€‹](#sample-function "Direct link to Sample function")

Let's look at the steps to allow a model to use a real `get_weather` function defined below:

Sample get\_weather function implemented in your codebase

    import requestsdef get_weather(latitude, longitude):    response = requests.get(f"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m")    data = response.json()    return data['current']['temperature_2m']

    async function getWeather(latitude, longitude) {    const response = await fetch(`https://api.open-meteo.com/v1/forecast?latitude=${latitude}&longitude=${longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m`);    const data = await response.json();    return data.current.temperature_2m;}

Let's walk through the complete cycle of executing a function call with ASI1, using the weather example:

##### Step 1: Initial Request with Tools[â€‹](#step-1-initial-request-with-tools "Direct link to Step 1: Initial Request with Tools")

First, we make a request to the model with the function definition and user message:

    # Define the get_weather toolget_weather_tool = {    "type": "function",    "function": {        "name": "get_weather",        "description": "Get current temperature for a given location (latitude and longitude).",        "parameters": {            "type": "object",            "properties": {                "latitude": {"type": "number"},                "longitude": {"type": "number"}            },            "required": ["latitude", "longitude"]        }    }}# User's questioninitial_message = {    "role": "user",    "content": "What's the current weather like in London right now?"}# First call to modelpayload = {    "model": "asi1-mini",    "messages": [initial_message],    "tools": [get_weather_tool],    "temperature": 0.7,    "max_tokens": 1024}first_response = requests.post(    f"{BASE_URL}/chat/completions",    headers=headers,    json=payload)

##### Step 2: Parse Tool Calls from Response[â€‹](#step-2-parse-tool-calls-from-response "Direct link to Step 2: Parse Tool Calls from Response")

The model responds with a function call that we need to parse:

    first_response.raise_for_status()first_response_json = first_response.json()tool_calls = first_response_json["choices"][0]["message"].get("tool_calls", [])messages_history = [    initial_message,    first_response_json["choices"][0]["message"]]

##### Step 3: Execute Tools and Format Results[â€‹](#step-3-execute-tools-and-format-results "Direct link to Step 3: Execute Tools and Format Results")

Next, we execute the function and format the results:

    # Simulate execution of get_weather tooldef get_weather(lat, lon):    response = requests.get(        f"https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={lon}&current=temperature_2m,wind_speed_10m"    )    data = response.json()    return data['current']['temperature_2m']# Process tool callfor tool_call in tool_calls:    function_name = tool_call["function"]["name"]    arguments = json.loads(tool_call["function"]["arguments"])    if function_name == "get_weather":        latitude = arguments["latitude"]        longitude = arguments["longitude"]        temperature = get_weather(latitude, longitude)        result = {            "temperature_celsius": temperature,            "location": f"lat: {latitude}, lon: {longitude}"        }    else:        result = {"error": f"Unknown tool: {function_name}"}    # Tool result message    tool_result_message = {        "role": "tool",        "tool_call_id": tool_call["id"],        "content": json.dumps(result)    }    messages_history.append(tool_result_message)

##### Step 4: Send Results Back to Model[â€‹](#step-4-send-results-back-to-model "Direct link to Step 4: Send Results Back to Model")

Now we send the function results back to the model:

    # Final call to model with tool resultsfinal_payload = {    "model": "asi1-mini",    "messages": messages_history,    "temperature": 0.7,    "max_tokens": 1024}final_response = requests.post(    f"{BASE_URL}/chat/completions",    headers=headers,    json=final_payload)

##### Step 5: Receive Final Answer[â€‹](#step-5-receive-final-answer "Direct link to Step 5: Receive Final Answer")

Finally, we get the model's response incorporating the function results:

    final_response.raise_for_status()final_response_json = final_response.json()# Final resultprint(final_response_json["choices"][0]["message"]["content"])

### Tool Result Handling in ASI1[â€‹](#tool-result-handling-in-asi1 "Direct link to Tool Result Handling in ASI1")

Here are key guidelines to ensure correct behavior and prevent common errors.

* * *

**Preserving Tool Call IDs**

Each tool call comes with a unique `id` that **must** be preserved when sending results back.

    # Correcttool_result_message = {  "role": "tool",  "tool_call_id": tool_call.id,  # Use the exact ID from the tool call  "content": json.dumps(result)}# Incorrect - Don't make up IDstool_result_message = {  "role": "tool",  "tool_call_id": "my_custom_id",  # This will cause an error  "content": json.dumps(result)}

**Message History Order**

The message history **must** maintain this exact order:

*   Original user message
*   Assistant message with `tool_calls` (content should be null or empty)
*   Tool result messages (one for each tool\_call, identified by `tool_call_id`)

**Content Formatting**

Tool results **must** be JSON-stringified within the `content` field.

    # Correct"content": json.dumps({"key": "value"})# Incorrect - Don't send raw objects"content": {"key": "value"}  # This will cause an error

**Error Handling**

If a tool fails, send back a result message indicating the error.

    try:  # Execute tool  result = execute_tool(function_name, arguments)  content_to_send = json.dumps(result)except Exception as e:  # Send error as tool result content  error_content = {      "error": f"Tool execution failed: {str(e)}",      "status": "failed"  }  content_to_send = json.dumps(error_content)tool_result_message = {  "role": "tool",  "tool_call_id": tool_call.id, # Still use the original tool_call.id  "content": content_to_send}messages_history.append(tool_result_message)

Function Definition[â€‹](#function-definition "Direct link to Function Definition")
---------------------------------------------------------------------------------

Functions are specified using the tools parameter in each API request, where each tool is described as a function object.

Each function is defined using a schema that tells the model what the function does and what input arguments it requires. The schema includes the following key fields:

*   **`name`** (string) : A unique, descriptive identifier for the function (e.g., `get_weather_forecast`, `send_email`). Use underscores or camelCase formatting. Avoid spaces or special characters.
    
*   **`description`** (string) : A detailed explanation of what the function does and when it should be used. Clear, specific descriptions improve the model's ability to use the function correctly.
    
*   **`parameters`** (object) : Defines the input parameters the function expects.
    
    *   **`type`** (string) : Usually set to `"object"` to represent structured input.
        
    *   **`properties`** (object) : Lists each input parameter and its details:
        
        *   **`type`** (string): The data type (e.g., `string`, `integer`, `boolean`, `array`).
        *   **`description`** (string): A clear explanation of the parameter's purpose and expected format.  
            _Example:_ `"City and country, e.g., 'Paris, France'"`
        *   **`enum`** _(optional)_: An array of allowed values, useful when inputs must be restricted.  
            _Example:_ `"enum": ["celsius", "fahrenheit"]`
    *   **`required`** (array of strings) : Lists the parameter names that must be included when calling the function.
        

Example Function Schema

    {    "type": "function",    "function": {        "name": "get_weather",        "description": "Retrieves current weather for the given location.",        "parameters": {            "type": "object",            "properties": {                "location": {                    "type": "string",                    "description": "City and country e.g. BogotÃ¡, Colombia"                },                "units": {                    "type": "string",                    "enum": [                        "celsius",                        "fahrenheit"                    ],                    "description": "Units the temperature will be returned in."                }            },            "required": [                "location",                "units"            ],            "additionalProperties": false        },        "strict": true    }}

Additional Configurations[â€‹](#additional-configurations "Direct link to Additional Configurations")
---------------------------------------------------------------------------------------------------

ASI1 provides several options to control how and when tools are called, as well as how strictly the model adheres to your function schemas.

### Tool Choice[â€‹](#tool-choice "Direct link to Tool Choice")

By default, the model determines when and how many tools to use. You can control this behavior with the `tool_choice` parameter:

*   **Auto (default):** The model may call zero, one, or multiple functions.
*   **Required:** The model must call at least one function.
    
        "tool_choice": "required"
    
*   **Forced Function:** Force the model to call a specific function.
    
        "tool_choice": {  "type": "function",  "function": { "name": "get_weather" }}
    
*   **None:** Prevent the model from calling any functions.

### Parallel Function Calling[â€‹](#parallel-function-calling "Direct link to Parallel Function Calling")

By default, the model may call multiple functions in a single turn. To restrict this and ensure only one (or zero) tool is called per turn, set:

    "parallel_tool_calls": false

> **Note:** If parallel tool calls are enabled, strict mode may be disabled for those calls.

### Strict Mode[â€‹](#strict-mode "Direct link to Strict Mode")

Setting `strict` to `true` ensures the model strictly follows your function schema. This is recommended for most use cases.

**Requirements for strict mode:**

1.  `additionalProperties` must be set to `false` for each object in the `parameters`.
2.  All fields in `properties` must be listed in `required`.

**Example with strict mode enabled:**

    {  "type": "function",  "function": {    "name": "get_weather",    "description": "Retrieves current weather for the given location.",    "strict": true,    "parameters": {      "type": "object",      "properties": {        "location": {          "type": "string",          "description": "City and country e.g. BogotÃ¡, Colombia"        },        "units": {          "type": ["string", "null"],          "enum": ["celsius", "fahrenheit"],          "description": "Units the temperature will be returned in."        }      },      "required": ["location", "units"],      "additionalProperties": false    }  }}

**Example with strict mode disabled:**

    {  "type": "function",  "function": {    "name": "get_weather",    "description": "Retrieves current weather for the given location.",    "parameters": {      "type": "object",      "properties": {        "location": {          "type": "string",          "description": "City and country e.g. BogotÃ¡, Colombia"        },        "units": {          "type": "string",          "enum": ["celsius", "fahrenheit"],          "description": "Units the temperature will be returned in."        }      },      "required": ["location"]    }  }}

> **Tip:** We recommend enabling strict mode for reliable function calling.</content>
</page>

<page>
  <title>Chat Completion with ASI-1 Mini | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/asione/asi1-mini-chat-completion</url>
  <content>The Chat Completion API is the primary way to interact with ASI:One. This guide provides detailed information on how to use the API effectively, with examples and best practices.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

The Chat Completion API allows you to have conversational interactions with ASI:One. You provide a series of messages representing a conversation, and the model generates a response that continues the conversation in a natural way.

What sets ASI:One apart from other LLMs is its agentic capabilities - it can reason through complex problems, maintain context across long conversations, and execute multi-step tasks autonomously.

Endpoint[â€‹](#endpoint "Direct link to Endpoint")
------------------------------------------------

    POST https://api.asi1.ai/v1/chat/completions

Request Format[â€‹](#request-format "Direct link to Request Format")
------------------------------------------------------------------

A basic chat completion request includes the model name, a list of messages, and optional parameters to control the generation:

    {  "model": "asi1-mini",  "messages": [    {      "role": "user",      "content": "Your message here"    }  ],  "temperature": 0,  "stream": false,  "max_tokens": 500}

### Required Parameters[â€‹](#required-parameters "Direct link to Required Parameters")

*   `model`: Currently, only "asi1-mini" is available.
*   `messages`: An array of message objects representing the conversation history.

### Optional Parameters[â€‹](#optional-parameters "Direct link to Optional Parameters")

*   `temperature`: Controls randomness in the response. Range is 0-2, with lower values producing more deterministic outputs. Default is 1.0.
*   `stream`: When set to `true`, the API will stream the response as it's generated. Default is `false`.
*   `max_tokens`: The maximum number of tokens to generate. Default varies based on the model.

Message Roles[â€‹](#message-roles "Direct link to Message Roles")
---------------------------------------------------------------

Each message in the conversation has a `role` and `content`. The available roles are:

*   `system`: Used to set the behavior or context for the assistant. System messages help guide the model's behavior.
*   `user`: Represents messages from the user.
*   `assistant`: Represents previous responses from the assistant.

Basic Example[â€‹](#basic-example "Direct link to Basic Example")
---------------------------------------------------------------

Here's a simple example of a chat completion request and response:

### Request[â€‹](#request "Direct link to Request")

    import requestsimport jsonurl = "https://api.asi1.ai/v1/chat/completions"payload = json.dumps({  "model": "asi1-mini",  "messages": [    {      "role": "user",      "content": "Hi, tell me about giraffes"    }  ],  "temperature": 0,  "stream": False,  "max_tokens": 500})headers = {  'Content-Type': 'application/json',  'Accept': 'application/json',  'Authorization': 'Bearer YOUR_API_KEY'  # Replace with your actual API key}response = requests.request("POST", url, headers=headers, data=payload)print(response.text)

### Response[â€‹](#response "Direct link to Response")

    {  "id": "id_comqjiusZjAoyuXlh",  "object": "chat.completion",  "model": "asi1-mini",  "choices": [    {      "index": 0,      "message": {        "role": "assistant",        "content": "Giraffes are fascinating creatures known for their towering height and distinctive long necks. They are the tallest land animals, with heights ranging from 14 to 19 feet (4.3 to 5.8 meters). Males, called bulls, are typically taller and heavier than females (cows). Their long necks allow them to reach leaves, flowers, and fruits high up in trees, especially from their preferred food source, the acacia tree.\n\nThese animals are native to Africa and are commonly found in savannas, grasslands, and open woodlands. Giraffes live in loose social groups, often mingling with other herbivores like zebras and antelopes. They are generally peaceful but can defend themselves with powerful kicks if threatened by predators such as lions or hyenas.\n\nOne of the most unique traits of giraffes is their spotted coat pattern, which is unique to each individual, much like a human fingerprint. They also have a specialized cardiovascular system to manage blood flow due to their heightâ€”especially when lowering their heads to drink water. Giraffes only need to drink water occasionally, as they obtain most of their moisture from the plants they consume.\n\nTheir social behavior and communication are also intriguing. Giraffes communicate using low-frequency sounds that are often inaudible to human ears. Additionally, they engage in a behavior called \"necking,\" where males swing their necks to compete for dominance or mates."      },      "finish_reason": "stop",      "stop_reason": "stop"    }  ],  "usage": {    "prompt_tokens": 435,    "completion_tokens": 391,    "total_tokens": 826  }}

Multi-turn Conversations[â€‹](#multi-turn-conversations "Direct link to Multi-turn Conversations")
------------------------------------------------------------------------------------------------

ASI:One excels at maintaining context across multiple turns of conversation. To create a multi-turn conversation, include previous messages in the `messages` array:

    payload = json.dumps({  "model": "asi1-mini",  "messages": [    {      "role": "system",      "content": "You are an expert in Web3 technologies and decentralized systems."    },    {      "role": "user",      "content": "What is a blockchain?"    },    {      "role": "assistant",      "content": "A blockchain is a distributed, immutable ledger that records transactions across many computers. It's the underlying technology behind cryptocurrencies like Bitcoin, but its applications extend far beyond digital currencies. Each 'block' contains a set of transactions, and once verified, it's linked to the previous block, forming a chain. This design makes the data extremely difficult to alter retroactively, providing security and transparency."    },    {      "role": "user",      "content": "How does this relate to Web3?"    }  ],  "temperature": 0.7,  "max_tokens": 500})

Streaming Responses[â€‹](#streaming-responses "Direct link to Streaming Responses")
---------------------------------------------------------------------------------

For more interactive applications, you can stream the response as it's being generated:

    import requestsimport jsonurl = "https://api.asi1.ai/v1/chat/completions"payload = json.dumps({  "model": "asi1-mini",  "messages": [    {      "role": "user",      "content": "Explain the concept of decentralized AI"    }  ],  "temperature": 0.7,  "stream": True,  "max_tokens": 500})headers = {  'Content-Type': 'application/json',  'Accept': 'application/json',  'Authorization': 'Bearer YOUR_API_KEY'}response = requests.request("POST", url, headers=headers, data=payload, stream=True)for line in response.iter_lines():    if line:        line_text = line.decode('utf-8')        if line_text.startswith('data: '):            data_str = line_text[6:]  # Remove 'data: ' prefix            if data_str == '[DONE]':                break            try:                data = json.loads(data_str)                if 'choices' in data and len(data['choices']) > 0:                    delta = data['choices'][0].get('delta', {})                    if 'content' in delta:                        print(delta['content'], end='', flush=True)            except json.JSONDecodeError:                pass

When streaming is enabled, you'll receive a series of server-sent events (SSE), each containing a small piece of the response. The stream is terminated with a `data: [DONE]` message.

Accessing Model Thoughts[â€‹](#accessing-model-thoughts "Direct link to Accessing Model Thoughts")
------------------------------------------------------------------------------------------------

A unique feature of ASI:One is the ability to access the model's "thoughts" during generation when streaming is enabled. These thoughts provide insight into the model's reasoning process:

    for line in response.iter_lines():    if line:        line_text = line.decode('utf-8')        if line_text.startswith('data: '):            data_str = line_text[6:]  # Remove 'data: ' prefix            if data_str == '[DONE]':                break            try:                data = json.loads(data_str)                if 'thought' in data:                    print(f"Thought: {data['thought']}")                elif 'choices' in data and len(data['choices']) > 0:                    delta = data['choices'][0].get('delta', {})                    if 'content' in delta:                        print(f"Content: {delta['content']}", end='', flush=True)            except json.JSONDecodeError:                pass

Best Practices[â€‹](#best-practices "Direct link to Best Practices")
------------------------------------------------------------------

### System Messages[â€‹](#system-messages "Direct link to System Messages")

Use system messages to guide the model's behavior. For example:

    {  "role": "system",  "content": "You are an AI assistant specialized in blockchain technology. Provide concise, accurate information and use examples where appropriate."}

### Managing Context Length[â€‹](#managing-context-length "Direct link to Managing Context Length")

ASI:One has a limited context window. To manage long conversations:

1.  Summarize previous turns when necessary
2.  Remove less relevant messages from the history
3.  Focus on the most recent and relevant context

### Controlling Response Style[â€‹](#controlling-response-style "Direct link to Controlling Response Style")

Use the temperature parameter to control the creativity of responses:

*   Lower temperature (0.2-0.5): More deterministic, factual responses
*   Medium temperature (0.5-0.8): Balanced creativity and coherence
*   Higher temperature (0.8-1.0): More creative, diverse responses

### Error Handling[â€‹](#error-handling "Direct link to Error Handling")

Implement robust error handling in your application:

    try:    response = requests.request("POST", url, headers=headers, data=payload)    response.raise_for_status()  # Raise an exception for 4XX/5XX responses    result = response.json()    # Process the resultexcept requests.exceptions.HTTPError as http_err:    print(f"HTTP error occurred: {http_err}")    # Handle specific status codes if neededexcept requests.exceptions.ConnectionError as conn_err:    print(f"Connection error occurred: {conn_err}")except requests.exceptions.Timeout as timeout_err:    print(f"Timeout error occurred: {timeout_err}")except requests.exceptions.RequestException as req_err:    print(f"An error occurred: {req_err}")except json.JSONDecodeError as json_err:    print(f"JSON decode error: {json_err}")

Advanced Use Cases[â€‹](#advanced-use-cases "Direct link to Advanced Use Cases")
------------------------------------------------------------------------------

ASI:One's agentic capabilities make it particularly well-suited for:

1.  **Multi-step reasoning tasks**: Problems that require breaking down into steps
2.  **Autonomous agents**: Creating AI assistants that can plan and execute tasks
3.  **Web3 integrations**: Interacting with blockchain data and smart contracts
4.  **Context-aware applications**: Systems that need to maintain state and adapt to changing information

Conclusion[â€‹](#conclusion "Direct link to Conclusion")
------------------------------------------------------

The Chat Completion API provides a powerful interface to ASI-1 Mini's capabilities. By understanding how to structure your requests and leverage the model's agentic reasoning, you can build sophisticated applications that go beyond simple text generation.

For more information, refer to the [API Reference](https://innovationlab.fetch.ai/resources/docs/asione/asi1-mini-api-reference) documentation.</content>
</page>

<page>
  <title>LangGraph Agent with MCP adapter | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/examples/mcp-integration/langgraph-mcp-agent-example</url>
  <content>    import osimport timefrom dotenv import load_dotenvfrom langchain_openai import ChatOpenAIfrom langchain_core.messages import HumanMessagefrom mcp import ClientSession, StdioServerParametersfrom mcp.client.stdio import stdio_clientimport asynciofrom langchain_mcp_adapters.tools import load_mcp_toolsfrom langgraph.prebuilt import create_react_agentfrom uagents_adapter import LangchainRegisterTool, cleanup_uagentfrom uagents_adapter.langchain import AgentManager# Load environment variablesload_dotenv()# Set your API keysOPENAI_API_KEY = os.getenv("OPENAI_API_KEY")API_TOKEN = os.getenv("AGENTVERSE_API_KEY")# Initialize the modelmodel = ChatOpenAI(model="gpt-4o")# Store the agent globally so it can be accessed by the wrapper functionagent = Noneasync def setup_math_agent():    global agent        print("Setting up math agent...")    server_params = StdioServerParameters(        command="python",        args=["math_server.py"],    )        async with stdio_client(server_params) as (read, write):        async with ClientSession(read, write) as session:            await session.initialize()            tools = await load_mcp_tools(session)            agent = create_react_agent(model, tools)                        # Test the agent            test_response = await agent.ainvoke({                "messages": [HumanMessage(content="what's (3 + 5) x 12?")]            })            print(f"Test response: {test_response['messages'][-1].content}")                        # Keep the connection alive            while True:                await asyncio.sleep(1)def main():    # Initialize agent manager    manager = AgentManager()        # Create agent wrapper    async def agent_func(x):        response = await agent.ainvoke({"messages": [HumanMessage(content=x)]})        return response["messages"][-1].content        agent_wrapper = manager.create_agent_wrapper(agent_func)        # Start the agent in background    manager.start_agent(setup_math_agent)        # Register with uAgents    print("Registering math agent...")    tool = LangchainRegisterTool()    agent_info = tool.invoke(        {            "agent_obj": agent_wrapper,            "name": "math_agent_langchain_mcp",            "port": 8080,            "description": "A math calculation agent",            "api_token": API_TOKEN,            "mailbox": True        }    )        print(f"âœ… Registered math agent: {agent_info}")        try:        manager.run_forever()    except KeyboardInterrupt:        print("ğŸ›‘ Shutting down...")        cleanup_uagent("math_agent")        print("âœ… Agent stopped.")if __name__ == "__main__":    main()</content>
</page>

<page>
  <title>Creating a MCP Server on Agentverse | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/examples/mcp-integration/mcp-adapter-example</url>
  <content>This example demonstrates how to deploy a **Model Control Protocol (MCP) Server** on [Agentverse](https://agentverse.ai/). The MCP Server Adapter allows MCP servers to be easily discoverable by other agents on Agentverse and [ASI:One](https://asi1.ai/).

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

The MCP Server Adapter makes it easy to bring your tools into the Agentverse ecosystem by:

*   Wrapping MCP servers as uAgents for seamless, decentralized communication
*   Exposing MCP tools to other agents on Agentverse for easy discovery and reuse
*   Enabling the Chat Protocol, allowing natural language conversations with the MCP Server directly or through ASI:One

Example Implementation[â€‹](#example-implementation "Direct link to Example Implementation")
------------------------------------------------------------------------------------------

In this example, the MCP Server provides weather-related tools, including:

*   `get_alerts`: Returns weather alerts for a given US state.
    
*   `get_forecast`: Returns a weather forecast for a specific latitude and longitude. You can define your own tools by following this pattern, making it easy to bring any Python-based service into the Agentverse network.
    
*   Create a FastMCP Server that implements the MCP Server logic.
    
*   Create an Agent that uses the `MCPServerAdapter` from the `uagents-adapter` package to wrap the MCP Server as a uAgent.
    

To get started,

1.  Navigate to [Agentverse](https://agentverse.ai/) â†’ Agents tab â†’ + New Agent.
2.  Choose Blank Agent
3.  Provide a name for your new Agent and click on Create.

Refer to the [Hosted Agents](https://innovationlab.fetch.ai/resources/docs/agent-creation/uagent-creation#hosted-agents) section to understand the detailed steps for agent creation on Agentverse.

### Step 1: Create a FastMCP Server[â€‹](#step-1-create-a-fastmcp-server "Direct link to Step 1: Create a FastMCP Server")

Create a `server.py` file implementing your MCP server.

1.  Click on New File.

2.  Rename the file to `server.py`

3.  Directory Structure

3.  Copy the following MCP Server Implementation and paste in your `server.py` file on Agentverse.

    from typing import Anyimport httpxfrom mcp.server.fastmcp import FastMCP# Create a FastMCP server instancemcp = FastMCP("weather")NWS_API_BASE = "https://api.weather.gov"USER_AGENT = "weather-app/1.0"async def make_nws_request(url: str) -> dict[str, Any] | None:    headers = {        "User-Agent": USER_AGENT,        "Accept": "application/geo+json"    }    async with httpx.AsyncClient() as client:        try:            response = await client.get(url, headers=headers, timeout=30.0)            response.raise_for_status()            return response.json()        except Exception:            return Nonedef format_alert(feature: dict) -> str:    props = feature["properties"]    return f""" Event: {props.get('event', 'Unknown')} Area: {props.get('areaDesc', 'Unknown')} Severity: {props.get('severity', 'Unknown')} Description: {props.get('description', 'No description available')} Instructions: {props.get('instruction', 'No specific instructions provided')}"""@mcp.tool()async def get_alerts(state: str) -> str:    """Get weather alerts for a US state."""    url = f"{NWS_API_BASE}/alerts/active/area/{state}"    data = await make_nws_request(url)    if not data or "features" not in data:        return "Unable to fetch alerts or no alerts found."    if not data["features"]:        return "No active alerts for this state."    alerts = [format_alert(feature) for feature in data["features"]]    return "\n---\n".join(alerts)@mcp.tool()async def get_forecast(latitude: float, longitude: float) -> str:    """Get weather forecast for a location."""        points_url = f"{NWS_API_BASE}/points/{latitude},{longitude}"    points_data = await make_nws_request(points_url)    if not points_data:        return "Unable to fetch forecast data for this location."    forecast_url = points_data["properties"]["forecast"]    forecast_data = await make_nws_request(forecast_url)    if not forecast_data:        return "Unable to fetch detailed forecast."    periods = forecast_data["properties"]["periods"]    forecasts = []    for period in periods[:5]:        forecast = f"""{period['name']}: Temperature: {period['temperature']}Â°{period['temperatureUnit']} Wind: {period['windSpeed']} {period['windDirection']} Forecast: {period['detailedForecast']}"""        forecasts.append(forecast)    return "\n---\n".join(forecasts)if __name__ == "__main__":    # Initialize and run the server    mcp.run(transport='stdio')

note

**Important:** When creating MCP tools, always include detailed docstrings using triple quotes (""") to describe what each tool in the MCP Server as these descriptions play a critical role in selecting the right MCP Tool based on the user's query.

### Step 2: Create an Agent for Your FastMCP Server[â€‹](#step-2-create-an-agent-for-your-fastmcp-server "Direct link to Step 2: Create an Agent for Your FastMCP Server")

To create an Agent for your Fast MCP Server, we will import the `MCPServerAdapter` from the `uagents-adapter` package. We will also import the MCP server instance `mcp` from `server.py`.

    from uagents_adapter import MCPServerAdapterfrom server import mcp  # This is your FastMCP server instance from server.py

To enable intelligent tool selection, the adapter leverages the ASI:One LLM. You'll need an ASI:One API key, which you can obtain by logging into [ASI:One](https://asi1.ai/) and navigating to the "API Keys" tab.

**Instantiate the MCPServerAdapter:**

    mcp_adapter = MCPServerAdapter(    mcp_server=mcp,                # (FastMCP) Your MCP server instance    asi1_api_key="your-asi1-api-key",  # (str) Your ASI:One API key    model="asi1-mini"              # (str) Model to use: "asi1-mini", "asi1-extended", or "asi1-fast")

#### MCPServerAdapter Parameters[â€‹](#mcpserveradapter-parameters "Direct link to MCPServerAdapter Parameters")

| Parameter | Type | Description | Required |
| --- | --- | --- | --- |
| `mcp_server` | FastMCP | The FastMCP server instance exposing your tools. | Yes |
| `asi1_api_key` | str | Your ASI:One API key for LLM-powered tool selection. | Yes |
| `model` | str | The ASI:One model to use (`"asi1-mini"`, `"asi1-extended"`, or `"asi1-fast"`). | Yes |

**Whole Script:**

    from uagents_adapter import MCPServerAdapterfrom server import mcp# Create an MCP adapter with your MCP servermcp_adapter = MCPServerAdapter(    mcp_server=mcp,                     # (FastMCP) Your MCP server instance    asi1_api_key="your-asi1-api-key",  # (str) Your ASI:One API key    model="asi1-mini"              # (str) Model to use: "asi1-mini", "asi1-extended", or "asi1-fast")# Create a uAgentagent = Agent()# Include protocols from the adapterfor protocol in mcp_adapter.protocols:    agent.include(protocol, publish_manifest=True)if __name__ == "__main__":    # Run the MCP adapter with the agent    mcp_adapter.run(agent)

note

The MCPServerAdapter only supports FastMCP Servers at the moment.

This setup ensures your agent can intelligently select and execute the right tool from the MCP Server based on the user queries.

#### Adding a README to your Agent[â€‹](#adding-a-readme-to-your-agent "Direct link to Adding a README to your Agent")

1.  Go to the Overview section in the Editor.
    
2.  Click on Edit and add a good description for your Agent so that it can be easily searchable by the ASI1 LLM. Please refer the [Importance of Good Readme](https://innovationlab.fetch.ai/resources/docs/agentverse/searching#importance-of-good-readme) section for more details.
    

### Step 3: Test Your Agent[â€‹](#step-3-test-your-agent "Direct link to Step 3: Test Your Agent")

1.  Start your Agent.
    
2.  Switch to the Overview Tab and use the "Chat with Agent" button to start talking to your agent.
    

#### Query your Agent from ASI1 LLM[â€‹](#query-your-agent-from-asi1-llm "Direct link to Query your Agent from ASI1 LLM")

1.  Login to the [ASI1 LLM](https://asi1.ai/), either using your Google Account or the ASI1 Wallet and Start a New Chat.
    
2.  Toggle the "Agents" switch to enable ASI1 to connect with Agents on Agentverse.
    

3.  To query your specific agent, you can copy the agent's address and mention in your query to explicitly connect with your agent. For instance, "Please ask the agent1qgggh8wy6ux2xwkc267cfpxk390c4ve0ts23yz5d9l6qsnckyvs2zpx08gq for weather alerts San Diego"

You can click on the Agent URL to check the agent that answered your question.

note

**Note:** If you ask about the weather without mentioning the address of your specific agent, ASI:One LLM might select another agent as it uses the Agent Ranking mechanism to select the most appropriate agent for each query. To test your agent directly, use the "Chat with Agent" button on your Agentverse agent page.</content>
</page>

<page>
  <title>Multi-Server MCP Langgraph Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/examples/mcp-integration/multi-server-agent-example</url>
  <content>This guide demonstrates two approaches for building LangGraph agents that connect to multiple MCP servers, then wrap them as uAgents and register them on [Agentverse](https://innovationlab.fetch.ai/resources/docs/agentverse) for discovery and use by ASI:One LLM.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

Both examples in this guide:

*   Connect to multiple MCP servers (math and weather) using `langchain_mcp_adapters.MultiServerMCPClient`
*   Support multiple transport methods (stdio and SSE) for different MCP servers
*   Wrap the LangGraph agent using `uagents_adapter` to become a uAgent
*   Register the uAgent on Agentverse, making it discoverable and callable by ASI:One

The key difference is in the agent architecture:

*   **Basic Multi-Server Agent**: Uses a simple LangGraph agent with ReAct framework
*   **Advanced State Graph Agent**: Uses a LangGraph state graph for more complex workflows and state management

Transport Methods[â€‹](#transport-methods "Direct link to Transport Methods")
---------------------------------------------------------------------------

In both examples, we use two different transport methods for the MCP servers:

1.  **stdio Transport** (Math Server):
    
    *   Used for local MCP servers that run as subprocesses
    *   Communication happens through standard input/output
    *   Good for local development and testing
    *   Example: `mcp.run(transport="stdio")`
2.  **SSE Transport** (Weather Server):
    
    *   Server-Sent Events (SSE) for real-time communication
    *   Used for remote or web-based MCP servers
    *   Supports long-lived connections
    *   Example: `mcp.run(transport="sse", port=8000)`

The `MultiServerMCPClient` handles both transport types seamlessly, allowing you to mix and match different transport methods based on your needs.

Common Server Setup[â€‹](#common-server-setup "Direct link to Common Server Setup")
---------------------------------------------------------------------------------

Both examples use the same MCP servers. Let's set those up first:

### 1\. Create the Math MCP Server[â€‹](#1-create-the-math-mcp-server "Direct link to 1. Create the Math MCP Server")

math\_server.py

    from mcp.server.fastmcp import FastMCPmcp = FastMCP("Math")@mcp.tool()def add(a: int, b: int) -> int:    """Add two numbers"""    return a + b@mcp.tool()def multiply(a: int, b: int) -> int:    """Multiply two numbers"""    return a * bif __name__ == "__main__":    mcp.run(transport="stdio")

### 2\. Create the Weather MCP Server[â€‹](#2-create-the-weather-mcp-server "Direct link to 2. Create the Weather MCP Server")

weather\_server.py

    from mcp.server.fastmcp import FastMCPmcp = FastMCP("Weather")@mcp.tool()def get_weather(city: str) -> str:    """Get the current weather for a city"""    # This is a mock implementation    return f"The weather in {city} is sunny and 25Â°C"if __name__ == "__main__":    mcp.run(transport="sse", port=8000)

Approach 1: Basic Multi-Server Agent[â€‹](#approach-1-basic-multi-server-agent "Direct link to Approach 1: Basic Multi-Server Agent")
-----------------------------------------------------------------------------------------------------------------------------------

This approach uses LangGraph's `create_react_agent` to create a simple agent that can access tools from multiple MCP servers.

### Create and Register the Basic Multi-Server Agent[â€‹](#create-and-register-the-basic-multi-server-agent "Direct link to Create and Register the Basic Multi-Server Agent")

basic\_agent.py

    import osimport timefrom dotenv import load_dotenvfrom langchain_openai import ChatOpenAIfrom langchain_core.messages import HumanMessagefrom langchain_mcp_adapters.client import MultiServerMCPClientfrom langgraph.prebuilt import create_react_agentfrom uagents_adapter import LangchainRegisterTool, cleanup_uagentfrom uagents_adapter.langchain import AgentManager# Load environment variablesload_dotenv()# Set your API keysOPENAI_API_KEY = os.getenv("OPENAI_API_KEY")API_TOKEN = os.getenv("AGENTVERSE_API_KEY")# Initialize the modelmodel = ChatOpenAI(model="gpt-4o")# Store the agent globally so it can be accessed by the wrapper functionagent = Noneasync def setup_multi_server_agent():    global agent        print("Setting up multi-server agent...")    async with MultiServerMCPClient(        {            "math": {                "command": "python",                "args": ["math_server.py"],                "transport": "stdio",            },            "weather": {                "url": "http://localhost:8000/sse",                "transport": "sse",            }        }    ) as client:        tools = client.get_tools()        agent = create_react_agent(model, tools)                # Test the agent with both services        print("Testing math capabilities...")        math_response = await agent.ainvoke({            "messages": [HumanMessage(content="what's (3 + 5) x 12?")]        })        print(f"Math test response: {math_response['messages'][-1].content}")                print("Testing weather capabilities...")        weather_response = await agent.ainvoke({            "messages": [HumanMessage(content="what's the weather in NYC?")]        })        print(f"Weather test response: {weather_response['messages'][-1].content}")                # Keep the connection alive        while True:            await asyncio.sleep(1)def main():    # Initialize agent manager    manager = AgentManager()        # Create agent wrapper    async def agent_func(x):        response = await agent.ainvoke({"messages": [HumanMessage(content=x)]})        return response["messages"][-1].content        agent_wrapper = manager.create_agent_wrapper(agent_func)        # Start the agent in background    manager.start_agent(setup_multi_server_agent)        # Register with uAgents    print("Registering multi-server agent...")    tool = LangchainRegisterTool()    agent_info = tool.invoke(        {            "agent_obj": agent_wrapper,            "name": "multi_server_agent_math_langchain_mcp",            "port": 8080,            "description": "A multi-service agent that can handle math calculations and weather queries",            "api_token": API_TOKEN,            "mailbox": True        }    )        print(f"âœ… Registered multi-server agent: {agent_info}")        try:        manager.run_forever()    except KeyboardInterrupt:        print("ğŸ›‘ Shutting down...")        cleanup_uagent("multi_server_agent")        print("âœ… Agent stopped.")if __name__ == "__main__":    import asyncio    main()

Approach 2: Advanced State Graph Agent[â€‹](#approach-2-advanced-state-graph-agent "Direct link to Approach 2: Advanced State Graph Agent")
-----------------------------------------------------------------------------------------------------------------------------------------

This approach uses LangGraph's `StateGraph` to create a more sophisticated agent with explicit state management and conditional workflow branching.

### Create and Register the Multi-Server Graph Agent[â€‹](#create-and-register-the-multi-server-graph-agent "Direct link to Create and Register the Multi-Server Graph Agent")

graph\_agent.py

    import osimport timeimport asynciofrom dotenv import load_dotenvfrom langchain_openai import ChatOpenAIfrom langchain_core.messages import HumanMessagefrom langchain_mcp_adapters.client import MultiServerMCPClientfrom langgraph.graph import StateGraph, MessagesState, STARTfrom langgraph.prebuilt import ToolNode, tools_conditionfrom uagents_adapter import LangchainRegisterTool, cleanup_uagentfrom uagents_adapter.langchain import AgentManager# Load environment variablesload_dotenv()# Set your API keysOPENAI_API_KEY = os.getenv("OPENAI_API_KEY")API_TOKEN = os.getenv("AGENTVERSE_API_KEY")# Initialize the modelmodel = ChatOpenAI(model="gpt-4o")# Store the graph globally so it can be accessed by the wrapper function_global_graph = None# Add an event to signal when the graph is readygraph_ready = asyncio.Event()async def setup_multi_server_graph_agent():    global _global_graph        print("Setting up multi-server graph agent...")    try:        # Create the client without async with        client = MultiServerMCPClient(            {                "math": {                    "command": "python",                    "args": ["./math_server.py"],                    "transport": "stdio",                },                "weather": {                    "url": "http://localhost:8000/sse",                    "transport": "sse",                }            }        )                # Get tools directly        tools = await client.get_tools()        print(f"Successfully loaded {len(tools)} tools")                # Define call_model function        def call_model(state: MessagesState):            response = model.bind_tools(tools).invoke(state["messages"])            return {"messages": response}        # Build the graph        builder = StateGraph(MessagesState)        builder.add_node(call_model)        builder.add_node(ToolNode(tools))        builder.add_edge(START, "call_model")        builder.add_conditional_edges(            "call_model",            tools_condition,        )        builder.add_edge("tools", "call_model")        _global_graph = builder.compile()        print("Graph successfully compiled")                # Test the graph        try:            print("Testing math capabilities...")            math_response = await _global_graph.ainvoke({"messages": "what's (3 + 5) x 12?"})            print(f"Math test response: {math_response['messages'][-1].content}")                        print("Testing weather capabilities...")            weather_response = await _global_graph.ainvoke({"messages": "what's the weather in NYC?"})            print(f"Weather test response: {weather_response['messages'][-1].content}")        except Exception as e:            print(f"Error during testing: {e}")                # Signal that the graph is ready        graph_ready.set()                # Keep the connection alive        while True:            await asyncio.sleep(1)    except Exception as e:        print(f"Error setting up graph: {e}")        # Set the event even in case of error to avoid deadlock        graph_ready.set()def main():    print("Initializing agent...")    # Initialize agent manager    manager = AgentManager()        # Create graph wrapper with proper error handling    async def graph_func(x):        # Wait for the graph to be ready before trying to use it        await graph_ready.wait()                if _global_graph is None:            error_msg = "Error: Graph not initialized properly. Please try again later."            print(f"Response: {error_msg}")            return error_msg                try:            # Print the incoming message            print(f"\nReceived query: {x}")                        # Process the message            if isinstance(x, str):                response = await _global_graph.ainvoke({"messages": x})            else:                response = await _global_graph.ainvoke({"messages": x})                        # Extract and print the response            result = response["messages"][-1].content            print(f"\nâœ… Response: {result}\n")            return result        except Exception as e:            error_msg = f"Error processing request: {str(e)}"            print(f"\nâŒ {error_msg}\n")            return error_msg        agent_wrapper = manager.create_agent_wrapper(graph_func)        # Start the graph in background    manager.start_agent(setup_multi_server_graph_agent)        # Register with uAgents    print("Registering multi-server graph agent...")    tool = LangchainRegisterTool()    try:        agent_info = tool.invoke(            {                "agent_obj": agent_wrapper,                "name": "multi_server_graph_agent_math_langchain_mcp",                "port": 8080,                "description": "A multi-service graph agent that can handle math calculations and weather queries",                "api_token": API_TOKEN,                "mailbox": True            }        )        print(f"âœ… Registered multi-server graph agent: {agent_info}")    except Exception as e:        print(f"âš ï¸ Error registering agent: {e}")        print("Continuing with local agent only...")        try:        manager.run_forever()    except KeyboardInterrupt:        print("ğŸ›‘ Shutting down...")        cleanup_uagent("multi_server_graph_agent")        print("âœ… Agent stopped.")if __name__ == "__main__":    import asyncio    main()

Key Differences Between the Two Approaches[â€‹](#key-differences-between-the-two-approaches "Direct link to Key Differences Between the Two Approaches")
------------------------------------------------------------------------------------------------------------------------------------------------------

1.  **Architecture**:
    
    *   **Basic Agent**: Uses LangGraph's `create_react_agent` for a simple ReAct-style agent.
    *   **Graph Agent**: Uses LangGraph's `StateGraph` for explicit state management and workflow control.
2.  **Control Flow**:
    
    *   **Basic Agent**: The control flow is managed internally by the ReAct framework.
    *   **Graph Agent**: The control flow is explicitly defined with nodes, edges, and conditional branching.
3.  **State Management**:
    
    *   **Basic Agent**: State is managed implicitly within the ReAct agent.
    *   **Graph Agent**: State is explicitly managed and can be more easily inspected and modified.
4.  **Extensibility**:
    
    *   **Basic Agent**: Simpler to set up but less flexible for complex workflows.
    *   **Graph Agent**: More complex setup but offers greater flexibility for sophisticated agent behaviors.

Getting Started[â€‹](#getting-started "Direct link to Getting Started")
---------------------------------------------------------------------

1.  **Get your Agentverse API Key**:
    
    *   Follow the [Agentverse API Key guide](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key) to obtain your API key
    *   Make sure to save your API key securely as it cannot be regenerated
2.  **Set up environment variables** in a `.env` file:
    
        OPENAI_API_KEY=your_openai_api_keyAGENTVERSE_API_KEY=your_agentverse_api_key
    
3.  **Install dependencies**:
    
        pip install langchain-openai mcp langchain-mcp-adapters uagents-adapter python-dotenv
    
4.  **Create the files**:
    
    *   Save the math server code as `math_server.py`
    *   Save the weather server code as `weather_server.py`
    *   Save the basic agent code as `basic_agent.py`
    *   Save the graph agent code as `graph_agent.py`
5.  **Start the servers and agents**:
    
        # Terminal 1: Start the weather serverpython weather_server.py# Terminal 2: Start the basic agentpython basic_agent.py# OR to run the graph agent insteadpython graph_agent.py
    
6.  **Test your agent** by querying it from Agentverse chat UI.
    

*   Open your Agentverse account and goto [local agents](https://agentverse.ai/agents/local).

*   Click on your agent and Use the "Chat with Agent" button on your Agentverse agent page

When to Use Each Approach[â€‹](#when-to-use-each-approach "Direct link to When to Use Each Approach")
---------------------------------------------------------------------------------------------------

*   **Use the Basic Agent** when:
    
    *   You need a simple agent that can access multiple tools
    *   You want a quick setup with minimal boilerplate
    *   The agent's decision-making process is relatively straightforward
*   **Use the Graph Agent** when:
    
    *   You need more control over the agent's workflow
    *   You want explicit state management
    *   You need complex conditional branching in your agent's behavior
    *   You're building an agent with multiple specialized steps or phases

note

**Note:** These examples demonstrate how to connect to multiple MCP servers using different transport methods and agent architectures. You can extend these patterns to include any number of MCP servers with different capabilities and create more sophisticated agent behaviors.</content>
</page>

<page>
  <title>Connect an Agent to Multiple Remote MCP Servers | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/examples/mcp-integration/connect-an-agent-to-multiple-remote-mcp-servers</url>
  <content>This example demonstrates how to build a uAgent client that connects to multiple remote MCP servers hosted on [Smithery.ai](https://smithery.ai/) (such as PubMed, clinical trials, medical calculators, and web search), handles user queries using Claude for intelligent tool selection, and registers itself on [Agentverse](https://innovationlab.fetch.ai/resources/docs/agentverse) for discovery and use by ASI:One LLM.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

*   **uAgent client** connects to multiple remote MCP servers via HTTP using [Smithery.ai](https://smithery.ai/)'s platform
*   Uses Claude to intelligently select and call the appropriate tools based on user queries
*   Formats responses using Claude for better readability and user experience
*   The agent is registered on [Agentverse](https://innovationlab.fetch.ai/resources/docs/agentverse), making it discoverable and callable by ASI:One LLM
*   Supports multiple specialized MCP servers for different domains (medical research, web search, etc.)

MCP Servers Used[â€‹](#mcp-servers-used "Direct link to MCP Servers Used")
------------------------------------------------------------------------

This example connects to several MCP servers hosted on Smithery.ai:

1.  **Medical Calculators** (`@vitaldb/medcalc`):
    
    *   BMI calculation
    *   HOMA-IR (insulin resistance)
    *   Other medical formulas
2.  **Clinical Trials** (`@JackKuo666/clinicaltrials-mcp-server`):
    
    *   Search clinical trial databases
    *   Get trial details and status
3.  **PubMed** (`@JackKuo666/pubmed-mcp-server`):
    
    *   Search biomedical literature
    *   Get article metadata
4.  **Paper Search** (`@openags/paper-search-mcp`):
    
    *   Search scientific research metadata
    *   Get paper details
5.  **DuckDuckGo** (`@nickclyde/duckduckgo-mcp-server`):
    
    *   Perform DuckDuckGo web searches
    *   Get real-time web results

Example: Medical Research Agent[â€‹](#example-medical-research-agent "Direct link to Example: Medical Research Agent")
--------------------------------------------------------------------------------------------------------------------

### Configure the uAgent Client[â€‹](#configure-the-uagent-client "Direct link to Configure the uAgent Client")

mcp\_agent.py

    from anthropic import Anthropicfrom dotenv import load_dotenvfrom uagents_core.contrib.protocols.chat import (    chat_protocol_spec,    ChatMessage,    ChatAcknowledgement,    TextContent,    StartSessionContent,)from uagents import Agent, Context, Protocolfrom uagents.setup import fund_agent_if_lowfrom datetime import datetime, timezone, timedeltafrom uuid import uuid4import mcpfrom mcp.client.streamable_http import streamablehttp_clientimport jsonimport base64import asynciofrom typing import Dict, List, Optional, Anyfrom contextlib import AsyncExitStackimport os# Load environment variablesload_dotenv()# Get API keys from environment variablesANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")if not ANTHROPIC_API_KEY:    raise ValueError("Please set the ANTHROPIC_API_KEY environment variable in your .env file")SMITHERY_API_KEY = os.getenv("SMITHERY_API_KEY")if not SMITHERY_API_KEY:    raise ValueError("Please set the SMITHERY_API_KEY environment variable in your .env file")class MedicalResearchMCPClient:    def __init__(self):        self.sessions: Dict[str, mcp.ClientSession] = {}        self.exit_stack = AsyncExitStack()        self.anthropic = Anthropic(api_key=ANTHROPIC_API_KEY)        self.all_tools = []        self.tool_server_map = {}        self.server_configs = {}        self.default_timeout = timedelta(seconds=30)    def get_server_config(self, server_path: str) -> dict:        """Get or create server configuration"""        if server_path not in self.server_configs:            config_templates = {                "@JackKuo666/pubmed-mcp-server": {},                "@openags/paper-search-mcp": {},                "@JackKuo666/clinicaltrials-mcp-server": {},                "@vitaldb/medcalc": {},            }            self.server_configs[server_path] = config_templates.get(server_path, {})        return self.server_configs[server_path]    async def connect_to_servers(self, ctx: Context):        """Connect to all MCP servers and collect their tools"""        base_config = {            "ignoreRobotsTxt": True        }        servers = [            "@JackKuo666/pubmed-mcp-server",            "@openags/paper-search-mcp",            "@JackKuo666/clinicaltrials-mcp-server",            "@vitaldb/medcalc",        ]        for server_path in servers:            try:                ctx.logger.info(f"Connecting to server: {server_path}")                server_config = self.get_server_config(server_path)                config = {**base_config, **server_config}                config_b64 = base64.b64encode(json.dumps(config).encode()).decode()                url = f"https://server.smithery.ai/{server_path}/mcp?config={config_b64}&api_key={SMITHERY_API_KEY}"                try:                    read_stream, write_stream, _ = await self.exit_stack.enter_async_context(                        streamablehttp_client(url)                    )                    session = await self.exit_stack.enter_async_context(                        mcp.ClientSession(read_stream, write_stream)                    )                    await session.initialize()                    tools_result = await session.list_tools()                    tools = tools_result.tools                    self.sessions[server_path] = session                    for tool in tools:                        tool_info = {                            "name": tool.name,                            "description": f"[{server_path}] {tool.description}",                            "input_schema": tool.inputSchema,                            "server": server_path,                            "tool_name": tool.name                        }                        self.all_tools.append(tool_info)                        self.tool_server_map[tool.name] = server_path                    ctx.logger.info(f"Successfully connected to {server_path}")                    ctx.logger.info(f"Available tools: {', '.join([t.name for t in tools])}")                except Exception as e:                    ctx.logger.error(f"Error during connection setup: {str(e)}")                    raise            except Exception as e:                ctx.logger.error(f"Error connecting to {server_path}: {str(e)}")                continue    async def process_query(self, query: str, ctx: Context) -> str:        try:            messages = [{"role": "user", "content": query}]            claude_tools = [{                "name": tool["name"],                "description": tool["description"],                "input_schema": tool["input_schema"]            } for tool in self.all_tools]            response = self.anthropic.messages.create(                model="claude-3-5-sonnet-20241022",                max_tokens=1000,                messages=messages,                tools=claude_tools            )            tool_response = None            for content in response.content:                if content.type == 'tool_use':                    tool_name = content.name                    tool_args = content.input                    server_path = self.tool_server_map.get(tool_name)                    if server_path and server_path in self.sessions:                        ctx.logger.info(f"Calling tool {tool_name} from {server_path}")                        try:                            result = await asyncio.wait_for(                                self.sessions[server_path].call_tool(tool_name, tool_args),                                timeout=self.default_timeout.total_seconds()                            )                            if isinstance(result.content, str):                                tool_response = result.content                            elif isinstance(result.content, list):                                tool_response = "\n".join([str(item) for item in result.content])                            else:                                tool_response = str(result.content)                        except asyncio.TimeoutError:                            return f"Error: The MCP server did not respond. Please try again later."                        except Exception as e:                            return f"Error calling tool {tool_name}: {str(e)}"            if tool_response:                format_prompt = f"""Please format the following response in a clear, user-friendly way. Do not add any additional information or knowledge, just format what is provided: {tool_response} Instructions: 1. If the response contains multiple records (like clinical trials), present ALL records in a clear format, do not say something like "Saved to a CSV file" or anything similar. 2. Use appropriate headings and sections 3. Maintain all the original information 4. Do not add any external knowledge or commentary 5. Do not summarize or modify the content 6. Keep the formatting simple and clean 7. If the response mentions a CSV file, do not include that information in the response. 9. For long responses, ensure all records are shown, not just a subset"""                format_response = self.anthropic.messages.create(                    model="claude-3-5-sonnet-20241022",                    max_tokens=2000,                    messages=[{"role": "user", "content": format_prompt}]                )                if format_response.content and len(format_response.content) > 0:                    return format_response.content[0].text                else:                    return tool_response            else:                return "No response received from the tool."        except Exception as e:            ctx.logger.error(f"Error processing query: {str(e)}")            return f"An error occurred while processing your query: {str(e)}"    async def cleanup(self):        await self.exit_stack.aclose()# Initialize chat protocol and agentchat_proto = Protocol(spec=chat_protocol_spec)mcp_agent = Agent(    name='MedicalResearchMCPAgent',    port=8001,    mailbox=True)client = MedicalResearchMCPClient()@chat_proto.on_message(model=ChatMessage)async def handle_chat_message(ctx: Context, sender: str, msg: ChatMessage):    try:        ack = ChatAcknowledgement(            timestamp=datetime.now(timezone.utc),            acknowledged_msg_id=msg.msg_id        )        await ctx.send(sender, ack)        if not client.sessions:            await client.connect_to_servers(ctx)        for item in msg.content:            if isinstance(item, StartSessionContent):                ctx.logger.info(f"Got a start session message from {sender}")                continue            elif isinstance(item, TextContent):                ctx.logger.info(f"Got a message from {sender}: {item.text}")                response_text = await client.process_query(item.text, ctx)                ctx.logger.info(f"Response text: {response_text}")                response = ChatMessage(                    timestamp=datetime.now(timezone.utc),                    msg_id=uuid4(),                    content=[TextContent(type="text", text=response_text)]                )                await ctx.send(sender, response)            else:                ctx.logger.info(f"Got unexpected content from {sender}")    except Exception as e:        ctx.logger.error(f"Error handling chat message: {str(e)}")        error_response = ChatMessage(            timestamp=datetime.now(timezone.utc),            msg_id=uuid4(),            content=[TextContent(type="text", text=f"An error occurred: {str(e)}")]        )        await ctx.send(sender, error_response)@chat_proto.on_message(model=ChatAcknowledgement)async def handle_chat_acknowledgement(ctx: Context, sender: str, msg: ChatAcknowledgement):    ctx.logger.info(f"Received acknowledgement from {sender} for message {msg.acknowledged_msg_id}")    if msg.metadata:        ctx.logger.info(f"Metadata: {msg.metadata}")mcp_agent.include(chat_proto)if __name__ == "__main__":    try:        mcp_agent.run()    except Exception as e:        print(f"Error running agent: {str(e)}")    finally:        asyncio.run(client.cleanup())

Script Breakdown[â€‹](#script-breakdown "Direct link to Script Breakdown")
------------------------------------------------------------------------

This section walks through the main components of the `mcp_agent.py` script to help you understand how each part contributes to building a Claude-powered uAgent that connects to multiple remote MCP servers.

### 1\. Loading Configuration and Dependencies[â€‹](#1-loading-configuration-and-dependencies "Direct link to 1. Loading Configuration and Dependencies")

    ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")SMITHERY_API_KEY = os.getenv("SMITHERY_API_KEY")

Environment variables are loaded using `dotenv` to securely manage API keys for Anthropic and [Smithery.ai](https://smithery.ai/). These credentials are required to authenticate and connect with the MCP Servers and process natural language queries.

### 2\. Creating the MCP Client[â€‹](#2-creating-the-mcp-client "Direct link to 2. Creating the MCP Client")

    class MedicalResearchMCPClient:    def __init__(self):        ...

The `MedicalResearchMCPClient` class handles all interactions with MCP servers. It manages session lifecycle, tool discovery, and query execution. The `exit_stack` allows multiple async context managers to be managed together.

### 3\. MCP Server Configuration[â€‹](#3-mcp-server-configuration "Direct link to 3. MCP Server Configuration")

    def get_server_config(self, server_path: str) -> dict:    ...

First, identify the MCP servers you want to integrate from the [Smithery.ai](https://smithery.ai/) platform. Once selected, navigate to the serverâ€™s API tab to locate its configuration schema. This schema outlines required parameters, authentication methods, and any necessary API keys for establishing a connection.

note

**Note:** For this example, we've selected MCP Servers that do not require extra authentication or server parameters.

### 4\. Connecting to Multiple Remote MCP Servers[â€‹](#4-connecting-to-multiple-remote-mcp-servers "Direct link to 4. Connecting to Multiple Remote MCP Servers")

    async def connect_to_servers(self, ctx: Context):    ...

Establishes connections to several predefined MCP servers using their Smithery-hosted URLs. The `config` is encoded and passed as a base64 parameter to the server endpoint. Each tool exposed by the server is retrieved and stored for later use.

    config_b64 = base64.b64encode(json.dumps(config).encode()).decode()url = f"https://server.smithery.ai/{server_path}/mcp?config={config_b64}&api_key={SMITHERY_API_KEY}"

MCP sessions and tool metadata are stored in local dictionaries, enabling tool invocation at runtime.

### 5\. Processing User Queries[â€‹](#5-processing-user-queries "Direct link to 5. Processing User Queries")

    async def process_query(self, query: str, ctx: Context) -> str:    ...

To process the user's query, it fetches all the tools of the MCP Servers that the client is connected to and then uses Claude to select the appropriate tool and provide input arguments for invocation.

    claude_tools = [{    "name": tool["name"],    "description": tool["description"],    "input_schema": tool["input_schema"]} for tool in self.all_tools]

Once Claude returns a tool selection, the agent locates the matching MCP session using the tool name, then calls the tool with the arguments returned by Claude. The output is parsed and passed back for formatting.

    if content.type == 'tool_use':    tool_name = content.name    tool_args = content.input    server_path = self.tool_server_map.get(tool_name)    if server_path and server_path in self.sessions:        ctx.logger.info(f"Calling tool {tool_name} from {server_path}")        try:            result = await asyncio.wait_for(                self.sessions[server_path].call_tool(tool_name, tool_args),                timeout=self.default_timeout.total_seconds()            )            if isinstance(result.content, str):                tool_response = result.content            elif isinstance(result.content, list):                tool_response = "\n".join([str(item) for item in result.content])            else:                tool_response = str(result.content)        except asyncio.TimeoutError:            return f"Error: The MCP server did not respond. Please try again later."        except Exception as e:            return f"Error calling tool {tool_name}: {str(e)}"

The tool is invoked via the relevant MCP session. If the response is successful, it's passed back to Claude with specific formatting instructions.

    format_response = self.anthropic.messages.create(...)

This second call ensures the output is easy to read while preserving all original data.

### 6\. Initializing the uAgent[â€‹](#6-initializing-the-uagent "Direct link to 6. Initializing the uAgent")

    mcp_agent = Agent(    name='MedicalResearchMCPAgent',    port=8001,    mailbox=True)

Since we are creating this Agent locally, we will enable `mailbox` to connect it to Agentverse so that it is discoverable and callable by ASI:One.

### 7\. Implementing the Chat Protocol[â€‹](#7-implementing-the-chat-protocol "Direct link to 7. Implementing the Chat Protocol")

    chat_proto = Protocol(spec=chat_protocol_spec)

The agent includes the standardized `chat_protocol_spec` to communicate with other agents and ASI:One.

    @chat_proto.on_message(model=ChatMessage)async def handle_chat_message(ctx: Context, sender: str, msg: ChatMessage):    ...

When ASI:One selects this agent to handle a user query, it sends the request as a ChatMessage to this handler. Upon receiving the message, the agent invokes the `process_query()` to connect with the remote MCP Servers to address the user's question and send back the formatted response as a `ChatMessage` to ASI:One. You can refer the [Chat Protocol documentation](https://innovationlab.fetch.ai/resources/docs/agent-communication/agent-chat-protocol) for more information.

### 8\. Cleanup on Shutdown[â€‹](#8-cleanup-on-shutdown "Direct link to 8. Cleanup on Shutdown")

    async def cleanup(self):    await self.exit_stack.aclose()

Gracefully closes all active MCP sessions to prevent resource leaks when the agent is stopped or restarted.

### 9\. Running the Agent[â€‹](#9-running-the-agent "Direct link to 9. Running the Agent")

    if __name__ == "__main__":    mcp_agent.run()

Runs the agent on the defined port. On termination, all active connections to remote MCP servers are closed via the `cleanup()` method.s

### Register the Agent on Agentverse[â€‹](#register-the-agent-on-agentverse "Direct link to Register the Agent on Agentverse")

*   Use the Agent inspector link upon agent startup to register your agent on Agentverse, making it discoverable by ASI:One LLM and other agents.
*   Add a comprehensive README.md in the Overview tab of your agent to improve discoverability.

Getting Started[â€‹](#getting-started "Direct link to Getting Started")
---------------------------------------------------------------------

1.  **Get your API Keys**:
    
    *   Follow the [Agentverse API Key guide](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key) to obtain your API key
    *   Get your [Smithery.ai API key](https://smithery.ai/account/api-keys) from their platform
    *   Get your [Anthropic API key](https://console.anthropic.com/settings/keys) from their platform
    *   Make sure to save your API keys securely
2.  **Set up environment variables** in a `.env` file:
    
        ANTHROPIC_API_KEY=your_anthropic_api_keySMITHERY_API_KEY=your_smithery_api_keyAGENTVERSE_API_KEY=your_agentverse_api_key
    
3.  **Install dependencies**:
    
        pip install uagents anthropic mcp python-dotenv
    
4.  **Create the agent file**:
    
    *   Save the code above as `mcp_agent.py`
    *   Create a `README.md` file in the same directory
    *   Update the API keys in your `.env` file
5.  **Run the agent**:
    
6.  **Test your agent**:
    
    *   Open your Agentverse account and goto [local agents](https://agentverse.ai/agents/local).
    *   Click on your agent and Use the "Chat with Agent" button on your Agentverse agent page

*   Query your agent through ASI:One LLM (make sure to enable the "Agents" switch)

note

**Note:** The ASI:One LLM uses an Agent Ranking mechanism to select the most appropriate agent for each query. To test your agent directly, use the "Chat with Agent" button on your Agentverse agent page. For more information about the Chat Protocol and ASI:One integration, check out the [Chat Protocol documentation](https://innovationlab.fetch.ai/resources/docs/agent-communication/agent-chat-protocol).</content>
</page>

<page>
  <title>Mettalex - A Practical Implementation of AI Agents in the Web3 Ecosystem | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/examples/on-chain-examples/mettalex-agents</url>
  <content>Mettalex: A Practical Implementation of AI Agents in the Web3 Ecosystem - Powered by Fetch.ai Agent Tech
--------------------------------------------------------------------------------------------------------

Introduction[â€‹](#introduction "Direct link to Introduction")
------------------------------------------------------------

[Mettalex](https://www.mettalex.ai/) stands out as the **first P2P orderbook** and **agent-based DEX** for commodity and digital (tokenized) assets trading. It harnesses **Fetch.aiâ€™s uAgents** to power **autonomous order matching**, **secure on-chain escrow**, and **cross-chain operations**. By eliminating reliance on centralized order books or liquidity pools, Mettalex aims to offer **slippage-free**, trustless trades with **maximum transparency**.

Key uAgents features in this use case include their **wallet- and chain-agnostic** capabilities. Additionally, the agents provide a robust **communication** and **execution** layer, making the trading process seamless and efficient.

Mettalex: Agent-Based Commodity Trading in a Nutshell[â€‹](#mettalex-agent-based-commodity-trading-in-a-nutshell "Direct link to Mettalex: Agent-Based Commodity Trading in a Nutshell")
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

1.  Direct P2P Order Matching
    
    *   **No Liquidity Pools:** Mettalex uses uAgents to match buyers and sellers directly, eliminating AMMs and reducing slippage.
    *   **Zero Slippage Execution:** Final prices match exactly what each party has agreed toâ€”crucial for volatile or low-liquidity commodity markets.
2.  Escrow-Backed Settlement
    
    *   **On-Chain Escrow:** Traders lock funds in a smart contract. The agent only completes a trade if both sides have met the exact terms.
    *   **Fail-Safe Mechanism:** If either side fails to finalize the transaction, agents revert the escrow to protect user funds.
3.  Multi-Wallet & Cross-Chain Support
    
    *   **Wallet-Agnostic:** Users may use MetaMask, Ledger, or any Web3-compatible wallet. The agent logic remains the same, ensuring a uniform trading experience.
    *   **Chain-Agnostic:** Mettalex agents can run on multiple blockchains in parallel (e.g., Ethereum, BNB Chain, Cosmos). They coordinate escrow locks and trades across networks if bridging solutions exist.
4.  On-Chain Registration & Discovery
    
    *   **Almanac Registry:** Agents register on a Cosmos-based on-chain Almanac contract, allowing other agents/dApps to verify their identity.
5.  Transparent Execution & Governance
    
    *   **Public Transaction Logs:** Every escrow creation, signature, and fund release is recorded on-chain. Users can view agent logs and track progress in real time.
    *   **MTLX Governance:** Mettalexâ€™s governance token (MTLX) can let agents automate fee changes or protocol upgrades, broadening the platformâ€™s agent-driven ecosystem.

Putting It All Together[â€‹](#putting-it-all-together "Direct link to Putting It All Together")
---------------------------------------------------------------------------------------------

*   User places a trade -> Funds locked in escrow.
*   Agent checks buyer/seller positions -> Confirms each sideâ€™s escrow.
*   Agents sign trade parameters on-chain -> Escrows release funds upon successful match.
*   Settlement -> Both parties receive respective assets with no slippage and full transparency.

If you wish to learn more about Mettalex, please visit [Mettalex Docs](https://www.mettalex.com/docs).</content>
</page>

<page>
  <title>Solana Agent Integration with Fetch.ai uAgents | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/examples/on-chain-examples/solana-agents</url>
  <content>This example shows how to integrate Solana wallets within **Fetch.aiâ€™s uAgents** framework. Weâ€™ll walk through the **EscrowAgent, PlayerAgent, and ChallengerAgent** scripts, detailing how each agent:

1.  Registers with the **Almanac** contract (for discoverability)
2.  Loads Solana **private keys** from environment variables
3.  Executes transfers, checks balances, and handles bet-based business logic via **Solana Devnet**
4.  Communicates with other agents through **uAgents** messaging

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

*   **Solana CLI** (configured to Devnet)
*   **Poetry** (for dependency management)
*   **Python 3.8+**
*   Fetch.aiâ€™s **uagents** library
*   **Solders, requests**, etc. (handled by `poetry install`)
*   `.env` with your Solana private keys (Base64 arrays) for each agent

note

**Note:** Each agent script runs on a different portâ€”EscrowAgent uses `:8000`, PlayerAgent uses `:8001`, and ChallengerAgent uses `:8002` by default.

High-Level Architecture[â€‹](#high-level-architecture "Direct link to High-Level Architecture")
---------------------------------------------------------------------------------------------

1.  **PlayerAgent** & **ChallengerAgent** each place a bet by transferring SOL to the **Escrow** wallet (managed by the EscrowAgent).
2.  **EscrowAgent** collects two bets, checks the BTC price via an external API, and decides a winner. 90% of the total stake is transferred to the winnerâ€™s Solana wallet; the loser forfeits.

Escrow Agent[â€‹](#escrow-agent "Direct link to Escrow Agent")
------------------------------------------------------------

### Overview[â€‹](#overview "Direct link to Overview")

The **EscrowAgent**:

*   Registers on **Almanac** so other agents (Player/Challenger) can discover it
*   Waits for two **escrowRequest** messages
*   Fetches **BTC price from Binance**
*   **Transfers** the correct portion of **SOL** to the winner

### Script Breakdown[â€‹](#script-breakdown "Direct link to Script Breakdown")

**Required Libraries**

    import osimport base58import astfrom uagents import Agent, Context, Modelfrom solders.keypair import Keypairfrom functions import get_latest_btc_price, transfer_solimport time

*   os & ast for environment handling
*   uagents for agent creation
*   solders.keypair for Solana KeyPair
*   functions for helper utilities (price fetch & SOL transfer)

**Key Classes & Models**

    class escrowRequest(Model):    amount: float    price: float    public_key: strclass escrowResponse(Model):    result: str

*   **escrowRequest** holds the userâ€™s desired bet: `amount`, `price`, and the userâ€™s **Solana public key**.
*   **escrowResponse** returns the result to the user: either `"You Won"` or `"You Lost"`.

**Initialization & Identity**

    # Retrieve ESCROW_SECRET_LIST from the .envescrow_secret_key_str = os.getenv('ESCROW_SECRET_LIST')escrow_secret_key_list = ast.literal_eval(escrow_secret_key_str)escrow_secret_key_bytes = bytes(escrow_secret_key_list)escrow_keypair = Keypair.from_bytes(escrow_secret_key_bytes)escrow_pubkey_base58 = base58.b58encode(bytes(escrow_keypair.pubkey())).decode('utf-8')agent = Agent(    name="EscrowAgent",    port=8000,    seed="Escrow Wallet",    endpoint=["http://127.0.0.1:8000/submit"],)

*   We decode the **Solana private key** from `.env`.
*   Create a `Keypair` for the Escrowâ€™s wallet.
*   Instantiate a `uagents.Agent` with the name â€œEscrowAgentâ€ listening on port `8000`.

**On Startup**

    @agent.on_event('startup')async def saf(ctx: Context):    ctx.logger.info("Escrow agent initialized, ready for bids.")    ctx.logger.info(f"Escrow agent address: {agent.address}")    ctx.storage.set("bids_count", 0)

*   Logs that the Escrow is online.
*   Initializes a storage key bids\_count=0 to track how many requests have come in.

**Message Handling**

Receiving Bets

    @agent.on_message(model=escrowRequest, replies={escrowResponse})async def escrow_request_handler(ctx: Context, sender: str, msg: escrowRequest):    current_count = ctx.storage.get("bids_count") or 0    ...    if current_count == 0:        # Store first bet    elif current_count == 1:        # Store second bet        # Compare        # Transfer to winner        # Respond with escrowResponse        # Reset storage

*   **First Bet:** If `bids_count=0`, store the details (amount, price, userâ€™s pubkey).
*   **Second Bet:** If `bids_count=1`, store the second userâ€™s bet, then call `get_latest_btc_price()`.
*   Calculate each userâ€™s distance from the real BTC price.
*   Transfer 90% of the total stake to the winnerâ€™s public key with `transfer_sol()`.
*   Send escrowResponse messages to both the winner `(You Won)` and loser `(You Lost)`.
*   Reset internal storage.

**Running the EscrowAgent**

    if __name__ == "__main__":    agent.run()

*   Save the script as `escrow_agent.py`
*   Simply run `poetry run python escrow_agent.py`.

Player Agent[â€‹](#player-agent "Direct link to Player Agent")
------------------------------------------------------------

### Overview[â€‹](#overview-1 "Direct link to Overview")

The **PlayerAgent** simulates a user placing a bet on BTCâ€™s future price.

### Script Breakdown[â€‹](#script-breakdown-1 "Direct link to Script Breakdown")

**Required Libraries**

    import osimport astimport base58from uagents import Agent, Context, Modelfrom solders.keypair import Keypairfrom functions import check_balance, transfer_solfrom uagents.setup import fund_agent_if_lowcheck_balance and transfer_sol from functions.py to manage SOL balances/transfersfund_agent_if_low from uagents.setup can top up the agentâ€™s fetch-side address if needed

**Key Classes**

    class escrowRequest(Model):    amount: float    price: float    public_key: strclass escrowResponse(Model):    result: str

*   Re-used from the Escrow flow: escrowRequest is how we send the userâ€™s bet to the EscrowAgent.

**Initialization & Identity**

    secret_key_str = os.getenv('PLAYER_SECRET_LIST')secret_key_list = ast.literal_eval(secret_key_str)secret_key_bytes = bytes(secret_key_list)agent_keypair = Keypair.from_bytes(secret_key_bytes)agent_pubkey_base58 = base58.b58encode(bytes(agent_keypair.pubkey())).decode('utf-8')agent = Agent(    name="PlayerAgent",    port=8001,    seed="Player Escrow Wallet 1",    endpoint=["http://127.0.0.1:8001/submit"],)

*   Decodes the **PLAYER\_SECRET\_LIST** from `.env`.
*   Assigns the Playerâ€™s Solana wallet keypair.
*   Sets up a uAgent on port `8001`.

**Startup Sequence**

    @agent.on_event('startup')async def starter_function(ctx: Context):    initial_balance = check_balance(agent_keypair.pubkey())    ctx.logger.info(f"Initial agent balance: {initial_balance} SOL")    amount = float(input('What is the amount of SOL you want to deposit? '))    price = float(input('What is the price of Bitcoin you want to bid at? '))    # Send bet to Escrow    await ctx.send(        'agent1qd6ts50kuy3vqq36s5yg2dkzujq60x0l0sr2acfafnp5zea749yvvvq2qm7',        escrowRequest(amount=amount, price=price, public_key=agent_pubkey_base58)    )    # Transfer SOL to escrowâ€™s base58 key    transfer_result = transfer_sol(agent_keypair, '8WMWFo13At1REkwy5t7ck6sLgCUrJ9dn66mbaccPiJ26', amount)    ctx.logger.info(f"Transfer result: {transfer_result}")    final_balance = check_balance(agent_keypair.pubkey())    ctx.logger.info(f"Final agent balance: {final_balance} SOL")

*   **Check initial SOL** in the userâ€™s wallet.
*   **Ask** for deposit & BTC price guess.
*   **Send** an `escrowRequest` message to the known Escrow agent address.
*   `transfer_sol` to the Escrow agentâ€™s public key.
*   Log final SOL.

**Receiving Escrow Responses**

    @agent.on_message(model=escrowResponse)async def escrow_request_handler(ctx: Context, sender: str, msg: escrowResponse):    balance = check_balance(agent_keypair.pubkey())    ctx.logger.info(f'{msg.result}. Updated account balance: {balance} SOL')

*   When the **EscrowAgent** decides the outcome, it sends an `escrowResponse`.
*   This handler logs either â€œYou Wonâ€ or â€œYou Lostâ€ plus the updated balance.

**Running PlayerAgent**

    if __name__ == "__main__":    agent.run()

*   Save the script as `player_agent.py`
*   Run with poetry `run python player_agent.py`.

Challenger Agent[â€‹](#challenger-agent "Direct link to Challenger Agent")
------------------------------------------------------------------------

### Overview[â€‹](#overview-2 "Direct link to Overview")

Nearly identical to `player_agent.py`, but simulates another user (Challenger) placing a competing bet.

### Script Breakdown[â€‹](#script-breakdown-2 "Direct link to Script Breakdown")

**Required Libraries**

    import osimport astimport base58from uagents import Agent, Context, Modelfrom solders.keypair import Keypairfrom functions import check_balance, transfer_solfrom uagents.setup import fund_agent_if_low

**Key Classes**

    class escrowRequest(Model):    amount: float    price: float    public_key: strclass escrowResponse(Model):    result: str

**Initialization & Startup**

    secret_key_str = os.getenv('CHALLENGER_SECRET_LIST')secret_key_list = ast.literal_eval(secret_key_str)secret_key_bytes = bytes(secret_key_list)agent_keypair = Keypair.from_bytes(secret_key_bytes)agent_pubkey_base58 = base58.b58encode(bytes(agent_keypair.pubkey())).decode('utf-8')agent = Agent(    name="Challenger",    port=8002,    seed="Challenger Escrow Wallet 2",    endpoint=["http://127.0.0.1:8002/submit"],)fund_agent_if_low(agent.wallet.address())@agent.on_event('startup')async def starter_function(ctx: Context):    initial_balance = check_balance(agent_keypair.pubkey())    ctx.logger.info(f"Initial agent balance: {initial_balance} SOL")    amount = float(input('What is the amount of SOL you want to deposit? '))    price = float(input('What is the price of Bitcoin you want to bid at? '))    # Send the bet    await ctx.send(        'agent1qd6ts50kuy3vqq36s5yg2dkzujq60x0l0sr2acfafnp5zea749yvvvq2qm7',        escrowRequest(amount=amount, price=price, public_key=agent_pubkey_base58)    )    # Transfer SOL    transfer_result = transfer_sol(agent_keypair, '8WMWFo13At1REkwy5t7ck6sLgCUrJ9dn66mbaccPiJ26', amount)    ctx.logger.info(f"Transfer result: {transfer_result}")    final_balance = check_balance(agent_keypair.pubkey())    ctx.logger.info(f"Final agent balance: {final_balance} SOL")@agent.on_message(model=escrowResponse)async def escrow_request_handler(ctx: Context, sender: str, msg: escrowResponse):    balance = check_balance(agent_keypair.pubkey())    ctx.logger.info(f'{msg.result}. Updated account balance: {balance} SOL')if __name__ == "__main__":    agent.run()

*   Exactly the same flow: read user input, transfer SOL, wait for a response from the Escrow agent.

Utility Script : functions.py[â€‹](#utility-script--functionspy "Direct link to Utility Script : functions.py")
-------------------------------------------------------------------------------------------------------------

Below is a brief summary of the key utility functions. For full code, see the repository.

    import base58from solders.keypair import Keypairfrom solders.pubkey import Pubkeyfrom solders.transaction import Transactionfrom solders.system_program import TransferParams, transferfrom solana.rpc.api import Clientfrom solana.rpc.types import TxOptsimport requestsdef get_keypair_details(secret_key_list):    """    Given a list of secret key bytes (integers), returns a dictionary with the keypair, public key,    private key in bytes, and private key in Base58 encoding.    """    # Convert the list of integers into a bytes object (private key)    secret_key_bytes = bytes(secret_key_list)    # Restore the Keypair using the secret key    keypair = Keypair.from_bytes(secret_key_bytes)    # Public key (from keypair)    public_key = keypair.pubkey()    # Private key in Base58 encoding (for readability)    private_key_base58 = base58.b58encode(secret_key_bytes).decode()    # Return all necessary details in a dictionary    return {        "keypair": keypair,        "public_key": public_key,  # Solders Pubkey object        "private_key_bytes": secret_key_bytes,  # Private key in bytes        "private_key_base58": private_key_base58  # Private key in Base58    }client = Client("https://api.devnet.solana.com")# Function to check balancedef check_balance(pubkey):    balance_resp = client.get_balance(Pubkey.from_bytes(bytes(pubkey)))    print(f'balance_resp :{balance_resp}')    balance = balance_resp.value  # Extract balance in lamports    return balance / 1_000_000_000  # Convert lamports to SOLdef transfer_sol(from_keypair, to_pubkey_base58, amount_sol):    # Convert SOL to lamports (1 SOL = 1 billion lamports)    lamports = int(amount_sol * 1_000_000_000)    # Convert the recipient's Base58 public key string to a Pubkey object    to_pubkey = Pubkey.from_string(to_pubkey_base58)    # Get latest blockhash    blockhash_resp = client.get_latest_blockhash()    recent_blockhash = blockhash_resp.value.blockhash    # Create a transfer instruction    transfer_instruction = transfer(        TransferParams(from_pubkey=from_keypair.pubkey(), to_pubkey=to_pubkey, lamports=lamports)    )    # Create the transaction with the instruction directly    transaction = Transaction.new_signed_with_payer(        [transfer_instruction],  # Pass the list of instructions directly        from_keypair.pubkey(),  # Fee-payer (challenger)        [from_keypair],  # Signers (challenger)        recent_blockhash  # Use recent blockhash directly    )    # Send the transaction    result = client.send_raw_transaction(bytes(transaction), opts=TxOpts(skip_confirmation=False))    return resultdef get_latest_btc_price():    try:        # Binance API endpoint for fetching the latest BTC price in USDT        url = 'https://api.binance.com/api/v3/ticker/price?symbol=BTCUSDT'        response = requests.get(url)        response.raise_for_status()  # Raise an error for bad responses        data = response.json()        return float(data['price'])  # Return the latest BTC price as a float    except requests.exceptions.RequestException as e:        print(f"Error fetching BTC price: {e}")        return None

.env File Example[â€‹](#env-file-example "Direct link to .env File Example")
--------------------------------------------------------------------------

    # .envAGENTVERSE_API_KEY="<Your_FetchAI_Agentverse_Token>"PLAYER_SECRET_LIST="[79,79,237,8,87,104,75,156,47,204,53,127,171,9,114,244,...]"CHALLENGER_SECRET_LIST="[134,53,148,91,88,30,254,53,171,183,219,91,33,67,24,9,65,...]"ESCROW_SECRET_LIST="[251,164,58,0,121,167,133,83,114,82,162,22,88,214,195,91,82,...]"

Ensure each secret list matches the integer arrays from your `player-wallet.json`, `challenger-wallet.json`, and `escrow-wallet.json`. Also, do not commit your `.env` to source control.

Steps to Run the Agents[â€‹](#steps-to-run-the-agents "Direct link to Steps to Run the Agents")
---------------------------------------------------------------------------------------------

1.  Set Up Virtual Environment & Dependencies

2.  Fund Each Wallet on Devnet

    solana airdrop 5 <PLAYER_PUBKEY> --url devnetsolana airdrop 5 <CHALLENGER_PUBKEY> --url devnetsolana airdrop 5 <ESCROW_PUBKEY> --url devnet

3.  Start EscrowAgent

    poetry run python escrow_agent.py

*   Wait for it to display Escrow agent initialized, ready for bids.

4.  Start PlayerAgent

    poetry run python player_agent.py

*   Input the deposit amount & BTC guess when prompted.

5.  Start ChallengerAgent

    poetry run python challenger_agent.py

*   Similarly input deposit & guess. Once the Escrow receives the second bet, it decides a winner.

### Sample Output[â€‹](#sample-output "Direct link to Sample Output")

**EscrowAgent:**

    INFO: [EscrowAgent]: Escrow agent initialized, ready for bids.INFO: [EscrowAgent]: Received escrowRequest messageINFO: [EscrowAgent]: Storing first request ...INFO: [EscrowAgent]: Received escrowRequest messageINFO: [EscrowAgent]: Storing second request ...INFO: [EscrowAgent]: Processing bids to determine the winner.INFO: [EscrowAgent]: First difference: 1820.0, Second difference: 586820.0INFO: [EscrowAgent]: Transferring 0.9 SOL to winner ...INFO: [EscrowAgent]: Notifying winner and loser.

**PlayerAgent:**

    What is the amount of SOL you want to deposit? 0.5What is the price of Bitcoin you want to bid at? 65000INFO: [PlayerAgent]: Transfer result: ...INFO: [PlayerAgent]: Final agent balance: 4.31998 SOLINFO: [PlayerAgent]: You Won. Updated account balance: 5.21998 SOL

Debugging Common Issues[â€‹](#debugging-common-issues "Direct link to Debugging Common Issues")
---------------------------------------------------------------------------------------------

1.  Key Decoding Errors
    
    *   Ensure your `.env` secret lists are valid JSON arrays of integers.
    *   If you see `ValueError` or `Cannot decode secret key`, confirm you have no trailing commas.
2.  Faucet or Balance Issues
    
    *   Double-check `solana balance <PUBKEY> --url devnet`. If less then 1 SOL, some transactions might fail due to insufficient lamports for fees.
3.  Agent Registration Problems
    
    *   Confirm **AGENTVERSE\_API\_KEY** is correct in your `.env`.
    *   Make sure each agent can reach the default Almanac endpoint (requires internet connection).
4.  BTC Price Fetch Errors
    
    *   If Binance is unreachable or rate-limits your IP, consider adding retry logic or a fallback endpoint.

By following this **Solana + uAgents** guide, youâ€™ve set up three distinct agents that:

*   Load private keys from .env
*   Register on Fetch.aiâ€™s Almanac for discovery
*   Communicate using @agent.on\_message and typed models (escrowRequest, escrowResponse)
*   Interact with Solana Devnet for safe, low-cost experimentation

This architecture can be extended for **NFT auctions, DeFi ops, cross-chain bridging**, or any scenario where you need **agent-driven** logic plus on-chain Solana transactions. Enjoy building!

note

**Note:** GitHub repository for this example is available [here](https://github.com/abhifetch/solana-fetch-uagents-integration).</content>
</page>

<page>
  <title>AI Language Tutor with ASI-1 Mini | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/examples/asione/asi1-mini-language-tutor</url>
  <content>Building an AI Language Tutor uAgent with ASI-1 Mini
----------------------------------------------------

This guide explains how to create a simple uAgent that serves as an AI language tutor using the ASI-1 Mini API. The agent accepts a user query and returns language learning assistanceâ€”such as translation, grammar correction, or pronunciation tipsâ€”based on the provided prompt.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

### Purpose[â€‹](#purpose "Direct link to Purpose")

This project demonstrates how to integrate ASI-1 Mini, Fetch.ai's Web3-native large language model, into a uAgent. The agent, named **AI\_Language\_Tutor**, is designed to:

*   Call the **ASI-1 Mini API** with a prompt tailored for language learning.
*   Provide translation, grammar correction, or pronunciation tips based on user input.
*   Log the response upon agent startup.

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

Before running this project, ensure you have:

*   Python 3.11 installed.
*   uAgents library installed. If not already installed, use:

*   Requests library installed:

*   A valid API key for ASI-1 Mini. Get your API Key [here](https://innovationlab.fetch.ai/resources/docs/asione/asi1-mini-getting-started#how-to-get-an-api-key).

Script Breakdown[â€‹](#script-breakdown "Direct link to Script Breakdown")
------------------------------------------------------------------------

**1\. Importing Required Libraries**

The script begins by importing the necessary modules:

*   **requests:** To perform HTTP requests to the ASI-1 Mini API.
*   **json:** For handling JSON responses and potential JSON decode errors.
*   **uagents:** To create the uAgent and manage events.
*   **Context:** Provides access to the agent's runtime context, including logging.

    import requestsimport jsonfrom uagents import Agent, Context

**2\. Initializing the uAgent**

An instance of the uAgent is created with a name, port, and endpoint.

    agent = Agent(    name="AI_Language_Tutor",    port=8000,  # You can change this to any available port    endpoint="http://localhost:8000/submit")

**3\. Defining the ASI-1 Mini API Helper Function**

The function `get_language_help` constructs a custom prompt and sends a POST request to the ASI-1 Mini API. Based on the user's query and target language, it asks the API to either translate, correct grammar, or offer pronunciation tips.

    def get_language_help(query: str, target_language: str = "Spanish") -> str:    url = "https://api.asi1.ai/v1/chat/completions"    headers = {        'Content-Type': 'application/json',        'Authorization': f'Bearer <Your_ASI1_Mini_API_Key>'  # Replace with your API Key    }    prompt = f"""You are an AI language tutor. Help the user with their language learning request:    - If they ask for a **translation**, provide it in {target_language}.    - If they provide a sentence, **correct any grammar mistakes**.    - If they ask for pronunciation tips, explain how to say it.    User request: "{query}"    """    payload = {        "model": "asi1-mini",        "messages": [{"role": "user", "content": prompt}],        "temperature": 0.7,        "max_tokens": 0    }    try:        response = requests.post(url, headers=headers, json=payload)        response.raise_for_status()        return response.json()['choices'][0]['message']['content']    except requests.exceptions.RequestException as e:        return f"API Request Error: {str(e)}"    except json.JSONDecodeError:        return "API Error: Unable to parse JSON response"

**4\. Startup Event Handler**

The agent registers an `on_event("startup")` handler. When the agent starts, it:

*   Logs a sample query.
*   Calls the `get_language_help` function with an example query.
*   Logs the returned response from the ASI-1 Mini API.

    @agent.on_event("startup")async def language_tutor_demo(ctx: Context):    query = "How do you say 'Good morning' in French?"  # Example query    ctx.logger.info(f"User query: {query}")    response = get_language_help(query, target_language="French")    ctx.logger.info(f"ğŸŒ AI Tutor Response: {response}")

**5\. Running the Agent**

The script concludes by running the agent, which starts the server and registers the startup handler.

    if __name__ == "__main__":    agent.run()

Running the Agent[â€‹](#running-the-agent "Direct link to Running the Agent")
---------------------------------------------------------------------------

**1\. Activate Your Virtual Environment:**

    source venv/bin/activate   # On Windows: venv\Scripts\activate

**2\. Run the Script:**

    python my_language_tutor.py

**3\. Expected Output:**

Upon startup, you should see log messages similar to:

    INFO: [AI_Language_Tutor]: Starting agent with address: agent1...INFO: [AI_Language_Tutor]: User query: How do you say 'Good morning' in French?INFO: [AI_Language_Tutor]: ğŸŒ AI Tutor Response: Bonjour (Good morning/Good day).

GitHub Repository[â€‹](#github-repository "Direct link to GitHub Repository")
---------------------------------------------------------------------------

For the complete code and additional examples, visit the [ASI-1-Mini-simple-Examples](https://github.com/abhifetch/ASI-1-Mini-simple-Examples/blob/main/language_tutor.py) repository.

note

**Note:** You can learn more about ASI-1 Mini APIs [**here**](https://docs.asi1.ai/docs/).</content>
</page>

<page>
  <title>BNB Chain Agents | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/examples/on-chain-examples/bnb-chain-agents</url>
  <content>Version: 1.0.4

This guide demonstrates how to create and use AI agents for interacting with the BNB Chain using uAgents. We'll build a system of three agents that work together to send transactions, validate them, and monitor wallet activity.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

The system consists of three main agents:

1.  **Transaction Sender Agent (Agent1)**: Handles BNB transfer requests and initiates transactions
2.  **Transaction Validator Agent (Agent2)**: Verifies transaction status using BscScan API
3.  **Wallet Monitor Agent**: Monitors specified wallet addresses for any transaction activity

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

Before getting started, you'll need:

*   Python 3.11 or higher
*   A BNB Testnet account with some test BNB
*   [BscScan API Key](https://docs.bscscan.com/getting-started/viewing-api-usage-statistics)
*   Basic understanding of Web3 and blockchain concepts

Architecture Diagram[â€‹](#architecture-diagram "Direct link to Architecture Diagram")
------------------------------------------------------------------------------------

Installation[â€‹](#installation "Direct link to Installation")
------------------------------------------------------------

1.  Create a new directory and set up your virtual environment:

    mkdir bnb-chain-agentscd bnb-chain-agentspython -m venv venvsource venv/bin/activate  # On macOS/Linuxvenv\Scripts\activate     # On Windows

2.  Install the required packages:

    pip install uagents web3 python-dotenv requests

3.  Create a `.env` file with your credentials:

    USER_WALLET=your_wallet_addressUSER_KEY=your_private_keyBSCSCAN_API_KEY=your_bscscan_api_key

Implementation[â€‹](#implementation "Direct link to Implementation")
------------------------------------------------------------------

### 1\. Transaction Sender Agent (agent1.py)[â€‹](#1-transaction-sender-agent-agent1py "Direct link to 1. Transaction Sender Agent (agent1.py)")

This agent handles BNB transfer requests and communicates with the validator agent:

    from uagents import Agent, Context, Modelfrom web3 import Web3from web3.middleware import geth_poa_middlewareimport osfrom dotenv import load_dotenv# Load environment variablesload_dotenv()agent_user = Agent(    name='User BNB Agent to make transactions',    port=8000,    endpoint=['http://localhost:8000/submit'])class RequestTransfer(Model):    to_address: str    amount: float    agent_to_address: strclass RequestDetails(Model):    tx_hash: strclass ResponseTransfer(Model):    response: str# Connect to BNB Chain Testnetprovider_url = "https://data-seed-prebsc-1-s1.binance.org:8545/"web3 = Web3(Web3.HTTPProvider(provider_url))web3.middleware_onion.inject(geth_poa_middleware, layer=0)@agent_user.on_rest_post("/send/bnb", RequestTransfer, ResponseTransfer)async def handle_post(ctx: Context, req: RequestTransfer) -> ResponseTransfer:    try:        # Get wallet credentials        user_wallet = os.getenv("USER_WALLET")        user_key = os.getenv("USER_KEY")                # Prepare transaction        to_address = web3.to_checksum_address(req.to_address)        nonce = web3.eth.get_transaction_count(user_wallet)                tx = {            'to': to_address,            'value': web3.to_wei(req.amount, 'ether'),            'gas': 21000,            'gasPrice': web3.eth.gas_price,            'nonce': nonce,            'chainId': 97  # BNB Testnet        }        # Sign and send transaction        signed_tx = web3.eth.account.sign_transaction(tx, user_key)        tx_hash = web3.eth.send_raw_transaction(signed_tx.raw_transaction)                # Send to validator agent        message, status = await ctx.send_and_receive(            req.agent_to_address,            RequestDetails(tx_hash=web3.to_hex(tx_hash)),            response_type=ResponseTransfer        )                return ResponseTransfer(response=message.response)        except Exception as e:        return ResponseTransfer(response=f"Transaction Failed: {str(e)}")if __name__ == "__main__":    agent_user.run()

### 2\. Transaction Validator Agent (agent2.py)[â€‹](#2-transaction-validator-agent-agent2py "Direct link to 2. Transaction Validator Agent (agent2.py)")

This agent verifies transaction status using the BscScan API:

    from uagents import Agent, Context, Modelfrom web3 import Web3import osfrom dotenv import load_dotenvimport asyncioimport requestsagent_dummy = Agent(    name='Dummy BNB Agent to make transactions',    port=8001,    endpoint=['http://localhost:8001/submit'])load_dotenv()BSCSCAN_API_KEY = os.getenv("BSCSCAN_API_KEY")class RequestDetails(Model):    tx_hash: strclass ResponseTransfer(Model):    response: strasync def get_transaction_status(tx_hash: str) -> dict:    base_url = "https://api.bscscan.com/api"    params = {        "module": "transaction",        "action": "getstatus",        "txhash": tx_hash,        "apikey": BSCSCAN_API_KEY    }        response = await asyncio.to_thread(requests.get, base_url, params=params)    return response.json() if response.status_code == 200 else {"error": f"HTTP error {response.status_code}"}@agent_dummy.on_message(model=RequestDetails, replies={ResponseTransfer})async def startup_handler(ctx: Context, sender: str, msg: RequestDetails):    tx_status = await get_transaction_status(msg.tx_hash)        if "error" in tx_status:        reply = f"API Error: {tx_status['error']}"    else:        if tx_status.get("status") == "1" and tx_status.get("message") == "OK":            if tx_status["result"].get("isError") == "0":                reply = f"Successful transfer confirmed by receiver.âœ… tx_hash: {msg.tx_hash}"            else:                err_desc = tx_status["result"].get("errDescription", "Unknown error")                reply = f"Transfer rejected: {err_desc}"        else:            reply = "Transfer status unknown or API response error."    await ctx.send(sender, ResponseTransfer(response=reply))if __name__ == "__main__":    agent_dummy.run()

### 3\. Wallet Monitor Agent (monitor\_wallet.py)[â€‹](#3-wallet-monitor-agent-monitor_walletpy "Direct link to 3. Wallet Monitor Agent (monitor_wallet.py)")

This agent monitors wallet activity in real-time:

    from uagents import Agent, Contextfrom web3 import Web3from web3.middleware import geth_poa_middlewareimport jsonimport osfrom dotenv import load_dotenvload_dotenv()user_wallet = os.getenv("USER_WALLET")monitor_agent = Agent(    name='Monitor BNB Agent to monitor address',    port=8003,    endpoint=['http://localhost:8003/submit'])# Connect to BNB Chain Testnetprovider_url = "https://data-seed-prebsc-1-s1.binance.org:8545/"web3 = Web3(Web3.HTTPProvider(provider_url))web3.middleware_onion.inject(geth_poa_middleware, layer=0)monitored_address = web3.to_checksum_address(user_wallet)last_scanned_block = web3.eth.block_number@monitor_agent.on_interval(period=10)async def monitor_handler(ctx: Context):    global last_scanned_block    try:        current_block = web3.eth.block_number        if current_block <= last_scanned_block:            return        for block_num in range(last_scanned_block + 1, current_block + 1):            block = web3.eth.get_block(block_num, full_transactions=True)            for tx in block['transactions']:                tx_from = tx['from']                tx_to = tx['to']                                if (tx_from and tx_from.lower() == monitored_address.lower()) or \                   (tx_to and tx_to.lower() == monitored_address.lower()):                    details = {                        "blockNumber": block_num,                        "hash": tx['hash'].hex(),                        "from": tx_from,                        "to": tx_to if tx_to else "Contract Creation",                        "value": str(web3.from_wei(tx['value'], 'ether')) + " BNB"                    }                    ctx.logger.info(f"Recorded transaction: {json.dumps(details, indent=2)}")        last_scanned_block = current_block    except Exception as e:        ctx.logger.error(f"Error during monitoring: {e}")if __name__ == "__main__":    monitor_agent.run()

Usage[â€‹](#usage "Direct link to Usage")
---------------------------------------

1.  Start all three agents in separate terminal windows:

    # Terminal 1python agent1.py# Terminal 2python agent2.py# Terminal 3python monitor_wallet.py

2.  Send a BNB transfer request using curl:

    curl -d '{    "to_address": "RECIPIENT_ADDRESS",    "amount": 0.01,    "agent_to_address": "AGENT2_ADDRESS"}' \-H "Content-Type: application/json" \-X POST http://localhost:8000/send/bnb

Replace `RECIPIENT_ADDRESS` with the destination wallet address and `AGENT2_ADDRESS` with the address of your validator agent.

System Flow[â€‹](#system-flow "Direct link to System Flow")
---------------------------------------------------------

1.  The Transaction Sender Agent receives a transfer request via HTTP POST
2.  It creates and signs the transaction using the private key
3.  The transaction is sent to the BNB Chain Testnet
4.  The transaction hash is forwarded to the Validator Agent
5.  The Validator Agent checks the transaction status using BscScan API
6.  The Wallet Monitor Agent continuously scans for new transactions
7.  All agents log their activities and transaction details

Best Practices[â€‹](#best-practices "Direct link to Best Practices")
------------------------------------------------------------------

1.  **Security**:
    
    *   Never commit your `.env` file or expose private keys
    *   Use environment variables for sensitive data
    *   Validate input data before processing
2.  **Error Handling**:
    
    *   Implement proper error handling for API calls
    *   Log errors and transaction details
    *   Provide meaningful error messages
3.  **Monitoring**:
    
    *   Use the monitoring agent to track transactions
    *   Implement alerts for suspicious activities
    *   Keep logs for auditing purposes

Conclusion[â€‹](#conclusion "Direct link to Conclusion")
------------------------------------------------------

This example demonstrates how to create a system of AI agents that interact with the BNB Chain. The agents work together to handle transactions, validate them, and monitor wallet activity. This system can be extended to include more complex functionality like smart contract interactions, automated trading, or blockchain analytics.</content>
</page>

<page>
  <title>Creating ASI1 Compatible uAgent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/examples/chat-protocol/asi-compatible-uagents</url>
  <content>This guide demonstrates how to make your agents accessible via ASI1 LLM by integrating the chat protocol. We'll use a Football Team Agent example to show how the chat protocol enables seamless communication between agents and the LLM.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

In this example, you'll learn how to build a uAgent compatible with Fetch.ai's ASI1 Large Language Model (LLM). Using a Football Team Agent as an example, the guide shows how you can enable your agent to understand and respond to natural language queries.

Message Flow[â€‹](#message-flow "Direct link to Message Flow")
------------------------------------------------------------

The communication flow between ASI1 LLM, the Football Team Agent, and OpenAI Agent follows this sequence:

1.  **Query Initiation (1.1)**
    
    *   ASI1 LLM sends a natural language query (e.g., "Give me the list of players in Manchester United Football Team") as a `ChatMessage` to the Football Team Agent on the `ChatMessage handler`.
2.  **Parameter Extraction (2, 3)**
    
    *   The Football Team Agent forwards the query to OpenAI Agent for parameter extraction
    *   OpenAI Agent processes the natural language and extracts structured parameters (e.g., team\_name="Manchester United")
    *   The parameters are returned in a Pydantic Model format as `StructuredOutputResponse` on the `StructuredOutputResponse handler`.
3.  **Team Data Processing (4, 5)**
    
    *   The Football Team Agent calls the `get_team_info` function with the extracted parameters
    *   The function returns the team details.
4.  **Football Team Agent Response (6.1)**
    
    *   The Football Team Agent sends the formatted response back as a `ChatMessage` to ASI1 LLM.
5.  **Message Acknowledgements (1.2, 6.2)**
    
    *   Each message exchanged using the chat protocol is automatically acknowledged by the receiving agent using `ChatAcknowledgement`.

Here's a visual representation of the flow:

Implementation[â€‹](#implementation "Direct link to Implementation")
------------------------------------------------------------------

In this example we will create an agent and its associated files on [Agentverse](https://agentverse.ai/) that communicate using the chat protocol with the ASI1 LLM. Refer to the [Hosted Agents](https://innovationlab.fetch.ai/resources/docs/agent-creation/uagent-creation#hosted-agents) section to understand the detailed steps for agent creation on Agentverse.

Create a new agent named "FootballTeamAgent" on Agentverse and create the following files:

    agent.py        # Main agent file football.py   # Football team service implementation and API integrationchat_proto.py   # Chat protocol implementation for enabling text based communication 

To create a new file on Agentverse:

1.  Click on the New File icon

2.  Assign a name to the File

3.  Directory Structure

### 1\. Football Team Function and Data Models[â€‹](#1-football-team-function-and-data-models "Direct link to 1. Football Team Function and Data Models")

Let's start by defining our data models and the function to retrieve the list of players in a football team. These models will define how we request team information and receive responses. We'll use the AllSports API to fetch team and player information. You can obtain your API key by signing up at [AllSports API](https://allsportsapi.com/), which provides comprehensive sports data feeds including football (soccer) team and player information.

To implement the football team service add the following chat protocol in the `football.py` file created on Agentverse:

football.py

    import requestsfrom uagents import Model, FieldAPI_KEY = "YOUR_ALLSPORTS_API_KEY"BASE_URL = "https://apiv2.allsportsapi.com/football/"class FootballTeamRequest(Model):    team_name: strclass FootballTeamResponse(Model):    results: strasync def get_team_info(team_name: str) -> str:    """    Fetch team information from AllSportsAPI and return as plain text    """    try:        params = {            "met": "Teams",            "teamName": team_name,            "APIkey": API_KEY        }        response = requests.get(BASE_URL, params=params)        data = response.json()        if data.get("success") == 1 and data.get("result"):            team_info = data["result"][0]            result = f"\nTeam Name: {team_info['team_name']}\n"            result += f"Team Logo: {team_info['team_logo']}\n\n"            result += "Players:\n"                        for player in team_info.get("players", []):                result += f"- Name: {player['player_name']}\n"                result += f"  Type: {player['player_type']}\n"                result += f"  Image: {player['player_image']}\n\n"                        return result        else:            return "Team not found or invalid API key."                except Exception as e:        return f"Error fetching team information: {str(e)}"

### 2\. Chat Protocol Integration[â€‹](#2-chat-protocol-integration "Direct link to 2. Chat Protocol Integration")

The `chat_proto.py` file is essential for enabling natural language communication between your agent and ASI1 LLM.

#### LLM Integration[â€‹](#llm-integration "Direct link to LLM Integration")

To extract the important parameters from a user query passed by the LLM, you can use any of the following LLMs that support structured output:

*   OpenAI Agent: `agent1q0h70caed8ax769shpemapzkyk65uscw4xwk6dc4t3emvp5jdcvqs9xs32y`
*   Claude.ai Agent: `agent1qvk7q2av3e2y5gf5s90nfzkc8a48q3wdqeevwrtgqfdl0k78rspd6f2l4dx`

> **Note**: To ensure fair usage, each agent is limited to 6 requests per hour. Please implement appropriate rate limiting in your application.

#### Message Flow[â€‹](#message-flow-1 "Direct link to Message Flow")

1.  When a user sends a query (e.g., "Show me Manchester United players"):
    
    *   The `ChatMessage` handler receives the message
    *   Acknowledges receipt to the sender
    *   Forwards the query to the chosen LLM for processing
2.  The LLM processes the query and returns a `StructuredOutputResponse`:
    
    *   Extracts relevant parameters (e.g., team\_name="Manchester United")
    *   Returns data in the format specified by your agent's schema
3.  Your agent processes the structured data:
    
    *   Calls appropriate functions (e.g., `get_team_info`)
    *   Formats the response and sends it back.

#### Customizing the Handlers[â€‹](#customizing-the-handlers "Direct link to Customizing the Handlers")

When implementing a new agent for a different use case, you'll need to:

*   Update the schema in `StructuredOutputPrompt` to match your agent's data model
*   Modify the response handling in `handle_structured_output_response` function for your use case.

To enable natural language communication with your agent add the following chat protocol in the `chat_proto.py` file created on Agentverse:

chat\_proto.py

    from datetime import datetimefrom uuid import uuid4from typing import Anyfrom uagents import Context, Model, Protocol#Import the necessary components of the chat protocolfrom uagents_core.contrib.protocols.chat import (    ChatAcknowledgement,    ChatMessage,    EndSessionContent,    StartSessionContent,    TextContent,    chat_protocol_spec,)from football import get_team_info, FootballTeamRequest#Replace the AI Agent Address with anyone of the following LLMs as they support StructuredOutput required for the processing of this agent. AI_AGENT_ADDRESS = 'agent1q0h70caed8ax769shpemapzkyk65uscw4xwk6dc4t3emvp5jdcvqs9xs32y'if not AI_AGENT_ADDRESS:    raise ValueError("AI_AGENT_ADDRESS not set")def create_text_chat(text: str, end_session: bool = False) -> ChatMessage:    content = [TextContent(type="text", text=text)]    if end_session:        content.append(EndSessionContent(type="end-session"))    return ChatMessage(        timestamp=datetime.utcnow(),        msg_id=uuid4(),        content=content,    )chat_proto = Protocol(spec=chat_protocol_spec)struct_output_client_proto = Protocol(    name="StructuredOutputClientProtocol", version="0.1.0")class StructuredOutputPrompt(Model):    prompt: str    output_schema: dict[str, Any]class StructuredOutputResponse(Model):    output: dict[str, Any]@chat_proto.on_message(ChatMessage)async def handle_message(ctx: Context, sender: str, msg: ChatMessage):    ctx.logger.info(f"Got a message from {sender}: {msg.content}")    ctx.storage.set(str(ctx.session), sender)    await ctx.send(        sender,        ChatAcknowledgement(timestamp=datetime.utcnow(), acknowledged_msg_id=msg.msg_id),    )    for item in msg.content:        if isinstance(item, StartSessionContent):            ctx.logger.info(f"Got a start session message from {sender}")            continue        elif isinstance(item, TextContent):            ctx.logger.info(f"Got a message from {sender}: {item.text}")            ctx.storage.set(str(ctx.session), sender)            await ctx.send(                AI_AGENT_ADDRESS,                StructuredOutputPrompt(                    prompt=item.text, output_schema=FootballTeamRequest.schema()                ),            )        else:            ctx.logger.info(f"Got unexpected content from {sender}")@chat_proto.on_message(ChatAcknowledgement)async def handle_ack(ctx: Context, sender: str, msg: ChatAcknowledgement):    ctx.logger.info(        f"Got an acknowledgement from {sender} for {msg.acknowledged_msg_id}"    )@struct_output_client_proto.on_message(StructuredOutputResponse)async def handle_structured_output_response(    ctx: Context, sender: str, msg: StructuredOutputResponse):    session_sender = ctx.storage.get(str(ctx.session))    if session_sender is None:        ctx.logger.error(            "Discarding message because no session sender found in storage"        )        return    if "<UNKNOWN>" in str(msg.output):        await ctx.send(            session_sender,            create_text_chat(                "Sorry, I couldn't process your location request. Please try again later."            ),        )        return    prompt = FootballTeamRequest.parse_obj(msg.output)    try:        team_info = await get_team_info(prompt.team_name)    except Exception as err:        ctx.logger.error(err)        await ctx.send(            session_sender,            create_text_chat(                "Sorry, I couldn't process your request. Please try again later."            ),        )        return    if "error" in team_info:        await ctx.send(session_sender, create_text_chat(str(team_info["error"])))        return    chat_message = create_text_chat(team_info)    await ctx.send(session_sender, chat_message)

### 3\. Football Team Agent Setup[â€‹](#3-football-team-agent-setup "Direct link to 3. Football Team Agent Setup")

The `agent.py` file is the core of your application. Think of it as the main control center that:

*   Sets up your agent
*   Handles incoming requests
*   Manages rate limiting
*   Monitors the agent's health

Let's break down each component:

#### Basic Setup[â€‹](#basic-setup "Direct link to Basic Setup")

    from uagents import Agent, Context, Modelfrom uagents.experimental.quota import QuotaProtocol, RateLimitagent = Agent()  # Create a new agent instance

Import the necessary packages and initialise your agent.

#### Rate Limiting Setup[â€‹](#rate-limiting-setup "Direct link to Rate Limiting Setup")

    proto = QuotaProtocol(    storage_reference=agent.storage,    name="Football-Team-Protocol",    version="0.1.0",    default_rate_limit=RateLimit(window_size_minutes=60, max_requests=30),)

To ensure fair usage and prevent API abuse, we recommend implementing the QuotaProtocol. While optional, this protocol helps manage service usage by limiting requests. In this example, we've set a limit of 30 requests per hour per user, but you can adjust this threshold based on your agent's functionality. This helps maintain service reliability and protects your agent from excessive requests.

#### Request Handler[â€‹](#request-handler "Direct link to Request Handler")

    @proto.on_message(    FootballTeamRequest, replies={FootballTeamResponse, ErrorMessage})async def handle_request(ctx: Context, sender: str, msg: FootballTeamRequest):    ctx.logger.info("Received team info request")    try:        results = await get_team_info(msg.team_name)        await ctx.send(sender, FootballTeamResponse(results=results))    except Exception as err:        await ctx.send(sender, ErrorMessage(error=str(err)))

This message handler allows your agent to receive direct requests from other agents in a structured format. While we're using it with ASI1 LLM in this example, it's versatile enough to work with agents that don't have the chat protocol enabled. This makes it particularly useful in multi-agent systems, where other agents can directly request information directly.

#### Health Monitoring[â€‹](#health-monitoring "Direct link to Health Monitoring")

    ### Health check related codedef agent_is_healthy() -> bool:    """    Implement the actual health check logic here.    For example, check if the agent can connect to the AllSports API.    """    try:        import asyncio        asyncio.run(get_team_info("Manchester United"))        return True    except Exception:        return Falseclass HealthCheck(Model):    passclass HealthStatus(str, Enum):    HEALTHY = "healthy"    UNHEALTHY = "unhealthy"class AgentHealth(Model):    agent_name: str    status: HealthStatushealth_protocol = QuotaProtocol(    storage_reference=agent.storage, name="HealthProtocol", version="0.1.0")@health_protocol.on_message(HealthCheck, replies={AgentHealth})async def handle_health_check(ctx: Context, sender: str, msg: HealthCheck):    status = HealthStatus.UNHEALTHY    try:        if agent_is_healthy():            status = HealthStatus.HEALTHY    except Exception as err:        ctx.logger.error(err)    finally:        await ctx.send(sender, AgentHealth(agent_name="football_agent", status=status))

The HealthProtocol, while optional, provides a periodic health check system to monitor your agent's performance and reliability. It tests the agent's ability to fetch information about a team and verifies that all components are working correctly. This monitoring helps quickly identify and address any issues that might affect your agent's service, ensuring reliable operation for your users.

#### Protocol Registration[â€‹](#protocol-registration "Direct link to Protocol Registration")

    agent.include(proto, publish_manifest=True)agent.include(health_protocol, publish_manifest=True)agent.include(chat_proto, publish_manifest=True)agent.include(struct_output_client_proto, publish_manifest=True)

This registers all the necessary protocols with your agent:

*   Main protocol for handling team requests
*   Health monitoring protocol
*   Chat protocol for natural language communication
*   Structured output protocol for formatted responses

Here's the complete implementation:

agent.py

    import osfrom enum import Enumfrom uagents import Agent, Context, Modelfrom uagents.experimental.quota import QuotaProtocol, RateLimitfrom uagents_core.models import ErrorMessagefrom chat_proto import chat_proto, struct_output_client_protofrom football import get_team_info, FootballTeamRequest, FootballTeamResponseagent = Agent()proto = QuotaProtocol(    storage_reference=agent.storage,    name="Football-Team-Protocol",    version="0.1.0",    default_rate_limit=RateLimit(window_size_minutes=60, max_requests=30),)@proto.on_message(    FootballTeamRequest, replies={FootballTeamResponse, ErrorMessage})async def handle_request(ctx: Context, sender: str, msg: FootballTeamRequest):    ctx.logger.info("Received team info request")    try:        results = await get_team_info(msg.team_name)        ctx.logger.info(f'printing results in function {results}')        ctx.logger.info("Successfully fetched team information")        await ctx.send(sender, FootballTeamResponse(results=results))    except Exception as err:        ctx.logger.error(err)        await ctx.send(sender, ErrorMessage(error=str(err)))agent.include(proto, publish_manifest=True)### Health check related codedef agent_is_healthy() -> bool:    """    Implement the actual health check logic here.    For example, check if the agent can connect to the AllSports API.    """    try:        import asyncio        asyncio.run(get_team_info("Manchester United"))        return True    except Exception:        return Falseclass HealthCheck(Model):    passclass HealthStatus(str, Enum):    HEALTHY = "healthy"    UNHEALTHY = "unhealthy"class AgentHealth(Model):    agent_name: str    status: HealthStatushealth_protocol = QuotaProtocol(    storage_reference=agent.storage, name="HealthProtocol", version="0.1.0")@health_protocol.on_message(HealthCheck, replies={AgentHealth})async def handle_health_check(ctx: Context, sender: str, msg: HealthCheck):    status = HealthStatus.UNHEALTHY    try:        if agent_is_healthy():            status = HealthStatus.HEALTHY    except Exception as err:        ctx.logger.error(err)    finally:        await ctx.send(sender, AgentHealth(agent_name="football_agent", status=status))agent.include(health_protocol, publish_manifest=True)agent.include(chat_proto, publish_manifest=True)agent.include(struct_output_client_proto, publish_manifest=True)if __name__ == "__main__":    agent.run() 

Adding a README to your Agent[â€‹](#adding-a-readme-to-your-agent "Direct link to Adding a README to your Agent")
---------------------------------------------------------------------------------------------------------------

1.  Go to the Overview section in the Editor.
    
2.  Click on Edit and add a good description for your Agent so that it can be easily searchable by the ASI1 LLM. Please refer the [Importance of Good Readme](https://innovationlab.fetch.ai/resources/docs/agentverse/searching#importance-of-good-readme) section for more details.
    
3.  Make sure the Agent has the right `AgentChatProtocol`.
    

Test your Agent[â€‹](#test-your-agent "Direct link to Test your Agent")
---------------------------------------------------------------------

1.  Start your Agent

2.  To test your agent, use the [Agentverse Chat Interface](https://chat.agentverse.ai/). You can either search for your Agent by the Agent's name or by the Agent's address.

3.  Select your Agent from the list and type in a query to ask your Agent and it should return a response back with the Team Details.

Query your Agent from ASI1 LLM[â€‹](#query-your-agent-from-asi1-llm "Direct link to Query your Agent from ASI1 LLM")
------------------------------------------------------------------------------------------------------------------

1.  Login to the [ASI1 LLM](https://asi1.ai/), either using your Google Account or the ASI1 Wallet and Start a New Chat.
    
2.  Toggle the "Agents" switch to enable ASI1 to connect with Agents on Agentverse.
    

3.  Type in a query to ask your Agent for instance 'I want to get the player details for the Manchester City Football Team'.

> **Note**: The ASI1 LLM may not always select your agent for answering the query as it is designed to pick the best agent for a task based on a number of parameters.</content>
</page>

<page>
  <title>CrewAI Adapter Example | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/examples/adapters/crewai-adapter-example</url>
  <content>CrewAI Adapter for uAgents
--------------------------

This example demonstrates how to integrate a **CrewAI multi-agent system** with the **uAgents ecosystem** using the uAgents Adapter package. CrewAI allows you to create collaborative teams of AI agents working together to accomplish complex tasks.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

The CrewAI adapter enables:

*   Creating specialized agent teams with distinct roles and responsibilities
*   Orchestrating complex workflows between different AI agents
*   Exposing CrewAI teams as uAgents for seamless communication with the broader agent ecosystem
*   Deploying CrewAI applications to the Agentverse network

Trip Planner Example[â€‹](#trip-planner-example "Direct link to Trip Planner Example")
------------------------------------------------------------------------------------

Let's look at a real-world example of a trip planning system with multiple specialized agents working together to create a complete travel itinerary. We'll compare the standard CrewAI implementation with the uAgents-integrated version.

### Standard CrewAI Implementation[â€‹](#standard-crewai-implementation "Direct link to Standard CrewAI Implementation")

First, let's look at how a standard CrewAI system is implemented without uAgents integration:

    # Standard main.pyfrom textwrap import dedentfrom crewai import Crewfrom dotenv import load_dotenvfrom trip_agents import TripAgentsfrom trip_tasks import TripTasksload_dotenv()class TripCrew:    def __init__(self, origin, cities, date_range, interests):        self.cities = cities        self.origin = origin        self.interests = interests        self.date_range = date_range    def run(self):        agents = TripAgents()        tasks = TripTasks()        city_selector_agent = agents.city_selection_agent()        local_expert_agent = agents.local_expert()        travel_concierge_agent = agents.travel_concierge()        identify_task = tasks.identify_task(            city_selector_agent,            self.origin,            self.cities,            self.interests,            self.date_range,        )        gather_task = tasks.gather_task(local_expert_agent, self.origin, self.interests, self.date_range)        plan_task = tasks.plan_task(travel_concierge_agent, self.origin, self.interests, self.date_range)        crew = Crew(            agents=[city_selector_agent, local_expert_agent, travel_concierge_agent],            tasks=[identify_task, gather_task, plan_task],            verbose=True,        )        result = crew.kickoff()        return resultif __name__ == "__main__":    print("## Welcome to Trip Planner Crew")    print("-------------------------------")    location = input(        dedent("""      From where will you be traveling from?    """)    )    cities = input(        dedent("""      What are the cities options you are interested in visiting?    """)    )    date_range = input(        dedent("""      What is the date range you are interested in traveling?    """)    )    interests = input(        dedent("""      What are some of your high level interests and hobbies?    """)    )    trip_crew = TripCrew(location, cities, date_range, interests)    result = trip_crew.run()    print("\n\n########################")    print("## Here is you Trip Plan")    print("########################\n")    print(result)

### uAgents Integration[â€‹](#uagents-integration "Direct link to uAgents Integration")

Now, let's see how we can integrate this same CrewAI system with uAgents to enable network communication:

    #!/usr/bin/env python3"""Trip Planner script using CrewAI adapter for uAgents."""import osfrom crewai import Crewfrom dotenv import load_dotenvfrom uagents_adapter import CrewaiRegisterToolfrom trip_agents import TripAgentsfrom trip_tasks import TripTasksclass TripCrew:    def __init__(self, origin, cities, date_range, interests):        self.cities = cities        self.origin = origin        self.interests = interests        self.date_range = date_range    def run(self):        agents = TripAgents()        tasks = TripTasks()        city_selector_agent = agents.city_selection_agent()        local_expert_agent = agents.local_expert()        travel_concierge_agent = agents.travel_concierge()        identify_task = tasks.identify_task(            city_selector_agent,            self.origin,            self.cities,            self.interests,            self.date_range,        )        gather_task = tasks.gather_task(local_expert_agent, self.origin, self.interests, self.date_range)        plan_task = tasks.plan_task(travel_concierge_agent, self.origin, self.interests, self.date_range)        crew = Crew(            agents=[city_selector_agent, local_expert_agent, travel_concierge_agent],            tasks=[identify_task, gather_task, plan_task],            verbose=True,        )        result = crew.kickoff()        return result    def kickoff(self, inputs=None):        """        Compatibility method for uAgents integration.        Accepts a dictionary of inputs and calls run() with them.        """        if inputs:            self.origin = inputs.get("origin", self.origin)            self.cities = inputs.get("cities", self.cities)            self.date_range = inputs.get("date_range", self.date_range)            self.interests = inputs.get("interests", self.interests)        return self.run()def main():    """Main function to demonstrate Trip Planner with CrewAI adapter."""    # Load API key from environment    load_dotenv()    api_key = os.getenv("AGENTVERSE_API_KEY")    openai_api_key = os.getenv("OPENAI_API_KEY")    if not api_key:        print("Error: AGENTVERSE_API_KEY not found in environment")        return    if not openai_api_key:        print("Error: OPENAI_API_KEY not found in environment")        return    # Set OpenAI API key in environment    os.environ["OPENAI_API_KEY"] = openai_api_key    # Create an instance of TripCrew with default empty values    trip_crew = TripCrew("", "", "", "")    # Create tool for registering the crew with Agentverse    register_tool = CrewaiRegisterTool()    # Define parameters schema for the trip planner    query_params = {        "origin": {"type": "str", "required": True},        "cities": {"type": "str", "required": True},        "date_range": {"type": "str", "required": True},        "interests": {"type": "str", "required": True},    }    # Register the crew with parameter schema    result = register_tool.run(        tool_input={            "crew_obj": trip_crew,            "name": "Trip Planner Crew AI Agent adapters",            "port": 8080,            "description": "A CrewAI agent that helps plan trips based on preferences",            "api_token": api_key,            "mailbox": True,            "query_params": query_params,            "example_query": "Plan a trip from New York to Paris in June, I'm interested in art and history other than museums.",        }    )    # Get the agent address from the result    if isinstance(result, dict) and "address" in result:        result["address"]    print(f"\nCrewAI agent registration result: {result}")    # Keep the program running    try:        while True:            import time            time.sleep(1)    except KeyboardInterrupt:        print("\nExiting...")if __name__ == "__main__":    main()

Key Differences in uAgents Integration[â€‹](#key-differences-in-uagents-integration "Direct link to Key Differences in uAgents Integration")
------------------------------------------------------------------------------------------------------------------------------------------

When integrating a CrewAI system with uAgents, there are several important differences:

1.  **CrewaiRegisterTool**:
    
    *   Uses the specialized `CrewaiRegisterTool` instead of the generic `UAgentRegisterTool`.
    *   This tool is specifically designed to handle CrewAI's collaborative agent structure.
2.  **Kickoff Method**:
    
    *   The `TripCrew` class has an additional `kickoff` method that serves as an adapter between uAgents messages and the CrewAI system.
    *   It extracts parameters from the input dictionary and passes them to the actual execution method.
3.  **Parameter Schema**:
    
    *   A `query_params` schema is defined to validate and structure inputs to the CrewAI system.
    *   This allows for better error handling and client guidance when using the agent.
4.  **Example Query**:
    
    *   An example query is provided to help users understand the expected input format.
    *   This improves usability when interacting with the agent through chat protocols.

Specialized Agents in the Trip Planner[â€‹](#specialized-agents-in-the-trip-planner "Direct link to Specialized Agents in the Trip Planner")
------------------------------------------------------------------------------------------------------------------------------------------

The trip planning system uses three specialized agents, defined in `trip_agents.py`:

1.  **City Selection Agent**: Analyzes client preferences to select the optimal city to visit
2.  **Local Expert**: Identifies authentic local experiences and hidden gems
3.  **Travel Concierge**: Creates detailed itineraries and plans logistics

Each agent is assigned specific tasks through the `trip_tasks.py` file:

1.  **Identify Task**: Determines the best city based on client preferences
2.  **Gather Task**: Collects detailed information about activities and attractions
3.  **Plan Task**: Creates a comprehensive itinerary with transportation details

Interacting with the Trip Planner[â€‹](#interacting-with-the-trip-planner "Direct link to Interacting with the Trip Planner")
---------------------------------------------------------------------------------------------------------------------------

Once registered as a uAgent, you can interact with the CrewAI trip planner using any uAgent client:

    from datetime import datetime, timezonefrom uuid import uuid4from uagents import Agent, Protocol, Context#import the necessary components from the chat protocolfrom uagents_core.contrib.protocols.chat import (    ChatAcknowledgement,    ChatMessage,    TextContent,    chat_protocol_spec,)# Initialise agent2agent2 = Agent(name="client_agent",               port = 8082,               mailbox=True,               seed="client agent testing seed"               )# Initialize the chat protocolchat_proto = Protocol(spec=chat_protocol_spec)langgraph_agent_address = "agent1q0zyxrneyaury3f5c7aj67hfa5w65cykzplxkst5f5mnyf4y3em3kplxn4t"# Startup Handler - Print agent details@agent2.on_event("startup")async def startup_handler(ctx: Context):    # Print agent details    ctx.logger.info(f"My name is {ctx.agent.name} and my address is {ctx.agent.address}")    # Send initial message to agent2    initial_message = ChatMessage(        timestamp=datetime.now(timezone.utc),        msg_id=uuid4(),        content=[TextContent(type="text", text="Plan a trip for me from london to paris starting on 22nd of April 2025 and I am interested in a mountains beaches and history")]    )    await ctx.send(langgraph_agent_address, initial_message)# Message Handler - Process received messages and send acknowledgements@chat_proto.on_message(ChatMessage)async def handle_message(ctx: Context, sender: str, msg: ChatMessage):    for item in msg.content:        if isinstance(item, TextContent):            # Log received message            ctx.logger.info(f"Received message from {sender}: {item.text}")                        # Send acknowledgment            ack = ChatAcknowledgement(                timestamp=datetime.now(timezone.utc),                acknowledged_msg_id=msg.msg_id            )            await ctx.send(sender, ack)            # Acknowledgement Handler - Process received acknowledgements@chat_proto.on_message(ChatAcknowledgement)async def handle_acknowledgement(ctx: Context, sender: str, msg: ChatAcknowledgement):    ctx.logger.info(f"Received acknowledgement from {sender} for message: {msg.acknowledged_msg_id}")# Include the protocol in the agent to enable the chat functionality# This allows the agent to send/receive messages and handle acknowledgements using the chat protocolagent2.include(chat_proto, publish_manifest=True)if __name__ == '__main__':    agent2.run()

Also you can interact with crewai agent using `Chat with Agent` button on agent's profile.

Benefits of the uAgents Integration[â€‹](#benefits-of-the-uagents-integration "Direct link to Benefits of the uAgents Integration")
---------------------------------------------------------------------------------------------------------------------------------

Integrating CrewAI with uAgents provides several significant advantages:

*   **Network Communication**: Enables remote access to your CrewAI system over networks
*   **Structured Inputs**: Validates inputs through a defined parameter schema
*   **Persistent Mailbox**: Allows asynchronous communication with message storage
*   **Agentverse Integration**: Makes your CrewAI system discoverable in the agent ecosystem
*   **NL Processing**: Optional AI agent integration for processing natural language queries

Getting Started[â€‹](#getting-started "Direct link to Getting Started")
---------------------------------------------------------------------

1.  Clone the [Trip Planner repository](https://github.com/abhifetch/crewai-example/tree/main/trip_planner)
    
2.  Install dependencies:
    
        pip install uagents==0.22.3 "crewai[tools]"==0.105.0 uagents-adapter==0.4.1 python-dotenv==1.0.0 langchain_openai==0.2.13
    
    Or use the provided requirements.txt:
    
        pip install -r requirements.txt
    
3.  Set up your environment variables:
    
        OPENAI_API_KEY=your_openai_keyAGENTVERSE_API_KEY=your_agentverse_keyAGENT_SEED=your_agent_seed_phrase
    
4.  Run the CrewAI trip planner with uAgents adapter:
    
        cd crewai-example/trip_plannerpython main_uagents.py
    
5.  Check the inspector link from agent ouput and go to agent profile, or directly in your [local agents](https://agentverse.ai/agents/local) look for crewai agent.
    
6.  You can communicate to this agent using below options
    

*   Click on chat with agent button and ask it to plan a trip.

*   You can also try to interact with client agent shown in section (above)\[#\].

Expected Outputs[â€‹](#expected-outputs "Direct link to Expected Outputs")
------------------------------------------------------------------------

When running the examples, you should expect to see outputs similar to these:

### Standard CrewAI (`main.py`)[â€‹](#standard-crewai-mainpy "Direct link to standard-crewai-mainpy")

    ## Welcome to Trip Planner Crew-------------------------------From where will you be traveling from?> New YorkWhat are the cities options you are interested in visiting?> Paris, Rome, BarcelonaWhat is the date range you are interested in traveling?> June 10-20, 2023What are some of your high level interests and hobbies?> Food, art, architecture, and history[City Selection Specialist] I'll analyze which city would be the best fit based on the traveler's preferences...Working on: Analyze the traveler's preferences and determine which city from the options would be the best fit...[... search and reasoning details ...]########################## Here is you Trip Plan######################### PARIS: 3-DAY FOOD & ART JOURNEY*A curated itinerary for experiencing the best of Parisian cuisine and artistic treasures*## RECOMMENDED ACCOMMODATIONSLe Marais district or Saint-Germain-des-PrÃ©s would be ideal locations, offering central positioning with charming atmosphere and proximity to key attractions.[... detailed itinerary continues ...]

### uAgents Integration (`main_uagents.py`)[â€‹](#uagents-integration-main_uagentspy "Direct link to uagents-integration-main_uagentspy")

First terminal:

    (venv) abhi@Fetchs-MacBook-Pro test examples % python3 trip_planner/main_uagents.pyINFO:     [Trip Planner Crew AI Agent adapters]: Starting agent with address: agent1q2sgs58jzw70e8vvsrlx8k3yukdqc9gwkhp8p7q6tslcxhy0eqtxyq4fv07INFO:     [Trip Planner Crew AI Agent adapters]: Agent 'Trip Planner Crew AI Agent adapters' started with address: agent1q2sgs58jzw70e8vvsrlx8k3yukdqc9gwkhp8p7q6tslcxhy0eqtxyq4fv07INFO:     [Trip Planner Crew AI Agent adapters]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8080&address=agent1q2sgs58jzw70e8vvsrlx8k3yukdqc9gwkhp8p7q6tslcxhy0eqtxyq4fv07INFO:     [Trip Planner Crew AI Agent adapters]: Starting server on http://0.0.0.0:8080 (Press CTRL+C to quit)INFO:     [Trip Planner Crew AI Agent adapters]: Starting mailbox client for https://agentverse.aiINFO:     [Trip Planner Crew AI Agent adapters]: Mailbox access token acquiredConnecting agent 'Trip Planner Crew AI Agent adapters' to Agentverse...INFO:     [mailbox]: Successfully registered as mailbox agent in AgentverseSuccessfully connected agent 'Trip Planner Crew AI Agent adapters' to AgentverseUpdating agent 'Trip Planner Crew AI Agent adapters' README on Agentverse...Successfully updated agent 'Trip Planner Crew AI Agent adapters' README on AgentverseCrewAI agent registration result: Agent 'Trip Planner Crew AI Agent adapters' registered with address: agent1q2sgs58jzw70e8vvsrlx8k3yukdqc9gwkhp8p7q6tslcxhy0eqtxyq4fv07 with mailbox (Parameters: origin, cities, date_range, interests)INFO:     [mailbox]: Successfully registered as mailbox agent in AgentverseINFO:     [Trip Planner Crew AI Agent adapters]: Got a message from agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49INFO:     [Trip Planner Crew AI Agent adapters]: Received message model digest: timestamp=datetime.datetime(2025, 4, 21, 10, 13, 39, 989489, tzinfo=datetime.timezone.utc) msg_id=UUID('7930acf1-b16e-4b20-896b-7d801763eaa6') content=[TextContent(type='text', text='Plan a trip for me from london to paris starting on 22nd of April 2025 and I am interested in a mountains beaches and history')]INFO:     [Trip Planner Crew AI Agent adapters]: Got a text message from agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49: Plan a trip for me from london to paris starting on 22nd of April 2025 and I am interested in a mountains beaches and historyINFO:     [Trip Planner Crew AI Agent adapters]: Using crew object: <__main__.TripCrew object at 0x12c1f79d0>INFO:     [Trip Planner Crew AI Agent adapters]: Extracting parameters using keys: ['origin', 'cities', 'date_range', 'interests']INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"INFO:     [Trip Planner Crew AI Agent adapters]: Extracted parameters: {'origin': 'london', 'cities': 'paris', 'date_range': '22nd of April 2025', 'interests': 'mountains beaches and history'}INFO:     [Trip Planner Crew AI Agent adapters]: Running crew with extracted parametersâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Crew Execution Started â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®â”‚                                                                                                                                      â”‚â”‚  Crew Execution Started                                                                                                              â”‚â”‚  Name: crew                                                                                                                          â”‚â”‚  ID: 1462f3ae-5ce4-4ea3-b1af-5639aac04dd2                                                                                            â”‚â”‚                                                                                                                                      â”‚â”‚                                                                                                                                      â”‚â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ğŸš€ Crew: crewâ””â”€â”€ ğŸ“‹ Task: c181e31b-6b7f-4471-ab8f-fa5f06078365       Status: Executing Task...[... crew execution continues ...]

### Client Agent (`client_agent.py`)[â€‹](#client-agent-client_agentpy "Direct link to client-agent-client_agentpy")

Second terminal:

    (venv) abhi@Fetchs-MacBook-Pro crewai-example % python3 trip_planner/client_agent.py INFO:     [client_agent]: Starting agent with address: agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49INFO:     [client_agent]: My name is client_agent and my address is agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49INFO:     [client_agent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8082&address=agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49INFO:     [client_agent]: Starting server on http://0.0.0.0:8082 (Press CTRL+C to quit)INFO:     [client_agent]: Starting mailbox client for https://agentverse.aiINFO:     [client_agent]: Manifest published successfully: AgentChatProtocolINFO:     [client_agent]: Mailbox access token acquiredINFO:     [client_agent]: Received acknowledgement from agent1q2sgs58jzw70e8vvsrlx8k3yukdqc9gwkhp8p7q6tslcxhy0eqtxyq4fv07 for message: 7930acf1-b16e-4b20-896b-7d801763eaa6INFO:     [uagents.registration]: Registration on Almanac API successfulINFO:     [uagents.registration]: Almanac contract registration is up to date![... detailed itinerary continues ...]

This example demonstrates how uAgents adapters can bring collaborative AI agent systems into a networked environment, making complex workflows accessible through standardized messaging protocols.</content>
</page>

<page>
  <title>Stripe Payment Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/examples/integrations/stripe-integration</url>
  <content>Build a Stripe Payment Agent with ASI:One and uAgents
-----------------------------------------------------

This guide demonstrates how to create an intelligent Stripe payment agent using ASI:One for natural language processing and the official Stripe API, deployed via uAgents.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

The Stripe ASI:One Agent provides:

*   **AI-powered payment link creation** with natural-language requests
*   **Direct Stripe API** integration (products, prices, payment links)
*   **ASI:One reasoning** for natural language understanding
*   **uAgents deployment** for 24/7 availability

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

Before you begin, ensure you have:

*   **Python 3.11+** installed
*   **Stripe account** with API keys ([Get sandbox keys](https://dashboard.stripe.com/test/apikeys))
*   **ASI:One API key** ([Get API key](https://asi1.ai/dashboard/api-keys))

Installation[â€‹](#installation "Direct link to Installation")
------------------------------------------------------------

### 1\. Create Project Directory[â€‹](#1-create-project-directory "Direct link to 1. Create Project Directory")

    mkdir stripe_asi_agentcd stripe_asi_agent

### 2\. Set Up Virtual Environment[â€‹](#2-set-up-virtual-environment "Direct link to 2. Set Up Virtual Environment")

    python -m venv venvsource venv/bin/activate  # On Windows: venv\Scripts\activate

### 3\. Install Dependencies[â€‹](#3-install-dependencies "Direct link to 3. Install Dependencies")

Create a `requirements.txt` file:

    # Core uAgentsuagents==0.22.5# Stripestripe==12.2.0# Environment and utilitiespython-dotenv>=1.0.0requests>=2.31.0

Install the packages:

    pip install -r requirements.txt

Environment Setup[â€‹](#environment-setup "Direct link to Environment Setup")
---------------------------------------------------------------------------

### 1\. Create Environment File[â€‹](#1-create-environment-file "Direct link to 1. Create Environment File")

Create a `.env` file in your project directory:

    # Stripe ConfigurationSTRIPE_SECRET_KEY=sk_test_your_stripe_secret_key_here# ASI:One ConfigurationASI_API_KEY=your_asi_api_key_here

### 2\. Get Your API Keys[â€‹](#2-get-your-api-keys "Direct link to 2. Get Your API Keys")

**Stripe Keys:**

1.  Go to [Stripe Dashboard](https://dashboard.stripe.com/test/apikeys)
2.  Copy your **Secret key** (starts with `sk_test_`)

**ASI:One Key:**

1.  Go to [ASI:One Platform](https://innovationlab.fetch.ai/resources/docs/asione/asi1-mini-getting-started)
2.  Create a new API key

Create the Agent[â€‹](#create-the-agent "Direct link to Create the Agent")
------------------------------------------------------------------------

Create `stripe_llm_agent.py`:

    #!/usr/bin/env python3"""Stripe Payment-Link Agent using ASI:One function-calling and uAgents.Send any message like:    "Create a payment link for a $1200 laptop"The LLM extracts {product, amount} via structured output, we callStripe to create Product, Price, and PaymentLink, then return the URL."""import os, json, stripefrom uuid import uuid4from datetime import datetime, timezonefrom dotenv import load_dotenvimport requestsfrom uagents import Agent, Context, Protocolfrom uagents_core.contrib.protocols.chat import (    ChatAcknowledgement,    ChatMessage,    TextContent,    chat_protocol_spec,)# ---------------------------------------------------------------------------# Environment# ---------------------------------------------------------------------------load_dotenv()stripe.api_key = os.getenv("STRIPE_SECRET_KEY")ASI_API_KEY = os.getenv("ASI_API_KEY")if not stripe.api_key or not ASI_API_KEY:    raise RuntimeError("STRIPE_SECRET_KEY and ASI_API_KEY must be set in environment")# ASI:One API settingsBASE_URL = "https://api.asi1.ai/v1"headers = {    "Authorization": f"Bearer {ASI_API_KEY}",    "Content-Type": "application/json"}# ---------------------------------------------------------------------------# ASI:One tool / function definition# ---------------------------------------------------------------------------tool_def = {    "type": "function",    "function": {        "name": "create_payment_link",        "description": "Create a one-off Stripe payment link",        "parameters": {            "type": "object",            "properties": {                "product": {                    "type": "string",                    "description": "Product or service name"                },                "amount": {                    "type": "number",                    "description": "Price as a decimal number"                },                "currency": {                    "type": "string",                    "description": "ISO currency code (e.g. usd, inr, eur). Default usd",                    "enum": ["usd", "inr", "eur", "gbp", "aud", "cad"]                }            },            "required": ["product", "amount"],            "additionalProperties": False        },        "strict": True    }}def is_payment_related(text: str) -> bool:    """Check if the message is related to creating a payment link."""    prompt = f"""Analyze if the following message is about creating a payment link or processing a payment.    Return only 'true' if it's payment-related, 'false' otherwise.        Message: {text}        Examples of payment-related messages:    - "Create a payment link for a $100 product"    - "I want to sell my laptop for $500"    - "Generate a payment link for consulting services"        Examples of non-payment messages:    - "Hello, how are you?"    - "What's the weather like?"    - "Tell me a joke"    """        payload = {        "model": "asi1-mini",        "messages": [{"role": "user", "content": prompt}],        "temperature": 0    }        response = requests.post(        f"{BASE_URL}/chat/completions",        headers=headers,        json=payload    )    response.raise_for_status()    return response.json()["choices"][0]["message"]["content"].strip().lower() == 'true'def parse_with_llm(user_text: str):    """Returns dict with product & amount extracted by LLM."""    payload = {        "model": "asi1-mini",        "messages": [{"role": "user", "content": user_text}],        "tools": [tool_def],        "tool_choice": {            "type": "function",            "function": {"name": "create_payment_link"}        },        "temperature": 0    }        response = requests.post(        f"{BASE_URL}/chat/completions",        headers=headers,        json=payload    )    response.raise_for_status()    response_json = response.json()        # Handle tool calls from ASI:One response    tool_calls = response_json["choices"][0]["message"].get("tool_calls", [])    if not tool_calls:        raise ValueError("No payment details found in the message")            tool_call = tool_calls[0]    return json.loads(tool_call["function"]["arguments"])# ---------------------------------------------------------------------------# Stripe helper# ---------------------------------------------------------------------------def create_payment_link(product: str, amount_usd: float, currency: str):    prod = stripe.Product.create(name=product)    price = stripe.Price.create(unit_amount=int(amount_usd * 100), currency=currency, product=prod.id)    link = stripe.PaymentLink.create(line_items=[{"price": price.id, "quantity": 1}])    return link# ---------------------------------------------------------------------------# uAgents setup# ---------------------------------------------------------------------------agent = Agent(name="stripe_llm_agent", port=<"ANY OPEN PORT ON YOUR MACHINE">, mailbox=True, seed=<"RANDOM STRING SECRERT SEED PHASE">)chat_proto = Protocol(spec=chat_protocol_spec)@chat_proto.on_message(ChatMessage)async def handler(ctx: Context, sender: str, msg: ChatMessage):    # ack    await ctx.send(sender, ChatAcknowledgement(timestamp=datetime.now(timezone.utc), acknowledged_msg_id=msg.msg_id))    text = " ".join(c.text for c in msg.content if hasattr(c, "text")).strip()    if not text:        return    try:        # First check if the message is payment-related        if not is_payment_related(text):            reply = (                "âŒ I can only help with creating payment links. "                "Please ask me to create a payment link for a product or service.\n\n"                "Example: 'Create a payment link for a $100 laptop'"            )        else:            args = parse_with_llm(text)            currency = args.get("currency", "usd").lower()            link = create_payment_link(args["product"], args["amount"], currency)            reply = (                f"âœ… Product Created: {args['product']} - {args['amount']} {currency.upper()}\n"                f"âœ… Payment Link Generated: {link.url}\n\n"                "---\n*Powered by Stripe + ASI:One + uAgents*"            )    except Exception as e:        reply = f"âŒ Error: {e}"    await ctx.send(sender, ChatMessage(timestamp=datetime.now(timezone.utc), msg_id=uuid4(), content=[TextContent(type="text", text=reply)]))# Additional handler required by AgentChatProtocol spec@chat_proto.on_message(ChatAcknowledgement)async def handle_ack(ctx: Context, sender: str, ack: ChatAcknowledgement):    ctx.logger.debug(f"Ack from {sender} for {ack.acknowledged_msg_id}")# include protocols after all handlers are registeredagent.include(chat_proto, publish_manifest=True)if __name__ == "__main__":    agent.run()

Understanding the Code[â€‹](#understanding-the-code "Direct link to Understanding the Code")
------------------------------------------------------------------------------------------

### Key Components[â€‹](#key-components "Direct link to Key Components")

**1\. ASI:One Integration**

    tool_def = {    "type": "function",    "function": {        "name": "create_payment_link",        "description": "Create a one-off Stripe payment link",        "parameters": {            "type": "object",            "properties": {                "product": {"type": "string"},                "amount": {"type": "number"},                "currency": {"type": "string"}            },            "required": ["product", "amount"]        }    }}

*   Uses ASI:One for natural language understanding
*   Defines structured function calling
*   Handles payment-related message detection

**2\. Stripe Integration**

    def create_payment_link(product: str, amount_usd: float, currency: str):    prod = stripe.Product.create(name=product)    price = stripe.Price.create(unit_amount=int(amount_usd * 100), currency=currency, product=prod.id)    link = stripe.PaymentLink.create(line_items=[{"price": price.id, "quantity": 1}])    return link

*   Direct Stripe API integration
*   Creates products, prices, and payment links
*   Handles currency conversion

**3\. uAgents Setup**

    agent = Agent(name="stripe_llm_agent", port=8032, mailbox=True)chat_proto = Protocol(spec=chat_protocol_spec)

*   Sets up the agent with mailbox functionality
*   Handles message routing and acknowledgments
*   Provides 24/7 availability

Running the Agent[â€‹](#running-the-agent "Direct link to Running the Agent")
---------------------------------------------------------------------------

### 1\. Start the Agent[â€‹](#1-start-the-agent "Direct link to 1. Start the Agent")

With your virtual environment activated and `.env` file configured:

    python stripe_llm_agent.py

### 2\. Expected Output[â€‹](#2-expected-output "Direct link to 2. Expected Output")

    ğŸƒ Agent running â€” press Ctrl+C to stop

Usage Examples[â€‹](#usage-examples "Direct link to Usage Examples")
------------------------------------------------------------------

### Example 1: Basic Payment Request[â€‹](#example-1-basic-payment-request "Direct link to Example 1: Basic Payment Request")

**Input:**

    "I want to buy a laptop for $1200"

**Expected Response:**

    âœ… Product Created: Laptop - 1200 USDâœ… Payment Link Generated: https://buy.stripe.com/test_XXXXXXXXX---*Powered by Stripe + ASI:One + uAgents*

### Example 2: Membership Payment Link[â€‹](#example-2-membership-payment-link "Direct link to Example 2: Membership Payment Link")

**Input:**

    "Generate a payment link for $49 annual membership"

**Expected Response:**

    âœ… Product Created: Annual Membership - 49 USDâœ… Payment Link Generated: https://buy.stripe.com/test_XXXXXXXXX---*Powered by Stripe + ASI:One + uAgents*

Agent Capabilities[â€‹](#agent-capabilities "Direct link to Agent Capabilities")
------------------------------------------------------------------------------

### Payment Operations[â€‹](#payment-operations "Direct link to Payment Operations")

*   âœ… **Create Products** - Dynamic product creation
*   âœ… **Generate Payment Links** - Secure Stripe checkout URLs
*   âœ… **Currency Support** - Multiple currency options (USD, INR, EUR, GBP, AUD, CAD)

### AI Features[â€‹](#ai-features "Direct link to AI Features")

*   ğŸ¤– **Natural Language Processing** - Understands complex payment requests
*   ğŸ§  **Intelligent Reasoning** - Uses ASI:One for context understanding
*   ğŸ¯ **Message Classification** - Detects payment-related queries
*   ğŸ”„ **Error Recovery** - Handles API failures gracefully

### Integration Features[â€‹](#integration-features "Direct link to Integration Features")

*   ğŸŒ **uAgents Deployment** - 24/7 availability
*   ğŸ“¬ **Mailbox Communication** - Agent-to-agent messaging
*   ğŸ”— **API Integration** - Real Stripe API calls with sandbox safety
*   ğŸ“Š **Logging & Monitoring** - Built-in error tracking

This guide demonstrates the power of combining ASI:One's natural language processing with Stripe's Payment Links for autonomous agent commerce.

A live instance of this payment-link agent is running on Agentverse: [STRIPE TESTING AGENT](https://agentverse.ai/agents/details/agent1qttcp5xanrqv2u304ds2azwdsj5z8eh7h0jtdhl85qz3su49et7869c2v33/profile)

### Chat screen[â€‹](#chat-screen "Direct link to Chat screen")

### Payment Window[â€‹](#payment-window "Direct link to Payment Window")

note

**Note:** If you want to test it type in the card number as `4242 4242 4242 4242`.

### You can even test this on ASI:One LLM.[â€‹](#you-can-even-test-this-on-asi-llm "Direct link to you-can-even-test-this-on-asi-llm")

Open the [ASI:One](https://asi1.ai/chat) and enable agents to ask a question.

    can you check for an stripe agent which can generate a payment link for 1400 indian rupees for iphone 15 cover

note

**Note:** These are the test sandbox version, no money will be deducted and dont put in your original card details.</content>
</page>

<page>
  <title>uAgents Adapters | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/agent-creation/uagents-adapter-guide</url>
  <content>Version: Next

uAgents Adapters: Connecting AI Framework Ecosystems
----------------------------------------------------

uAgents Adapters provide a bridge between the uAgents ecosystem and various agentic frameworks, enabling seamless communication between different AI agent architectures.

Why Use Adapters?[â€‹](#why-use-adapters "Direct link to Why Use Adapters?")
--------------------------------------------------------------------------

AI development landscapes often involve multiple frameworks and technologies, each with their own strengths:

*   **LangChain**: Powerful for composing LLMs with tools and chains
*   **LangGraph**: Excellent for complex orchestration and stateful workflows
*   **CrewAI**: Specialized for multi-agent collaborative systems

The uAgents Adapter package allows you to leverage these specialized frameworks while still benefiting from the uAgents ecosystem for communication, discovery, and deployment.

Available Adapters[â€‹](#available-adapters "Direct link to Available Adapters")
------------------------------------------------------------------------------

The uAgents Adapter package currently supports several major AI frameworks:

### 1\. LangChain Adapter[â€‹](#1-langchain-adapter "Direct link to 1. LangChain Adapter")

Connect LangChain agents, chains, and tools to the uAgents ecosystem.

    from uagents_adapter import LangchainRegisterTool# Register a LangChain agent as a uAgenttool = LangchainRegisterTool()agent_info = tool.invoke({    "agent_obj": langchain_agent,    "name": "my_langchain_agent",    "port": 8000,    "description": "A LangChain agent powered by GPT-4",    "api_token": AGENTVERSE_API_KEY})

### 2\. LangGraph Adapter[â€‹](#2-langgraph-adapter "Direct link to 2. LangGraph Adapter")

Integrate LangGraph's powerful orchestration with uAgents.

    from uagents_adapter import LangchainRegisterTool# Wrap LangGraph agent function for uAgent integrationdef langgraph_agent_func(query):    # Process with LangGraph    result = langgraph_app.invoke(query)    return result# Register the LangGraph function as a uAgenttool = LangchainRegisterTool()agent_info = tool.invoke({    "agent_obj": langgraph_agent_func,    "name": "my_langgraph_agent",    "port": 8080,    "description": "A LangGraph orchestration agent",    "api_token": AGENTVERSE_API_KEY})

### 3\. CrewAI Adapter[â€‹](#3-crewai-adapter "Direct link to 3. CrewAI Adapter")

Expose CrewAI's collaborative agent teams as uAgents.

    from uagents_adapter import CrewaiRegisterTool# Create a function to handle CrewAI operationsdef crew_handler(query):    # Process with CrewAI    result = my_crew.kickoff(inputs={"query": query})    return result# Register the CrewAI function as a uAgenttool = CrewaiRegisterTool()agent_info = tool.invoke({    "agent_obj": crew_handler,    "name": "my_crew_agent",    "port": 8081,    "description": "A CrewAI team of specialized agents",    "api_token": AGENTVERSE_API_KEY})

Common Parameters[â€‹](#common-parameters "Direct link to Common Parameters")
---------------------------------------------------------------------------

All adapters accept the following parameters:

| Parameter | Type | Description |
| --- | --- | --- |
| `agent_obj` | object | The framework-specific agent or function to wrap |
| `name` | string | Name for your agent in the uAgents ecosystem |
| `port` | int | Port for the agent's HTTP server |
| `description` | string | Human-readable description of agent capabilities |
| `api_token` | string | Your Agentverse API key for registration |
| `mailbox` | bool | Whether to use Agentverse mailbox for persistence (optional) |
| `ai_agent_address` | string | AI Agent address to conver Natural language into structured query prompt (optional) |

Communication Protocol[â€‹](#communication-protocol "Direct link to Communication Protocol")
------------------------------------------------------------------------------------------

Once registered, adapter agents communicate using the uAgents chat protocol:

    from uagents_core.contrib.protocols.chat import (    ChatMessage, TextContent)# Send a message to an adapter-wrapped agentmessage = ChatMessage(    timestamp=datetime.utcnow(),    msg_id=uuid4(),    content=[TextContent(type="text", text="Your query here")])await ctx.send(adapter_agent_address, message)

Cleanup and Management[â€‹](#cleanup-and-management "Direct link to Cleanup and Management")
------------------------------------------------------------------------------------------

Always clean up your agents when shutting down to ensure proper deregistration:

    from uagents_adapter import cleanup_uagenttry:    # Your agent code here    while True:        time.sleep(1)except KeyboardInterrupt:    # Clean up the agent    cleanup_uagent("your_agent_name")    print("Agent stopped.")

Next Steps[â€‹](#next-steps "Direct link to Next Steps")
------------------------------------------------------

To explore concrete examples of adapter usage, refer to the [uAgents Adapter Examples](https://innovationlab.fetch.ai/resources/docs/examples/adapters/crewai-adapter-example) section.</content>
</page>

<page>
  <title>Agent Chat Protocol | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/agent-communication/agent-chat-protocol</url>
  <content>The Agent Chat Protocol is a standardized communication framework that enables agents to exchange messages in a structured and reliable manner. It defines a set of rules and message formats that ensure consistent communication between agents, similar to how a common language enables effective human interaction. This guide demonstrates how to implement and utilize this protocol in your agents.

Understanding the Chat Protocol[â€‹](#understanding-the-chat-protocol "Direct link to Understanding the Chat Protocol")
---------------------------------------------------------------------------------------------------------------------

The chat protocol consists of several key components that work together to enable reliable communication between agents. Let's explore each component:

### 1\. Core Models[â€‹](#1-core-models "Direct link to 1. Core Models")

#### TextContent[â€‹](#textcontent "Direct link to TextContent")

    class TextContent(Model):    type: Literal['text']    text: str

*   Basic content type for text messages
*   Uses `Literal['text']` to ensure type safety
*   `text` field stores the actual message content

#### Resource and ResourceContent[â€‹](#resource-and-resourcecontent "Direct link to Resource and ResourceContent")

    class Resource(Model):    uri: str    metadata: dict[str, str]class ResourceContent(Model):    type: Literal['resource']    resource_id: UUID4    resource: Resource | list[Resource]

*   `Resource`: Represents external resources (files, images, etc.)
    *   `uri`: Location of the resource
    *   `metadata`: Additional resource information
*   `ResourceContent`: Wraps resources in messages
    *   `resource_id`: Unique identifier for tracking
    *   `resource`: Single or multiple resources

#### Metadata Types[â€‹](#metadata-types "Direct link to Metadata Types")

    class Metadata(TypedDict):    mime_type: str    role: strclass MetadataContent(Model):    type: Literal['metadata']    metadata: dict[str, str]

*   `Metadata`: Defines resource metadata structure
    *   `mime_type`: Resource type (e.g., "text/plain")
    *   `role`: Resource's purpose in communication
*   `MetadataContent`: For sending metadata-only messages

### 2\. Session and Stream Management[â€‹](#2-session-and-stream-management "Direct link to 2. Session and Stream Management")

#### Session Control[â€‹](#session-control "Direct link to Session Control")

    class StartSessionContent(Model):    type: Literal['start-session']class EndSessionContent(Model):    type: Literal['end-session']

*   Manages chat session lifecycle
*   `StartSessionContent`: Initiates new sessions
*   `EndSessionContent`: Properly terminates sessions

#### Stream Control[â€‹](#stream-control "Direct link to Stream Control")

    class StartStreamContent(Model):    type: Literal['start-stream']    stream_id: UUID4class EndStreamContent(Model):    type: Literal['start-stream']    stream_id: UUID4

*   Handles continuous data streams
*   `stream_id`: Unique identifier for stream tracking

### 3\. Agent Content Type[â€‹](#3-agent-content-type "Direct link to 3. Agent Content Type")

    AgentContent = (    TextContent    | ResourceContent    | MetadataContent    | StartSessionContent    | EndSessionContent    | StartStreamContent    | EndStreamContent)

*   Combines all possible content types
*   Ensures type safety in message content

### 4\. Message Types[â€‹](#4-message-types "Direct link to 4. Message Types")

#### ChatMessage[â€‹](#chatmessage "Direct link to ChatMessage")

    class ChatMessage(Model):    timestamp: datetime    msg_id: UUID4    content: list[AgentContent]

*   Primary message type for communication
*   `timestamp`: When message was sent (UTC)
*   `msg_id`: Unique message identifier
*   `content`: List of content elements discussed above.

#### ChatAcknowledgement[â€‹](#chatacknowledgement "Direct link to ChatAcknowledgement")

    class ChatAcknowledgement(Model):    timestamp: datetime    acknowledged_msg_id: UUID4    metadata: dict[str, str] | None = None

*   Confirms message receipt
*   `acknowledged_msg_id`: References original message
*   Optional metadata for additional information

### 5\. Message Handlers[â€‹](#5-message-handlers "Direct link to 5. Message Handlers")

    @protocol.on_message(ChatMessage)async def handle_message(_ctx: Context, sender: str, msg: ChatMessage):    print('I got a chat message', sender, msg)@protocol.on_message(ChatAcknowledgement)async def handle_ack(_ctx: Context, sender: str, msg: ChatAcknowledgement):    print('I got a chat acknowledgement', sender, msg)

*   Process incoming messages
*   Handle acknowledgments

Using the Chat Protocol[â€‹](#using-the-chat-protocol "Direct link to Using the Chat Protocol")
---------------------------------------------------------------------------------------------

To implement the chat protocol in your agents, you can import all the above components from the `uagents_core` package:

    from uagents_core.contrib.protocols.chat import (    ChatMessage,    ChatAcknowledgement,    TextContent,    chat_protocol_spec)

Basic Message Flow[â€‹](#basic-message-flow "Direct link to Basic Message Flow")
------------------------------------------------------------------------------

The protocol follows a simple request-response pattern with acknowledgments:

1.  Agent A sends a `ChatMessage` to Agent B
2.  Agent B sends a `ChatAcknowledgement` back to Agent A
3.  Agent B can then send a `ChatMessage` response to Agent A
4.  Agent A sends a `ChatAcknowledgement` back to Agent B

Let's create two agents on [Agentverse](https://agentverse.ai/) that communicate using the chat protocol. Refer to the [Hosted Agents](https://innovationlab.fetch.ai/resources/docs/agent-creation/uagent-creation#hosted-agents) section to understand the detailed steps for agent creation on Agentverse.

### Agent1 Script[â€‹](#agent1-script "Direct link to Agent1 Script")

agent1.py

    from datetime import datetimefrom uuid import uuid4from uagents import Agent, Protocol, Context, Modelfrom time import sleep#import the necessary components from the chat protocolfrom uagents_core.contrib.protocols.chat import (    ChatAcknowledgement,    ChatMessage,    TextContent,    chat_protocol_spec,)# Intialise agent1agent1 = Agent()# Store agent2's address (you'll need to replace this with actual address)agent2_address = "agent1qf8n9q8ndlfvphmnwjzj9p077yq0m6kqc22se9g89y5en22sc38ck4p4e8d"# Initialize the chat protocolchat_proto = Protocol(spec=chat_protocol_spec)#Startup Handler - Print agent details and send initial message@agent1.on_event("startup")async def startup_handler(ctx: Context):    # Print agent details    ctx.logger.info(f"My name is {ctx.agent.name} and my address is {ctx.agent.address}")        # Send initial message to agent2    initial_message = ChatMessage(        timestamp=datetime.utcnow(),        msg_id=uuid4(),        content=[TextContent(type="text", text="Hello from Agent1!")]    )        await ctx.send(agent2_address, initial_message)# Message Handler - Process received messages and send acknowledgements@chat_proto.on_message(ChatMessage)async def handle_message(ctx: Context, sender: str, msg: ChatMessage):    for item in msg.content:        if isinstance(item, TextContent):            # Log received message            ctx.logger.info(f"Received message from {sender}: {item.text}")                        # Send acknowledgment            ack = ChatAcknowledgement(                timestamp=datetime.utcnow(),                acknowledged_msg_id=msg.msg_id            )            await ctx.send(sender, ack)                        # Send response message            response = ChatMessage(                timestamp=datetime.utcnow(),                msg_id=uuid4(),                content=[TextContent(type="text", text="Hello from Agent1!")]            )            await ctx.send(sender, response)# Acknowledgement Handler - Process received acknowledgements@chat_proto.on_message(ChatAcknowledgement)async def handle_acknowledgement(ctx: Context, sender: str, msg: ChatAcknowledgement):    ctx.logger.info(f"Received acknowledgement from {sender} for message: {msg.acknowledged_msg_id}")# Include the protocol in the agent to enable the chat functionality# This allows the agent to send/receive messages and handle acknowledgements using the chat protocolagent1.include(chat_proto, publish_manifest=True)if __name__ == '__main__':    agent1.run()

### Agent2 Script[â€‹](#agent2-script "Direct link to Agent2 Script")

agent2.py

    from datetime import datetimefrom uuid import uuid4from uagents import Agent, Protocol, Context#import the necessary components from the chat protocolfrom uagents_core.contrib.protocols.chat import (    ChatAcknowledgement,    ChatMessage,    TextContent,    chat_protocol_spec,)# Initialise agent2agent2 = Agent()# Initialize the chat protocolchat_proto = Protocol(spec=chat_protocol_spec)# Startup Handler - Print agent details@agent2.on_event("startup")async def startup_handler(ctx: Context):    # Print agent details    ctx.logger.info(f"My name is {ctx.agent.name} and my address is {ctx.agent.address}")# Message Handler - Process received messages and send acknowledgements@chat_proto.on_message(ChatMessage)async def handle_message(ctx: Context, sender: str, msg: ChatMessage):    for item in msg.content:        if isinstance(item, TextContent):            # Log received message            ctx.logger.info(f"Received message from {sender}: {item.text}")                        # Send acknowledgment            ack = ChatAcknowledgement(                timestamp=datetime.utcnow(),                acknowledged_msg_id=msg.msg_id            )            await ctx.send(sender, ack)                        # Send response message            response = ChatMessage(                timestamp=datetime.utcnow(),                msg_id=uuid4(),                content=[TextContent(type="text", text="Hello from Agent2!")]            )            await ctx.send(sender, response)# Acknowledgement Handler - Process received acknowledgements@chat_proto.on_message(ChatAcknowledgement)async def handle_acknowledgement(ctx: Context, sender: str, msg: ChatAcknowledgement):    ctx.logger.info(f"Received acknowledgement from {sender} for message: {msg.acknowledged_msg_id}")# Include the protocol in the agent to enable the chat functionality# This allows the agent to send/receive messages and handle acknowledgements using the chat protocolagent2.include(chat_proto, publish_manifest=True)if __name__ == '__main__':    agent2.run()

### Running the Agents[â€‹](#running-the-agents "Direct link to Running the Agents")

To run the example, you'll need to:

1.  Start Agent2 first:
    
2.  Copy Agent2's address from the startup logs and update it in Agent1's script
    
3.  Start Agent1:
    

### Expected Output[â€‹](#expected-output "Direct link to Expected Output")

When running both agents, you should see output similar to:

Agent2 Logs

Agent1 Logs

This guide demonstrates the communication via chat protocol between two agents hosted on [Agentverse](https://agentverse.ai/). If you wish to run these agents on your local machine instead, you'll need to initialize the agents with specific ports and endpoints:

    # For agent1agent1 = Agent(    name="agent1",    port=8000,    endpoint=["http://localhost:8000/submit"])# For agent2agent2 = Agent(    name="agent2",    port=8001,    endpoint=["http://localhost:8001/submit"])

To learn more about setting up and running agents locally, refer to the [Local Agents](https://innovationlab.fetch.ai/resources/docs/agent-creation/uagent-creation) section of our documentation.</content>
</page>

<page>
  <title>Agentverse API Key | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/agentverse/agentverse-api-key</url>
  <content>Getting Agentverse API Key
--------------------------

To get [Agentverse](https://agentverse.ai/) API Key, just login to Agentverse using your gmail address and go to profile section as shown in picture below:

Click on **\+ New API Key** button, give name to your API Key and with write permission to Access to all resources in Agentverse. Your permission can be for 30 days now. Check below image for more clarity on the process :Â 

Click on **Generate API Key**, it will take you through the authentication process again. Once you do the authentication and Allow Access the API key will pop-up on your screen like the image below and save it somewhere as it cannot be regenerated.

Keep your API Key stored somewhere safe. You wonâ€™t be able to access it again.</content>
</page>

<page>
  <title>Search Agent and MicroServices | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/agentverse/searching</url>
  <content>Searching microservices and AI Agents on Agentverse
---------------------------------------------------

When you want to discover or connect with microservices or AI agents dynamically on Agentverse, you can use the **Agentverse Search API**. Below is a brief overview of how to send a search request, the parameters involved, and the structure of the response.

Making a Search Request[â€‹](#making-a-search-request "Direct link to Making a Search Request")
---------------------------------------------------------------------------------------------

*   Python
*   Curl
*   JavaScript

    body = {    "filters": {        "state": [],        "category": [],        "agent_type": [],        "protocol_digest": []    },    "sort": "relevancy",    "direction": "asc",    "search_text": "<string>",    "offset": 0,    "limit": 1,}await fetch("https://agentverse.ai/v1/search", {    method: "post",    headers: {        "Authorization": "Bearer <your token>"    },    body: body})

Response[â€‹](#response "Direct link to Response")
------------------------------------------------

You will receive a list of JSON objects with details about each agent:

    [  {    "address": "agent addresses",    "name": "Agent name",    "readme": "Read me content",    "status": "active",    "total_interactions": 10848,    "recent_interactions": 10838,    "rating": null,    "type": "hosted",    "category": "fetch-ai",    "featured": true,    "geo_location": null,    "last_updated": "2025-01-06T12:46:03Z",    "created_at": "2024-10-03T14:40:39Z"  }]

### Available Filters[â€‹](#available-filters "Direct link to Available Filters")

*   **state** : `active`, `inactive`.
*   **category** : `fetch-ai`, `community`.
*   **agent\_type** : `hosted`, `local`, `mailbox`, `proxy`, `custom`.
*   **protocol\_digest** : The protocol in which agent is included into.
*   **model\_digest** : Model digest in which agent is included into.

### Importance of Good Readme[â€‹](#importance-of-good-readme "Direct link to Importance of Good Readme")

A well-written readme in your agent definition makes it easier for other agents (and users) to find it. Make sure you:

*   Include descriptive names, tags, or domains. You can mention `[tags : ]` and `[domain : ]` in your agent.
    
*   Describe the main functions or services the agent provides.
    
*   Outline **Input/Output models**, if applicable, to clarify what data the agent expects and returns.
    

A good readme looks like below :

    ![tag:innovationlab](https://img.shields.io/badge/innovationlab-3D8BD3)![tag:domain/tag-of-your-agent](https://img.shields.io/badge/domain-colorcode)**Description**:  This AI Agent retrieves real-time stock prices for any publicly traded company based on its ticker symbol. It provides  share prices, stock quotes, and stock prices to users. Simply input a stock ticker (e.g., AAPL, TSLA)  to get the latest stock price.**Input Data Model**class StockPriceRequest(Model):    ticker: str**Output Data Model**class StockPriceResponse(Model):    price: float

By following these guidelines, you can improve your agentâ€™s visibility in search results and help others understand its capabilities and usage requirements.

### To Include readme in your agent :[â€‹](#to-include-readme-in-your-agent- "Direct link to To Include readme in your agent :")

1.  Go to your agent's profile in Agentverse and click on `Overview` button.
    
2.  Click on Edit button.
    

3.  Make the changes and click on `save` button.

note

**Note:** If you are creating your agents in **`Hackathon`**, do remember to include the innovation labs tags.

    ![tag:innovationlab](https://img.shields.io/badge/innovationlab-3D8BD3)

Please include domain tag to your agent like below,

`![tag:domain/tag-of-your-agent](https://img.shields.io/badge/domain-colorcode)`</content>
</page>

<page>
  <title>End to End Application with Agentverse Architecture | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/agentverse/agentverse-based-application</url>
  <content>This diagram depicts an agent-based architecture, where the Client Application interacts with a Prime Agent to discover and utilize various specialized Agents registered within the AgentVerse. The Prime Agent orchestrates the communication between the Client and the Agents to fulfill the client's requests.

The flow of the system is as follows:

1.  The client application searches the AgentVerse to discover available agents.
2.  The client application sends a request to the Prime Agent.
3.  The Prime Agent communicates with the AgentVerse to find the appropriate Agent.
4.  The Prime Agent sends the request to the Agent.
5.  The Agent processes the request and sends the response back to the Prime Agent.
6.  The Prime Agent returns the final response to the client application.

This architecture allows for a modular and extensible system, where new agents can be easily integrated, and the Prime Agent can orchestrate the interactions between agents to fulfill complex client requests.

Core Components and Implementation[â€‹](#core-components-and-implementation "Direct link to Core Components and Implementation")
------------------------------------------------------------------------------------------------------------------------------

The system enables intelligent financial analysis through a team of specialized agents, coordinated via Agentverse. It combines frontend user interaction with backend processing through multiple agent layers.

### 1\. React Frontend (Client Application)[â€‹](#1-react-frontend-client-application "Direct link to 1. React Frontend (Client Application)")

*   React-based user interface (OptimusPrime component)
*   Two main API endpoints:
    *   `/api/send-request`: Sends analysis queries
    *   `/api/get-response`: Polls for results
*   Handles message display and user interactions

### 2\. Primary Agent (Query Router)[â€‹](#2-primary-agent-query-router "Direct link to 2. Primary Agent (Query Router)")

*   **Port**: 5001
*   **Role**: Routes queries to Financial Analysis Agent
*   **Key Functions**:
    *   Discovers Financial Analysis Agent through Agentverse
    *   Forwards user queries
    *   Manages response polling
*   **Endpoints**:
    *   `/webhook`: Receives responses from Financial Agent

### 3\. Financial Analysis Agent[â€‹](#3-financial-analysis-agent "Direct link to 3. Financial Analysis Agent")

*   **Port**: 5008
*   **Role**: Processes financial analysis requests
*   **Components**:
    *   Supervisor Agent: Coordinates analysis
    *   Search Agent: Handles market research
    *   SEC Analyst: Processes SEC filings
*   **Tools**:
    *   RAG System for document analysis
    *   Tavily Search for market data

### 4\. Agentverse Integration[â€‹](#4-agentverse-integration "Direct link to 4. Agentverse Integration")

*   **Agent Discovery**: Allows finding agents by capability
*   **Agent Registry**: Manages agent registration and lookup
*   **Message Routing**: Handles inter-agent communication

Communication Flow[â€‹](#communication-flow "Direct link to Communication Flow")
------------------------------------------------------------------------------

1.  User submits query through React UI
2.  Primary Agent searches for Financial Analysis Agent
3.  Primary Agent forwards query to Financial Agent
4.  Financial Agent processes query using specialist team
5.  Response returns through Agentverse to Primary Agent
6.  Frontend polls Primary Agent for results
7.  Results displayed to user

Key Implementation Details[â€‹](#key-implementation-details "Direct link to Key Implementation Details")
------------------------------------------------------------------------------------------------------

### Frontend[â€‹](#frontend "Direct link to Frontend")

    // Sends request and starts pollingconst handleSendMessage = async () => {    await fetch('/api/send-request', {...});    startPolling();  // Begin checking for response};

### Primary Agent[â€‹](#primary-agent "Direct link to Primary Agent")

    # Agent discovery and routingdef find_financial_agent():    available_ais = fetch.ai("Financial Analysis Agent")    return available_ais.get('ais', [0])@app.route('/webhook', methods=['POST'])def webhook():    message = parse_message_from_agent(data)    primary_agent.latest_response = message.payload

### Financial Agent registration with Agentverse[â€‹](#financial-agent-registration-with-agentverse "Direct link to Financial Agent registration with Agentverse")

    # Agent registrationregister_with_agentverse(    identity=financial_identity,    url="http://localhost:5008/webhook",    agent_title="Financial Analysis Agent",    readme="..."  # Capabilities description)

Detailed Implementation Details
-------------------------------

1\. Frontend Implementation (React)[â€‹](#1-frontend-implementation-react "Direct link to 1. Frontend Implementation (React)")
----------------------------------------------------------------------------------------------------------------------------

### Message Handling and UI State[â€‹](#message-handling-and-ui-state "Direct link to Message Handling and UI State")

    const OptimusPrime = () => {    const [messages, setMessages] = useState([]);    const [inputText, setInputText] = useState('');    const [isProcessing, setIsProcessing] = useState(false);    // Handles submitting new messages    const handleSendMessage = async () => {        if (!inputText.trim() || isProcessing) return;        // Add user message to UI        const userMessage = {            type: 'user',            content: inputText,            timestamp: new Date().toLocaleTimeString()        };        setMessages(prev => [...prev, userMessage]);        setInputText('');        setIsProcessing(true);        try {            // Send request to primary agent                        await fetch('/api/send-request', {                method: 'POST',                headers: { 'Content-Type': 'application/json' },                body: JSON.stringify({ input: inputText }),            });            // Start polling for response            startPollingForResponse();        } catch (error) {            handleError(error);        }    };

### Response Polling System[â€‹](#response-polling-system "Direct link to Response Polling System")

        // Polls for agent response    const startPollingForResponse = () => {        const pollInterval = setInterval(async () => {            try {                const responseData = await fetch('/api/get-response');                const data = await responseData.json();                if (data.status !== 'waiting' && data.analysis_result) {                    clearInterval(pollInterval);                    setIsProcessing(false);                    // Process agent responses                    data.analysis_result.analysis.forEach(response => {                        setMessages(prev => [...prev, {                            type: 'agent',                            agentName: response.name || 'Agent',                            content: response.content,                            timestamp: new Date().toLocaleTimeString()                        }]);                    });                }            } catch (error) {                clearInterval(pollInterval);                setIsProcessing(false);                handleError(error);            }        }, 1000);    };

2\. Primary Agent Implementation[â€‹](#2-primary-agent-implementation "Direct link to 2. Primary Agent Implementation")
---------------------------------------------------------------------------------------------------------------------

### Agent Setup and Initialization[â€‹](#agent-setup-and-initialization "Direct link to Agent Setup and Initialization")

    class PrimaryAgent:    def __init__(self):        self.identity = None        self.latest_response = None        def initialize(self):        try:            # Initialize agent identity            self.identity = Identity.from_seed(                os.getenv("PRIMARY_AGENT_KEY"),                 0            )                        # Register with Agentverse            register_with_agentverse(                identity=self.identity,                url="http://localhost:5001/webhook",                agentverse_token=os.getenv("AGENTVERSE_API_KEY"),                agent_title="Financial Query Router",                readme="<description>Routes queries to Financial Analysis Agent</description>"            )        except Exception as e:            logger.error(f"Initialization error: {e}")            raise

### Financial Agent Discovery and Communication[â€‹](#financial-agent-discovery-and-communication "Direct link to Financial Agent Discovery and Communication")

        def find_financial_agent(self):        """Find registered financial analysis agent"""        try:            # Search for financial agent in Agentverse            available_ais = fetch.ai("Financial Analysis Agent")            agents = available_ais.get('ais', [])                        if agents:                logger.info(f"Found financial agent at address: {agents[0]['address']}")                return agents[0]            return None                    except Exception as e:            logger.error(f"Error finding financial agent: {e}")            return None    @app.route('/api/send-request', methods=['POST'])    def send_request():        try:            # Extract user query            data = request.json            user_input = data.get('input')                        # Find and validate financial agent            agent = primary_agent.find_financial_agent()            if not agent:                return jsonify({"error": "Financial analysis agent not available"}), 404                        # Forward request to financial agent            send_message_to_agent(                primary_agent.identity,                agent['address'],                {"request": user_input}            )                        return jsonify({"status": "request_sent"})                    except Exception as e:            logger.error(f"Error processing request: {e}")            return jsonify({"error": str(e)}), 500

### Response Management[â€‹](#response-management "Direct link to Response Management")

        @app.route('/webhook', methods=['POST'])    def webhook():        try:            # Parse incoming agent message            data = request.get_data().decode("utf-8")            message = parse_message_from_agent(data)                        # Store response for polling            primary_agent.latest_response = message.payload                        return jsonify({"status": "success"})                    except Exception as e:            logger.error(f"Error in webhook: {e}")            return jsonify({"error": str(e)}), 500    @app.route('/api/get-response', methods=['GET'])    def get_response():        try:            if primary_agent.latest_response:                response = primary_agent.latest_response                primary_agent.latest_response = None                return jsonify(response)            return jsonify({"status": "waiting"})        except Exception as e:            logger.error(f"Error getting response: {e}")            return jsonify({"error": str(e)}), 500

3\. Financial Analysis Agent Registration[â€‹](#3-financial-analysis-agent-registration "Direct link to 3. Financial Analysis Agent Registration")
------------------------------------------------------------------------------------------------------------------------------------------------

### Agent Registration and Setup[â€‹](#agent-registration-and-setup "Direct link to Agent Registration and Setup")

    def init_agent():    """Initialize and register the agent with agentverse"""    global financial_identity, research_chain    try:        # Initialize the research chain        research_chain = init_financial_system()                # Initialize identity and register with Agentverse        financial_identity = Identity.from_seed(            os.getenv("FINANCIAL_AGENT_KEY"),             0        )                # Register with detailed capabilities description        register_with_agentverse(            identity=financial_identity,            url="http://localhost:5008/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="Financial Analysis Agent",            readme = """                <description>A comprehensive financial analysis agent that combines                 SEC filing analysis with real-time market data for Apple Inc.</description>                <use_cases>                    <use_case>Get detailed revenue analysis from SEC filings</use_case>                    <use_case>Analyze risk factors from latest 10-K</use_case>                    <use_case>Track financial metrics and trends</use_case>                </use_cases>                <payload_requirements>                    <payload>                        <requirement>                            <parameter>query</parameter>                            <description>What would you like to know about Apple's financials?</description>                        </requirement>                    </payload>                </payload_requirements>            """        )

### Query Processing and Response[â€‹](#query-processing-and-response "Direct link to Query Processing and Response")

    @app.route('/webhook', methods=['POST'])def webhook():    try:        # Parse incoming message        data = request.get_data().decode('utf-8')        message = parse_message_from_agent(data)        query = message.payload.get("request", "")        agent_address = message.sender        # Validate query        if not query:            return jsonify({"status": "error", "message": "No query provided"}), 400        # Process query using research chain        result = research_chain.invoke({            "messages": [HumanMessage(content=query)],            "team_members": ["Search", "SECAnalyst"]        })        # Format response for client        formatted_result = {            "analysis": [                {                    "role": msg.type if hasattr(msg, 'type') else "message",                    "content": msg.content,                    "name": msg.name if hasattr(msg, 'name') else None                }                for msg in result.get('messages', [])            ]        }        # Send response back through Agentverse        send_message_to_agent(            financial_identity,            agent_address,            {'analysis_result': formatted_result}        )        return jsonify({"status": "analysis_sent"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"status": "error", "message": str(e)}), 500</content>
</page>

<page>
  <title>TransactAI Payment Example | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/examples/transactAI/</url>
  <content>This example demonstrates how to use TransactAI for agent-to-agent payments on Agentverse with [TransactAI](https://agentverse.ai/agents/details/agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrq/profile). The example shows a complete payment flow between two agents: Alice (sender) and Bob (receiver).

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

The example demonstrates the following:

1.  Registering agents with TransactAI
2.  Linking on-chain wallet addresses
3.  Making on-chain deposits to fund the TransactAI account
4.  Sending payments between agents
5.  Receiving payments
6.  Withdrawing funds to on-chain wallets

Workflow Diagram[â€‹](#workflow-diagram "Direct link to Workflow Diagram")
------------------------------------------------------------------------

1.  **Registration Process**: Before using TransactAI, agents must register themselves and their wallet addresses.
    
2.  **Denominations**: All amounts are expressed in "atestfet" (smallest unit), where 1 TESTFET = 10^18 atestfet.
    
3.  **Deposit Process**:
    
    *   Make an on-chain deposit to TransactAI's wallet
    *   Notify TransactAI with the transaction hash
    *   Wait for TransactAI to confirm the deposit (may require several blockchain confirmations)
4.  **Payment Flow**:
    
    *   Sender sends payment command to TransactAI
    *   TransactAI updates internal balances
    *   TransactAI sends confirmation to sender
    *   TransactAI sends notification to recipient
5.  **Withdrawal Process**:
    
    *   Send withdrawal request to TransactAI with amount and wallet address
    *   TransactAI executes on-chain transaction
    *   TransactAI sends confirmation with transaction hash
6.  **Acknowledgements**: All messages must be acknowledged to confirm receipt.
    

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

*   [Agnetverse](https://agentverse.ai/) account.
*   Access to Fetch.ai Dorado testnet.
*   Testnet tokens (can be obtained from [Fetch.ai Dorado Faucet](https://companion.fetch.ai/dorado-1/accounts/fetch1n9vnna29c8us5pk9fxqhzltmgpeyn3hl4le28h#Transactions))

Example Code[â€‹](#example-code "Direct link to Example Code")
------------------------------------------------------------

### agent\_protocol.py[â€‹](#agent_protocolpy "Direct link to agent_protocol.py")

First, we need the custom protocol that enables communication with TransactAI:

agent\_protocol.py

    """Custom Agent Protocol for TransactAIThis protocol allows agents to communicate with the TransactAI payment agentby defining message structures and content types."""from uagents import Protocolfrom datetime import datetimefrom typing import Literal, TypedDict, Dict, List, Unionimport uuidfrom pydantic.v1 import UUID4, Fieldfrom uagents_core.models import Modelfrom uagents_core.protocol import ProtocolSpecification# --- Content Model Definitions ---class Metadata(TypedDict, total=False):    mime_type: str    role: strclass TextContent(Model):    type: Literal["text"] = "text"    text: strclass MetadataContent(Model):    type: Literal["metadata"] = "metadata"    metadata: dict[str, str]# Combined content typesAgentContent = Union[TextContent, MetadataContent]# --- Main Protocol Message Models ---class AgentMessage(Model):    timestamp: datetime = Field(default_factory=datetime.utcnow)    msg_id: UUID4 = Field(default_factory=uuid.uuid4)    content: list[AgentContent]class AgentAcknowledgement(Model):    timestamp: datetime = Field(default_factory=datetime.utcnow)    acknowledged_msg_id: UUID4    metadata: dict[str, str] | None = None# --- Protocol Specification ---agent_protocol_spec = ProtocolSpecification(    name="AgentProtocol",    version="1.0.0",    interactions={        AgentMessage: {AgentAcknowledgement},        AgentAcknowledgement: set(),    },)# --- Protocol Instance ---agent_proto = Protocol(spec=agent_protocol_spec)# --- Helper Functions ---def create_metadata_message(metadata: Dict[str, str]) -> AgentMessage:    """Create an agent message with metadata content"""    return AgentMessage(        content=[MetadataContent(metadata=metadata)]    )def create_text_message(text: str) -> AgentMessage:    """Create an agent message with text content"""    return AgentMessage(        content=[TextContent(text=text)]    )

### Alice (Sender) Agent[â€‹](#alice-sender-agent "Direct link to Alice (Sender) Agent")

alice\_agent.py

    """Alice Agent - Sends payments via TransactAI"""import asynciofrom uagents import Agent, Contextfrom uagents.network import get_ledgerfrom datetime import datetime# Import the custom agent protocol# Ensure agent_protocol.py is in the same directory or accessible in PYTHONPATHtry:    from agent_protocol import (        agent_proto,        AgentMessage,        AgentAcknowledgement,        create_metadata_message    )except ImportError:    print("Error: agent_protocol.py not found. Please ensure it's in the correct path.")    # Define minimal models if import fails, to allow basic understanding    from uagents import Model, Protocol    from typing import List, Dict, Union, Literal, Optional    from pydantic.v1 import Field, UUID4    import uuid    class MetadataContent(Model):        type: Literal["metadata"] = "metadata"        metadata: dict[str, str]    AgentContent = Union[MetadataContent]    class AgentMessage(Model):        timestamp: datetime = Field(default_factory=datetime.utcnow)        msg_id: UUID4 = Field(default_factory=uuid.uuid4)        content: list[AgentContent]    class AgentAcknowledgement(Model):        timestamp: datetime = Field(default_factory=datetime.utcnow)        acknowledged_msg_id: UUID4        metadata: Optional[dict[str, str]] = None    def create_metadata_message(metadata: Dict[str, str]) -> AgentMessage:        return AgentMessage(content=[MetadataContent(metadata=metadata)])    # Define a dummy protocol if needed for basic structure    agent_proto = Protocol("DummyAgentProto", version="1.0")# TransactAI agent address (ensure this is correct)TRANSACTAI_ADDRESS = "agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrq"TRANSACTAI_WALLET = "fetch1uyxsdlejg7axp4dzmqpq54g0uwde5nv6fflhkv" # TransactAI's on-chain wallet# Bob's agent addressBOB_ADDRESS = "agent1q2v8ck694z7h6qww90kvv4qafrnvlaquwpeuqtarsyhdmsve4v6k7r0a8l0" # Replace it with your bob's address # Alice agent setup# CRITICAL: Replace this seed phrase with your own secure one for actual use!alice = Agent()DEPOSIT_CONFIRMED_FLAG = "deposit_confirmed"PAYMENT_ATTEMPTED_FLAG = "payment_attempted"@alice.on_event("startup")async def startup(ctx: Context):    ctx.logger.info(f"Alice started. Address: {alice.address}")    ctx.logger.info(f"Wallet address: {alice.wallet.address()}")    # Initialize flags    ctx.storage.set(DEPOSIT_CONFIRMED_FLAG, False)    ctx.storage.set(PAYMENT_ATTEMPTED_FLAG, False)    # 1. Register Agent with TransactAI    ctx.logger.info("Registering agent with TransactAI...")    register_msg = create_metadata_message({'command': 'register'})    await ctx.send(TRANSACTAI_ADDRESS, register_msg)    await asyncio.sleep(2.0) # Wait for agent registration    # 2. Register Wallet with TransactAI    ctx.logger.info("Registering wallet with TransactAI...")    wallet_address = str(alice.wallet.address()) # Get Alice's wallet address    register_wallet_msg = create_metadata_message({        'command': 'register_wallet',        'wallet_address': wallet_address    })    await ctx.send(TRANSACTAI_ADDRESS, register_wallet_msg)    await asyncio.sleep(5.0) # Wait for wallet registration    # 3. On-chain deposit to TransactAI wallet    ctx.logger.info("Sending on-chain deposit to TransactAI wallet...")    deposit_amount = 100000000000000000 # 0.1 testfet (needs enough for payment)    tx_hash = None    try:        ledger = get_ledger("dorado") # Get ledger instance right before use        # Ensure wallet has funds from faucet: https://companion.fetch.ai/dorado-1/accounts        ctx.logger.info(f"Attempting to send {deposit_amount} atestfet to {TRANSACTAI_WALLET}")        tx = ledger.send_tokens(TRANSACTAI_WALLET, deposit_amount, "atestfet", alice.wallet)        result = tx.wait_to_complete()        tx_hash = result.tx_hash        ctx.logger.info(f"Deposit transaction hash: {tx_hash}")    except Exception as e:        ctx.logger.error(f"Error sending on-chain deposit: {e}")        ctx.logger.error("Ensure Alice's wallet has sufficient 'atestfet' from the faucet.")        return    # 4. Send deposit confirmation command to TransactAI    if tx_hash:        ctx.logger.info(f"Sending deposit confirmation command for tx_hash: {tx_hash}")        deposit_confirm_msg = create_metadata_message({            'command': 'deposit',            'tx_hash': tx_hash,            'amount': str(deposit_amount),            'denom': "atestfet"        })        await ctx.send(TRANSACTAI_ADDRESS, deposit_confirm_msg)        # Wait for deposit confirmation state change        MAX_WAIT_TIME = 60 # seconds        WAIT_INTERVAL = 5 # seconds        time_waited = 0        deposit_confirmed = False        while time_waited < MAX_WAIT_TIME:            if ctx.storage.get(DEPOSIT_CONFIRMED_FLAG) is True:                deposit_confirmed = True                ctx.logger.info("Deposit confirmed by TransactAI.")                break            ctx.logger.info(f"Waiting for deposit confirmation... ({time_waited}/{MAX_WAIT_TIME}s)")            await asyncio.sleep(WAIT_INTERVAL)            time_waited += WAIT_INTERVAL        if not deposit_confirmed:            ctx.logger.error("Timed out waiting for deposit confirmation from TransactAI.")            return # Stop if deposit not confirmed    else:        ctx.logger.error("On-chain deposit failed, cannot proceed.")        return # Stop if deposit failed    # 5. Send payment to Bob via TransactAI (only if deposit confirmed and payment not already attempted)    # This part is triggered by handle_transactai_response upon successful deposit confirmation    # We call maybe_send_payment here just in case the confirmation message arrived *during* the wait loop    await maybe_send_payment(ctx)# Handle responses from TransactAI@agent_proto.on_message(model=AgentMessage)async def handle_transactai_response(ctx: Context, sender: str, msg: AgentMessage):    ctx.logger.info(f"Received message from {sender}")    response_handled = False # Flag to ensure ack is sent even if no specific handler matches    for content in msg.content:        if content.type == "metadata":            metadata = content.metadata            ctx.logger.info(f"Metadata: {metadata}")                        command = metadata.get('command')            status = metadata.get('status')            if command == 'register_response':                 ctx.logger.info(f"Registration response: {status}")                 response_handled = True            elif command == 'register_wallet_response':                 ctx.logger.info(f"Wallet registration response: {status}")                 response_handled = True            elif command == 'deposit_response':                 ctx.logger.info(f"Deposit response received: {metadata}")                 if status == 'success':                     ctx.storage.set(DEPOSIT_CONFIRMED_FLAG, True)                     # If payment hasn't been attempted yet, trigger it now                     asyncio.create_task(maybe_send_payment(ctx))                 elif status == 'pending_confirmation':                     ctx.logger.info("Deposit is still pending confirmation.")                 else: # Failed                     ctx.logger.error(f"Deposit failed: {metadata.get('reason')}")                     # Consider setting a 'deposit_failed' flag if needed                 response_handled = True            elif command == 'payment_confirmation':                 if status == 'success':                     ctx.logger.info(f"Payment successful! New balance: {metadata.get('balance')}")                 else:                     ctx.logger.error(f"Payment failed! Reason: {metadata.get('reason')}, Balance: {metadata.get('balance')}")                 response_handled = True            # Handle other responses if needed (e.g., balance_response)                elif content.type == "text":            ctx.logger.info(f"Text: {content.text}")            response_handled = True # Acknowledge text messages too    # Send acknowledgement back to TransactAI if message was processed    if response_handled:        await ctx.send(sender, AgentAcknowledgement(            timestamp=datetime.utcnow(),            acknowledged_msg_id=msg.msg_id        ))    else:        ctx.logger.warning(f"Received unhandled message content types from {sender}: {[c.type for c in msg.content]}")# Separate function to attempt payment after deposit confirmationasync def maybe_send_payment(ctx: Context):    # Ensure deposit is confirmed and payment hasn't been attempted    if ctx.storage.get(DEPOSIT_CONFIRMED_FLAG) is True and not ctx.storage.get(PAYMENT_ATTEMPTED_FLAG):        payment_amount = 100000000000000000 # Example amount (0.1 atestfet)        ctx.logger.info(f"Deposit confirmed, now attempting to pay {payment_amount} to Bob ({BOB_ADDRESS})...")        ctx.storage.set(PAYMENT_ATTEMPTED_FLAG, True) # Mark as attempted        payment_msg = create_metadata_message({            'command': 'payment',            'recipient': BOB_ADDRESS,            'amount': str(payment_amount),            'reference': f"payment-{datetime.utcnow().isoformat()}"        })        await ctx.send(TRANSACTAI_ADDRESS, payment_msg)# Handle acknowledgements (optional)@agent_proto.on_message(model=AgentAcknowledgement)async def handle_ack(ctx: Context, sender: str, msg: AgentAcknowledgement):    ctx.logger.info(f"Received acknowledgement from {sender} for message {msg.acknowledged_msg_id}")# Include the protocolalice.include(agent_proto)if __name__ == "__main__":    print(f"Alice starting. Address: {alice.address}")    print("Ensure agent_protocol.py is accessible.")    print("CRITICAL: Replace the example ALICE_SEED in the code if using for anything beyond this demo.")    alice.run()

### Bob (Receiver) Agent[â€‹](#bob-receiver-agent "Direct link to Bob (Receiver) Agent")

bob\_agent.py

    """Bob Agent - Receives payments via TransactAI"""import asynciofrom uagents import Agent, Contextfrom datetime import datetime# Import the custom agent protocolfrom agent_protocol import (    agent_proto,    AgentMessage,    AgentAcknowledgement,    create_metadata_message)# TransactAI agent address (ensure this is correct)TRANSACTAI_ADDRESS = "agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrq"# Bob agent setupbob = Agent()@bob.on_event("startup")async def startup(ctx: Context):    ctx.logger.info(f"Bob started. Address: {bob.address}")    ctx.logger.info(f"Wallet address: {bob.wallet.address()}")    # Give agents time to register etc.    await asyncio.sleep(5.0)    # 1. Register Agent with TransactAI    ctx.logger.info("Registering agent with TransactAI...")    register_msg = create_metadata_message({'command': 'register'})    await ctx.send(TRANSACTAI_ADDRESS, register_msg)    await asyncio.sleep(2.0) # Wait briefly    # 2. Register Wallet with TransactAI    ctx.logger.info("Registering wallet with TransactAI...")    wallet_address = str(bob.wallet.address()) # Get Bob's wallet address    register_wallet_msg = create_metadata_message({        'command': 'register_wallet',        'wallet_address': wallet_address    })    await ctx.send(TRANSACTAI_ADDRESS, register_wallet_msg)    await asyncio.sleep(2.0) # Wait briefly# Handle responses/notifications from TransactAI@agent_proto.on_message(model=AgentMessage)async def handle_transactai_message(ctx: Context, sender: str, msg: AgentMessage):    ctx.logger.info(f"Received message from {sender}")    for content in msg.content:        if content.type == "metadata":            metadata = content.metadata            ctx.logger.info(f"Metadata: {metadata}")                        command = metadata.get('command')            status = metadata.get('status')            if command == 'register_response':                 ctx.logger.info(f"Registration response: {status}")            elif command == 'payment_received':                 ctx.logger.info(f"Payment received from {metadata.get('from')}!")                 amount_received_str = metadata.get('amount')                 ctx.logger.info(f"Amount: {amount_received_str}, Reference: {metadata.get('reference')}")                 ctx.logger.info(f"New balance: {metadata.get('balance')}")                 # Attempt to withdraw the received amount                 try:                     amount_to_withdraw = int(amount_received_str)                     if amount_to_withdraw > 0:                         ctx.logger.info(f"Attempting to withdraw received amount: {amount_to_withdraw}")                         withdraw_msg = create_metadata_message({                             'command': 'withdraw',                             'amount': str(amount_to_withdraw),                             'wallet_address': str(bob.wallet.address()), # Bob's own wallet                             'denom': "atestfet"                         })                         # Send withdrawal request asynchronously                         asyncio.create_task(ctx.send(TRANSACTAI_ADDRESS, withdraw_msg))                     else:                         ctx.logger.info("Received payment amount is zero or invalid, not withdrawing.")                 except (ValueError, TypeError) as e:                     ctx.logger.error(f"Could not parse amount for withdrawal: {amount_received_str}, Error: {e}")            elif command == 'withdraw_confirmation':                 ctx.logger.info(f"Withdrawal confirmation received: {metadata}")                 if status == 'success':                     ctx.logger.info(f"Withdrawal successful! Tx: {metadata.get('tx_hash')}, New balance: {metadata.get('balance')}")                 else:                     ctx.logger.error(f"Withdrawal failed! Reason: {metadata.get('reason')}")            # Handle other relevant messages like escrow notifications if needed                elif content.type == "text":            ctx.logger.info(f"Text: {content.text}")    # Send acknowledgement back to TransactAI    await ctx.send(sender, AgentAcknowledgement(        timestamp=datetime.utcnow(),        acknowledged_msg_id=msg.msg_id    ))# Handle acknowledgements (optional)@agent_proto.on_message(model=AgentAcknowledgement)async def handle_ack(ctx: Context, sender: str, msg: AgentAcknowledgement):    ctx.logger.info(f"Received acknowledgement from {sender} for message {msg.acknowledged_msg_id}")# Include the protocolbob.include(agent_proto)if __name__ == "__main__":    print(f"Bob starting. Address: {bob.address}")    bob.run()

Running the Example[â€‹](#running-the-example "Direct link to Running the Example")
---------------------------------------------------------------------------------

To run this example:

1.  Create two agents Alice and Bob on Agentverse.
2.  Create `agent_protocol.py` in both the agents and put in the code from above.
3.  Get some test tokens for the sender's wallet from the [Fetch.ai Dorado Faucet](https://companion.fetch.ai/dorado-1/accounts)
4.  Start the receiver agent (bob) first from agentverse.
5.  Copy the bob's address and update `RECIPIENT_ADDRESS` in alice's agent.py
6.  Start the alice agent as well.

Expected Output[â€‹](#expected-output "Direct link to Expected Output")
---------------------------------------------------------------------

### Receiver Output (Bob)[â€‹](#receiver-output-bob "Direct link to Receiver Output (Bob)")

    Receiver agent started. Address: agent1q2v8ck694z7h6qww90kvv4qafrnvlaquwpeuqtarsyhdmsve4v6k7r0a8l0Wallet address: fetch1ljd6tn7kr27wv0jwn6g63a7x38zj5ltfvwm8dwRegistering with TransactAI...Registering wallet fetch1ljd6tn7kr27wv0jwn6g63a7x38zj5ltfvwm8dw...âœ… Setup complete - Ready to receive paymentsReceived message from agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrqMetadata: {'command': 'register_response', 'status': 'success', 'balance': '0'}Registration response: successReceived message from agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrqMetadata: {'command': 'register_wallet_response', 'status': 'success', 'wallet_address': 'fetch1ljd6tn7kr27wv0jwn6g63a7x38zj5ltfvwm8dw'}Wallet registration response: successReceived message from agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrqMetadata: {'command': 'payment_received', 'from': 'agent1q2gxyqnwyr85v6ek7rplk09etnqgn4cfllph6gx3gtsuqzzn5p856gway5s', 'amount': '50000000000000000', 'reference': 'Example payment 2025-04-29T13:18:27.441861', 'balance': '5.0E+16'}ğŸ’° Payment received!  From: agent1q2gxyqnwyr85v6ek7rplk09etnqgn4cfllph6gx3gtsuqzzn5p856gway5s  Amount: 50000000000000000 atestfet  Reference: Example payment 2025-04-29T13:18:27.441861  New balance: 5.0E+16Withdrawing 50000000000000000 atestfet to fetch1ljd6tn7kr27wv0jwn6g63a7x38zj5ltfvwm8dw...Received message from agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrqMetadata: {'command': 'withdraw_confirmation', 'status': 'success', 'amount': '50000000000000000', 'wallet_address': 'fetch1ljd6tn7kr27wv0jwn6g63a7x38zj5ltfvwm8dw', 'tx_hash': 'A44295DCE532825CE257D63C79A63C01180FCC8DEF44258271C5E5FB9AE0F561', 'balance': '0', 'message': 'Withdrawal processed. Funds sent to your wallet.'}âœ… Withdrawal successful!  Transaction hash: A44295DCE532825CE257D63C79A63C01180FCC8DEF44258271C5E5FB9AE0F561  New balance: 0

### Sender Output[â€‹](#sender-output "Direct link to Sender Output")

    Sender agent started. Address: agent1q2gxyqnwyr85v6ek7rplk09etnqgn4cfllph6gx3gtsuqzzn5p856gway5sWallet address: fetch1n9vnna29c8us5pk9fxqhzltmgpeyn3hl4le28hRegistering with TransactAI...Registering wallet fetch1n9vnna29c8us5pk9fxqhzltmgpeyn3hl4le28h...Making on-chain deposit of 100000000000000000 atestfet...Attempting to send 100000000000000000 atestfet to fetch1uyxsdlejg7axp4dzmqpq54g0uwde5nv6fflhkvDeposit transaction hash: AE4A482EFE4215D6A54C63D1F4EE9EFA50437572BDECBD347FAA752BD9B3E0FAWaiting for deposit confirmation... (0/60s)Received message from agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrqMetadata: {'command': 'register_response', 'status': 'success', 'balance': '0'}Registration response: successReceived message from agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrqMetadata: {'command': 'register_wallet_response', 'status': 'success', 'wallet_address': 'fetch1n9vnna29c8us5pk9fxqhzltmgpeyn3hl4le28h'}Wallet registration response: successReceived message from agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrqMetadata: {'command': 'deposit_response', 'status': 'pending_confirmation', 'reason': 'Awaiting confirmations (2/6)'}Deposit pending: Awaiting confirmations (2/6)Waiting for deposit confirmation... (5/60s)Waiting for deposit confirmation... (10/60s)Received message from agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrqMetadata: {'command': 'deposit_response', 'status': 'success', 'amount': '100000000000000000', 'denom': 'atestfet', 'balance': '1.0E+17', 'tx_hash': 'AE4A482EFE4215D6A54C63D1F4EE9EFA50437572BDECBD347FAA752BD9B3E0FA'}Deposit confirmed! New balance: 1.0E+17âœ… Deposit confirmed!Sending payment of 50000000000000000 atestfet to agent1q2v8ck694z7h6qww90kvv4qafrnvlaquwpeuqtarsyhdmsve4v6k7r0a8l0...Received message from agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrqMetadata: {'command': 'payment_confirmation', 'status': 'success', 'recipient': 'agent1q2v8ck694z7h6qww90kvv4qafrnvlaquwpeuqtarsyhdmsve4v6k7r0a8l0', 'amount': '50000000000000000', 'balance': '5.0E+16'}Payment successful! New balance: 5.0E+16

Additional Features[â€‹](#additional-features "Direct link to Additional Features")
---------------------------------------------------------------------------------

In addition to basic payments, TransactAI also supports:

*   **Escrow Services**: Hold funds conditionally until released by the sender
*   **Balance Queries**: Check your current balance at any time
*   **Automated Withdrawals**: Set up automatic withdrawals for received payments

Refer to the complete [TransactAI documentation](https://innovationlab.fetch.ai/resources/docs/next/agent-transaction/agent-transaction) for more details on these advanced features.</content>
</page>

<page>
  <title>Getting Started with ASI-1 Mini | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/asione/asi1-mini-getting-started</url>
  <content>This guide will walk you through the process of setting up and making your first API call to ASI:One, the Web3-native Large Language Model designed for agentic AI.

Before you can start using ASI:One, you'll need to obtain an API key. Follow these steps:

Once you have your API key, you can start making requests to the ASI:One API. Here's a simple example using Python:

    import requestsimport jsonurl = "https://api.asi1.ai/v1/chat/completions"payload = json.dumps({  "model": "asi1-mini",  "messages": [    {      "role": "user",      "content": "Hello, tell me about agentic AI"    }  ],  "temperature": 0.7,  "stream": False,  "max_tokens": 500})headers = {  'Content-Type': 'application/json',  'Accept': 'application/json',  'Authorization': 'Bearer YOUR_API_KEY'  # Replace with your actual API key}response = requests.request("POST", url, headers=headers, data=payload)print(response.text)

Replace `YOUR_API_KEY` with the API key you obtained in the previous steps.

The API will return a JSON response containing the model's reply. If you set `stream` to `True`, the response will be streamed as it's generated, which is useful for creating more responsive applications.

For a more interactive experience, you can set the `stream` parameter to `True` to receive the response as it's being generated:

    import requestsimport jsonurl = "https://api.asi1.ai/v1/chat/completions"payload = json.dumps({  "model": "asi1-mini",  "messages": [    {      "role": "user",      "content": "Explain the concept of decentralized AI"    }  ],  "temperature": 0.7,  "stream": True,  "max_tokens": 500})headers = {  'Content-Type': 'application/json',  'Accept': 'application/json',  'Authorization': 'Bearer YOUR_API_KEY'  # Replace with your actual API key}response = requests.request("POST", url, headers=headers, data=payload, stream=True)for line in response.iter_lines():    if line:        line_text = line.decode('utf-8')        if line_text.startswith('data: '):            data_str = line_text[6:]  # Remove 'data: ' prefix            if data_str == '[DONE]':                break            try:                data = json.loads(data_str)                if 'choices' in data and len(data['choices']) > 0:                    delta = data['choices'][0].get('delta', {})                    if 'content' in delta:                        print(delta['content'], end='', flush=True)            except json.JSONDecodeError:                pass

Now that you've made your first API call to ASI:One, you can explore more advanced features:

By leveraging ASI-1 Mini's agentic capabilities, you can build sophisticated AI applications that can reason, plan, and execute complex tasks autonomously within the Web3 ecosystem.</content>
</page>

<page>
  <title>ASI-1 Mini API Reference | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/asione/asi1-mini-api-reference</url>
  <content>This API Reference describes the RESTful and streaming interfaces of the ASI:One platform.

Introduction[â€‹](#introduction "Direct link to Introduction")
------------------------------------------------------------

ASI:One provides a powerful API that allows developers to integrate advanced agentic AI capabilities into their applications. The API is designed to be easy to use while providing access to the full range of ASI:One's capabilities.

OpenAI Compatibility[â€‹](#openai-compatibility "Direct link to OpenAI Compatibility")
------------------------------------------------------------------------------------

Where possible, the ASI:One API conforms to the OpenAI API specification. This means that users can often plug the ASI:One API into existing code that uses the OpenAI API with minimal changes.

API keys can be created from your account when you log into ASI:One. Authorization is done by adding the following header into your requests:

    Authorization: Bearer <api token>

Remember your API key is a secret, do not share it with anyone. If you need to revoke access for a particular key, simply log into your account and delete it from your profile.

Base URL[â€‹](#base-url "Direct link to Base URL")
------------------------------------------------

All API requests should be made to the following base URL:

Endpoints[â€‹](#endpoints "Direct link to Endpoints")
---------------------------------------------------

### Chat Completions[â€‹](#chat-completions "Direct link to Chat Completions")

Creates a model response for the given chat conversation.

#### Request Parameters[â€‹](#request-parameters "Direct link to Request Parameters")

| Parameter | Type | Required | Description |
| --- | --- | --- | --- |
| model | string | Yes | ID of the model to use. Currently, only "asi1-mini" is available. |
| messages | array | Yes | An array of message objects representing the conversation history. |
| temperature | number | No | What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. Default is 1.0. |
| stream | boolean | No | If set to true, partial message deltas will be sent as they become available. Default is false. |
| max\_tokens | integer | No | The maximum number of tokens that can be generated in the chat completion. This can be used to control costs for text generated via API. |

#### Message Object[â€‹](#message-object "Direct link to Message Object")

Each message in the `messages` array should have the following structure:

| Field | Type | Description |
| --- | --- | --- |
| role | string | The role of the message author. Must be one of "system", "user", or "assistant". |
| content | string | The content of the message. |

#### Example Request[â€‹](#example-request "Direct link to Example Request")

    {  "model": "asi1-mini",  "messages": [    {      "role": "system",      "content": "You are a helpful assistant specialized in Web3 technologies."    },    {      "role": "user",      "content": "Explain the concept of decentralized AI."    }  ],  "temperature": 0.7,  "stream": false,  "max_tokens": 500}

#### Response Format[â€‹](#response-format "Direct link to Response Format")

The API returns a JSON object with the following structure:

| Field | Type | Description |
| --- | --- | --- |
| id | string | The completion request ID. |
| model | string | The name of the model being used. |
| thought | string | (Optional) The thoughts that were generated as part of the chat completion request. Only present when streaming is enabled. |
| choices | array | An array of completion choices. |
| usage | object | (Optional) Information about token usage. |

#### Choice Object[â€‹](#choice-object "Direct link to Choice Object")

Each choice in the `choices` array has the following structure:

| Field | Type | Description |
| --- | --- | --- |
| index | integer | The index of the choice in the array. |
| delta | object | (When streaming) Contains the incremental content being streamed. |
| finish\_reason | string | The reason the model stopped generating text. Can be "stop", "length", etc. |
| stop\_reason | string | Additional information about why generation stopped. |

#### Example Response (Non-streaming)[â€‹](#example-response-non-streaming "Direct link to Example Response (Non-streaming)")

    {  "id": "id_comqjiusZjAoyuXlh",  "object": "chat.completion",  "model": "asi1-mini",  "choices": [    {      "index": 0,      "message": {        "role": "assistant",        "content": "Decentralized AI refers to artificial intelligence systems that operate on distributed networks rather than centralized servers. This approach aligns with Web3 principles by removing central points of control and enabling more democratic access to AI capabilities..."      },      "finish_reason": "stop",      "stop_reason": "stop"    }  ],  "usage": {    "prompt_tokens": 435,    "completion_tokens": 391,    "total_tokens": 826  }}

#### Streaming Responses[â€‹](#streaming-responses "Direct link to Streaming Responses")

When `stream` is set to `true`, the API will send a series of server-sent events (SSE) with partial completions as they become available. Each event is prefixed with `data:` and contains a JSON object with incremental updates.

The stream is terminated with a `data: [DONE]` message.

Error Handling[â€‹](#error-handling "Direct link to Error Handling")
------------------------------------------------------------------

The API uses standard HTTP status codes to indicate the success or failure of requests:

*   200: Success
*   400: Bad Request (invalid parameters)
*   401: Unauthorized (invalid API key)
*   422: Unprocessable Entity (valid parameters but request cannot be processed)
*   429: Too Many Requests (rate limit exceeded)
*   500: Internal Server Error

Error responses include a JSON object with an `error` field containing details about the error.

Rate Limits[â€‹](#rate-limits "Direct link to Rate Limits")
---------------------------------------------------------

API usage is subject to rate limits based on your account tier. If you exceed these limits, you'll receive a 429 status code. The response headers include information about your current rate limit status.

Best Practices[â€‹](#best-practices "Direct link to Best Practices")
------------------------------------------------------------------

*   Store your API key securely and never expose it in client-side code.
*   Implement proper error handling to gracefully handle API errors.
*   For long-running conversations, consider maintaining context on your side to reduce token usage.
*   Use streaming for more responsive user interfaces when generating longer responses.

For more detailed examples of how to use the API, see the [Chat Completion](https://innovationlab.fetch.ai/resources/docs/next/asione/asi1-mini-chat-completion) guide.</content>
</page>

<page>
  <title>LangGraph Agent with MCP adapter | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/examples/mcp-integration/langgraph-mcp-agent-example</url>
  <content>    import osimport timefrom dotenv import load_dotenvfrom langchain_openai import ChatOpenAIfrom langchain_core.messages import HumanMessagefrom mcp import ClientSession, StdioServerParametersfrom mcp.client.stdio import stdio_clientimport asynciofrom langchain_mcp_adapters.tools import load_mcp_toolsfrom langgraph.prebuilt import create_react_agentfrom uagents_adapter import LangchainRegisterTool, cleanup_uagentfrom uagents_adapter.langchain import AgentManager# Load environment variablesload_dotenv()# Set your API keysOPENAI_API_KEY = os.getenv("OPENAI_API_KEY")API_TOKEN = os.getenv("AGENTVERSE_API_KEY")# Initialize the modelmodel = ChatOpenAI(model="gpt-4o")# Store the agent globally so it can be accessed by the wrapper functionagent = Noneasync def setup_math_agent():    global agent        print("Setting up math agent...")    server_params = StdioServerParameters(        command="python",        args=["math_server.py"],    )        async with stdio_client(server_params) as (read, write):        async with ClientSession(read, write) as session:            await session.initialize()            tools = await load_mcp_tools(session)            agent = create_react_agent(model, tools)                        # Test the agent            test_response = await agent.ainvoke({                "messages": [HumanMessage(content="what's (3 + 5) x 12?")]            })            print(f"Test response: {test_response['messages'][-1].content}")                        # Keep the connection alive            while True:                await asyncio.sleep(1)def main():    # Initialize agent manager    manager = AgentManager()        # Create agent wrapper    async def agent_func(x):        response = await agent.ainvoke({"messages": [HumanMessage(content=x)]})        return response["messages"][-1].content        agent_wrapper = manager.create_agent_wrapper(agent_func)        # Start the agent in background    manager.start_agent(setup_math_agent)        # Register with uAgents    print("Registering math agent...")    tool = LangchainRegisterTool()    agent_info = tool.invoke(        {            "agent_obj": agent_wrapper,            "name": "math_agent_langchain_mcp",            "port": 8080,            "description": "A math calculation agent",            "api_token": API_TOKEN,            "mailbox": True        }    )        print(f"âœ… Registered math agent: {agent_info}")        try:        manager.run_forever()    except KeyboardInterrupt:        print("ğŸ›‘ Shutting down...")        cleanup_uagent("math_agent")        print("âœ… Agent stopped.")if __name__ == "__main__":    main()</content>
</page>

<page>
  <title>Chat Completion with ASI-1 Mini | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/asione/asi1-mini-chat-completion</url>
  <content>The Chat Completion API is the primary way to interact with ASI:One. This guide provides detailed information on how to use the API effectively, with examples and best practices.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

The Chat Completion API allows you to have conversational interactions with ASI:One. You provide a series of messages representing a conversation, and the model generates a response that continues the conversation in a natural way.

What sets ASI:One apart from other LLMs is its agentic capabilities - it can reason through complex problems, maintain context across long conversations, and execute multi-step tasks autonomously.

Endpoint[â€‹](#endpoint "Direct link to Endpoint")
------------------------------------------------

    POST https://api.asi1.ai/v1/chat/completions

Request Format[â€‹](#request-format "Direct link to Request Format")
------------------------------------------------------------------

A basic chat completion request includes the model name, a list of messages, and optional parameters to control the generation:

    {  "model": "asi1-mini",  "messages": [    {      "role": "user",      "content": "Your message here"    }  ],  "temperature": 0,  "stream": false,  "max_tokens": 500}

### Required Parameters[â€‹](#required-parameters "Direct link to Required Parameters")

*   `model`: Currently, only "asi1-mini" is available.
*   `messages`: An array of message objects representing the conversation history.

### Optional Parameters[â€‹](#optional-parameters "Direct link to Optional Parameters")

*   `temperature`: Controls randomness in the response. Range is 0-2, with lower values producing more deterministic outputs. Default is 1.0.
*   `stream`: When set to `true`, the API will stream the response as it's generated. Default is `false`.
*   `max_tokens`: The maximum number of tokens to generate. Default varies based on the model.

Message Roles[â€‹](#message-roles "Direct link to Message Roles")
---------------------------------------------------------------

Each message in the conversation has a `role` and `content`. The available roles are:

*   `system`: Used to set the behavior or context for the assistant. System messages help guide the model's behavior.
*   `user`: Represents messages from the user.
*   `assistant`: Represents previous responses from the assistant.

Basic Example[â€‹](#basic-example "Direct link to Basic Example")
---------------------------------------------------------------

Here's a simple example of a chat completion request and response:

### Request[â€‹](#request "Direct link to Request")

    import requestsimport jsonurl = "https://api.asi1.ai/v1/chat/completions"payload = json.dumps({  "model": "asi1-mini",  "messages": [    {      "role": "user",      "content": "Hi, tell me about giraffes"    }  ],  "temperature": 0,  "stream": False,  "max_tokens": 500})headers = {  'Content-Type': 'application/json',  'Accept': 'application/json',  'Authorization': 'Bearer YOUR_API_KEY'  # Replace with your actual API key}response = requests.request("POST", url, headers=headers, data=payload)print(response.text)

### Response[â€‹](#response "Direct link to Response")

    {  "id": "id_comqjiusZjAoyuXlh",  "object": "chat.completion",  "model": "asi1-mini",  "choices": [    {      "index": 0,      "message": {        "role": "assistant",        "content": "Giraffes are fascinating creatures known for their towering height and distinctive long necks. They are the tallest land animals, with heights ranging from 14 to 19 feet (4.3 to 5.8 meters). Males, called bulls, are typically taller and heavier than females (cows). Their long necks allow them to reach leaves, flowers, and fruits high up in trees, especially from their preferred food source, the acacia tree.\n\nThese animals are native to Africa and are commonly found in savannas, grasslands, and open woodlands. Giraffes live in loose social groups, often mingling with other herbivores like zebras and antelopes. They are generally peaceful but can defend themselves with powerful kicks if threatened by predators such as lions or hyenas.\n\nOne of the most unique traits of giraffes is their spotted coat pattern, which is unique to each individual, much like a human fingerprint. They also have a specialized cardiovascular system to manage blood flow due to their heightâ€”especially when lowering their heads to drink water. Giraffes only need to drink water occasionally, as they obtain most of their moisture from the plants they consume.\n\nTheir social behavior and communication are also intriguing. Giraffes communicate using low-frequency sounds that are often inaudible to human ears. Additionally, they engage in a behavior called \"necking,\" where males swing their necks to compete for dominance or mates."      },      "finish_reason": "stop",      "stop_reason": "stop"    }  ],  "usage": {    "prompt_tokens": 435,    "completion_tokens": 391,    "total_tokens": 826  }}

Multi-turn Conversations[â€‹](#multi-turn-conversations "Direct link to Multi-turn Conversations")
------------------------------------------------------------------------------------------------

ASI:One excels at maintaining context across multiple turns of conversation. To create a multi-turn conversation, include previous messages in the `messages` array:

    payload = json.dumps({  "model": "asi1-mini",  "messages": [    {      "role": "system",      "content": "You are an expert in Web3 technologies and decentralized systems."    },    {      "role": "user",      "content": "What is a blockchain?"    },    {      "role": "assistant",      "content": "A blockchain is a distributed, immutable ledger that records transactions across many computers. It's the underlying technology behind cryptocurrencies like Bitcoin, but its applications extend far beyond digital currencies. Each 'block' contains a set of transactions, and once verified, it's linked to the previous block, forming a chain. This design makes the data extremely difficult to alter retroactively, providing security and transparency."    },    {      "role": "user",      "content": "How does this relate to Web3?"    }  ],  "temperature": 0.7,  "max_tokens": 500})

Streaming Responses[â€‹](#streaming-responses "Direct link to Streaming Responses")
---------------------------------------------------------------------------------

For more interactive applications, you can stream the response as it's being generated:

    import requestsimport jsonurl = "https://api.asi1.ai/v1/chat/completions"payload = json.dumps({  "model": "asi1-mini",  "messages": [    {      "role": "user",      "content": "Explain the concept of decentralized AI"    }  ],  "temperature": 0.7,  "stream": True,  "max_tokens": 500})headers = {  'Content-Type': 'application/json',  'Accept': 'application/json',  'Authorization': 'Bearer YOUR_API_KEY'}response = requests.request("POST", url, headers=headers, data=payload, stream=True)for line in response.iter_lines():    if line:        line_text = line.decode('utf-8')        if line_text.startswith('data: '):            data_str = line_text[6:]  # Remove 'data: ' prefix            if data_str == '[DONE]':                break            try:                data = json.loads(data_str)                if 'choices' in data and len(data['choices']) > 0:                    delta = data['choices'][0].get('delta', {})                    if 'content' in delta:                        print(delta['content'], end='', flush=True)            except json.JSONDecodeError:                pass

When streaming is enabled, you'll receive a series of server-sent events (SSE), each containing a small piece of the response. The stream is terminated with a `data: [DONE]` message.

Accessing Model Thoughts[â€‹](#accessing-model-thoughts "Direct link to Accessing Model Thoughts")
------------------------------------------------------------------------------------------------

A unique feature of ASI:One is the ability to access the model's "thoughts" during generation when streaming is enabled. These thoughts provide insight into the model's reasoning process:

    for line in response.iter_lines():    if line:        line_text = line.decode('utf-8')        if line_text.startswith('data: '):            data_str = line_text[6:]  # Remove 'data: ' prefix            if data_str == '[DONE]':                break            try:                data = json.loads(data_str)                if 'thought' in data:                    print(f"Thought: {data['thought']}")                elif 'choices' in data and len(data['choices']) > 0:                    delta = data['choices'][0].get('delta', {})                    if 'content' in delta:                        print(f"Content: {delta['content']}", end='', flush=True)            except json.JSONDecodeError:                pass

Best Practices[â€‹](#best-practices "Direct link to Best Practices")
------------------------------------------------------------------

### System Messages[â€‹](#system-messages "Direct link to System Messages")

Use system messages to guide the model's behavior. For example:

    {  "role": "system",  "content": "You are an AI assistant specialized in blockchain technology. Provide concise, accurate information and use examples where appropriate."}

### Managing Context Length[â€‹](#managing-context-length "Direct link to Managing Context Length")

ASI:One has a limited context window. To manage long conversations:

1.  Summarize previous turns when necessary
2.  Remove less relevant messages from the history
3.  Focus on the most recent and relevant context

### Controlling Response Style[â€‹](#controlling-response-style "Direct link to Controlling Response Style")

Use the temperature parameter to control the creativity of responses:

*   Lower temperature (0.2-0.5): More deterministic, factual responses
*   Medium temperature (0.5-0.8): Balanced creativity and coherence
*   Higher temperature (0.8-1.0): More creative, diverse responses

### Error Handling[â€‹](#error-handling "Direct link to Error Handling")

Implement robust error handling in your application:

    try:    response = requests.request("POST", url, headers=headers, data=payload)    response.raise_for_status()  # Raise an exception for 4XX/5XX responses    result = response.json()    # Process the resultexcept requests.exceptions.HTTPError as http_err:    print(f"HTTP error occurred: {http_err}")    # Handle specific status codes if neededexcept requests.exceptions.ConnectionError as conn_err:    print(f"Connection error occurred: {conn_err}")except requests.exceptions.Timeout as timeout_err:    print(f"Timeout error occurred: {timeout_err}")except requests.exceptions.RequestException as req_err:    print(f"An error occurred: {req_err}")except json.JSONDecodeError as json_err:    print(f"JSON decode error: {json_err}")

Advanced Use Cases[â€‹](#advanced-use-cases "Direct link to Advanced Use Cases")
------------------------------------------------------------------------------

ASI:One's agentic capabilities make it particularly well-suited for:

1.  **Multi-step reasoning tasks**: Problems that require breaking down into steps
2.  **Autonomous agents**: Creating AI assistants that can plan and execute tasks
3.  **Web3 integrations**: Interacting with blockchain data and smart contracts
4.  **Context-aware applications**: Systems that need to maintain state and adapt to changing information

Conclusion[â€‹](#conclusion "Direct link to Conclusion")
------------------------------------------------------

The Chat Completion API provides a powerful interface to ASI-1 Mini's capabilities. By understanding how to structure your requests and leverage the model's agentic reasoning, you can build sophisticated applications that go beyond simple text generation.

For more information, refer to the [API Reference](https://innovationlab.fetch.ai/resources/docs/next/asione/asi1-mini-api-reference) documentation.</content>
</page>

<page>
  <title>Function Calling with ASI-1 Mini | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/asione/asi1-mini-function-calling</url>
  <content>Function calling in ASI1 allows models to go beyond text generation by invoking external functions with the right parameters. This enables integration with APIs, tools, or your own code to retrieve live data, perform tasks, or trigger actions based on user input.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

Function calling enables you to integrate your custom code with the Chat Completion API in ASI1. When given access to defined tools, the model can choose to call them based on the conversation context. After the function is called, you execute the corresponding code, return the results, and the model incorporates the output into its final reply. This guide provides instructions for connecting ASI1 models to your custom functions to retrieve data or perform specific actions.

ASI:One offers three model variants to suit different needs:

*   **asi1-mini**: The standard model with balanced performance and speed
*   **asi1-extended**: Enhanced capabilities for more complex tasks
*   **asi1-fast**: Optimized for quicker response times
*   **asi1-graph**: Optimized for data analytics and creation of graphs

Here's a basic example of how function calling works with ASI1:

*   Python
*   cURL

    import requestsimport json# ASI1 API settingsAPI_KEY = "your_api_key"BASE_URL = "https://api.asi1.ai/v1"headers = {    "Authorization": f"Bearer {API_KEY}",    "Content-Type": "application/json"}# Define the get_weather toolget_weather_tool = {    "type": "function",    "function": {        "name": "get_weather",        "description": "Get current temperature for a given location (latitude and longitude).",        "parameters": {            "type": "object",            "properties": {                "latitude": {"type": "number"},                "longitude": {"type": "number"}            },            "required": ["latitude", "longitude"]        }    }}# Initial message setupinitial_message = [    {        "role": "system",        "content": "You are a weather assistant. When a user asks for the weather in a location, use the get_weather tool with the appropriate latitude and longitude for that location."    },    {        "role": "user",        "content": "What's the current weather like in New York right now?"    }]# First call to modelpayload = {    "model": "asi1-mini",    "messages": [initial_message],    "tools": [get_weather_tool],    "temperature": 0.7,    "max_tokens": 1024}response = requests.post(    f"{BASE_URL}/chat/completions",    headers=headers,    json=payload)

### Example Response[â€‹](#example-response "Direct link to Example Response")

When you make a function call request, the model will respond with a structured output that includes the function call details. Here's an example response:

    {  "id": "id_t0m4Pzm0KDbMg0WaX",  "model": "asi1-mini",  "executable_data": [],  "conversation_id": null,  "thought": [],  "choices": [    {      "index": 0,      "finish_reason": "tool_calls",      "message": {        "role": "assistant",        "content": "",        "reasoning": null,        "tool_calls": [          {            "id": "call_WzB5g",            "index": 0,            "type": "function",            "function": {              "name": "get_weather",              "arguments": "{\"latitude\":40.7128,\"longitude\":-74.006}"            }          }        ]      }    }  ],  "usage": {    "prompt_tokens": 36,    "completion_tokens": 8,    "total_tokens": 44  }}

### Sample function[â€‹](#sample-function "Direct link to Sample function")

Let's look at the steps to allow a model to use a real `get_weather` function defined below:

Sample get\_weather function implemented in your codebase

    import requestsdef get_weather(latitude, longitude):    response = requests.get(f"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m")    data = response.json()    return data['current']['temperature_2m']

    async function getWeather(latitude, longitude) {    const response = await fetch(`https://api.open-meteo.com/v1/forecast?latitude=${latitude}&longitude=${longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m`);    const data = await response.json();    return data.current.temperature_2m;}

Let's walk through the complete cycle of executing a function call with ASI1, using the weather example:

##### Step 1: Initial Request with Tools[â€‹](#step-1-initial-request-with-tools "Direct link to Step 1: Initial Request with Tools")

First, we make a request to the model with the function definition and user message:

    # Define the get_weather toolget_weather_tool = {    "type": "function",    "function": {        "name": "get_weather",        "description": "Get current temperature for a given location (latitude and longitude).",        "parameters": {            "type": "object",            "properties": {                "latitude": {"type": "number"},                "longitude": {"type": "number"}            },            "required": ["latitude", "longitude"]        }    }}# User's questioninitial_message = {    "role": "user",    "content": "What's the current weather like in London right now?"}# First call to modelpayload = {    "model": "asi1-mini",    "messages": [initial_message],    "tools": [get_weather_tool],    "temperature": 0.7,    "max_tokens": 1024}first_response = requests.post(    f"{BASE_URL}/chat/completions",    headers=headers,    json=payload)

##### Step 2: Parse Tool Calls from Response[â€‹](#step-2-parse-tool-calls-from-response "Direct link to Step 2: Parse Tool Calls from Response")

The model responds with a function call that we need to parse:

    first_response.raise_for_status()first_response_json = first_response.json()tool_calls = first_response_json["choices"][0]["message"].get("tool_calls", [])messages_history = [    initial_message,    first_response_json["choices"][0]["message"]]

##### Step 3: Execute Tools and Format Results[â€‹](#step-3-execute-tools-and-format-results "Direct link to Step 3: Execute Tools and Format Results")

Next, we execute the function and format the results:

    # Simulate execution of get_weather tooldef get_weather(lat, lon):    response = requests.get(        f"https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={lon}&current=temperature_2m,wind_speed_10m"    )    data = response.json()    return data['current']['temperature_2m']# Process tool callfor tool_call in tool_calls:    function_name = tool_call["function"]["name"]    arguments = json.loads(tool_call["function"]["arguments"])    if function_name == "get_weather":        latitude = arguments["latitude"]        longitude = arguments["longitude"]        temperature = get_weather(latitude, longitude)        result = {            "temperature_celsius": temperature,            "location": f"lat: {latitude}, lon: {longitude}"        }    else:        result = {"error": f"Unknown tool: {function_name}"}    # Tool result message    tool_result_message = {        "role": "tool",        "tool_call_id": tool_call["id"],        "content": json.dumps(result)    }    messages_history.append(tool_result_message)

##### Step 4: Send Results Back to Model[â€‹](#step-4-send-results-back-to-model "Direct link to Step 4: Send Results Back to Model")

Now we send the function results back to the model:

    # Final call to model with tool resultsfinal_payload = {    "model": "asi1-mini",    "messages": messages_history,    "temperature": 0.7,    "max_tokens": 1024}final_response = requests.post(    f"{BASE_URL}/chat/completions",    headers=headers,    json=final_payload)

##### Step 5: Receive Final Answer[â€‹](#step-5-receive-final-answer "Direct link to Step 5: Receive Final Answer")

Finally, we get the model's response incorporating the function results:

    final_response.raise_for_status()final_response_json = final_response.json()# Final resultprint(final_response_json["choices"][0]["message"]["content"])

### Tool Result Handling in ASI1[â€‹](#tool-result-handling-in-asi1 "Direct link to Tool Result Handling in ASI1")

Here are key guidelines to ensure correct behavior and prevent common errors.

* * *

**Preserving Tool Call IDs**

Each tool call comes with a unique `id` that **must** be preserved when sending results back.

    # Correcttool_result_message = {  "role": "tool",  "tool_call_id": tool_call.id,  # Use the exact ID from the tool call  "content": json.dumps(result)}# Incorrect - Don't make up IDstool_result_message = {  "role": "tool",  "tool_call_id": "my_custom_id",  # This will cause an error  "content": json.dumps(result)}

**Message History Order**

The message history **must** maintain this exact order:

*   Original user message
*   Assistant message with `tool_calls` (content should be null or empty)
*   Tool result messages (one for each tool\_call, identified by `tool_call_id`)

**Content Formatting**

Tool results **must** be JSON-stringified within the `content` field.

    # Correct"content": json.dumps({"key": "value"})# Incorrect - Don't send raw objects"content": {"key": "value"}  # This will cause an error

**Error Handling**

If a tool fails, send back a result message indicating the error.

    try:  # Execute tool  result = execute_tool(function_name, arguments)  content_to_send = json.dumps(result)except Exception as e:  # Send error as tool result content  error_content = {      "error": f"Tool execution failed: {str(e)}",      "status": "failed"  }  content_to_send = json.dumps(error_content)tool_result_message = {  "role": "tool",  "tool_call_id": tool_call.id, # Still use the original tool_call.id  "content": content_to_send}messages_history.append(tool_result_message)

Function Definition[â€‹](#function-definition "Direct link to Function Definition")
---------------------------------------------------------------------------------

Functions are specified using the tools parameter in each API request, where each tool is described as a function object.

Each function is defined using a schema that tells the model what the function does and what input arguments it requires. The schema includes the following key fields:

*   **`name`** (string) : A unique, descriptive identifier for the function (e.g., `get_weather_forecast`, `send_email`). Use underscores or camelCase formatting. Avoid spaces or special characters.
    
*   **`description`** (string) : A detailed explanation of what the function does and when it should be used. Clear, specific descriptions improve the model's ability to use the function correctly.
    
*   **`parameters`** (object) : Defines the input parameters the function expects.
    
    *   **`type`** (string) : Usually set to `"object"` to represent structured input.
        
    *   **`properties`** (object) : Lists each input parameter and its details:
        
        *   **`type`** (string): The data type (e.g., `string`, `integer`, `boolean`, `array`).
        *   **`description`** (string): A clear explanation of the parameter's purpose and expected format.  
            _Example:_ `"City and country, e.g., 'Paris, France'"`
        *   **`enum`** _(optional)_: An array of allowed values, useful when inputs must be restricted.  
            _Example:_ `"enum": ["celsius", "fahrenheit"]`
    *   **`required`** (array of strings) : Lists the parameter names that must be included when calling the function.
        

Example Function Schema

    {    "type": "function",    "function": {        "name": "get_weather",        "description": "Retrieves current weather for the given location.",        "parameters": {            "type": "object",            "properties": {                "location": {                    "type": "string",                    "description": "City and country e.g. BogotÃ¡, Colombia"                },                "units": {                    "type": "string",                    "enum": [                        "celsius",                        "fahrenheit"                    ],                    "description": "Units the temperature will be returned in."                }            },            "required": [                "location",                "units"            ],            "additionalProperties": false        },        "strict": true    }}

Additional Configurations[â€‹](#additional-configurations "Direct link to Additional Configurations")
---------------------------------------------------------------------------------------------------

ASI1 provides several options to control how and when tools are called, as well as how strictly the model adheres to your function schemas.

### Tool Choice[â€‹](#tool-choice "Direct link to Tool Choice")

By default, the model determines when and how many tools to use. You can control this behavior with the `tool_choice` parameter:

*   **Auto (default):** The model may call zero, one, or multiple functions.
*   **Required:** The model must call at least one function.
    
        "tool_choice": "required"
    
*   **Forced Function:** Force the model to call a specific function.
    
        "tool_choice": {  "type": "function",  "function": { "name": "get_weather" }}
    
*   **None:** Prevent the model from calling any functions.

### Parallel Function Calling[â€‹](#parallel-function-calling "Direct link to Parallel Function Calling")

By default, the model may call multiple functions in a single turn. To restrict this and ensure only one (or zero) tool is called per turn, set:

    "parallel_tool_calls": false

> **Note:** If parallel tool calls are enabled, strict mode may be disabled for those calls.

### Strict Mode[â€‹](#strict-mode "Direct link to Strict Mode")

Setting `strict` to `true` ensures the model strictly follows your function schema. This is recommended for most use cases.

**Requirements for strict mode:**

1.  `additionalProperties` must be set to `false` for each object in the `parameters`.
2.  All fields in `properties` must be listed in `required`.

**Example with strict mode enabled:**

    {  "type": "function",  "function": {    "name": "get_weather",    "description": "Retrieves current weather for the given location.",    "strict": true,    "parameters": {      "type": "object",      "properties": {        "location": {          "type": "string",          "description": "City and country e.g. BogotÃ¡, Colombia"        },        "units": {          "type": ["string", "null"],          "enum": ["celsius", "fahrenheit"],          "description": "Units the temperature will be returned in."        }      },      "required": ["location", "units"],      "additionalProperties": false    }  }}

**Example with strict mode disabled:**

    {  "type": "function",  "function": {    "name": "get_weather",    "description": "Retrieves current weather for the given location.",    "parameters": {      "type": "object",      "properties": {        "location": {          "type": "string",          "description": "City and country e.g. BogotÃ¡, Colombia"        },        "units": {          "type": "string",          "enum": ["celsius", "fahrenheit"],          "description": "Units the temperature will be returned in."        }      },      "required": ["location"]    }  }}

> **Tip:** We recommend enabling strict mode for reliable function calling.</content>
</page>

<page>
  <title>Multi-Server MCP Langgraph Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/examples/mcp-integration/multi-server-agent-example</url>
  <content>This guide demonstrates two approaches for building LangGraph agents that connect to multiple MCP servers, then wrap them as uAgents and register them on [Agentverse](https://innovationlab.fetch.ai/resources/docs/agentverse) for discovery and use by ASI:One LLM.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

Both examples in this guide:

*   Connect to multiple MCP servers (math and weather) using `langchain_mcp_adapters.MultiServerMCPClient`
*   Support multiple transport methods (stdio and SSE) for different MCP servers
*   Wrap the LangGraph agent using `uagents_adapter` to become a uAgent
*   Register the uAgent on Agentverse, making it discoverable and callable by ASI:One

The key difference is in the agent architecture:

*   **Basic Multi-Server Agent**: Uses a simple LangGraph agent with ReAct framework
*   **Advanced State Graph Agent**: Uses a LangGraph state graph for more complex workflows and state management

Transport Methods[â€‹](#transport-methods "Direct link to Transport Methods")
---------------------------------------------------------------------------

In both examples, we use two different transport methods for the MCP servers:

1.  **stdio Transport** (Math Server):
    
    *   Used for local MCP servers that run as subprocesses
    *   Communication happens through standard input/output
    *   Good for local development and testing
    *   Example: `mcp.run(transport="stdio")`
2.  **SSE Transport** (Weather Server):
    
    *   Server-Sent Events (SSE) for real-time communication
    *   Used for remote or web-based MCP servers
    *   Supports long-lived connections
    *   Example: `mcp.run(transport="sse", port=8000)`

The `MultiServerMCPClient` handles both transport types seamlessly, allowing you to mix and match different transport methods based on your needs.

Common Server Setup[â€‹](#common-server-setup "Direct link to Common Server Setup")
---------------------------------------------------------------------------------

Both examples use the same MCP servers. Let's set those up first:

### 1\. Create the Math MCP Server[â€‹](#1-create-the-math-mcp-server "Direct link to 1. Create the Math MCP Server")

math\_server.py

    from mcp.server.fastmcp import FastMCPmcp = FastMCP("Math")@mcp.tool()def add(a: int, b: int) -> int:    """Add two numbers"""    return a + b@mcp.tool()def multiply(a: int, b: int) -> int:    """Multiply two numbers"""    return a * bif __name__ == "__main__":    mcp.run(transport="stdio")

### 2\. Create the Weather MCP Server[â€‹](#2-create-the-weather-mcp-server "Direct link to 2. Create the Weather MCP Server")

weather\_server.py

    from mcp.server.fastmcp import FastMCPmcp = FastMCP("Weather")@mcp.tool()def get_weather(city: str) -> str:    """Get the current weather for a city"""    # This is a mock implementation    return f"The weather in {city} is sunny and 25Â°C"if __name__ == "__main__":    mcp.run(transport="sse", port=8000)

Approach 1: Basic Multi-Server Agent[â€‹](#approach-1-basic-multi-server-agent "Direct link to Approach 1: Basic Multi-Server Agent")
-----------------------------------------------------------------------------------------------------------------------------------

This approach uses LangGraph's `create_react_agent` to create a simple agent that can access tools from multiple MCP servers.

### Create and Register the Basic Multi-Server Agent[â€‹](#create-and-register-the-basic-multi-server-agent "Direct link to Create and Register the Basic Multi-Server Agent")

basic\_agent.py

    import osimport timefrom dotenv import load_dotenvfrom langchain_openai import ChatOpenAIfrom langchain_core.messages import HumanMessagefrom langchain_mcp_adapters.client import MultiServerMCPClientfrom langgraph.prebuilt import create_react_agentfrom uagents_adapter import LangchainRegisterTool, cleanup_uagentfrom uagents_adapter.langchain import AgentManager# Load environment variablesload_dotenv()# Set your API keysOPENAI_API_KEY = os.getenv("OPENAI_API_KEY")API_TOKEN = os.getenv("AGENTVERSE_API_KEY")# Initialize the modelmodel = ChatOpenAI(model="gpt-4o")# Store the agent globally so it can be accessed by the wrapper functionagent = Noneasync def setup_multi_server_agent():    global agent        print("Setting up multi-server agent...")    async with MultiServerMCPClient(        {            "math": {                "command": "python",                "args": ["math_server.py"],                "transport": "stdio",            },            "weather": {                "url": "http://localhost:8000/sse",                "transport": "sse",            }        }    ) as client:        tools = client.get_tools()        agent = create_react_agent(model, tools)                # Test the agent with both services        print("Testing math capabilities...")        math_response = await agent.ainvoke({            "messages": [HumanMessage(content="what's (3 + 5) x 12?")]        })        print(f"Math test response: {math_response['messages'][-1].content}")                print("Testing weather capabilities...")        weather_response = await agent.ainvoke({            "messages": [HumanMessage(content="what's the weather in NYC?")]        })        print(f"Weather test response: {weather_response['messages'][-1].content}")                # Keep the connection alive        while True:            await asyncio.sleep(1)def main():    # Initialize agent manager    manager = AgentManager()        # Create agent wrapper    async def agent_func(x):        response = await agent.ainvoke({"messages": [HumanMessage(content=x)]})        return response["messages"][-1].content        agent_wrapper = manager.create_agent_wrapper(agent_func)        # Start the agent in background    manager.start_agent(setup_multi_server_agent)        # Register with uAgents    print("Registering multi-server agent...")    tool = LangchainRegisterTool()    agent_info = tool.invoke(        {            "agent_obj": agent_wrapper,            "name": "multi_server_agent_math_langchain_mcp",            "port": 8080,            "description": "A multi-service agent that can handle math calculations and weather queries",            "api_token": API_TOKEN,            "mailbox": True        }    )        print(f"âœ… Registered multi-server agent: {agent_info}")        try:        manager.run_forever()    except KeyboardInterrupt:        print("ğŸ›‘ Shutting down...")        cleanup_uagent("multi_server_agent")        print("âœ… Agent stopped.")if __name__ == "__main__":    import asyncio    main()

Approach 2: Advanced State Graph Agent[â€‹](#approach-2-advanced-state-graph-agent "Direct link to Approach 2: Advanced State Graph Agent")
-----------------------------------------------------------------------------------------------------------------------------------------

This approach uses LangGraph's `StateGraph` to create a more sophisticated agent with explicit state management and conditional workflow branching.

### Create and Register the Multi-Server Graph Agent[â€‹](#create-and-register-the-multi-server-graph-agent "Direct link to Create and Register the Multi-Server Graph Agent")

graph\_agent.py

    import osimport timeimport asynciofrom dotenv import load_dotenvfrom langchain_openai import ChatOpenAIfrom langchain_core.messages import HumanMessagefrom langchain_mcp_adapters.client import MultiServerMCPClientfrom langgraph.graph import StateGraph, MessagesState, STARTfrom langgraph.prebuilt import ToolNode, tools_conditionfrom uagents_adapter import LangchainRegisterTool, cleanup_uagentfrom uagents_adapter.langchain import AgentManager# Load environment variablesload_dotenv()# Set your API keysOPENAI_API_KEY = os.getenv("OPENAI_API_KEY")API_TOKEN = os.getenv("AGENTVERSE_API_KEY")# Initialize the modelmodel = ChatOpenAI(model="gpt-4o")# Store the graph globally so it can be accessed by the wrapper function_global_graph = None# Add an event to signal when the graph is readygraph_ready = asyncio.Event()async def setup_multi_server_graph_agent():    global _global_graph        print("Setting up multi-server graph agent...")    try:        # Create the client without async with        client = MultiServerMCPClient(            {                "math": {                    "command": "python",                    "args": ["./math_server.py"],                    "transport": "stdio",                },                "weather": {                    "url": "http://localhost:8000/sse",                    "transport": "sse",                }            }        )                # Get tools directly        tools = await client.get_tools()        print(f"Successfully loaded {len(tools)} tools")                # Define call_model function        def call_model(state: MessagesState):            response = model.bind_tools(tools).invoke(state["messages"])            return {"messages": response}        # Build the graph        builder = StateGraph(MessagesState)        builder.add_node(call_model)        builder.add_node(ToolNode(tools))        builder.add_edge(START, "call_model")        builder.add_conditional_edges(            "call_model",            tools_condition,        )        builder.add_edge("tools", "call_model")        _global_graph = builder.compile()        print("Graph successfully compiled")                # Test the graph        try:            print("Testing math capabilities...")            math_response = await _global_graph.ainvoke({"messages": "what's (3 + 5) x 12?"})            print(f"Math test response: {math_response['messages'][-1].content}")                        print("Testing weather capabilities...")            weather_response = await _global_graph.ainvoke({"messages": "what's the weather in NYC?"})            print(f"Weather test response: {weather_response['messages'][-1].content}")        except Exception as e:            print(f"Error during testing: {e}")                # Signal that the graph is ready        graph_ready.set()                # Keep the connection alive        while True:            await asyncio.sleep(1)    except Exception as e:        print(f"Error setting up graph: {e}")        # Set the event even in case of error to avoid deadlock        graph_ready.set()def main():    print("Initializing agent...")    # Initialize agent manager    manager = AgentManager()        # Create graph wrapper with proper error handling    async def graph_func(x):        # Wait for the graph to be ready before trying to use it        await graph_ready.wait()                if _global_graph is None:            error_msg = "Error: Graph not initialized properly. Please try again later."            print(f"Response: {error_msg}")            return error_msg                try:            # Print the incoming message            print(f"\nReceived query: {x}")                        # Process the message            if isinstance(x, str):                response = await _global_graph.ainvoke({"messages": x})            else:                response = await _global_graph.ainvoke({"messages": x})                        # Extract and print the response            result = response["messages"][-1].content            print(f"\nâœ… Response: {result}\n")            return result        except Exception as e:            error_msg = f"Error processing request: {str(e)}"            print(f"\nâŒ {error_msg}\n")            return error_msg        agent_wrapper = manager.create_agent_wrapper(graph_func)        # Start the graph in background    manager.start_agent(setup_multi_server_graph_agent)        # Register with uAgents    print("Registering multi-server graph agent...")    tool = LangchainRegisterTool()    try:        agent_info = tool.invoke(            {                "agent_obj": agent_wrapper,                "name": "multi_server_graph_agent_math_langchain_mcp",                "port": 8080,                "description": "A multi-service graph agent that can handle math calculations and weather queries",                "api_token": API_TOKEN,                "mailbox": True            }        )        print(f"âœ… Registered multi-server graph agent: {agent_info}")    except Exception as e:        print(f"âš ï¸ Error registering agent: {e}")        print("Continuing with local agent only...")        try:        manager.run_forever()    except KeyboardInterrupt:        print("ğŸ›‘ Shutting down...")        cleanup_uagent("multi_server_graph_agent")        print("âœ… Agent stopped.")if __name__ == "__main__":    import asyncio    main()

Key Differences Between the Two Approaches[â€‹](#key-differences-between-the-two-approaches "Direct link to Key Differences Between the Two Approaches")
------------------------------------------------------------------------------------------------------------------------------------------------------

1.  **Architecture**:
    
    *   **Basic Agent**: Uses LangGraph's `create_react_agent` for a simple ReAct-style agent.
    *   **Graph Agent**: Uses LangGraph's `StateGraph` for explicit state management and workflow control.
2.  **Control Flow**:
    
    *   **Basic Agent**: The control flow is managed internally by the ReAct framework.
    *   **Graph Agent**: The control flow is explicitly defined with nodes, edges, and conditional branching.
3.  **State Management**:
    
    *   **Basic Agent**: State is managed implicitly within the ReAct agent.
    *   **Graph Agent**: State is explicitly managed and can be more easily inspected and modified.
4.  **Extensibility**:
    
    *   **Basic Agent**: Simpler to set up but less flexible for complex workflows.
    *   **Graph Agent**: More complex setup but offers greater flexibility for sophisticated agent behaviors.

Getting Started[â€‹](#getting-started "Direct link to Getting Started")
---------------------------------------------------------------------

1.  **Get your Agentverse API Key**:
    
    *   Follow the [Agentverse API Key guide](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key) to obtain your API key
    *   Make sure to save your API key securely as it cannot be regenerated
2.  **Set up environment variables** in a `.env` file:
    
        OPENAI_API_KEY=your_openai_api_keyAGENTVERSE_API_KEY=your_agentverse_api_key
    
3.  **Install dependencies**:
    
        pip install langchain-openai mcp langchain-mcp-adapters uagents-adapter python-dotenv
    
4.  **Create the files**:
    
    *   Save the math server code as `math_server.py`
    *   Save the weather server code as `weather_server.py`
    *   Save the basic agent code as `basic_agent.py`
    *   Save the graph agent code as `graph_agent.py`
5.  **Start the servers and agents**:
    
        # Terminal 1: Start the weather serverpython weather_server.py# Terminal 2: Start the basic agentpython basic_agent.py# OR to run the graph agent insteadpython graph_agent.py
    
6.  **Test your agent** by querying it from Agentverse chat UI.
    

*   Open your Agentverse account and goto [local agents](https://agentverse.ai/agents/local).

*   Click on your agent and Use the "Chat with Agent" button on your Agentverse agent page

When to Use Each Approach[â€‹](#when-to-use-each-approach "Direct link to When to Use Each Approach")
---------------------------------------------------------------------------------------------------

*   **Use the Basic Agent** when:
    
    *   You need a simple agent that can access multiple tools
    *   You want a quick setup with minimal boilerplate
    *   The agent's decision-making process is relatively straightforward
*   **Use the Graph Agent** when:
    
    *   You need more control over the agent's workflow
    *   You want explicit state management
    *   You need complex conditional branching in your agent's behavior
    *   You're building an agent with multiple specialized steps or phases

note

**Note:** These examples demonstrate how to connect to multiple MCP servers using different transport methods and agent architectures. You can extend these patterns to include any number of MCP servers with different capabilities and create more sophisticated agent behaviors.</content>
</page>

<page>
  <title>Creating a MCP Server on Agentverse | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/examples/mcp-integration/mcp-adapter-example</url>
  <content>This example demonstrates how to deploy a **Model Control Protocol (MCP) Server** on [Agentverse](https://agentverse.ai/). The MCP Server Adapter allows MCP servers to be easily discoverable by other agents on Agentverse and [ASI:One](https://asi1.ai/).

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

The MCP Server Adapter makes it easy to bring your tools into the Agentverse ecosystem by:

*   Wrapping MCP servers as uAgents for seamless, decentralized communication
*   Exposing MCP tools to other agents on Agentverse for easy discovery and reuse
*   Enabling the Chat Protocol, allowing natural language conversations with the MCP Server directly or through ASI:One

Example Implementation[â€‹](#example-implementation "Direct link to Example Implementation")
------------------------------------------------------------------------------------------

In this example, the MCP Server provides weather-related tools, including:

*   `get_alerts`: Returns weather alerts for a given US state.
    
*   `get_forecast`: Returns a weather forecast for a specific latitude and longitude. You can define your own tools by following this pattern, making it easy to bring any Python-based service into the Agentverse network.
    
*   Create a FastMCP Server that implements the MCP Server logic.
    
*   Create an Agent that uses the `MCPServerAdapter` from the `uagents-adapter` package to wrap the MCP Server as a uAgent.
    

To get started,

1.  Navigate to [Agentverse](https://agentverse.ai/) â†’ Agents tab â†’ + New Agent.
2.  Choose Blank Agent
3.  Provide a name for your new Agent and click on Create.

Refer to the [Hosted Agents](https://innovationlab.fetch.ai/resources/docs/agent-creation/uagent-creation#hosted-agents) section to understand the detailed steps for agent creation on Agentverse.

### Step 1: Create a FastMCP Server[â€‹](#step-1-create-a-fastmcp-server "Direct link to Step 1: Create a FastMCP Server")

Create a `server.py` file implementing your MCP server.

1.  Click on New File.

2.  Rename the file to `server.py`

3.  Directory Structure

3.  Copy the following MCP Server Implementation and paste in your `server.py` file on Agentverse.

    from typing import Anyimport httpxfrom mcp.server.fastmcp import FastMCP# Create a FastMCP server instancemcp = FastMCP("weather")NWS_API_BASE = "https://api.weather.gov"USER_AGENT = "weather-app/1.0"async def make_nws_request(url: str) -> dict[str, Any] | None:    headers = {        "User-Agent": USER_AGENT,        "Accept": "application/geo+json"    }    async with httpx.AsyncClient() as client:        try:            response = await client.get(url, headers=headers, timeout=30.0)            response.raise_for_status()            return response.json()        except Exception:            return Nonedef format_alert(feature: dict) -> str:    props = feature["properties"]    return f""" Event: {props.get('event', 'Unknown')} Area: {props.get('areaDesc', 'Unknown')} Severity: {props.get('severity', 'Unknown')} Description: {props.get('description', 'No description available')} Instructions: {props.get('instruction', 'No specific instructions provided')}"""@mcp.tool()async def get_alerts(state: str) -> str:    """Get weather alerts for a US state."""    url = f"{NWS_API_BASE}/alerts/active/area/{state}"    data = await make_nws_request(url)    if not data or "features" not in data:        return "Unable to fetch alerts or no alerts found."    if not data["features"]:        return "No active alerts for this state."    alerts = [format_alert(feature) for feature in data["features"]]    return "\n---\n".join(alerts)@mcp.tool()async def get_forecast(latitude: float, longitude: float) -> str:    """Get weather forecast for a location."""        points_url = f"{NWS_API_BASE}/points/{latitude},{longitude}"    points_data = await make_nws_request(points_url)    if not points_data:        return "Unable to fetch forecast data for this location."    forecast_url = points_data["properties"]["forecast"]    forecast_data = await make_nws_request(forecast_url)    if not forecast_data:        return "Unable to fetch detailed forecast."    periods = forecast_data["properties"]["periods"]    forecasts = []    for period in periods[:5]:        forecast = f"""{period['name']}: Temperature: {period['temperature']}Â°{period['temperatureUnit']} Wind: {period['windSpeed']} {period['windDirection']} Forecast: {period['detailedForecast']}"""        forecasts.append(forecast)    return "\n---\n".join(forecasts)if __name__ == "__main__":    # Initialize and run the server    mcp.run(transport='stdio')

note

**Important:** When creating MCP tools, always include detailed docstrings using triple quotes (""") to describe what each tool in the MCP Server as these descriptions play a critical role in selecting the right MCP Tool based on the user's query.

### Step 2: Create an Agent for Your FastMCP Server[â€‹](#step-2-create-an-agent-for-your-fastmcp-server "Direct link to Step 2: Create an Agent for Your FastMCP Server")

To create an Agent for your Fast MCP Server, we will import the `MCPServerAdapter` from the `uagents-adapter` package. We will also import the MCP server instance `mcp` from `server.py`.

    from uagents_adapter import MCPServerAdapterfrom server import mcp  # This is your FastMCP server instance from server.py

To enable intelligent tool selection, the adapter leverages the ASI:One LLM. You'll need an ASI:One API key, which you can obtain by logging into [ASI:One](https://asi1.ai/) and navigating to the "API Keys" tab.

**Instantiate the MCPServerAdapter:**

    mcp_adapter = MCPServerAdapter(    mcp_server=mcp,                # (FastMCP) Your MCP server instance    asi1_api_key="your-asi1-api-key",  # (str) Your ASI:One API key    model="asi1-mini"              # (str) Model to use: "asi1-mini", "asi1-extended", or "asi1-fast")

#### MCPServerAdapter Parameters[â€‹](#mcpserveradapter-parameters "Direct link to MCPServerAdapter Parameters")

| Parameter | Type | Description | Required |
| --- | --- | --- | --- |
| `mcp_server` | FastMCP | The FastMCP server instance exposing your tools. | Yes |
| `asi1_api_key` | str | Your ASI:One API key for LLM-powered tool selection. | Yes |
| `model` | str | The ASI:One model to use (`"asi1-mini"`, `"asi1-extended"`, or `"asi1-fast"`). | Yes |

**Whole Script:**

    from uagents_adapter import MCPServerAdapterfrom server import mcp# Create an MCP adapter with your MCP servermcp_adapter = MCPServerAdapter(    mcp_server=mcp,                     # (FastMCP) Your MCP server instance    asi1_api_key="your-asi1-api-key",  # (str) Your ASI:One API key    model="asi1-mini"              # (str) Model to use: "asi1-mini", "asi1-extended", or "asi1-fast")# Create a uAgentagent = Agent()# Include protocols from the adapterfor protocol in mcp_adapter.protocols:    agent.include(protocol, publish_manifest=True)if __name__ == "__main__":    # Run the MCP adapter with the agent    mcp_adapter.run(agent)

note

The MCPServerAdapter only supports FastMCP Servers at the moment.

This setup ensures your agent can intelligently select and execute the right tool from the MCP Server based on the user queries.

#### Adding a README to your Agent[â€‹](#adding-a-readme-to-your-agent "Direct link to Adding a README to your Agent")

1.  Go to the Overview section in the Editor.
    
2.  Click on Edit and add a good description for your Agent so that it can be easily searchable by the ASI1 LLM. Please refer the [Importance of Good Readme](https://innovationlab.fetch.ai/resources/docs/agentverse/searching#importance-of-good-readme) section for more details.
    

### Step 3: Test Your Agent[â€‹](#step-3-test-your-agent "Direct link to Step 3: Test Your Agent")

1.  Start your Agent.
    
2.  Switch to the Overview Tab and use the "Chat with Agent" button to start talking to your agent.
    

#### Query your Agent from ASI1 LLM[â€‹](#query-your-agent-from-asi1-llm "Direct link to Query your Agent from ASI1 LLM")

1.  Login to the [ASI1 LLM](https://asi1.ai/), either using your Google Account or the ASI1 Wallet and Start a New Chat.
    
2.  Toggle the "Agents" switch to enable ASI1 to connect with Agents on Agentverse.
    

3.  To query your specific agent, you can copy the agent's address and mention in your query to explicitly connect with your agent. For instance, "Please ask the agent1qgggh8wy6ux2xwkc267cfpxk390c4ve0ts23yz5d9l6qsnckyvs2zpx08gq for weather alerts San Diego"

You can click on the Agent URL to check the agent that answered your question.

note

**Note:** If you ask about the weather without mentioning the address of your specific agent, ASI:One LLM might select another agent as it uses the Agent Ranking mechanism to select the most appropriate agent for each query. To test your agent directly, use the "Chat with Agent" button on your Agentverse agent page.</content>
</page>

<page>
  <title>Connect an Agent to Multiple Remote MCP Servers | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/examples/mcp-integration/connect-an-agent-to-multiple-remote-mcp-servers</url>
  <content>This example demonstrates how to build a uAgent client that connects to multiple remote MCP servers hosted on [Smithery.ai](https://smithery.ai/) (such as PubMed, clinical trials, medical calculators, and web search), handles user queries using Claude for intelligent tool selection, and registers itself on [Agentverse](https://innovationlab.fetch.ai/resources/docs/agentverse) for discovery and use by ASI:One LLM.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

*   **uAgent client** connects to multiple remote MCP servers via HTTP using [Smithery.ai](https://smithery.ai/)'s platform
*   Uses Claude to intelligently select and call the appropriate tools based on user queries
*   Formats responses using Claude for better readability and user experience
*   The agent is registered on [Agentverse](https://innovationlab.fetch.ai/resources/docs/agentverse), making it discoverable and callable by ASI:One LLM
*   Supports multiple specialized MCP servers for different domains (medical research, web search, etc.)

MCP Servers Used[â€‹](#mcp-servers-used "Direct link to MCP Servers Used")
------------------------------------------------------------------------

This example connects to several MCP servers hosted on Smithery.ai:

1.  **Medical Calculators** (`@vitaldb/medcalc`):
    
    *   BMI calculation
    *   HOMA-IR (insulin resistance)
    *   Other medical formulas
2.  **Clinical Trials** (`@JackKuo666/clinicaltrials-mcp-server`):
    
    *   Search clinical trial databases
    *   Get trial details and status
3.  **PubMed** (`@JackKuo666/pubmed-mcp-server`):
    
    *   Search biomedical literature
    *   Get article metadata
4.  **Paper Search** (`@openags/paper-search-mcp`):
    
    *   Search scientific research metadata
    *   Get paper details
5.  **DuckDuckGo** (`@nickclyde/duckduckgo-mcp-server`):
    
    *   Perform DuckDuckGo web searches
    *   Get real-time web results

Example: Medical Research Agent[â€‹](#example-medical-research-agent "Direct link to Example: Medical Research Agent")
--------------------------------------------------------------------------------------------------------------------

### Configure the uAgent Client[â€‹](#configure-the-uagent-client "Direct link to Configure the uAgent Client")

mcp\_agent.py

    from anthropic import Anthropicfrom dotenv import load_dotenvfrom uagents_core.contrib.protocols.chat import (    chat_protocol_spec,    ChatMessage,    ChatAcknowledgement,    TextContent,    StartSessionContent,)from uagents import Agent, Context, Protocolfrom uagents.setup import fund_agent_if_lowfrom datetime import datetime, timezone, timedeltafrom uuid import uuid4import mcpfrom mcp.client.streamable_http import streamablehttp_clientimport jsonimport base64import asynciofrom typing import Dict, List, Optional, Anyfrom contextlib import AsyncExitStackimport os# Load environment variablesload_dotenv()# Get API keys from environment variablesANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")if not ANTHROPIC_API_KEY:    raise ValueError("Please set the ANTHROPIC_API_KEY environment variable in your .env file")SMITHERY_API_KEY = os.getenv("SMITHERY_API_KEY")if not SMITHERY_API_KEY:    raise ValueError("Please set the SMITHERY_API_KEY environment variable in your .env file")class MedicalResearchMCPClient:    def __init__(self):        self.sessions: Dict[str, mcp.ClientSession] = {}        self.exit_stack = AsyncExitStack()        self.anthropic = Anthropic(api_key=ANTHROPIC_API_KEY)        self.all_tools = []        self.tool_server_map = {}        self.server_configs = {}        self.default_timeout = timedelta(seconds=30)    def get_server_config(self, server_path: str) -> dict:        """Get or create server configuration"""        if server_path not in self.server_configs:            config_templates = {                "@JackKuo666/pubmed-mcp-server": {},                "@openags/paper-search-mcp": {},                "@JackKuo666/clinicaltrials-mcp-server": {},                "@vitaldb/medcalc": {},            }            self.server_configs[server_path] = config_templates.get(server_path, {})        return self.server_configs[server_path]    async def connect_to_servers(self, ctx: Context):        """Connect to all MCP servers and collect their tools"""        base_config = {            "ignoreRobotsTxt": True        }        servers = [            "@JackKuo666/pubmed-mcp-server",            "@openags/paper-search-mcp",            "@JackKuo666/clinicaltrials-mcp-server",            "@vitaldb/medcalc",        ]        for server_path in servers:            try:                ctx.logger.info(f"Connecting to server: {server_path}")                server_config = self.get_server_config(server_path)                config = {**base_config, **server_config}                config_b64 = base64.b64encode(json.dumps(config).encode()).decode()                url = f"https://server.smithery.ai/{server_path}/mcp?config={config_b64}&api_key={SMITHERY_API_KEY}"                try:                    read_stream, write_stream, _ = await self.exit_stack.enter_async_context(                        streamablehttp_client(url)                    )                    session = await self.exit_stack.enter_async_context(                        mcp.ClientSession(read_stream, write_stream)                    )                    await session.initialize()                    tools_result = await session.list_tools()                    tools = tools_result.tools                    self.sessions[server_path] = session                    for tool in tools:                        tool_info = {                            "name": tool.name,                            "description": f"[{server_path}] {tool.description}",                            "input_schema": tool.inputSchema,                            "server": server_path,                            "tool_name": tool.name                        }                        self.all_tools.append(tool_info)                        self.tool_server_map[tool.name] = server_path                    ctx.logger.info(f"Successfully connected to {server_path}")                    ctx.logger.info(f"Available tools: {', '.join([t.name for t in tools])}")                except Exception as e:                    ctx.logger.error(f"Error during connection setup: {str(e)}")                    raise            except Exception as e:                ctx.logger.error(f"Error connecting to {server_path}: {str(e)}")                continue    async def process_query(self, query: str, ctx: Context) -> str:        try:            messages = [{"role": "user", "content": query}]            claude_tools = [{                "name": tool["name"],                "description": tool["description"],                "input_schema": tool["input_schema"]            } for tool in self.all_tools]            response = self.anthropic.messages.create(                model="claude-3-5-sonnet-20241022",                max_tokens=1000,                messages=messages,                tools=claude_tools            )            tool_response = None            for content in response.content:                if content.type == 'tool_use':                    tool_name = content.name                    tool_args = content.input                    server_path = self.tool_server_map.get(tool_name)                    if server_path and server_path in self.sessions:                        ctx.logger.info(f"Calling tool {tool_name} from {server_path}")                        try:                            result = await asyncio.wait_for(                                self.sessions[server_path].call_tool(tool_name, tool_args),                                timeout=self.default_timeout.total_seconds()                            )                            if isinstance(result.content, str):                                tool_response = result.content                            elif isinstance(result.content, list):                                tool_response = "\n".join([str(item) for item in result.content])                            else:                                tool_response = str(result.content)                        except asyncio.TimeoutError:                            return f"Error: The MCP server did not respond. Please try again later."                        except Exception as e:                            return f"Error calling tool {tool_name}: {str(e)}"            if tool_response:                format_prompt = f"""Please format the following response in a clear, user-friendly way. Do not add any additional information or knowledge, just format what is provided: {tool_response} Instructions: 1. If the response contains multiple records (like clinical trials), present ALL records in a clear format, do not say something like "Saved to a CSV file" or anything similar. 2. Use appropriate headings and sections 3. Maintain all the original information 4. Do not add any external knowledge or commentary 5. Do not summarize or modify the content 6. Keep the formatting simple and clean 7. If the response mentions a CSV file, do not include that information in the response. 9. For long responses, ensure all records are shown, not just a subset"""                format_response = self.anthropic.messages.create(                    model="claude-3-5-sonnet-20241022",                    max_tokens=2000,                    messages=[{"role": "user", "content": format_prompt}]                )                if format_response.content and len(format_response.content) > 0:                    return format_response.content[0].text                else:                    return tool_response            else:                return "No response received from the tool."        except Exception as e:            ctx.logger.error(f"Error processing query: {str(e)}")            return f"An error occurred while processing your query: {str(e)}"    async def cleanup(self):        await self.exit_stack.aclose()# Initialize chat protocol and agentchat_proto = Protocol(spec=chat_protocol_spec)mcp_agent = Agent(    name='MedicalResearchMCPAgent',    port=8001,    mailbox=True)client = MedicalResearchMCPClient()@chat_proto.on_message(model=ChatMessage)async def handle_chat_message(ctx: Context, sender: str, msg: ChatMessage):    try:        ack = ChatAcknowledgement(            timestamp=datetime.now(timezone.utc),            acknowledged_msg_id=msg.msg_id        )        await ctx.send(sender, ack)        if not client.sessions:            await client.connect_to_servers(ctx)        for item in msg.content:            if isinstance(item, StartSessionContent):                ctx.logger.info(f"Got a start session message from {sender}")                continue            elif isinstance(item, TextContent):                ctx.logger.info(f"Got a message from {sender}: {item.text}")                response_text = await client.process_query(item.text, ctx)                ctx.logger.info(f"Response text: {response_text}")                response = ChatMessage(                    timestamp=datetime.now(timezone.utc),                    msg_id=uuid4(),                    content=[TextContent(type="text", text=response_text)]                )                await ctx.send(sender, response)            else:                ctx.logger.info(f"Got unexpected content from {sender}")    except Exception as e:        ctx.logger.error(f"Error handling chat message: {str(e)}")        error_response = ChatMessage(            timestamp=datetime.now(timezone.utc),            msg_id=uuid4(),            content=[TextContent(type="text", text=f"An error occurred: {str(e)}")]        )        await ctx.send(sender, error_response)@chat_proto.on_message(model=ChatAcknowledgement)async def handle_chat_acknowledgement(ctx: Context, sender: str, msg: ChatAcknowledgement):    ctx.logger.info(f"Received acknowledgement from {sender} for message {msg.acknowledged_msg_id}")    if msg.metadata:        ctx.logger.info(f"Metadata: {msg.metadata}")mcp_agent.include(chat_proto)if __name__ == "__main__":    try:        mcp_agent.run()    except Exception as e:        print(f"Error running agent: {str(e)}")    finally:        asyncio.run(client.cleanup())

Script Breakdown[â€‹](#script-breakdown "Direct link to Script Breakdown")
------------------------------------------------------------------------

This section walks through the main components of the `mcp_agent.py` script to help you understand how each part contributes to building a Claude-powered uAgent that connects to multiple remote MCP servers.

### 1\. Loading Configuration and Dependencies[â€‹](#1-loading-configuration-and-dependencies "Direct link to 1. Loading Configuration and Dependencies")

    ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")SMITHERY_API_KEY = os.getenv("SMITHERY_API_KEY")

Environment variables are loaded using `dotenv` to securely manage API keys for Anthropic and [Smithery.ai](https://smithery.ai/). These credentials are required to authenticate and connect with the MCP Servers and process natural language queries.

### 2\. Creating the MCP Client[â€‹](#2-creating-the-mcp-client "Direct link to 2. Creating the MCP Client")

    class MedicalResearchMCPClient:    def __init__(self):        ...

The `MedicalResearchMCPClient` class handles all interactions with MCP servers. It manages session lifecycle, tool discovery, and query execution. The `exit_stack` allows multiple async context managers to be managed together.

### 3\. MCP Server Configuration[â€‹](#3-mcp-server-configuration "Direct link to 3. MCP Server Configuration")

    def get_server_config(self, server_path: str) -> dict:    ...

First, identify the MCP servers you want to integrate from the [Smithery.ai](https://smithery.ai/) platform. Once selected, navigate to the serverâ€™s API tab to locate its configuration schema. This schema outlines required parameters, authentication methods, and any necessary API keys for establishing a connection.

note

**Note:** For this example, we've selected MCP Servers that do not require extra authentication or server parameters.

### 4\. Connecting to Multiple Remote MCP Servers[â€‹](#4-connecting-to-multiple-remote-mcp-servers "Direct link to 4. Connecting to Multiple Remote MCP Servers")

    async def connect_to_servers(self, ctx: Context):    ...

Establishes connections to several predefined MCP servers using their Smithery-hosted URLs. The `config` is encoded and passed as a base64 parameter to the server endpoint. Each tool exposed by the server is retrieved and stored for later use.

    config_b64 = base64.b64encode(json.dumps(config).encode()).decode()url = f"https://server.smithery.ai/{server_path}/mcp?config={config_b64}&api_key={SMITHERY_API_KEY}"

MCP sessions and tool metadata are stored in local dictionaries, enabling tool invocation at runtime.

### 5\. Processing User Queries[â€‹](#5-processing-user-queries "Direct link to 5. Processing User Queries")

    async def process_query(self, query: str, ctx: Context) -> str:    ...

To process the user's query, it fetches all the tools of the MCP Servers that the client is connected to and then uses Claude to select the appropriate tool and provide input arguments for invocation.

    claude_tools = [{    "name": tool["name"],    "description": tool["description"],    "input_schema": tool["input_schema"]} for tool in self.all_tools]

Once Claude returns a tool selection, the agent locates the matching MCP session using the tool name, then calls the tool with the arguments returned by Claude. The output is parsed and passed back for formatting.

    if content.type == 'tool_use':    tool_name = content.name    tool_args = content.input    server_path = self.tool_server_map.get(tool_name)    if server_path and server_path in self.sessions:        ctx.logger.info(f"Calling tool {tool_name} from {server_path}")        try:            result = await asyncio.wait_for(                self.sessions[server_path].call_tool(tool_name, tool_args),                timeout=self.default_timeout.total_seconds()            )            if isinstance(result.content, str):                tool_response = result.content            elif isinstance(result.content, list):                tool_response = "\n".join([str(item) for item in result.content])            else:                tool_response = str(result.content)        except asyncio.TimeoutError:            return f"Error: The MCP server did not respond. Please try again later."        except Exception as e:            return f"Error calling tool {tool_name}: {str(e)}"

The tool is invoked via the relevant MCP session. If the response is successful, it's passed back to Claude with specific formatting instructions.

    format_response = self.anthropic.messages.create(...)

This second call ensures the output is easy to read while preserving all original data.

### 6\. Initializing the uAgent[â€‹](#6-initializing-the-uagent "Direct link to 6. Initializing the uAgent")

    mcp_agent = Agent(    name='MedicalResearchMCPAgent',    port=8001,    mailbox=True)

Since we are creating this Agent locally, we will enable `mailbox` to connect it to Agentverse so that it is discoverable and callable by ASI:One.

### 7\. Implementing the Chat Protocol[â€‹](#7-implementing-the-chat-protocol "Direct link to 7. Implementing the Chat Protocol")

    chat_proto = Protocol(spec=chat_protocol_spec)

The agent includes the standardized `chat_protocol_spec` to communicate with other agents and ASI:One.

    @chat_proto.on_message(model=ChatMessage)async def handle_chat_message(ctx: Context, sender: str, msg: ChatMessage):    ...

When ASI:One selects this agent to handle a user query, it sends the request as a ChatMessage to this handler. Upon receiving the message, the agent invokes the `process_query()` to connect with the remote MCP Servers to address the user's question and send back the formatted response as a `ChatMessage` to ASI:One. You can refer the [Chat Protocol documentation](https://innovationlab.fetch.ai/resources/docs/agent-communication/agent-chat-protocol) for more information.

### 8\. Cleanup on Shutdown[â€‹](#8-cleanup-on-shutdown "Direct link to 8. Cleanup on Shutdown")

    async def cleanup(self):    await self.exit_stack.aclose()

Gracefully closes all active MCP sessions to prevent resource leaks when the agent is stopped or restarted.

### 9\. Running the Agent[â€‹](#9-running-the-agent "Direct link to 9. Running the Agent")

    if __name__ == "__main__":    mcp_agent.run()

Runs the agent on the defined port. On termination, all active connections to remote MCP servers are closed via the `cleanup()` method.s

### Register the Agent on Agentverse[â€‹](#register-the-agent-on-agentverse "Direct link to Register the Agent on Agentverse")

*   Use the Agent inspector link upon agent startup to register your agent on Agentverse, making it discoverable by ASI:One LLM and other agents.
*   Add a comprehensive README.md in the Overview tab of your agent to improve discoverability.

Getting Started[â€‹](#getting-started "Direct link to Getting Started")
---------------------------------------------------------------------

1.  **Get your API Keys**:
    
    *   Follow the [Agentverse API Key guide](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key) to obtain your API key
    *   Get your [Smithery.ai API key](https://smithery.ai/account/api-keys) from their platform
    *   Get your [Anthropic API key](https://console.anthropic.com/settings/keys) from their platform
    *   Make sure to save your API keys securely
2.  **Set up environment variables** in a `.env` file:
    
        ANTHROPIC_API_KEY=your_anthropic_api_keySMITHERY_API_KEY=your_smithery_api_keyAGENTVERSE_API_KEY=your_agentverse_api_key
    
3.  **Install dependencies**:
    
        pip install uagents anthropic mcp python-dotenv
    
4.  **Create the agent file**:
    
    *   Save the code above as `mcp_agent.py`
    *   Create a `README.md` file in the same directory
    *   Update the API keys in your `.env` file
5.  **Run the agent**:
    
6.  **Test your agent**:
    
    *   Open your Agentverse account and goto [local agents](https://agentverse.ai/agents/local).
    *   Click on your agent and Use the "Chat with Agent" button on your Agentverse agent page

*   Query your agent through ASI:One LLM (make sure to enable the "Agents" switch)

note

**Note:** The ASI:One LLM uses an Agent Ranking mechanism to select the most appropriate agent for each query. To test your agent directly, use the "Chat with Agent" button on your Agentverse agent page. For more information about the Chat Protocol and ASI:One integration, check out the [Chat Protocol documentation](https://innovationlab.fetch.ai/resources/docs/agent-communication/agent-chat-protocol).</content>
</page>

<page>
  <title>Mettalex - A Practical Implementation of AI Agents in the Web3 Ecosystem | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/examples/on-chain-examples/mettalex-agents</url>
  <content>Mettalex: A Practical Implementation of AI Agents in the Web3 Ecosystem - Powered by Fetch.ai Agent Tech
--------------------------------------------------------------------------------------------------------

Introduction[â€‹](#introduction "Direct link to Introduction")
------------------------------------------------------------

[Mettalex](https://www.mettalex.ai/) stands out as the **first P2P orderbook** and **agent-based DEX** for commodity and digital (tokenized) assets trading. It harnesses **Fetch.aiâ€™s uAgents** to power **autonomous order matching**, **secure on-chain escrow**, and **cross-chain operations**. By eliminating reliance on centralized order books or liquidity pools, Mettalex aims to offer **slippage-free**, trustless trades with **maximum transparency**.

Key uAgents features in this use case include their **wallet- and chain-agnostic** capabilities. Additionally, the agents provide a robust **communication** and **execution** layer, making the trading process seamless and efficient.

Mettalex: Agent-Based Commodity Trading in a Nutshell[â€‹](#mettalex-agent-based-commodity-trading-in-a-nutshell "Direct link to Mettalex: Agent-Based Commodity Trading in a Nutshell")
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

1.  Direct P2P Order Matching
    
    *   **No Liquidity Pools:** Mettalex uses uAgents to match buyers and sellers directly, eliminating AMMs and reducing slippage.
    *   **Zero Slippage Execution:** Final prices match exactly what each party has agreed toâ€”crucial for volatile or low-liquidity commodity markets.
2.  Escrow-Backed Settlement
    
    *   **On-Chain Escrow:** Traders lock funds in a smart contract. The agent only completes a trade if both sides have met the exact terms.
    *   **Fail-Safe Mechanism:** If either side fails to finalize the transaction, agents revert the escrow to protect user funds.
3.  Multi-Wallet & Cross-Chain Support
    
    *   **Wallet-Agnostic:** Users may use MetaMask, Ledger, or any Web3-compatible wallet. The agent logic remains the same, ensuring a uniform trading experience.
    *   **Chain-Agnostic:** Mettalex agents can run on multiple blockchains in parallel (e.g., Ethereum, BNB Chain, Cosmos). They coordinate escrow locks and trades across networks if bridging solutions exist.
4.  On-Chain Registration & Discovery
    
    *   **Almanac Registry:** Agents register on a Cosmos-based on-chain Almanac contract, allowing other agents/dApps to verify their identity.
5.  Transparent Execution & Governance
    
    *   **Public Transaction Logs:** Every escrow creation, signature, and fund release is recorded on-chain. Users can view agent logs and track progress in real time.
    *   **MTLX Governance:** Mettalexâ€™s governance token (MTLX) can let agents automate fee changes or protocol upgrades, broadening the platformâ€™s agent-driven ecosystem.

Putting It All Together[â€‹](#putting-it-all-together "Direct link to Putting It All Together")
---------------------------------------------------------------------------------------------

*   User places a trade -> Funds locked in escrow.
*   Agent checks buyer/seller positions -> Confirms each sideâ€™s escrow.
*   Agents sign trade parameters on-chain -> Escrows release funds upon successful match.
*   Settlement -> Both parties receive respective assets with no slippage and full transparency.

If you wish to learn more about Mettalex, please visit [Mettalex Docs](https://www.mettalex.com/docs).</content>
</page>

<page>
  <title>AI Language Tutor with ASI-1 Mini | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/examples/asione/asi1-mini-language-tutor</url>
  <content>Building an AI Language Tutor uAgent with ASI-1 Mini
----------------------------------------------------

This guide explains how to create a simple uAgent that serves as an AI language tutor using the ASI-1 Mini API. The agent accepts a user query and returns language learning assistanceâ€”such as translation, grammar correction, or pronunciation tipsâ€”based on the provided prompt.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

### Purpose[â€‹](#purpose "Direct link to Purpose")

This project demonstrates how to integrate ASI-1 Mini, Fetch.ai's Web3-native large language model, into a uAgent. The agent, named **AI\_Language\_Tutor**, is designed to:

*   Call the **ASI-1 Mini API** with a prompt tailored for language learning.
*   Provide translation, grammar correction, or pronunciation tips based on user input.
*   Log the response upon agent startup.

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

Before running this project, ensure you have:

*   Python 3.11 installed.
*   uAgents library installed. If not already installed, use:

*   Requests library installed:

*   A valid API key for ASI-1 Mini. Get your API Key [here](https://innovationlab.fetch.ai/resources/docs/next/asione/asi1-mini-getting-started#how-to-get-an-api-key).

Script Breakdown[â€‹](#script-breakdown "Direct link to Script Breakdown")
------------------------------------------------------------------------

**1\. Importing Required Libraries**

The script begins by importing the necessary modules:

*   **requests:** To perform HTTP requests to the ASI-1 Mini API.
*   **json:** For handling JSON responses and potential JSON decode errors.
*   **uagents:** To create the uAgent and manage events.
*   **Context:** Provides access to the agent's runtime context, including logging.

    import requestsimport jsonfrom uagents import Agent, Context

**2\. Initializing the uAgent**

An instance of the uAgent is created with a name, port, and endpoint.

    agent = Agent(    name="AI_Language_Tutor",    port=8000,  # You can change this to any available port    endpoint="http://localhost:8000/submit")

**3\. Defining the ASI-1 Mini API Helper Function**

The function `get_language_help` constructs a custom prompt and sends a POST request to the ASI-1 Mini API. Based on the user's query and target language, it asks the API to either translate, correct grammar, or offer pronunciation tips.

    def get_language_help(query: str, target_language: str = "Spanish") -> str:    url = "https://api.asi1.ai/v1/chat/completions"    headers = {        'Content-Type': 'application/json',        'Authorization': f'Bearer <Your_ASI1_Mini_API_Key>'  # Replace with your API Key    }    prompt = f"""You are an AI language tutor. Help the user with their language learning request:    - If they ask for a **translation**, provide it in {target_language}.    - If they provide a sentence, **correct any grammar mistakes**.    - If they ask for pronunciation tips, explain how to say it.    User request: "{query}"    """    payload = {        "model": "asi1-mini",        "messages": [{"role": "user", "content": prompt}],        "temperature": 0.7,        "max_tokens": 0    }    try:        response = requests.post(url, headers=headers, json=payload)        response.raise_for_status()        return response.json()['choices'][0]['message']['content']    except requests.exceptions.RequestException as e:        return f"API Request Error: {str(e)}"    except json.JSONDecodeError:        return "API Error: Unable to parse JSON response"

**4\. Startup Event Handler**

The agent registers an `on_event("startup")` handler. When the agent starts, it:

*   Logs a sample query.
*   Calls the `get_language_help` function with an example query.
*   Logs the returned response from the ASI-1 Mini API.

    @agent.on_event("startup")async def language_tutor_demo(ctx: Context):    query = "How do you say 'Good morning' in French?"  # Example query    ctx.logger.info(f"User query: {query}")    response = get_language_help(query, target_language="French")    ctx.logger.info(f"ğŸŒ AI Tutor Response: {response}")

**5\. Running the Agent**

The script concludes by running the agent, which starts the server and registers the startup handler.

    if __name__ == "__main__":    agent.run()

Running the Agent[â€‹](#running-the-agent "Direct link to Running the Agent")
---------------------------------------------------------------------------

**1\. Activate Your Virtual Environment:**

    source venv/bin/activate   # On Windows: venv\Scripts\activate

**2\. Run the Script:**

    python my_language_tutor.py

**3\. Expected Output:**

Upon startup, you should see log messages similar to:

    INFO: [AI_Language_Tutor]: Starting agent with address: agent1...INFO: [AI_Language_Tutor]: User query: How do you say 'Good morning' in French?INFO: [AI_Language_Tutor]: ğŸŒ AI Tutor Response: Bonjour (Good morning/Good day).

GitHub Repository[â€‹](#github-repository "Direct link to GitHub Repository")
---------------------------------------------------------------------------

For the complete code and additional examples, visit the [ASI-1-Mini-simple-Examples](https://github.com/abhifetch/ASI-1-Mini-simple-Examples/blob/main/language_tutor.py) repository.

note

**Note:** You can learn more about ASI-1 Mini APIs [**here**](https://docs.asi1.ai/docs/).</content>
</page>

<page>
  <title>Solana Agent Integration with Fetch.ai uAgents | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/examples/on-chain-examples/solana-agents</url>
  <content>This example shows how to integrate Solana wallets within **Fetch.aiâ€™s uAgents** framework. Weâ€™ll walk through the **EscrowAgent, PlayerAgent, and ChallengerAgent** scripts, detailing how each agent:

1.  Registers with the **Almanac** contract (for discoverability)
2.  Loads Solana **private keys** from environment variables
3.  Executes transfers, checks balances, and handles bet-based business logic via **Solana Devnet**
4.  Communicates with other agents through **uAgents** messaging

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

*   **Solana CLI** (configured to Devnet)
*   **Poetry** (for dependency management)
*   **Python 3.8+**
*   Fetch.aiâ€™s **uagents** library
*   **Solders, requests**, etc. (handled by `poetry install`)
*   `.env` with your Solana private keys (Base64 arrays) for each agent

note

**Note:** Each agent script runs on a different portâ€”EscrowAgent uses `:8000`, PlayerAgent uses `:8001`, and ChallengerAgent uses `:8002` by default.

High-Level Architecture[â€‹](#high-level-architecture "Direct link to High-Level Architecture")
---------------------------------------------------------------------------------------------

1.  **PlayerAgent** & **ChallengerAgent** each place a bet by transferring SOL to the **Escrow** wallet (managed by the EscrowAgent).
2.  **EscrowAgent** collects two bets, checks the BTC price via an external API, and decides a winner. 90% of the total stake is transferred to the winnerâ€™s Solana wallet; the loser forfeits.

Escrow Agent[â€‹](#escrow-agent "Direct link to Escrow Agent")
------------------------------------------------------------

### Overview[â€‹](#overview "Direct link to Overview")

The **EscrowAgent**:

*   Registers on **Almanac** so other agents (Player/Challenger) can discover it
*   Waits for two **escrowRequest** messages
*   Fetches **BTC price from Binance**
*   **Transfers** the correct portion of **SOL** to the winner

### Script Breakdown[â€‹](#script-breakdown "Direct link to Script Breakdown")

**Required Libraries**

    import osimport base58import astfrom uagents import Agent, Context, Modelfrom solders.keypair import Keypairfrom functions import get_latest_btc_price, transfer_solimport time

*   os & ast for environment handling
*   uagents for agent creation
*   solders.keypair for Solana KeyPair
*   functions for helper utilities (price fetch & SOL transfer)

**Key Classes & Models**

    class escrowRequest(Model):    amount: float    price: float    public_key: strclass escrowResponse(Model):    result: str

*   **escrowRequest** holds the userâ€™s desired bet: `amount`, `price`, and the userâ€™s **Solana public key**.
*   **escrowResponse** returns the result to the user: either `"You Won"` or `"You Lost"`.

**Initialization & Identity**

    # Retrieve ESCROW_SECRET_LIST from the .envescrow_secret_key_str = os.getenv('ESCROW_SECRET_LIST')escrow_secret_key_list = ast.literal_eval(escrow_secret_key_str)escrow_secret_key_bytes = bytes(escrow_secret_key_list)escrow_keypair = Keypair.from_bytes(escrow_secret_key_bytes)escrow_pubkey_base58 = base58.b58encode(bytes(escrow_keypair.pubkey())).decode('utf-8')agent = Agent(    name="EscrowAgent",    port=8000,    seed="Escrow Wallet",    endpoint=["http://127.0.0.1:8000/submit"],)

*   We decode the **Solana private key** from `.env`.
*   Create a `Keypair` for the Escrowâ€™s wallet.
*   Instantiate a `uagents.Agent` with the name â€œEscrowAgentâ€ listening on port `8000`.

**On Startup**

    @agent.on_event('startup')async def saf(ctx: Context):    ctx.logger.info("Escrow agent initialized, ready for bids.")    ctx.logger.info(f"Escrow agent address: {agent.address}")    ctx.storage.set("bids_count", 0)

*   Logs that the Escrow is online.
*   Initializes a storage key bids\_count=0 to track how many requests have come in.

**Message Handling**

Receiving Bets

    @agent.on_message(model=escrowRequest, replies={escrowResponse})async def escrow_request_handler(ctx: Context, sender: str, msg: escrowRequest):    current_count = ctx.storage.get("bids_count") or 0    ...    if current_count == 0:        # Store first bet    elif current_count == 1:        # Store second bet        # Compare        # Transfer to winner        # Respond with escrowResponse        # Reset storage

*   **First Bet:** If `bids_count=0`, store the details (amount, price, userâ€™s pubkey).
*   **Second Bet:** If `bids_count=1`, store the second userâ€™s bet, then call `get_latest_btc_price()`.
*   Calculate each userâ€™s distance from the real BTC price.
*   Transfer 90% of the total stake to the winnerâ€™s public key with `transfer_sol()`.
*   Send escrowResponse messages to both the winner `(You Won)` and loser `(You Lost)`.
*   Reset internal storage.

**Running the EscrowAgent**

    if __name__ == "__main__":    agent.run()

*   Save the script as `escrow_agent.py`
*   Simply run `poetry run python escrow_agent.py`.

Player Agent[â€‹](#player-agent "Direct link to Player Agent")
------------------------------------------------------------

### Overview[â€‹](#overview-1 "Direct link to Overview")

The **PlayerAgent** simulates a user placing a bet on BTCâ€™s future price.

### Script Breakdown[â€‹](#script-breakdown-1 "Direct link to Script Breakdown")

**Required Libraries**

    import osimport astimport base58from uagents import Agent, Context, Modelfrom solders.keypair import Keypairfrom functions import check_balance, transfer_solfrom uagents.setup import fund_agent_if_lowcheck_balance and transfer_sol from functions.py to manage SOL balances/transfersfund_agent_if_low from uagents.setup can top up the agentâ€™s fetch-side address if needed

**Key Classes**

    class escrowRequest(Model):    amount: float    price: float    public_key: strclass escrowResponse(Model):    result: str

*   Re-used from the Escrow flow: escrowRequest is how we send the userâ€™s bet to the EscrowAgent.

**Initialization & Identity**

    secret_key_str = os.getenv('PLAYER_SECRET_LIST')secret_key_list = ast.literal_eval(secret_key_str)secret_key_bytes = bytes(secret_key_list)agent_keypair = Keypair.from_bytes(secret_key_bytes)agent_pubkey_base58 = base58.b58encode(bytes(agent_keypair.pubkey())).decode('utf-8')agent = Agent(    name="PlayerAgent",    port=8001,    seed="Player Escrow Wallet 1",    endpoint=["http://127.0.0.1:8001/submit"],)

*   Decodes the **PLAYER\_SECRET\_LIST** from `.env`.
*   Assigns the Playerâ€™s Solana wallet keypair.
*   Sets up a uAgent on port `8001`.

**Startup Sequence**

    @agent.on_event('startup')async def starter_function(ctx: Context):    initial_balance = check_balance(agent_keypair.pubkey())    ctx.logger.info(f"Initial agent balance: {initial_balance} SOL")    amount = float(input('What is the amount of SOL you want to deposit? '))    price = float(input('What is the price of Bitcoin you want to bid at? '))    # Send bet to Escrow    await ctx.send(        'agent1qd6ts50kuy3vqq36s5yg2dkzujq60x0l0sr2acfafnp5zea749yvvvq2qm7',        escrowRequest(amount=amount, price=price, public_key=agent_pubkey_base58)    )    # Transfer SOL to escrowâ€™s base58 key    transfer_result = transfer_sol(agent_keypair, '8WMWFo13At1REkwy5t7ck6sLgCUrJ9dn66mbaccPiJ26', amount)    ctx.logger.info(f"Transfer result: {transfer_result}")    final_balance = check_balance(agent_keypair.pubkey())    ctx.logger.info(f"Final agent balance: {final_balance} SOL")

*   **Check initial SOL** in the userâ€™s wallet.
*   **Ask** for deposit & BTC price guess.
*   **Send** an `escrowRequest` message to the known Escrow agent address.
*   `transfer_sol` to the Escrow agentâ€™s public key.
*   Log final SOL.

**Receiving Escrow Responses**

    @agent.on_message(model=escrowResponse)async def escrow_request_handler(ctx: Context, sender: str, msg: escrowResponse):    balance = check_balance(agent_keypair.pubkey())    ctx.logger.info(f'{msg.result}. Updated account balance: {balance} SOL')

*   When the **EscrowAgent** decides the outcome, it sends an `escrowResponse`.
*   This handler logs either â€œYou Wonâ€ or â€œYou Lostâ€ plus the updated balance.

**Running PlayerAgent**

    if __name__ == "__main__":    agent.run()

*   Save the script as `player_agent.py`
*   Run with poetry `run python player_agent.py`.

Challenger Agent[â€‹](#challenger-agent "Direct link to Challenger Agent")
------------------------------------------------------------------------

### Overview[â€‹](#overview-2 "Direct link to Overview")

Nearly identical to `player_agent.py`, but simulates another user (Challenger) placing a competing bet.

### Script Breakdown[â€‹](#script-breakdown-2 "Direct link to Script Breakdown")

**Required Libraries**

    import osimport astimport base58from uagents import Agent, Context, Modelfrom solders.keypair import Keypairfrom functions import check_balance, transfer_solfrom uagents.setup import fund_agent_if_low

**Key Classes**

    class escrowRequest(Model):    amount: float    price: float    public_key: strclass escrowResponse(Model):    result: str

**Initialization & Startup**

    secret_key_str = os.getenv('CHALLENGER_SECRET_LIST')secret_key_list = ast.literal_eval(secret_key_str)secret_key_bytes = bytes(secret_key_list)agent_keypair = Keypair.from_bytes(secret_key_bytes)agent_pubkey_base58 = base58.b58encode(bytes(agent_keypair.pubkey())).decode('utf-8')agent = Agent(    name="Challenger",    port=8002,    seed="Challenger Escrow Wallet 2",    endpoint=["http://127.0.0.1:8002/submit"],)fund_agent_if_low(agent.wallet.address())@agent.on_event('startup')async def starter_function(ctx: Context):    initial_balance = check_balance(agent_keypair.pubkey())    ctx.logger.info(f"Initial agent balance: {initial_balance} SOL")    amount = float(input('What is the amount of SOL you want to deposit? '))    price = float(input('What is the price of Bitcoin you want to bid at? '))    # Send the bet    await ctx.send(        'agent1qd6ts50kuy3vqq36s5yg2dkzujq60x0l0sr2acfafnp5zea749yvvvq2qm7',        escrowRequest(amount=amount, price=price, public_key=agent_pubkey_base58)    )    # Transfer SOL    transfer_result = transfer_sol(agent_keypair, '8WMWFo13At1REkwy5t7ck6sLgCUrJ9dn66mbaccPiJ26', amount)    ctx.logger.info(f"Transfer result: {transfer_result}")    final_balance = check_balance(agent_keypair.pubkey())    ctx.logger.info(f"Final agent balance: {final_balance} SOL")@agent.on_message(model=escrowResponse)async def escrow_request_handler(ctx: Context, sender: str, msg: escrowResponse):    balance = check_balance(agent_keypair.pubkey())    ctx.logger.info(f'{msg.result}. Updated account balance: {balance} SOL')if __name__ == "__main__":    agent.run()

*   Exactly the same flow: read user input, transfer SOL, wait for a response from the Escrow agent.

Utility Script : functions.py[â€‹](#utility-script--functionspy "Direct link to Utility Script : functions.py")
-------------------------------------------------------------------------------------------------------------

Below is a brief summary of the key utility functions. For full code, see the repository.

    import base58from solders.keypair import Keypairfrom solders.pubkey import Pubkeyfrom solders.transaction import Transactionfrom solders.system_program import TransferParams, transferfrom solana.rpc.api import Clientfrom solana.rpc.types import TxOptsimport requestsdef get_keypair_details(secret_key_list):    """    Given a list of secret key bytes (integers), returns a dictionary with the keypair, public key,    private key in bytes, and private key in Base58 encoding.    """    # Convert the list of integers into a bytes object (private key)    secret_key_bytes = bytes(secret_key_list)    # Restore the Keypair using the secret key    keypair = Keypair.from_bytes(secret_key_bytes)    # Public key (from keypair)    public_key = keypair.pubkey()    # Private key in Base58 encoding (for readability)    private_key_base58 = base58.b58encode(secret_key_bytes).decode()    # Return all necessary details in a dictionary    return {        "keypair": keypair,        "public_key": public_key,  # Solders Pubkey object        "private_key_bytes": secret_key_bytes,  # Private key in bytes        "private_key_base58": private_key_base58  # Private key in Base58    }client = Client("https://api.devnet.solana.com")# Function to check balancedef check_balance(pubkey):    balance_resp = client.get_balance(Pubkey.from_bytes(bytes(pubkey)))    print(f'balance_resp :{balance_resp}')    balance = balance_resp.value  # Extract balance in lamports    return balance / 1_000_000_000  # Convert lamports to SOLdef transfer_sol(from_keypair, to_pubkey_base58, amount_sol):    # Convert SOL to lamports (1 SOL = 1 billion lamports)    lamports = int(amount_sol * 1_000_000_000)    # Convert the recipient's Base58 public key string to a Pubkey object    to_pubkey = Pubkey.from_string(to_pubkey_base58)    # Get latest blockhash    blockhash_resp = client.get_latest_blockhash()    recent_blockhash = blockhash_resp.value.blockhash    # Create a transfer instruction    transfer_instruction = transfer(        TransferParams(from_pubkey=from_keypair.pubkey(), to_pubkey=to_pubkey, lamports=lamports)    )    # Create the transaction with the instruction directly    transaction = Transaction.new_signed_with_payer(        [transfer_instruction],  # Pass the list of instructions directly        from_keypair.pubkey(),  # Fee-payer (challenger)        [from_keypair],  # Signers (challenger)        recent_blockhash  # Use recent blockhash directly    )    # Send the transaction    result = client.send_raw_transaction(bytes(transaction), opts=TxOpts(skip_confirmation=False))    return resultdef get_latest_btc_price():    try:        # Binance API endpoint for fetching the latest BTC price in USDT        url = 'https://api.binance.com/api/v3/ticker/price?symbol=BTCUSDT'        response = requests.get(url)        response.raise_for_status()  # Raise an error for bad responses        data = response.json()        return float(data['price'])  # Return the latest BTC price as a float    except requests.exceptions.RequestException as e:        print(f"Error fetching BTC price: {e}")        return None

.env File Example[â€‹](#env-file-example "Direct link to .env File Example")
--------------------------------------------------------------------------

    # .envAGENTVERSE_API_KEY="<Your_FetchAI_Agentverse_Token>"PLAYER_SECRET_LIST="[79,79,237,8,87,104,75,156,47,204,53,127,171,9,114,244,...]"CHALLENGER_SECRET_LIST="[134,53,148,91,88,30,254,53,171,183,219,91,33,67,24,9,65,...]"ESCROW_SECRET_LIST="[251,164,58,0,121,167,133,83,114,82,162,22,88,214,195,91,82,...]"

Ensure each secret list matches the integer arrays from your `player-wallet.json`, `challenger-wallet.json`, and `escrow-wallet.json`. Also, do not commit your `.env` to source control.

Steps to Run the Agents[â€‹](#steps-to-run-the-agents "Direct link to Steps to Run the Agents")
---------------------------------------------------------------------------------------------

1.  Set Up Virtual Environment & Dependencies

2.  Fund Each Wallet on Devnet

    solana airdrop 5 <PLAYER_PUBKEY> --url devnetsolana airdrop 5 <CHALLENGER_PUBKEY> --url devnetsolana airdrop 5 <ESCROW_PUBKEY> --url devnet

3.  Start EscrowAgent

    poetry run python escrow_agent.py

*   Wait for it to display Escrow agent initialized, ready for bids.

4.  Start PlayerAgent

    poetry run python player_agent.py

*   Input the deposit amount & BTC guess when prompted.

5.  Start ChallengerAgent

    poetry run python challenger_agent.py

*   Similarly input deposit & guess. Once the Escrow receives the second bet, it decides a winner.

### Sample Output[â€‹](#sample-output "Direct link to Sample Output")

**EscrowAgent:**

    INFO: [EscrowAgent]: Escrow agent initialized, ready for bids.INFO: [EscrowAgent]: Received escrowRequest messageINFO: [EscrowAgent]: Storing first request ...INFO: [EscrowAgent]: Received escrowRequest messageINFO: [EscrowAgent]: Storing second request ...INFO: [EscrowAgent]: Processing bids to determine the winner.INFO: [EscrowAgent]: First difference: 1820.0, Second difference: 586820.0INFO: [EscrowAgent]: Transferring 0.9 SOL to winner ...INFO: [EscrowAgent]: Notifying winner and loser.

**PlayerAgent:**

    What is the amount of SOL you want to deposit? 0.5What is the price of Bitcoin you want to bid at? 65000INFO: [PlayerAgent]: Transfer result: ...INFO: [PlayerAgent]: Final agent balance: 4.31998 SOLINFO: [PlayerAgent]: You Won. Updated account balance: 5.21998 SOL

Debugging Common Issues[â€‹](#debugging-common-issues "Direct link to Debugging Common Issues")
---------------------------------------------------------------------------------------------

1.  Key Decoding Errors
    
    *   Ensure your `.env` secret lists are valid JSON arrays of integers.
    *   If you see `ValueError` or `Cannot decode secret key`, confirm you have no trailing commas.
2.  Faucet or Balance Issues
    
    *   Double-check `solana balance <PUBKEY> --url devnet`. If less then 1 SOL, some transactions might fail due to insufficient lamports for fees.
3.  Agent Registration Problems
    
    *   Confirm **AGENTVERSE\_API\_KEY** is correct in your `.env`.
    *   Make sure each agent can reach the default Almanac endpoint (requires internet connection).
4.  BTC Price Fetch Errors
    
    *   If Binance is unreachable or rate-limits your IP, consider adding retry logic or a fallback endpoint.

By following this **Solana + uAgents** guide, youâ€™ve set up three distinct agents that:

*   Load private keys from .env
*   Register on Fetch.aiâ€™s Almanac for discovery
*   Communicate using @agent.on\_message and typed models (escrowRequest, escrowResponse)
*   Interact with Solana Devnet for safe, low-cost experimentation

This architecture can be extended for **NFT auctions, DeFi ops, cross-chain bridging**, or any scenario where you need **agent-driven** logic plus on-chain Solana transactions. Enjoy building!

note

**Note:** GitHub repository for this example is available [here](https://github.com/abhifetch/solana-fetch-uagents-integration).</content>
</page>

<page>
  <title>BNB Chain Agents | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/examples/on-chain-examples/bnb-chain-agents</url>
  <content>Version: Next

This guide demonstrates how to create and use AI agents for interacting with the BNB Chain using uAgents. We'll build a system of three agents that work together to send transactions, validate them, and monitor wallet activity.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

The system consists of three main agents:

1.  **Transaction Sender Agent (Agent1)**: Handles BNB transfer requests and initiates transactions
2.  **Transaction Validator Agent (Agent2)**: Verifies transaction status using BscScan API
3.  **Wallet Monitor Agent**: Monitors specified wallet addresses for any transaction activity

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

Before getting started, you'll need:

*   Python 3.11 or higher
*   A BNB Testnet account with some test BNB
*   [BscScan API Key](https://docs.bscscan.com/getting-started/viewing-api-usage-statistics)
*   Basic understanding of Web3 and blockchain concepts

Architecture Diagram[â€‹](#architecture-diagram "Direct link to Architecture Diagram")
------------------------------------------------------------------------------------

Installation[â€‹](#installation "Direct link to Installation")
------------------------------------------------------------

1.  Create a new directory and set up your virtual environment:

    mkdir bnb-chain-agentscd bnb-chain-agentspython -m venv venvsource venv/bin/activate  # On macOS/Linuxvenv\Scripts\activate     # On Windows

2.  Install the required packages:

    pip install uagents web3 python-dotenv requests

3.  Create a `.env` file with your credentials:

    USER_WALLET=your_wallet_addressUSER_KEY=your_private_keyBSCSCAN_API_KEY=your_bscscan_api_key

Implementation[â€‹](#implementation "Direct link to Implementation")
------------------------------------------------------------------

### 1\. Transaction Sender Agent (agent1.py)[â€‹](#1-transaction-sender-agent-agent1py "Direct link to 1. Transaction Sender Agent (agent1.py)")

This agent handles BNB transfer requests and communicates with the validator agent:

    from uagents import Agent, Context, Modelfrom web3 import Web3from web3.middleware import geth_poa_middlewareimport osfrom dotenv import load_dotenv# Load environment variablesload_dotenv()agent_user = Agent(    name='User BNB Agent to make transactions',    port=8000,    endpoint=['http://localhost:8000/submit'])class RequestTransfer(Model):    to_address: str    amount: float    agent_to_address: strclass RequestDetails(Model):    tx_hash: strclass ResponseTransfer(Model):    response: str# Connect to BNB Chain Testnetprovider_url = "https://data-seed-prebsc-1-s1.binance.org:8545/"web3 = Web3(Web3.HTTPProvider(provider_url))web3.middleware_onion.inject(geth_poa_middleware, layer=0)@agent_user.on_rest_post("/send/bnb", RequestTransfer, ResponseTransfer)async def handle_post(ctx: Context, req: RequestTransfer) -> ResponseTransfer:    try:        # Get wallet credentials        user_wallet = os.getenv("USER_WALLET")        user_key = os.getenv("USER_KEY")                # Prepare transaction        to_address = web3.to_checksum_address(req.to_address)        nonce = web3.eth.get_transaction_count(user_wallet)                tx = {            'to': to_address,            'value': web3.to_wei(req.amount, 'ether'),            'gas': 21000,            'gasPrice': web3.eth.gas_price,            'nonce': nonce,            'chainId': 97  # BNB Testnet        }        # Sign and send transaction        signed_tx = web3.eth.account.sign_transaction(tx, user_key)        tx_hash = web3.eth.send_raw_transaction(signed_tx.raw_transaction)                # Send to validator agent        message, status = await ctx.send_and_receive(            req.agent_to_address,            RequestDetails(tx_hash=web3.to_hex(tx_hash)),            response_type=ResponseTransfer        )                return ResponseTransfer(response=message.response)        except Exception as e:        return ResponseTransfer(response=f"Transaction Failed: {str(e)}")if __name__ == "__main__":    agent_user.run()

### 2\. Transaction Validator Agent (agent2.py)[â€‹](#2-transaction-validator-agent-agent2py "Direct link to 2. Transaction Validator Agent (agent2.py)")

This agent verifies transaction status using the BscScan API:

    from uagents import Agent, Context, Modelfrom web3 import Web3import osfrom dotenv import load_dotenvimport asyncioimport requestsagent_dummy = Agent(    name='Dummy BNB Agent to make transactions',    port=8001,    endpoint=['http://localhost:8001/submit'])load_dotenv()BSCSCAN_API_KEY = os.getenv("BSCSCAN_API_KEY")class RequestDetails(Model):    tx_hash: strclass ResponseTransfer(Model):    response: strasync def get_transaction_status(tx_hash: str) -> dict:    base_url = "https://api.bscscan.com/api"    params = {        "module": "transaction",        "action": "getstatus",        "txhash": tx_hash,        "apikey": BSCSCAN_API_KEY    }        response = await asyncio.to_thread(requests.get, base_url, params=params)    return response.json() if response.status_code == 200 else {"error": f"HTTP error {response.status_code}"}@agent_dummy.on_message(model=RequestDetails, replies={ResponseTransfer})async def startup_handler(ctx: Context, sender: str, msg: RequestDetails):    tx_status = await get_transaction_status(msg.tx_hash)        if "error" in tx_status:        reply = f"API Error: {tx_status['error']}"    else:        if tx_status.get("status") == "1" and tx_status.get("message") == "OK":            if tx_status["result"].get("isError") == "0":                reply = f"Successful transfer confirmed by receiver.âœ… tx_hash: {msg.tx_hash}"            else:                err_desc = tx_status["result"].get("errDescription", "Unknown error")                reply = f"Transfer rejected: {err_desc}"        else:            reply = "Transfer status unknown or API response error."    await ctx.send(sender, ResponseTransfer(response=reply))if __name__ == "__main__":    agent_dummy.run()

### 3\. Wallet Monitor Agent (monitor\_wallet.py)[â€‹](#3-wallet-monitor-agent-monitor_walletpy "Direct link to 3. Wallet Monitor Agent (monitor_wallet.py)")

This agent monitors wallet activity in real-time:

    from uagents import Agent, Contextfrom web3 import Web3from web3.middleware import geth_poa_middlewareimport jsonimport osfrom dotenv import load_dotenvload_dotenv()user_wallet = os.getenv("USER_WALLET")monitor_agent = Agent(    name='Monitor BNB Agent to monitor address',    port=8003,    endpoint=['http://localhost:8003/submit'])# Connect to BNB Chain Testnetprovider_url = "https://data-seed-prebsc-1-s1.binance.org:8545/"web3 = Web3(Web3.HTTPProvider(provider_url))web3.middleware_onion.inject(geth_poa_middleware, layer=0)monitored_address = web3.to_checksum_address(user_wallet)last_scanned_block = web3.eth.block_number@monitor_agent.on_interval(period=10)async def monitor_handler(ctx: Context):    global last_scanned_block    try:        current_block = web3.eth.block_number        if current_block <= last_scanned_block:            return        for block_num in range(last_scanned_block + 1, current_block + 1):            block = web3.eth.get_block(block_num, full_transactions=True)            for tx in block['transactions']:                tx_from = tx['from']                tx_to = tx['to']                                if (tx_from and tx_from.lower() == monitored_address.lower()) or \                   (tx_to and tx_to.lower() == monitored_address.lower()):                    details = {                        "blockNumber": block_num,                        "hash": tx['hash'].hex(),                        "from": tx_from,                        "to": tx_to if tx_to else "Contract Creation",                        "value": str(web3.from_wei(tx['value'], 'ether')) + " BNB"                    }                    ctx.logger.info(f"Recorded transaction: {json.dumps(details, indent=2)}")        last_scanned_block = current_block    except Exception as e:        ctx.logger.error(f"Error during monitoring: {e}")if __name__ == "__main__":    monitor_agent.run()

Usage[â€‹](#usage "Direct link to Usage")
---------------------------------------

1.  Start all three agents in separate terminal windows:

    # Terminal 1python agent1.py# Terminal 2python agent2.py# Terminal 3python monitor_wallet.py

2.  Send a BNB transfer request using curl:

    curl -d '{    "to_address": "RECIPIENT_ADDRESS",    "amount": 0.01,    "agent_to_address": "AGENT2_ADDRESS"}' \-H "Content-Type: application/json" \-X POST http://localhost:8000/send/bnb

Replace `RECIPIENT_ADDRESS` with the destination wallet address and `AGENT2_ADDRESS` with the address of your validator agent.

System Flow[â€‹](#system-flow "Direct link to System Flow")
---------------------------------------------------------

1.  The Transaction Sender Agent receives a transfer request via HTTP POST
2.  It creates and signs the transaction using the private key
3.  The transaction is sent to the BNB Chain Testnet
4.  The transaction hash is forwarded to the Validator Agent
5.  The Validator Agent checks the transaction status using BscScan API
6.  The Wallet Monitor Agent continuously scans for new transactions
7.  All agents log their activities and transaction details

Best Practices[â€‹](#best-practices "Direct link to Best Practices")
------------------------------------------------------------------

1.  **Security**:
    
    *   Never commit your `.env` file or expose private keys
    *   Use environment variables for sensitive data
    *   Validate input data before processing
2.  **Error Handling**:
    
    *   Implement proper error handling for API calls
    *   Log errors and transaction details
    *   Provide meaningful error messages
3.  **Monitoring**:
    
    *   Use the monitoring agent to track transactions
    *   Implement alerts for suspicious activities
    *   Keep logs for auditing purposes

Conclusion[â€‹](#conclusion "Direct link to Conclusion")
------------------------------------------------------

This example demonstrates how to create a system of AI agents that interact with the BNB Chain. The agents work together to handle transactions, validate them, and monitor wallet activity. This system can be extended to include more complex functionality like smart contract interactions, automated trading, or blockchain analytics.</content>
</page>

<page>
  <title>Creating ASI1 Compatible uAgent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/examples/chat-protocol/asi-compatible-uagents</url>
  <content>This guide demonstrates how to make your agents accessible via ASI1 LLM by integrating the chat protocol. We'll use a Football Team Agent example to show how the chat protocol enables seamless communication between agents and the LLM.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

In this example, you'll learn how to build a uAgent compatible with Fetch.ai's ASI1 Large Language Model (LLM). Using a Football Team Agent as an example, the guide shows how you can enable your agent to understand and respond to natural language queries.

Message Flow[â€‹](#message-flow "Direct link to Message Flow")
------------------------------------------------------------

The communication flow between ASI1 LLM, the Football Team Agent, and OpenAI Agent follows this sequence:

1.  **Query Initiation (1.1)**
    
    *   ASI1 LLM sends a natural language query (e.g., "Give me the list of players in Manchester United Football Team") as a `ChatMessage` to the Football Team Agent on the `ChatMessage handler`.
2.  **Parameter Extraction (2, 3)**
    
    *   The Football Team Agent forwards the query to OpenAI Agent for parameter extraction
    *   OpenAI Agent processes the natural language and extracts structured parameters (e.g., team\_name="Manchester United")
    *   The parameters are returned in a Pydantic Model format as `StructuredOutputResponse` on the `StructuredOutputResponse handler`.
3.  **Team Data Processing (4, 5)**
    
    *   The Football Team Agent calls the `get_team_info` function with the extracted parameters
    *   The function returns the team details.
4.  **Football Team Agent Response (6.1)**
    
    *   The Football Team Agent sends the formatted response back as a `ChatMessage` to ASI1 LLM.
5.  **Message Acknowledgements (1.2, 6.2)**
    
    *   Each message exchanged using the chat protocol is automatically acknowledged by the receiving agent using `ChatAcknowledgement`.

Here's a visual representation of the flow:

Implementation[â€‹](#implementation "Direct link to Implementation")
------------------------------------------------------------------

In this example we will create an agent and its associated files on [Agentverse](https://agentverse.ai/) that communicate using the chat protocol with the ASI1 LLM. Refer to the [Hosted Agents](https://innovationlab.fetch.ai/resources/docs/agent-creation/uagent-creation#hosted-agents) section to understand the detailed steps for agent creation on Agentverse.

Create a new agent named "FootballTeamAgent" on Agentverse and create the following files:

    agent.py        # Main agent file football.py   # Football team service implementation and API integrationchat_proto.py   # Chat protocol implementation for enabling text based communication 

To create a new file on Agentverse:

1.  Click on the New File icon

2.  Assign a name to the File

3.  Directory Structure

### 1\. Football Team Function and Data Models[â€‹](#1-football-team-function-and-data-models "Direct link to 1. Football Team Function and Data Models")

Let's start by defining our data models and the function to retrieve the list of players in a football team. These models will define how we request team information and receive responses. We'll use the AllSports API to fetch team and player information. You can obtain your API key by signing up at [AllSports API](https://allsportsapi.com/), which provides comprehensive sports data feeds including football (soccer) team and player information.

To implement the football team service add the following chat protocol in the `football.py` file created on Agentverse:

football.py

    import requestsfrom uagents import Model, FieldAPI_KEY = "YOUR_ALLSPORTS_API_KEY"BASE_URL = "https://apiv2.allsportsapi.com/football/"class FootballTeamRequest(Model):    team_name: strclass FootballTeamResponse(Model):    results: strasync def get_team_info(team_name: str) -> str:    """    Fetch team information from AllSportsAPI and return as plain text    """    try:        params = {            "met": "Teams",            "teamName": team_name,            "APIkey": API_KEY        }        response = requests.get(BASE_URL, params=params)        data = response.json()        if data.get("success") == 1 and data.get("result"):            team_info = data["result"][0]            result = f"\nTeam Name: {team_info['team_name']}\n"            result += f"Team Logo: {team_info['team_logo']}\n\n"            result += "Players:\n"                        for player in team_info.get("players", []):                result += f"- Name: {player['player_name']}\n"                result += f"  Type: {player['player_type']}\n"                result += f"  Image: {player['player_image']}\n\n"                        return result        else:            return "Team not found or invalid API key."                except Exception as e:        return f"Error fetching team information: {str(e)}"

### 2\. Chat Protocol Integration[â€‹](#2-chat-protocol-integration "Direct link to 2. Chat Protocol Integration")

The `chat_proto.py` file is essential for enabling natural language communication between your agent and ASI1 LLM.

#### LLM Integration[â€‹](#llm-integration "Direct link to LLM Integration")

To extract the important parameters from a user query passed by the LLM, you can use any of the following LLMs that support structured output:

*   OpenAI Agent: `agent1q0h70caed8ax769shpemapzkyk65uscw4xwk6dc4t3emvp5jdcvqs9xs32y`
*   Claude.ai Agent: `agent1qvk7q2av3e2y5gf5s90nfzkc8a48q3wdqeevwrtgqfdl0k78rspd6f2l4dx`

> **Note**: To ensure fair usage, each agent is limited to 6 requests per hour. Please implement appropriate rate limiting in your application.

#### Message Flow[â€‹](#message-flow-1 "Direct link to Message Flow")

1.  When a user sends a query (e.g., "Show me Manchester United players"):
    
    *   The `ChatMessage` handler receives the message
    *   Acknowledges receipt to the sender
    *   Forwards the query to the chosen LLM for processing
2.  The LLM processes the query and returns a `StructuredOutputResponse`:
    
    *   Extracts relevant parameters (e.g., team\_name="Manchester United")
    *   Returns data in the format specified by your agent's schema
3.  Your agent processes the structured data:
    
    *   Calls appropriate functions (e.g., `get_team_info`)
    *   Formats the response and sends it back.

#### Customizing the Handlers[â€‹](#customizing-the-handlers "Direct link to Customizing the Handlers")

When implementing a new agent for a different use case, you'll need to:

*   Update the schema in `StructuredOutputPrompt` to match your agent's data model
*   Modify the response handling in `handle_structured_output_response` function for your use case.

To enable natural language communication with your agent add the following chat protocol in the `chat_proto.py` file created on Agentverse:

chat\_proto.py

    from datetime import datetimefrom uuid import uuid4from typing import Anyfrom uagents import Context, Model, Protocol#Import the necessary components of the chat protocolfrom uagents_core.contrib.protocols.chat import (    ChatAcknowledgement,    ChatMessage,    EndSessionContent,    StartSessionContent,    TextContent,    chat_protocol_spec,)from football import get_team_info, FootballTeamRequest#Replace the AI Agent Address with anyone of the following LLMs as they support StructuredOutput required for the processing of this agent. AI_AGENT_ADDRESS = 'agent1q0h70caed8ax769shpemapzkyk65uscw4xwk6dc4t3emvp5jdcvqs9xs32y'if not AI_AGENT_ADDRESS:    raise ValueError("AI_AGENT_ADDRESS not set")def create_text_chat(text: str, end_session: bool = False) -> ChatMessage:    content = [TextContent(type="text", text=text)]    if end_session:        content.append(EndSessionContent(type="end-session"))    return ChatMessage(        timestamp=datetime.utcnow(),        msg_id=uuid4(),        content=content,    )chat_proto = Protocol(spec=chat_protocol_spec)struct_output_client_proto = Protocol(    name="StructuredOutputClientProtocol", version="0.1.0")class StructuredOutputPrompt(Model):    prompt: str    output_schema: dict[str, Any]class StructuredOutputResponse(Model):    output: dict[str, Any]@chat_proto.on_message(ChatMessage)async def handle_message(ctx: Context, sender: str, msg: ChatMessage):    ctx.logger.info(f"Got a message from {sender}: {msg.content}")    ctx.storage.set(str(ctx.session), sender)    await ctx.send(        sender,        ChatAcknowledgement(timestamp=datetime.utcnow(), acknowledged_msg_id=msg.msg_id),    )    for item in msg.content:        if isinstance(item, StartSessionContent):            ctx.logger.info(f"Got a start session message from {sender}")            continue        elif isinstance(item, TextContent):            ctx.logger.info(f"Got a message from {sender}: {item.text}")            ctx.storage.set(str(ctx.session), sender)            await ctx.send(                AI_AGENT_ADDRESS,                StructuredOutputPrompt(                    prompt=item.text, output_schema=FootballTeamRequest.schema()                ),            )        else:            ctx.logger.info(f"Got unexpected content from {sender}")@chat_proto.on_message(ChatAcknowledgement)async def handle_ack(ctx: Context, sender: str, msg: ChatAcknowledgement):    ctx.logger.info(        f"Got an acknowledgement from {sender} for {msg.acknowledged_msg_id}"    )@struct_output_client_proto.on_message(StructuredOutputResponse)async def handle_structured_output_response(    ctx: Context, sender: str, msg: StructuredOutputResponse):    session_sender = ctx.storage.get(str(ctx.session))    if session_sender is None:        ctx.logger.error(            "Discarding message because no session sender found in storage"        )        return    if "<UNKNOWN>" in str(msg.output):        await ctx.send(            session_sender,            create_text_chat(                "Sorry, I couldn't process your location request. Please try again later."            ),        )        return    prompt = FootballTeamRequest.parse_obj(msg.output)    try:        team_info = await get_team_info(prompt.team_name)    except Exception as err:        ctx.logger.error(err)        await ctx.send(            session_sender,            create_text_chat(                "Sorry, I couldn't process your request. Please try again later."            ),        )        return    if "error" in team_info:        await ctx.send(session_sender, create_text_chat(str(team_info["error"])))        return    chat_message = create_text_chat(team_info)    await ctx.send(session_sender, chat_message)

### 3\. Football Team Agent Setup[â€‹](#3-football-team-agent-setup "Direct link to 3. Football Team Agent Setup")

The `agent.py` file is the core of your application. Think of it as the main control center that:

*   Sets up your agent
*   Handles incoming requests
*   Manages rate limiting
*   Monitors the agent's health

Let's break down each component:

#### Basic Setup[â€‹](#basic-setup "Direct link to Basic Setup")

    from uagents import Agent, Context, Modelfrom uagents.experimental.quota import QuotaProtocol, RateLimitagent = Agent()  # Create a new agent instance

Import the necessary packages and initialise your agent.

#### Rate Limiting Setup[â€‹](#rate-limiting-setup "Direct link to Rate Limiting Setup")

    proto = QuotaProtocol(    storage_reference=agent.storage,    name="Football-Team-Protocol",    version="0.1.0",    default_rate_limit=RateLimit(window_size_minutes=60, max_requests=30),)

To ensure fair usage and prevent API abuse, we recommend implementing the QuotaProtocol. While optional, this protocol helps manage service usage by limiting requests. In this example, we've set a limit of 30 requests per hour per user, but you can adjust this threshold based on your agent's functionality. This helps maintain service reliability and protects your agent from excessive requests.

#### Request Handler[â€‹](#request-handler "Direct link to Request Handler")

    @proto.on_message(    FootballTeamRequest, replies={FootballTeamResponse, ErrorMessage})async def handle_request(ctx: Context, sender: str, msg: FootballTeamRequest):    ctx.logger.info("Received team info request")    try:        results = await get_team_info(msg.team_name)        await ctx.send(sender, FootballTeamResponse(results=results))    except Exception as err:        await ctx.send(sender, ErrorMessage(error=str(err)))

This message handler allows your agent to receive direct requests from other agents in a structured format. While we're using it with ASI1 LLM in this example, it's versatile enough to work with agents that don't have the chat protocol enabled. This makes it particularly useful in multi-agent systems, where other agents can directly request information directly.

#### Health Monitoring[â€‹](#health-monitoring "Direct link to Health Monitoring")

    ### Health check related codedef agent_is_healthy() -> bool:    """    Implement the actual health check logic here.    For example, check if the agent can connect to the AllSports API.    """    try:        import asyncio        asyncio.run(get_team_info("Manchester United"))        return True    except Exception:        return Falseclass HealthCheck(Model):    passclass HealthStatus(str, Enum):    HEALTHY = "healthy"    UNHEALTHY = "unhealthy"class AgentHealth(Model):    agent_name: str    status: HealthStatushealth_protocol = QuotaProtocol(    storage_reference=agent.storage, name="HealthProtocol", version="0.1.0")@health_protocol.on_message(HealthCheck, replies={AgentHealth})async def handle_health_check(ctx: Context, sender: str, msg: HealthCheck):    status = HealthStatus.UNHEALTHY    try:        if agent_is_healthy():            status = HealthStatus.HEALTHY    except Exception as err:        ctx.logger.error(err)    finally:        await ctx.send(sender, AgentHealth(agent_name="football_agent", status=status))

The HealthProtocol, while optional, provides a periodic health check system to monitor your agent's performance and reliability. It tests the agent's ability to fetch information about a team and verifies that all components are working correctly. This monitoring helps quickly identify and address any issues that might affect your agent's service, ensuring reliable operation for your users.

#### Protocol Registration[â€‹](#protocol-registration "Direct link to Protocol Registration")

    agent.include(proto, publish_manifest=True)agent.include(health_protocol, publish_manifest=True)agent.include(chat_proto, publish_manifest=True)agent.include(struct_output_client_proto, publish_manifest=True)

This registers all the necessary protocols with your agent:

*   Main protocol for handling team requests
*   Health monitoring protocol
*   Chat protocol for natural language communication
*   Structured output protocol for formatted responses

Here's the complete implementation:

agent.py

    import osfrom enum import Enumfrom uagents import Agent, Context, Modelfrom uagents.experimental.quota import QuotaProtocol, RateLimitfrom uagents_core.models import ErrorMessagefrom chat_proto import chat_proto, struct_output_client_protofrom football import get_team_info, FootballTeamRequest, FootballTeamResponseagent = Agent()proto = QuotaProtocol(    storage_reference=agent.storage,    name="Football-Team-Protocol",    version="0.1.0",    default_rate_limit=RateLimit(window_size_minutes=60, max_requests=30),)@proto.on_message(    FootballTeamRequest, replies={FootballTeamResponse, ErrorMessage})async def handle_request(ctx: Context, sender: str, msg: FootballTeamRequest):    ctx.logger.info("Received team info request")    try:        results = await get_team_info(msg.team_name)        ctx.logger.info(f'printing results in function {results}')        ctx.logger.info("Successfully fetched team information")        await ctx.send(sender, FootballTeamResponse(results=results))    except Exception as err:        ctx.logger.error(err)        await ctx.send(sender, ErrorMessage(error=str(err)))agent.include(proto, publish_manifest=True)### Health check related codedef agent_is_healthy() -> bool:    """    Implement the actual health check logic here.    For example, check if the agent can connect to the AllSports API.    """    try:        import asyncio        asyncio.run(get_team_info("Manchester United"))        return True    except Exception:        return Falseclass HealthCheck(Model):    passclass HealthStatus(str, Enum):    HEALTHY = "healthy"    UNHEALTHY = "unhealthy"class AgentHealth(Model):    agent_name: str    status: HealthStatushealth_protocol = QuotaProtocol(    storage_reference=agent.storage, name="HealthProtocol", version="0.1.0")@health_protocol.on_message(HealthCheck, replies={AgentHealth})async def handle_health_check(ctx: Context, sender: str, msg: HealthCheck):    status = HealthStatus.UNHEALTHY    try:        if agent_is_healthy():            status = HealthStatus.HEALTHY    except Exception as err:        ctx.logger.error(err)    finally:        await ctx.send(sender, AgentHealth(agent_name="football_agent", status=status))agent.include(health_protocol, publish_manifest=True)agent.include(chat_proto, publish_manifest=True)agent.include(struct_output_client_proto, publish_manifest=True)if __name__ == "__main__":    agent.run() 

Adding a README to your Agent[â€‹](#adding-a-readme-to-your-agent "Direct link to Adding a README to your Agent")
---------------------------------------------------------------------------------------------------------------

1.  Go to the Overview section in the Editor.
    
2.  Click on Edit and add a good description for your Agent so that it can be easily searchable by the ASI1 LLM. Please refer the [Importance of Good Readme](https://innovationlab.fetch.ai/resources/docs/agentverse/searching#importance-of-good-readme) section for more details.
    
3.  Make sure the Agent has the right `AgentChatProtocol`.
    

Test your Agent[â€‹](#test-your-agent "Direct link to Test your Agent")
---------------------------------------------------------------------

1.  Start your Agent

2.  To test your agent, use the [Agentverse Chat Interface](https://chat.agentverse.ai/). You can either search for your Agent by the Agent's name or by the Agent's address.

3.  Select your Agent from the list and type in a query to ask your Agent and it should return a response back with the Team Details.

Query your Agent from ASI1 LLM[â€‹](#query-your-agent-from-asi1-llm "Direct link to Query your Agent from ASI1 LLM")
------------------------------------------------------------------------------------------------------------------

1.  Login to the [ASI1 LLM](https://asi1.ai/), either using your Google Account or the ASI1 Wallet and Start a New Chat.
    
2.  Toggle the "Agents" switch to enable ASI1 to connect with Agents on Agentverse.
    

3.  Type in a query to ask your Agent for instance 'I want to get the player details for the Manchester City Football Team'.

> **Note**: The ASI1 LLM may not always select your agent for answering the query as it is designed to pick the best agent for a task based on a number of parameters.</content>
</page>

<page>
  <title>CrewAI Adapter Example | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/examples/adapters/crewai-adapter-example</url>
  <content>CrewAI Adapter for uAgents
--------------------------

This example demonstrates how to integrate a **CrewAI multi-agent system** with the **uAgents ecosystem** using the uAgents Adapter package. CrewAI allows you to create collaborative teams of AI agents working together to accomplish complex tasks.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

The CrewAI adapter enables:

*   Creating specialized agent teams with distinct roles and responsibilities
*   Orchestrating complex workflows between different AI agents
*   Exposing CrewAI teams as uAgents for seamless communication with the broader agent ecosystem
*   Deploying CrewAI applications to the Agentverse network

Trip Planner Example[â€‹](#trip-planner-example "Direct link to Trip Planner Example")
------------------------------------------------------------------------------------

Let's look at a real-world example of a trip planning system with multiple specialized agents working together to create a complete travel itinerary. We'll compare the standard CrewAI implementation with the uAgents-integrated version.

### Standard CrewAI Implementation[â€‹](#standard-crewai-implementation "Direct link to Standard CrewAI Implementation")

First, let's look at how a standard CrewAI system is implemented without uAgents integration:

    # Standard main.pyfrom textwrap import dedentfrom crewai import Crewfrom dotenv import load_dotenvfrom trip_agents import TripAgentsfrom trip_tasks import TripTasksload_dotenv()class TripCrew:    def __init__(self, origin, cities, date_range, interests):        self.cities = cities        self.origin = origin        self.interests = interests        self.date_range = date_range    def run(self):        agents = TripAgents()        tasks = TripTasks()        city_selector_agent = agents.city_selection_agent()        local_expert_agent = agents.local_expert()        travel_concierge_agent = agents.travel_concierge()        identify_task = tasks.identify_task(            city_selector_agent,            self.origin,            self.cities,            self.interests,            self.date_range,        )        gather_task = tasks.gather_task(local_expert_agent, self.origin, self.interests, self.date_range)        plan_task = tasks.plan_task(travel_concierge_agent, self.origin, self.interests, self.date_range)        crew = Crew(            agents=[city_selector_agent, local_expert_agent, travel_concierge_agent],            tasks=[identify_task, gather_task, plan_task],            verbose=True,        )        result = crew.kickoff()        return resultif __name__ == "__main__":    print("## Welcome to Trip Planner Crew")    print("-------------------------------")    location = input(        dedent("""      From where will you be traveling from?    """)    )    cities = input(        dedent("""      What are the cities options you are interested in visiting?    """)    )    date_range = input(        dedent("""      What is the date range you are interested in traveling?    """)    )    interests = input(        dedent("""      What are some of your high level interests and hobbies?    """)    )    trip_crew = TripCrew(location, cities, date_range, interests)    result = trip_crew.run()    print("\n\n########################")    print("## Here is you Trip Plan")    print("########################\n")    print(result)

### uAgents Integration[â€‹](#uagents-integration "Direct link to uAgents Integration")

Now, let's see how we can integrate this same CrewAI system with uAgents to enable network communication:

    #!/usr/bin/env python3"""Trip Planner script using CrewAI adapter for uAgents."""import osfrom crewai import Crewfrom dotenv import load_dotenvfrom uagents_adapter import CrewaiRegisterToolfrom trip_agents import TripAgentsfrom trip_tasks import TripTasksclass TripCrew:    def __init__(self, origin, cities, date_range, interests):        self.cities = cities        self.origin = origin        self.interests = interests        self.date_range = date_range    def run(self):        agents = TripAgents()        tasks = TripTasks()        city_selector_agent = agents.city_selection_agent()        local_expert_agent = agents.local_expert()        travel_concierge_agent = agents.travel_concierge()        identify_task = tasks.identify_task(            city_selector_agent,            self.origin,            self.cities,            self.interests,            self.date_range,        )        gather_task = tasks.gather_task(local_expert_agent, self.origin, self.interests, self.date_range)        plan_task = tasks.plan_task(travel_concierge_agent, self.origin, self.interests, self.date_range)        crew = Crew(            agents=[city_selector_agent, local_expert_agent, travel_concierge_agent],            tasks=[identify_task, gather_task, plan_task],            verbose=True,        )        result = crew.kickoff()        return result    def kickoff(self, inputs=None):        """        Compatibility method for uAgents integration.        Accepts a dictionary of inputs and calls run() with them.        """        if inputs:            self.origin = inputs.get("origin", self.origin)            self.cities = inputs.get("cities", self.cities)            self.date_range = inputs.get("date_range", self.date_range)            self.interests = inputs.get("interests", self.interests)        return self.run()def main():    """Main function to demonstrate Trip Planner with CrewAI adapter."""    # Load API key from environment    load_dotenv()    api_key = os.getenv("AGENTVERSE_API_KEY")    openai_api_key = os.getenv("OPENAI_API_KEY")    if not api_key:        print("Error: AGENTVERSE_API_KEY not found in environment")        return    if not openai_api_key:        print("Error: OPENAI_API_KEY not found in environment")        return    # Set OpenAI API key in environment    os.environ["OPENAI_API_KEY"] = openai_api_key    # Create an instance of TripCrew with default empty values    trip_crew = TripCrew("", "", "", "")    # Create tool for registering the crew with Agentverse    register_tool = CrewaiRegisterTool()    # Define parameters schema for the trip planner    query_params = {        "origin": {"type": "str", "required": True},        "cities": {"type": "str", "required": True},        "date_range": {"type": "str", "required": True},        "interests": {"type": "str", "required": True},    }    # Register the crew with parameter schema    result = register_tool.run(        tool_input={            "crew_obj": trip_crew,            "name": "Trip Planner Crew AI Agent adapters",            "port": 8080,            "description": "A CrewAI agent that helps plan trips based on preferences",            "api_token": api_key,            "mailbox": True,            "query_params": query_params,            "example_query": "Plan a trip from New York to Paris in June, I'm interested in art and history other than museums.",        }    )    # Get the agent address from the result    if isinstance(result, dict) and "address" in result:        result["address"]    print(f"\nCrewAI agent registration result: {result}")    # Keep the program running    try:        while True:            import time            time.sleep(1)    except KeyboardInterrupt:        print("\nExiting...")if __name__ == "__main__":    main()

Key Differences in uAgents Integration[â€‹](#key-differences-in-uagents-integration "Direct link to Key Differences in uAgents Integration")
------------------------------------------------------------------------------------------------------------------------------------------

When integrating a CrewAI system with uAgents, there are several important differences:

1.  **CrewaiRegisterTool**:
    
    *   Uses the specialized `CrewaiRegisterTool` instead of the generic `UAgentRegisterTool`.
    *   This tool is specifically designed to handle CrewAI's collaborative agent structure.
2.  **Kickoff Method**:
    
    *   The `TripCrew` class has an additional `kickoff` method that serves as an adapter between uAgents messages and the CrewAI system.
    *   It extracts parameters from the input dictionary and passes them to the actual execution method.
3.  **Parameter Schema**:
    
    *   A `query_params` schema is defined to validate and structure inputs to the CrewAI system.
    *   This allows for better error handling and client guidance when using the agent.
4.  **Example Query**:
    
    *   An example query is provided to help users understand the expected input format.
    *   This improves usability when interacting with the agent through chat protocols.

Specialized Agents in the Trip Planner[â€‹](#specialized-agents-in-the-trip-planner "Direct link to Specialized Agents in the Trip Planner")
------------------------------------------------------------------------------------------------------------------------------------------

The trip planning system uses three specialized agents, defined in `trip_agents.py`:

1.  **City Selection Agent**: Analyzes client preferences to select the optimal city to visit
2.  **Local Expert**: Identifies authentic local experiences and hidden gems
3.  **Travel Concierge**: Creates detailed itineraries and plans logistics

Each agent is assigned specific tasks through the `trip_tasks.py` file:

1.  **Identify Task**: Determines the best city based on client preferences
2.  **Gather Task**: Collects detailed information about activities and attractions
3.  **Plan Task**: Creates a comprehensive itinerary with transportation details

Interacting with the Trip Planner[â€‹](#interacting-with-the-trip-planner "Direct link to Interacting with the Trip Planner")
---------------------------------------------------------------------------------------------------------------------------

Once registered as a uAgent, you can interact with the CrewAI trip planner using any uAgent client:

    from datetime import datetime, timezonefrom uuid import uuid4from uagents import Agent, Protocol, Context#import the necessary components from the chat protocolfrom uagents_core.contrib.protocols.chat import (    ChatAcknowledgement,    ChatMessage,    TextContent,    chat_protocol_spec,)# Initialise agent2agent2 = Agent(name="client_agent",               port = 8082,               mailbox=True,               seed="client agent testing seed"               )# Initialize the chat protocolchat_proto = Protocol(spec=chat_protocol_spec)langgraph_agent_address = "agent1q0zyxrneyaury3f5c7aj67hfa5w65cykzplxkst5f5mnyf4y3em3kplxn4t"# Startup Handler - Print agent details@agent2.on_event("startup")async def startup_handler(ctx: Context):    # Print agent details    ctx.logger.info(f"My name is {ctx.agent.name} and my address is {ctx.agent.address}")    # Send initial message to agent2    initial_message = ChatMessage(        timestamp=datetime.now(timezone.utc),        msg_id=uuid4(),        content=[TextContent(type="text", text="Plan a trip for me from london to paris starting on 22nd of April 2025 and I am interested in a mountains beaches and history")]    )    await ctx.send(langgraph_agent_address, initial_message)# Message Handler - Process received messages and send acknowledgements@chat_proto.on_message(ChatMessage)async def handle_message(ctx: Context, sender: str, msg: ChatMessage):    for item in msg.content:        if isinstance(item, TextContent):            # Log received message            ctx.logger.info(f"Received message from {sender}: {item.text}")                        # Send acknowledgment            ack = ChatAcknowledgement(                timestamp=datetime.now(timezone.utc),                acknowledged_msg_id=msg.msg_id            )            await ctx.send(sender, ack)            # Acknowledgement Handler - Process received acknowledgements@chat_proto.on_message(ChatAcknowledgement)async def handle_acknowledgement(ctx: Context, sender: str, msg: ChatAcknowledgement):    ctx.logger.info(f"Received acknowledgement from {sender} for message: {msg.acknowledged_msg_id}")# Include the protocol in the agent to enable the chat functionality# This allows the agent to send/receive messages and handle acknowledgements using the chat protocolagent2.include(chat_proto, publish_manifest=True)if __name__ == '__main__':    agent2.run()

Also you can interact with crewai agent using `Chat with Agent` button on agent's profile.

Benefits of the uAgents Integration[â€‹](#benefits-of-the-uagents-integration "Direct link to Benefits of the uAgents Integration")
---------------------------------------------------------------------------------------------------------------------------------

Integrating CrewAI with uAgents provides several significant advantages:

*   **Network Communication**: Enables remote access to your CrewAI system over networks
*   **Structured Inputs**: Validates inputs through a defined parameter schema
*   **Persistent Mailbox**: Allows asynchronous communication with message storage
*   **Agentverse Integration**: Makes your CrewAI system discoverable in the agent ecosystem
*   **NL Processing**: Optional AI agent integration for processing natural language queries

Getting Started[â€‹](#getting-started "Direct link to Getting Started")
---------------------------------------------------------------------

1.  Clone the [Trip Planner repository](https://github.com/abhifetch/crewai-example/tree/main/trip_planner)
    
2.  Install dependencies:
    
        pip install uagents==0.22.3 "crewai[tools]"==0.105.0 uagents-adapter==0.4.1 python-dotenv==1.0.0 langchain_openai==0.2.13
    
    Or use the provided requirements.txt:
    
        pip install -r requirements.txt
    
3.  Set up your environment variables:
    
        OPENAI_API_KEY=your_openai_keyAGENTVERSE_API_KEY=your_agentverse_keyAGENT_SEED=your_agent_seed_phrase
    
4.  Run the CrewAI trip planner with uAgents adapter:
    
        cd crewai-example/trip_plannerpython main_uagents.py
    
5.  Check the inspector link from agent ouput and go to agent profile, or directly in your [local agents](https://agentverse.ai/agents/local) look for crewai agent.
    
6.  You can communicate to this agent using below options
    

*   Click on chat with agent button and ask it to plan a trip.

*   You can also try to interact with client agent shown in section (above)\[#\].

Expected Outputs[â€‹](#expected-outputs "Direct link to Expected Outputs")
------------------------------------------------------------------------

When running the examples, you should expect to see outputs similar to these:

### Standard CrewAI (`main.py`)[â€‹](#standard-crewai-mainpy "Direct link to standard-crewai-mainpy")

    ## Welcome to Trip Planner Crew-------------------------------From where will you be traveling from?> New YorkWhat are the cities options you are interested in visiting?> Paris, Rome, BarcelonaWhat is the date range you are interested in traveling?> June 10-20, 2023What are some of your high level interests and hobbies?> Food, art, architecture, and history[City Selection Specialist] I'll analyze which city would be the best fit based on the traveler's preferences...Working on: Analyze the traveler's preferences and determine which city from the options would be the best fit...[... search and reasoning details ...]########################## Here is you Trip Plan######################### PARIS: 3-DAY FOOD & ART JOURNEY*A curated itinerary for experiencing the best of Parisian cuisine and artistic treasures*## RECOMMENDED ACCOMMODATIONSLe Marais district or Saint-Germain-des-PrÃ©s would be ideal locations, offering central positioning with charming atmosphere and proximity to key attractions.[... detailed itinerary continues ...]

### uAgents Integration (`main_uagents.py`)[â€‹](#uagents-integration-main_uagentspy "Direct link to uagents-integration-main_uagentspy")

First terminal:

    (venv) abhi@Fetchs-MacBook-Pro test examples % python3 trip_planner/main_uagents.pyINFO:     [Trip Planner Crew AI Agent adapters]: Starting agent with address: agent1q2sgs58jzw70e8vvsrlx8k3yukdqc9gwkhp8p7q6tslcxhy0eqtxyq4fv07INFO:     [Trip Planner Crew AI Agent adapters]: Agent 'Trip Planner Crew AI Agent adapters' started with address: agent1q2sgs58jzw70e8vvsrlx8k3yukdqc9gwkhp8p7q6tslcxhy0eqtxyq4fv07INFO:     [Trip Planner Crew AI Agent adapters]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8080&address=agent1q2sgs58jzw70e8vvsrlx8k3yukdqc9gwkhp8p7q6tslcxhy0eqtxyq4fv07INFO:     [Trip Planner Crew AI Agent adapters]: Starting server on http://0.0.0.0:8080 (Press CTRL+C to quit)INFO:     [Trip Planner Crew AI Agent adapters]: Starting mailbox client for https://agentverse.aiINFO:     [Trip Planner Crew AI Agent adapters]: Mailbox access token acquiredConnecting agent 'Trip Planner Crew AI Agent adapters' to Agentverse...INFO:     [mailbox]: Successfully registered as mailbox agent in AgentverseSuccessfully connected agent 'Trip Planner Crew AI Agent adapters' to AgentverseUpdating agent 'Trip Planner Crew AI Agent adapters' README on Agentverse...Successfully updated agent 'Trip Planner Crew AI Agent adapters' README on AgentverseCrewAI agent registration result: Agent 'Trip Planner Crew AI Agent adapters' registered with address: agent1q2sgs58jzw70e8vvsrlx8k3yukdqc9gwkhp8p7q6tslcxhy0eqtxyq4fv07 with mailbox (Parameters: origin, cities, date_range, interests)INFO:     [mailbox]: Successfully registered as mailbox agent in AgentverseINFO:     [Trip Planner Crew AI Agent adapters]: Got a message from agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49INFO:     [Trip Planner Crew AI Agent adapters]: Received message model digest: timestamp=datetime.datetime(2025, 4, 21, 10, 13, 39, 989489, tzinfo=datetime.timezone.utc) msg_id=UUID('7930acf1-b16e-4b20-896b-7d801763eaa6') content=[TextContent(type='text', text='Plan a trip for me from london to paris starting on 22nd of April 2025 and I am interested in a mountains beaches and history')]INFO:     [Trip Planner Crew AI Agent adapters]: Got a text message from agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49: Plan a trip for me from london to paris starting on 22nd of April 2025 and I am interested in a mountains beaches and historyINFO:     [Trip Planner Crew AI Agent adapters]: Using crew object: <__main__.TripCrew object at 0x12c1f79d0>INFO:     [Trip Planner Crew AI Agent adapters]: Extracting parameters using keys: ['origin', 'cities', 'date_range', 'interests']INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"INFO:     [Trip Planner Crew AI Agent adapters]: Extracted parameters: {'origin': 'london', 'cities': 'paris', 'date_range': '22nd of April 2025', 'interests': 'mountains beaches and history'}INFO:     [Trip Planner Crew AI Agent adapters]: Running crew with extracted parametersâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Crew Execution Started â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®â”‚                                                                                                                                      â”‚â”‚  Crew Execution Started                                                                                                              â”‚â”‚  Name: crew                                                                                                                          â”‚â”‚  ID: 1462f3ae-5ce4-4ea3-b1af-5639aac04dd2                                                                                            â”‚â”‚                                                                                                                                      â”‚â”‚                                                                                                                                      â”‚â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ğŸš€ Crew: crewâ””â”€â”€ ğŸ“‹ Task: c181e31b-6b7f-4471-ab8f-fa5f06078365       Status: Executing Task...[... crew execution continues ...]

### Client Agent (`client_agent.py`)[â€‹](#client-agent-client_agentpy "Direct link to client-agent-client_agentpy")

Second terminal:

    (venv) abhi@Fetchs-MacBook-Pro crewai-example % python3 trip_planner/client_agent.py INFO:     [client_agent]: Starting agent with address: agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49INFO:     [client_agent]: My name is client_agent and my address is agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49INFO:     [client_agent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8082&address=agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49INFO:     [client_agent]: Starting server on http://0.0.0.0:8082 (Press CTRL+C to quit)INFO:     [client_agent]: Starting mailbox client for https://agentverse.aiINFO:     [client_agent]: Manifest published successfully: AgentChatProtocolINFO:     [client_agent]: Mailbox access token acquiredINFO:     [client_agent]: Received acknowledgement from agent1q2sgs58jzw70e8vvsrlx8k3yukdqc9gwkhp8p7q6tslcxhy0eqtxyq4fv07 for message: 7930acf1-b16e-4b20-896b-7d801763eaa6INFO:     [uagents.registration]: Registration on Almanac API successfulINFO:     [uagents.registration]: Almanac contract registration is up to date![... detailed itinerary continues ...]

This example demonstrates how uAgents adapters can bring collaborative AI agent systems into a networked environment, making complex workflows accessible through standardized messaging protocols.</content>
</page>

<page>
  <title>Stripe Payment Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/examples/integrations/stripe-integration</url>
  <content>Build a Stripe Payment Agent with ASI:One and uAgents
-----------------------------------------------------

This guide demonstrates how to create an intelligent Stripe payment agent using ASI:One for natural language processing and the official Stripe API, deployed via uAgents.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

The Stripe ASI:One Agent provides:

*   **AI-powered payment link creation** with natural-language requests
*   **Direct Stripe API** integration (products, prices, payment links)
*   **ASI:One reasoning** for natural language understanding
*   **uAgents deployment** for 24/7 availability

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

Before you begin, ensure you have:

*   **Python 3.11+** installed
*   **Stripe account** with API keys ([Get sandbox keys](https://dashboard.stripe.com/test/apikeys))
*   **ASI:One API key** ([Get API key](https://asi1.ai/dashboard/api-keys))

Installation[â€‹](#installation "Direct link to Installation")
------------------------------------------------------------

### 1\. Create Project Directory[â€‹](#1-create-project-directory "Direct link to 1. Create Project Directory")

    mkdir stripe_asi_agentcd stripe_asi_agent

### 2\. Set Up Virtual Environment[â€‹](#2-set-up-virtual-environment "Direct link to 2. Set Up Virtual Environment")

    python -m venv venvsource venv/bin/activate  # On Windows: venv\Scripts\activate

### 3\. Install Dependencies[â€‹](#3-install-dependencies "Direct link to 3. Install Dependencies")

Create a `requirements.txt` file:

    # Core uAgentsuagents==0.22.5# Stripestripe==12.2.0# Environment and utilitiespython-dotenv>=1.0.0requests>=2.31.0

Install the packages:

    pip install -r requirements.txt

Environment Setup[â€‹](#environment-setup "Direct link to Environment Setup")
---------------------------------------------------------------------------

### 1\. Create Environment File[â€‹](#1-create-environment-file "Direct link to 1. Create Environment File")

Create a `.env` file in your project directory:

    # Stripe ConfigurationSTRIPE_SECRET_KEY=sk_test_your_stripe_secret_key_here# ASI:One ConfigurationASI_API_KEY=your_asi_api_key_here

### 2\. Get Your API Keys[â€‹](#2-get-your-api-keys "Direct link to 2. Get Your API Keys")

**Stripe Keys:**

1.  Go to [Stripe Dashboard](https://dashboard.stripe.com/test/apikeys)
2.  Copy your **Secret key** (starts with `sk_test_`)

**ASI:One Key:**

1.  Go to [ASI:One Platform](https://innovationlab.fetch.ai/resources/docs/asione/asi1-mini-getting-started)
2.  Create a new API key

Create the Agent[â€‹](#create-the-agent "Direct link to Create the Agent")
------------------------------------------------------------------------

Create `stripe_llm_agent.py`:

    #!/usr/bin/env python3"""Stripe Payment-Link Agent using ASI:One function-calling and uAgents.Send any message like:    "Create a payment link for a $1200 laptop"The LLM extracts {product, amount} via structured output, we callStripe to create Product, Price, and PaymentLink, then return the URL."""import os, json, stripefrom uuid import uuid4from datetime import datetime, timezonefrom dotenv import load_dotenvimport requestsfrom uagents import Agent, Context, Protocolfrom uagents_core.contrib.protocols.chat import (    ChatAcknowledgement,    ChatMessage,    TextContent,    chat_protocol_spec,)# ---------------------------------------------------------------------------# Environment# ---------------------------------------------------------------------------load_dotenv()stripe.api_key = os.getenv("STRIPE_SECRET_KEY")ASI_API_KEY = os.getenv("ASI_API_KEY")if not stripe.api_key or not ASI_API_KEY:    raise RuntimeError("STRIPE_SECRET_KEY and ASI_API_KEY must be set in environment")# ASI:One API settingsBASE_URL = "https://api.asi1.ai/v1"headers = {    "Authorization": f"Bearer {ASI_API_KEY}",    "Content-Type": "application/json"}# ---------------------------------------------------------------------------# ASI:One tool / function definition# ---------------------------------------------------------------------------tool_def = {    "type": "function",    "function": {        "name": "create_payment_link",        "description": "Create a one-off Stripe payment link",        "parameters": {            "type": "object",            "properties": {                "product": {                    "type": "string",                    "description": "Product or service name"                },                "amount": {                    "type": "number",                    "description": "Price as a decimal number"                },                "currency": {                    "type": "string",                    "description": "ISO currency code (e.g. usd, inr, eur). Default usd",                    "enum": ["usd", "inr", "eur", "gbp", "aud", "cad"]                }            },            "required": ["product", "amount"],            "additionalProperties": False        },        "strict": True    }}def is_payment_related(text: str) -> bool:    """Check if the message is related to creating a payment link."""    prompt = f"""Analyze if the following message is about creating a payment link or processing a payment.    Return only 'true' if it's payment-related, 'false' otherwise.        Message: {text}        Examples of payment-related messages:    - "Create a payment link for a $100 product"    - "I want to sell my laptop for $500"    - "Generate a payment link for consulting services"        Examples of non-payment messages:    - "Hello, how are you?"    - "What's the weather like?"    - "Tell me a joke"    """        payload = {        "model": "asi1-mini",        "messages": [{"role": "user", "content": prompt}],        "temperature": 0    }        response = requests.post(        f"{BASE_URL}/chat/completions",        headers=headers,        json=payload    )    response.raise_for_status()    return response.json()["choices"][0]["message"]["content"].strip().lower() == 'true'def parse_with_llm(user_text: str):    """Returns dict with product & amount extracted by LLM."""    payload = {        "model": "asi1-mini",        "messages": [{"role": "user", "content": user_text}],        "tools": [tool_def],        "tool_choice": {            "type": "function",            "function": {"name": "create_payment_link"}        },        "temperature": 0    }        response = requests.post(        f"{BASE_URL}/chat/completions",        headers=headers,        json=payload    )    response.raise_for_status()    response_json = response.json()        # Handle tool calls from ASI:One response    tool_calls = response_json["choices"][0]["message"].get("tool_calls", [])    if not tool_calls:        raise ValueError("No payment details found in the message")            tool_call = tool_calls[0]    return json.loads(tool_call["function"]["arguments"])# ---------------------------------------------------------------------------# Stripe helper# ---------------------------------------------------------------------------def create_payment_link(product: str, amount_usd: float, currency: str):    prod = stripe.Product.create(name=product)    price = stripe.Price.create(unit_amount=int(amount_usd * 100), currency=currency, product=prod.id)    link = stripe.PaymentLink.create(line_items=[{"price": price.id, "quantity": 1}])    return link# ---------------------------------------------------------------------------# uAgents setup# ---------------------------------------------------------------------------agent = Agent(name="stripe_llm_agent", port=<"ANY OPEN PORT ON YOUR MACHINE">, mailbox=True, seed=<"RANDOM STRING SECRERT SEED PHASE">)chat_proto = Protocol(spec=chat_protocol_spec)@chat_proto.on_message(ChatMessage)async def handler(ctx: Context, sender: str, msg: ChatMessage):    # ack    await ctx.send(sender, ChatAcknowledgement(timestamp=datetime.now(timezone.utc), acknowledged_msg_id=msg.msg_id))    text = " ".join(c.text for c in msg.content if hasattr(c, "text")).strip()    if not text:        return    try:        # First check if the message is payment-related        if not is_payment_related(text):            reply = (                "âŒ I can only help with creating payment links. "                "Please ask me to create a payment link for a product or service.\n\n"                "Example: 'Create a payment link for a $100 laptop'"            )        else:            args = parse_with_llm(text)            currency = args.get("currency", "usd").lower()            link = create_payment_link(args["product"], args["amount"], currency)            reply = (                f"âœ… Product Created: {args['product']} - {args['amount']} {currency.upper()}\n"                f"âœ… Payment Link Generated: {link.url}\n\n"                "---\n*Powered by Stripe + ASI:One + uAgents*"            )    except Exception as e:        reply = f"âŒ Error: {e}"    await ctx.send(sender, ChatMessage(timestamp=datetime.now(timezone.utc), msg_id=uuid4(), content=[TextContent(type="text", text=reply)]))# Additional handler required by AgentChatProtocol spec@chat_proto.on_message(ChatAcknowledgement)async def handle_ack(ctx: Context, sender: str, ack: ChatAcknowledgement):    ctx.logger.debug(f"Ack from {sender} for {ack.acknowledged_msg_id}")# include protocols after all handlers are registeredagent.include(chat_proto, publish_manifest=True)if __name__ == "__main__":    agent.run()

Understanding the Code[â€‹](#understanding-the-code "Direct link to Understanding the Code")
------------------------------------------------------------------------------------------

### Key Components[â€‹](#key-components "Direct link to Key Components")

**1\. ASI:One Integration**

    tool_def = {    "type": "function",    "function": {        "name": "create_payment_link",        "description": "Create a one-off Stripe payment link",        "parameters": {            "type": "object",            "properties": {                "product": {"type": "string"},                "amount": {"type": "number"},                "currency": {"type": "string"}            },            "required": ["product", "amount"]        }    }}

*   Uses ASI:One for natural language understanding
*   Defines structured function calling
*   Handles payment-related message detection

**2\. Stripe Integration**

    def create_payment_link(product: str, amount_usd: float, currency: str):    prod = stripe.Product.create(name=product)    price = stripe.Price.create(unit_amount=int(amount_usd * 100), currency=currency, product=prod.id)    link = stripe.PaymentLink.create(line_items=[{"price": price.id, "quantity": 1}])    return link

*   Direct Stripe API integration
*   Creates products, prices, and payment links
*   Handles currency conversion

**3\. uAgents Setup**

    agent = Agent(name="stripe_llm_agent", port=8032, mailbox=True)chat_proto = Protocol(spec=chat_protocol_spec)

*   Sets up the agent with mailbox functionality
*   Handles message routing and acknowledgments
*   Provides 24/7 availability

Running the Agent[â€‹](#running-the-agent "Direct link to Running the Agent")
---------------------------------------------------------------------------

### 1\. Start the Agent[â€‹](#1-start-the-agent "Direct link to 1. Start the Agent")

With your virtual environment activated and `.env` file configured:

    python stripe_llm_agent.py

### 2\. Expected Output[â€‹](#2-expected-output "Direct link to 2. Expected Output")

    ğŸƒ Agent running â€” press Ctrl+C to stop

Usage Examples[â€‹](#usage-examples "Direct link to Usage Examples")
------------------------------------------------------------------

### Example 1: Basic Payment Request[â€‹](#example-1-basic-payment-request "Direct link to Example 1: Basic Payment Request")

**Input:**

    "I want to buy a laptop for $1200"

**Expected Response:**

    âœ… Product Created: Laptop - 1200 USDâœ… Payment Link Generated: https://buy.stripe.com/test_XXXXXXXXX---*Powered by Stripe + ASI:One + uAgents*

### Example 2: Membership Payment Link[â€‹](#example-2-membership-payment-link "Direct link to Example 2: Membership Payment Link")

**Input:**

    "Generate a payment link for $49 annual membership"

**Expected Response:**

    âœ… Product Created: Annual Membership - 49 USDâœ… Payment Link Generated: https://buy.stripe.com/test_XXXXXXXXX---*Powered by Stripe + ASI:One + uAgents*

Agent Capabilities[â€‹](#agent-capabilities "Direct link to Agent Capabilities")
------------------------------------------------------------------------------

### Payment Operations[â€‹](#payment-operations "Direct link to Payment Operations")

*   âœ… **Create Products** - Dynamic product creation
*   âœ… **Generate Payment Links** - Secure Stripe checkout URLs
*   âœ… **Currency Support** - Multiple currency options (USD, INR, EUR, GBP, AUD, CAD)

### AI Features[â€‹](#ai-features "Direct link to AI Features")

*   ğŸ¤– **Natural Language Processing** - Understands complex payment requests
*   ğŸ§  **Intelligent Reasoning** - Uses ASI:One for context understanding
*   ğŸ¯ **Message Classification** - Detects payment-related queries
*   ğŸ”„ **Error Recovery** - Handles API failures gracefully

### Integration Features[â€‹](#integration-features "Direct link to Integration Features")

*   ğŸŒ **uAgents Deployment** - 24/7 availability
*   ğŸ“¬ **Mailbox Communication** - Agent-to-agent messaging
*   ğŸ”— **API Integration** - Real Stripe API calls with sandbox safety
*   ğŸ“Š **Logging & Monitoring** - Built-in error tracking

This guide demonstrates the power of combining ASI:One's natural language processing with Stripe's Payment Links for autonomous agent commerce.

A live instance of this payment-link agent is running on Agentverse: [STRIPE TESTING AGENT](https://agentverse.ai/agents/details/agent1qttcp5xanrqv2u304ds2azwdsj5z8eh7h0jtdhl85qz3su49et7869c2v33/profile)

### Chat screen[â€‹](#chat-screen "Direct link to Chat screen")

### Payment Window[â€‹](#payment-window "Direct link to Payment Window")

note

**Note:** If you want to test it type in the card number as `4242 4242 4242 4242`.

### You can even test this on ASI:One LLM.[â€‹](#you-can-even-test-this-on-asi-llm "Direct link to you-can-even-test-this-on-asi-llm")

Open the [ASI:One](https://asi1.ai/chat) and enable agents to ask a question.

    can you check for an stripe agent which can generate a payment link for 1400 indian rupees for iphone 15 cover

note

**Note:** These are the test sandbox version, no money will be deducted and dont put in your original card details.</content>
</page>

<page>
  <title>Foundation Core Concepts | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/concepts-ai-agents/foundation-core-concepts</url>
  <content>### Introduction to AI Agents[â€‹](#introduction-to-ai-agents "Direct link to Introduction to AI Agents")

In the rapidly evolving landscape of artificial intelligence, AI agents represent a significant advancement over traditional software applications, offering more flexibility, adaptability, and intelligence in tackling complex tasks across various domains.

At their core, agents are software entities designed to perform autonomous actions by:

1.  Observing their environment through various inputs (digital or physical)
2.  Processing, analyzing, and reasoning about information using advanced algorithms and large language models
3.  Making decisions and taking actions, often by leveraging external tools and APIs
4.  Learning from outcomes and adapting their behavior over time
5.  Utilizing memory to retain information and improve performance
6.  Engaging in self-reflection, evaluation, and course correction

### The Evolution of AI Systems[â€‹](#the-evolution-of-ai-systems "Direct link to The Evolution of AI Systems")

To understand AI agents, it's crucial to recognize the progression of AI systems:

1.  **Traditional Applications**
    
    *   Fixed logic and predefined rules
    *   Limited or no adaptation
    *   Direct input-to-output mapping
    *   Example : Rule-based expert systems
2.  **AI-Enhanced Applications**
    
    *   Foundation model integration (LLMs, neural networks)
    *   Task-specific intelligence, guided learning abilities, Human-directed operations
    *   Limited context awareness
    *   Example : Modern Chatbots, Specialized AI tools (image generators, coding assistants like cursor,windsurf)
3.  **Agentic Systems**
    
    *   Capable of taking autonomous decisions with or without human in the loop
    *   Multi-step planning and execution
    *   Dynamic tool discovery and usage, self-directed learning, continuous context awareness
    *   Example : Operator released by OpenAI, Manus AI, Self-driving cars

### Understanding System Types[â€‹](#understanding-system-types "Direct link to Understanding System Types")

AI systems can be categorized into two primary types:

1.  **AI Workflows:** These are predefined sequences where LLMs and other tools are orchestrated using explicit code paths. They follow structured logic and operate with a defined start and end point.
2.  **AI Agents:** These are more dynamic, allowing LLMs to take control of their processes and tool usage, making autonomous decisions on how to accomplish a task.

While the term "AI agent" is often used interchangeably, many practical applications don't need full agentic behavior. Instead, structured workflows are sufficient for most tasks, offering better control and predictability.

### Agentic Workflow and Degree of Autonomy[â€‹](#agentic-workflow-and-degree-of-autonomy "Direct link to Agentic Workflow and Degree of Autonomy")

Since full autonomy is neither possible (in majority of the systems) nor needed in most practical applications, the term 'Agentic Workflow' is gaining popularity as it combines the benefits of structured workflows with the flexibility of AI agents. This hybrid approach allows for more dynamic decision-making within a controlled framework, striking a balance between autonomy and predictability.

Agentic workflows represent a middle ground where AI agents operate within defined processes but have the ability to make decisions and adapt to changing circumstances. They leverage the strengths of both AI workflows and agents by:

1.  Providing a structured sequence of tasks for consistency and control
2.  Allowing AI agents to make autonomous decisions within these sequences
3.  Enabling dynamic problem-solving and adaptation to complex scenarios
4.  Maintaining oversight and predictability for critical business processes

This approach is particularly useful for tasks that require some level of flexibility but still need to operate within certain boundaries or comply with specific rules. As businesses seek to optimize their operations while managing risks, agentic workflows offer a practical solution that combines the efficiency of automation with the intelligence of AI agents.

The term 'AI agent' is widely used in the industry and by startups, often without a clear, universal definition. In practice, the autonomy of these so-called agents falls on a spectrum rather than being a binary classification. There is no definitive technical measure of autonomy, which leads to varying interpretations and implementations across different systems.

This spectrum of autonomy can range from:

1.  Highly structured workflows with minimal decision-making capabilities
2.  Semi-autonomous systems that can make decisions within predefined parameters
3.  More flexible agents that can adapt their approach based on context
4.  Highly autonomous systems that can formulate and pursue their own goals within a given domain

The degree of autonomy granted to an AI system often depends on factors such as:

*   The complexity of the task
*   The potential risks involved
*   The need for human oversight
*   The capabilities of the underlying AI technologies

As the field evolves, we may see more standardized ways to measure and describe the level of autonomy in AI systems. For now, it's important to understand that when someone claims to use 'AI agents', the actual level of autonomy can vary significantly, and it's crucial to delve deeper into the specific capabilities and limitations of each system.

This nuanced understanding of autonomy reinforces the value of agentic workflows, as they offer a flexible framework that can accommodate various degrees of AI decision-making while maintaining necessary control structures.

### Three Pillars of Agentic Workflows[â€‹](#three-pillars-of-agentic-workflows "Direct link to Three Pillars of Agentic Workflows")

The effectiveness of agentic workflows is based on three key elements.

1.  **Autonomy**: Handling tasks with minimal human input.
2.  **Adaptability**: Adjusting to unique business needs and changing conditions.
3.  **Optimization**: Continuously improving through machine learning.

### Implementation Challenges[â€‹](#implementation-challenges "Direct link to Implementation Challenges")

While the benefits are significant, it's important to note that implementing and managing these workflows can be complex. This complexity reinforces the need for a nuanced approach to autonomy and careful consideration of the specific use case and organizational context.

#### AI Workflow[â€‹](#ai-workflow "Direct link to AI Workflow")

*   Combine LLMs with predefined processes
*   Follow structured but flexible paths
*   Best for: Complex but predictable tasks where:
    *   Process steps are well-understood and consistent
    *   Predictability is more important than flexibility
    *   You need tight control over execution
    *   Performance and reliability are crucial
*   Example:
    
        # AI workflow exampleclass StockAnalysisWorkflow:    def analyze(self, stock_symbol):        # Uses LLM but in FIXED order:        # Always: price â†’ news â†’ recommendation        # Can't skip steps or change order                price_analysis = llm.analyze(f"Analyze {stock_symbol} price trends")        news_analysis = llm.analyze(f"Analyze {stock_symbol} recent news")        return llm.recommend(price_analysis, news_analysis)
    

#### AI Agents[â€‹](#ai-agents "Direct link to AI Agents")

*   Dynamically direct their own processes
*   Maintain control over task execution
*   Best for: Dynamic planning, adaptive execution, goal-oriented behavior where:
    *   Tasks require dynamic decision-making
    *   Problems have multiple valid solution paths
    *   Flexibility and adaptation are crucial
    *   Complex tool interactions are needed
    *   Tasks benefit from maintaining context
*   Example:
    
        # AI agent exampleclass StockAnalysisAgent:    def analyze(self, stock_symbol):        # LLM DECIDES everything:        # - What to analyze first (price? news? competitors?)        # - Whether to dig deeper into any area        # - When analysis is sufficient        # Can adapt strategy based on what it finds                strategy = llm.decide(f"How should we analyze {stock_symbol}?")        while not self.analysis_complete():            next_step = llm.decide("What should we analyze next?")            findings = self.execute_step(next_step)            if findings.need_different_approach:                strategy = llm.revise_strategy(findings)
    

The core difference is that AI agents have:

**Genuine Autonomy**

*   Not just following predefined steps with decision points
*   Actually reasoning about what actions to take
*   Ability to discover and adapt strategies

**Strategic Flexibility**

*   Can handle unexpected situations
*   Doesn't just choose from predefined options
*   Creates novel approaches to problems

**Contextual Understanding**

*   Understands the implications of its actions
*   Can reason about tool capabilities
*   Maintains meaningful context about its goals and progress

**Dynamic Goal Management**

*   Can reformulate goals when needed
*   Understands when to abandon or modify objectives
*   Can handle competing or conflicting goals

### Core Components of an Agent[â€‹](#core-components-of-an-agent "Direct link to Core Components of an Agent")

A generic agent architecture, consists of several key components:

#### 1\. Model Layer[â€‹](#1-model-layer "Direct link to 1. Model Layer")

*   Central decision-making engine
*   Processes input and context
*   Generates reasoning and plans

#### 2\. Orchestration Layer[â€‹](#2-orchestration-layer "Direct link to 2. Orchestration Layer")

*   Manages the execution flow
*   Coordinates tool usage
*   Monitors progress towards goals

#### 3\. Memory System[â€‹](#3-memory-system "Direct link to 3. Memory System")

*   Short-term working memory
*   Long-term knowledge storage
*   Context retention

#### 4\. Tool Integration[â€‹](#4-tool-integration "Direct link to 4. Tool Integration")

*   External API connections
    
*   Data processing capabilities
    
*   Action execution interfaces
    

This diagram shows a more detailed "Agent Runtime Environment" with three main layers that work together:

**Orchestration Layer (Top)**

*   Contains the high-level control components:
    *   Profile & Goals: Defines the agent's objectives and constraints
    *   Memory System: Implements both short and long-term memory
    *   Reasoning Engine: Coordinates decision-making and orchestrates the flow between components

**Tools Layer (Middle)**

*   Breaks down tool integration into three specific categories:
    *   API Tools: Interfaces for external API connections
    *   Data Processing: Tools for handling and transforming data
    *   External Services: Integration with third-party services
*   This layer implements the "Tool Integration" component from the core architecture

**Model Layer (Bottom)**

*   Contains three specialized components:
    *   Large Language Model: The foundation model for understanding and generation
    *   Planning System: Handles task decomposition and strategy
    *   Response Generation: Manages the creation of outputs

The arrows in the diagram show the information flow:

*   The Orchestration Layer controls the overall process flow
*   The Tools Layer acts as an intermediary between orchestration and models
*   There's a feedback loop from the Model Layer back to the Orchestration Layer, showing how the system can iteratively refine its responses

This implementation provides more concrete details about how the four core components (Model, Orchestration, Memory, and Tools) work together in a practical system.

### General Criteria for Agency[â€‹](#general-criteria-for-agency "Direct link to General Criteria for Agency")

Before implementing an agent, evaluate if your system really needs true agency by checking these criteria:

**Decision Autonomy**

*   Can the system choose different paths based on context?
*   Does it make meaningful decisions about tool usage?
*   Can it adapt its strategy during execution?

**State Management**

*   Does it maintain meaningful state?
*   Can it use past interactions to inform decisions?
*   Does it track progress toward goals?

**Tool Integration**

*   Can it choose tools dynamically?
*   Does it understand tool capabilities?
*   Can it combine tools in novel ways?

**Goal Orientation**

*   Does it understand and work toward specific objectives?
*   Can it recognize when goals are achieved?
*   Can it adjust goals based on new information?

### ReAct Pattern[â€‹](#react-pattern "Direct link to ReAct Pattern")

The simplest AI agents typically operate using the ReAct (Reason-Action) framework, which follows a cyclical pattern. ReAct is an iterative approach that alternates between thinking and acting, combining the reasoning capabilities of large language models (LLMs) with the ability to interact with external tools and environments. The core workflow includes:

1.  **Reasoning (Thought)**: The agent analyzes the current state, objectives, and available information.
2.  **Acting (Action)**: Based on its reasoning, the agent executes specific operations or uses tools.
3.  **Observation**: The agent obtains results from its actions.
4.  **Iteration**: The cycle continues, with the agent thinking and acting based on new observations until reaching a final answer.

#### Key Components:[â€‹](#key-components "Direct link to Key Components:")

1.  **Thought**:
    
    *   Internal reasoning about the current state and objectives
    *   Analysis of available information
    *   Planning next steps and formulating strategies
2.  **Action**:
    
    *   Execution of chosen steps
    *   Tool usage and integration (e.g., calculators, search engines, APIs)
    *   Interaction with external systems or environments
3.  **Observation**:
    
    *   Gathering results from actions
    *   Analyzing outcomes
    *   Updating understanding and knowledge base
4.  **Iteration**:
    
    *   Continuous loop of Thought-Action-Observation
    *   Dynamic adjustment of plans based on new information
    *   Progress towards final goal or answer

#### Implementation and Best Practices:[â€‹](#implementation-and-best-practices "Direct link to Implementation and Best Practices:")

1.  **Prompt Engineering**: Craft a clear system prompt that defines the agent's behavior and available tools.
    
2.  **Tool Integration**: Provide the agent with access to relevant external tools and APIs to expand its capabilities.
    
3.  **Memory Management**: Implement a mechanism for the agent to retain and utilize information from previous steps.
    
4.  **Error Handling**: Design the system to gracefully handle unexpected inputs or tool failures.
    
5.  **Performance Optimization**: Balance the number of reasoning steps with action execution to maintain efficiency.
    

### Advantages of ReAct:[â€‹](#advantages-of-react "Direct link to Advantages of ReAct:")

*   Combines internal knowledge with external information gathering
*   Enables complex problem-solving through iterative reasoning and action
*   Improves transparency and interpretability of AI decision-making
*   Allows for dynamic adaptation to new information and changing scenarios

### Applications:[â€‹](#applications "Direct link to Applications:")

ReAct has shown promise in various domains, including:

*   Question answering systems
*   Task planning and execution
*   Data analysis and interpretation
*   Decision-making in complex environments

By implementing the ReAct pattern, developers can create more versatile and capable AI agents that can handle a wide range of tasks requiring both reasoning and interaction with external resources.</content>
</page>

<page>
  <title>Foundation Core Concepts | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/concepts-ai-agents/foundation-core-concepts</url>
  <content>### Introduction to AI Agents[â€‹](#introduction-to-ai-agents "Direct link to Introduction to AI Agents")

In the rapidly evolving landscape of artificial intelligence, AI agents represent a significant advancement over traditional software applications, offering more flexibility, adaptability, and intelligence in tackling complex tasks across various domains.

At their core, agents are software entities designed to perform autonomous actions by:

1.  Observing their environment through various inputs (digital or physical)
2.  Processing, analyzing, and reasoning about information using advanced algorithms and large language models
3.  Making decisions and taking actions, often by leveraging external tools and APIs
4.  Learning from outcomes and adapting their behavior over time
5.  Utilizing memory to retain information and improve performance
6.  Engaging in self-reflection, evaluation, and course correction

### The Evolution of AI Systems[â€‹](#the-evolution-of-ai-systems "Direct link to The Evolution of AI Systems")

To understand AI agents, it's crucial to recognize the progression of AI systems:

1.  **Traditional Applications**
    
    *   Fixed logic and predefined rules
    *   Limited or no adaptation
    *   Direct input-to-output mapping
    *   Example : Rule-based expert systems
2.  **AI-Enhanced Applications**
    
    *   Foundation model integration (LLMs, neural networks)
    *   Task-specific intelligence, guided learning abilities, Human-directed operations
    *   Limited context awareness
    *   Example : Modern Chatbots, Specialized AI tools (image generators, coding assistants like cursor,windsurf)
3.  **Agentic Systems**
    
    *   Capable of taking autonomous decisions with or without human in the loop
    *   Multi-step planning and execution
    *   Dynamic tool discovery and usage, self-directed learning, continuous context awareness
    *   Example : Operator released by OpenAI, Manus AI, Self-driving cars

### Understanding System Types[â€‹](#understanding-system-types "Direct link to Understanding System Types")

AI systems can be categorized into two primary types:

1.  **AI Workflows:** These are predefined sequences where LLMs and other tools are orchestrated using explicit code paths. They follow structured logic and operate with a defined start and end point.
2.  **AI Agents:** These are more dynamic, allowing LLMs to take control of their processes and tool usage, making autonomous decisions on how to accomplish a task.

While the term "AI agent" is often used interchangeably, many practical applications don't need full agentic behavior. Instead, structured workflows are sufficient for most tasks, offering better control and predictability.

### Agentic Workflow and Degree of Autonomy[â€‹](#agentic-workflow-and-degree-of-autonomy "Direct link to Agentic Workflow and Degree of Autonomy")

Since full autonomy is neither possible (in majority of the systems) nor needed in most practical applications, the term 'Agentic Workflow' is gaining popularity as it combines the benefits of structured workflows with the flexibility of AI agents. This hybrid approach allows for more dynamic decision-making within a controlled framework, striking a balance between autonomy and predictability.

Agentic workflows represent a middle ground where AI agents operate within defined processes but have the ability to make decisions and adapt to changing circumstances. They leverage the strengths of both AI workflows and agents by:

1.  Providing a structured sequence of tasks for consistency and control
2.  Allowing AI agents to make autonomous decisions within these sequences
3.  Enabling dynamic problem-solving and adaptation to complex scenarios
4.  Maintaining oversight and predictability for critical business processes

This approach is particularly useful for tasks that require some level of flexibility but still need to operate within certain boundaries or comply with specific rules. As businesses seek to optimize their operations while managing risks, agentic workflows offer a practical solution that combines the efficiency of automation with the intelligence of AI agents.

The term 'AI agent' is widely used in the industry and by startups, often without a clear, universal definition. In practice, the autonomy of these so-called agents falls on a spectrum rather than being a binary classification. There is no definitive technical measure of autonomy, which leads to varying interpretations and implementations across different systems.

This spectrum of autonomy can range from:

1.  Highly structured workflows with minimal decision-making capabilities
2.  Semi-autonomous systems that can make decisions within predefined parameters
3.  More flexible agents that can adapt their approach based on context
4.  Highly autonomous systems that can formulate and pursue their own goals within a given domain

The degree of autonomy granted to an AI system often depends on factors such as:

*   The complexity of the task
*   The potential risks involved
*   The need for human oversight
*   The capabilities of the underlying AI technologies

As the field evolves, we may see more standardized ways to measure and describe the level of autonomy in AI systems. For now, it's important to understand that when someone claims to use 'AI agents', the actual level of autonomy can vary significantly, and it's crucial to delve deeper into the specific capabilities and limitations of each system.

This nuanced understanding of autonomy reinforces the value of agentic workflows, as they offer a flexible framework that can accommodate various degrees of AI decision-making while maintaining necessary control structures.

### Three Pillars of Agentic Workflows[â€‹](#three-pillars-of-agentic-workflows "Direct link to Three Pillars of Agentic Workflows")

The effectiveness of agentic workflows is based on three key elements.

1.  **Autonomy**: Handling tasks with minimal human input.
2.  **Adaptability**: Adjusting to unique business needs and changing conditions.
3.  **Optimization**: Continuously improving through machine learning.

### Implementation Challenges[â€‹](#implementation-challenges "Direct link to Implementation Challenges")

While the benefits are significant, it's important to note that implementing and managing these workflows can be complex. This complexity reinforces the need for a nuanced approach to autonomy and careful consideration of the specific use case and organizational context.

#### AI Workflow[â€‹](#ai-workflow "Direct link to AI Workflow")

*   Combine LLMs with predefined processes
*   Follow structured but flexible paths
*   Best for: Complex but predictable tasks where:
    *   Process steps are well-understood and consistent
    *   Predictability is more important than flexibility
    *   You need tight control over execution
    *   Performance and reliability are crucial
*   Example:
    
        # AI workflow exampleclass StockAnalysisWorkflow:    def analyze(self, stock_symbol):        # Uses LLM but in FIXED order:        # Always: price â†’ news â†’ recommendation        # Can't skip steps or change order                price_analysis = llm.analyze(f"Analyze {stock_symbol} price trends")        news_analysis = llm.analyze(f"Analyze {stock_symbol} recent news")        return llm.recommend(price_analysis, news_analysis)
    

#### AI Agents[â€‹](#ai-agents "Direct link to AI Agents")

*   Dynamically direct their own processes
*   Maintain control over task execution
*   Best for: Dynamic planning, adaptive execution, goal-oriented behavior where:
    *   Tasks require dynamic decision-making
    *   Problems have multiple valid solution paths
    *   Flexibility and adaptation are crucial
    *   Complex tool interactions are needed
    *   Tasks benefit from maintaining context
*   Example:
    
        # AI agent exampleclass StockAnalysisAgent:    def analyze(self, stock_symbol):        # LLM DECIDES everything:        # - What to analyze first (price? news? competitors?)        # - Whether to dig deeper into any area        # - When analysis is sufficient        # Can adapt strategy based on what it finds                strategy = llm.decide(f"How should we analyze {stock_symbol}?")        while not self.analysis_complete():            next_step = llm.decide("What should we analyze next?")            findings = self.execute_step(next_step)            if findings.need_different_approach:                strategy = llm.revise_strategy(findings)
    

The core difference is that AI agents have:

**Genuine Autonomy**

*   Not just following predefined steps with decision points
*   Actually reasoning about what actions to take
*   Ability to discover and adapt strategies

**Strategic Flexibility**

*   Can handle unexpected situations
*   Doesn't just choose from predefined options
*   Creates novel approaches to problems

**Contextual Understanding**

*   Understands the implications of its actions
*   Can reason about tool capabilities
*   Maintains meaningful context about its goals and progress

**Dynamic Goal Management**

*   Can reformulate goals when needed
*   Understands when to abandon or modify objectives
*   Can handle competing or conflicting goals

### Core Components of an Agent[â€‹](#core-components-of-an-agent "Direct link to Core Components of an Agent")

A generic agent architecture, consists of several key components:

#### 1\. Model Layer[â€‹](#1-model-layer "Direct link to 1. Model Layer")

*   Central decision-making engine
*   Processes input and context
*   Generates reasoning and plans

#### 2\. Orchestration Layer[â€‹](#2-orchestration-layer "Direct link to 2. Orchestration Layer")

*   Manages the execution flow
*   Coordinates tool usage
*   Monitors progress towards goals

#### 3\. Memory System[â€‹](#3-memory-system "Direct link to 3. Memory System")

*   Short-term working memory
*   Long-term knowledge storage
*   Context retention

#### 4\. Tool Integration[â€‹](#4-tool-integration "Direct link to 4. Tool Integration")

*   External API connections
    
*   Data processing capabilities
    
*   Action execution interfaces
    

This diagram shows a more detailed "Agent Runtime Environment" with three main layers that work together:

**Orchestration Layer (Top)**

*   Contains the high-level control components:
    *   Profile & Goals: Defines the agent's objectives and constraints
    *   Memory System: Implements both short and long-term memory
    *   Reasoning Engine: Coordinates decision-making and orchestrates the flow between components

**Tools Layer (Middle)**

*   Breaks down tool integration into three specific categories:
    *   API Tools: Interfaces for external API connections
    *   Data Processing: Tools for handling and transforming data
    *   External Services: Integration with third-party services
*   This layer implements the "Tool Integration" component from the core architecture

**Model Layer (Bottom)**

*   Contains three specialized components:
    *   Large Language Model: The foundation model for understanding and generation
    *   Planning System: Handles task decomposition and strategy
    *   Response Generation: Manages the creation of outputs

The arrows in the diagram show the information flow:

*   The Orchestration Layer controls the overall process flow
*   The Tools Layer acts as an intermediary between orchestration and models
*   There's a feedback loop from the Model Layer back to the Orchestration Layer, showing how the system can iteratively refine its responses

This implementation provides more concrete details about how the four core components (Model, Orchestration, Memory, and Tools) work together in a practical system.

### General Criteria for Agency[â€‹](#general-criteria-for-agency "Direct link to General Criteria for Agency")

Before implementing an agent, evaluate if your system really needs true agency by checking these criteria:

**Decision Autonomy**

*   Can the system choose different paths based on context?
*   Does it make meaningful decisions about tool usage?
*   Can it adapt its strategy during execution?

**State Management**

*   Does it maintain meaningful state?
*   Can it use past interactions to inform decisions?
*   Does it track progress toward goals?

**Tool Integration**

*   Can it choose tools dynamically?
*   Does it understand tool capabilities?
*   Can it combine tools in novel ways?

**Goal Orientation**

*   Does it understand and work toward specific objectives?
*   Can it recognize when goals are achieved?
*   Can it adjust goals based on new information?

### ReAct Pattern[â€‹](#react-pattern "Direct link to ReAct Pattern")

The simplest AI agents typically operate using the ReAct (Reason-Action) framework, which follows a cyclical pattern. ReAct is an iterative approach that alternates between thinking and acting, combining the reasoning capabilities of large language models (LLMs) with the ability to interact with external tools and environments. The core workflow includes:

1.  **Reasoning (Thought)**: The agent analyzes the current state, objectives, and available information.
2.  **Acting (Action)**: Based on its reasoning, the agent executes specific operations or uses tools.
3.  **Observation**: The agent obtains results from its actions.
4.  **Iteration**: The cycle continues, with the agent thinking and acting based on new observations until reaching a final answer.

#### Key Components:[â€‹](#key-components "Direct link to Key Components:")

1.  **Thought**:
    
    *   Internal reasoning about the current state and objectives
    *   Analysis of available information
    *   Planning next steps and formulating strategies
2.  **Action**:
    
    *   Execution of chosen steps
    *   Tool usage and integration (e.g., calculators, search engines, APIs)
    *   Interaction with external systems or environments
3.  **Observation**:
    
    *   Gathering results from actions
    *   Analyzing outcomes
    *   Updating understanding and knowledge base
4.  **Iteration**:
    
    *   Continuous loop of Thought-Action-Observation
    *   Dynamic adjustment of plans based on new information
    *   Progress towards final goal or answer

#### Implementation and Best Practices:[â€‹](#implementation-and-best-practices "Direct link to Implementation and Best Practices:")

1.  **Prompt Engineering**: Craft a clear system prompt that defines the agent's behavior and available tools.
    
2.  **Tool Integration**: Provide the agent with access to relevant external tools and APIs to expand its capabilities.
    
3.  **Memory Management**: Implement a mechanism for the agent to retain and utilize information from previous steps.
    
4.  **Error Handling**: Design the system to gracefully handle unexpected inputs or tool failures.
    
5.  **Performance Optimization**: Balance the number of reasoning steps with action execution to maintain efficiency.
    

### Advantages of ReAct:[â€‹](#advantages-of-react "Direct link to Advantages of ReAct:")

*   Combines internal knowledge with external information gathering
*   Enables complex problem-solving through iterative reasoning and action
*   Improves transparency and interpretability of AI decision-making
*   Allows for dynamic adaptation to new information and changing scenarios

### Applications:[â€‹](#applications "Direct link to Applications:")

ReAct has shown promise in various domains, including:

*   Question answering systems
*   Task planning and execution
*   Data analysis and interpretation
*   Decision-making in complex environments

By implementing the ReAct pattern, developers can create more versatile and capable AI agents that can handle a wide range of tasks requiring both reasoning and interaction with external resources.</content>
</page>

<page>
  <title>Understanding Agentic Systems | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/concepts-ai-agents/understanding-agentic-systems</url>
  <content>### Common Misconceptions[â€‹](#common-misconceptions "Direct link to Common Misconceptions")

The term "agent" in AI is overused and often applied inconsistently, diminishing its meaning and creating confusion about the actual capabilities of AI systems. Some of the common misconceptions are explained below:

#### Misconception 1: "My application uses multiple LLM calls, so it's an agent"[â€‹](#misconception-1-my-application-uses-multiple-llm-calls-so-its-an-agent "Direct link to Misconception 1: \"My application uses multiple LLM calls, so it's an agent\"")

    # This is NOT an agent - it's a multi-step LLM applicationdef process_document(doc):    summary = llm.generate_summary(doc)    keywords = llm.extract_keywords(summary)    sentiment = llm.analyze_sentiment(summary)    return {        "summary": summary,        "keywords": keywords,        "sentiment": sentiment    }

#### Misconception 2: "I'm using tools and APIs, so it's an agent"[â€‹](#misconception-2-im-using-tools-and-apis-so-its-an-agent "Direct link to Misconception 2: \"I'm using tools and APIs, so it's an agent\"")

    # This is NOT an agent - it's an automated workflowdef analyze_stock(symbol):    price_data = stock_api.get_price(symbol)    news = news_api.get_recent_news(symbol)    analysis = llm.analyze(f"Price: {price_data}, News: {news}")    return analysis

#### Misconception 3: "Having memory makes it an agent"[â€‹](#misconception-3-having-memory-makes-it-an-agent "Direct link to Misconception 3: \"Having memory makes it an agent\"")

    # This is NOT an agent - just stateful LLM interactionclass ChatSystem:    def __init__(self):        self.conversation_history = []        def respond(self, user_input):        self.conversation_history.append(user_input)        response = llm.generate(context=self.conversation_history)        self.conversation_history.append(response)        return response

#### Misconception 4: "Using planning means its an agent"[â€‹](#misconception-4-using-planning-means-its-an-agent "Direct link to Misconception 4: \"Using planning means its an agent\"")

    # This is NOT an agent - it's structured task decompositiondef handle_task(task):    # Fixed planning template    steps = llm.break_down_task(task)    results = []    for step in steps:        result = execute_step(step)        results.append(result)    return combine_results(results)

#### Misconception 4: "Complex prompt engineering makes it an agent"[â€‹](#misconception-4-complex-prompt-engineering-makes-it-an-agent "Direct link to Misconception 4: \"Complex prompt engineering makes it an agent\"")

    # This is NOT an agent - just sophisticated promptingdef analyze_with_cot(query):    prompt = f"""    Step 1: Understand the query    {query}    Step 2: Break down the components    Step 3: Analyze each component    Step 4: Synthesize findings    """    return llm.generate(prompt)

#### Misconception 5: "Having a feedback loop makes it an agent"[â€‹](#misconception-5-having-a-feedback-loop-makes-it-an-agent "Direct link to Misconception 5: \"Having a feedback loop makes it an agent\"")

    # This is NOT an agent - just iterative refinementdef iterative_response(query, max_iterations=3):    response = initial_response(query)    for _ in range(max_iterations):        quality = evaluate_response(response)        if quality > threshold:            break        response = improve_response(response)    return response

#### Misconception 6: "The LLM performs the actions in an agent"[â€‹](#misconception-6-the-llm-performs-the-actions-in-an-agent "Direct link to Misconception 6: \"The LLM performs the actions in an agent\"")

    # Common MISCONCEPTION: People think this actually performs actionsdef incorrect_understanding():    llm_response = llm.generate("Please save this file to disk")    # The LLM can't actually save files!    # REALITY: Tools perform actions, LLM orchestratesclass PropertyAgent:    def __init__(self):        self.tools = {            'database': DatabaseTool(),            'email': EmailTool(),            'calendar': CalendarTool()        }        def handle_request(self, query):        # LLM determines what needs to be done        action_plan = llm.plan_actions(query)                # TOOLS actually perform the actions        for action in action_plan:            if action.type == "schedule_viewing":                # Calendar tool performs the actual scheduling                self.tools['calendar'].create_appointment(action.details)            elif action.type == "send_confirmation":                # Email tool performs the actual sending                self.tools['email'].send_message(action.details)

### Key Points About LLM's Role[â€‹](#key-points-about-llms-role "Direct link to Key Points About LLM's Role")

1.  **LLM's Actual Functions:**
    *   Planning and strategizing actions
    *   Reasoning about which tools to use
    *   Interpreting results from tools
    *   Generating natural language responses
2.  **Tools' Actual Functions:**
    *   File operations
    *   Database queries
    *   API calls
    *   Network requests
    *   System modifications
    *   Real-world interactions

    # Clear separation of responsibilitiesclass AgentSystem:    def process_task(self, task):        # LLM PLANS the action        plan = self.llm.create_execution_plan(task)                # TOOLS EXECUTE the action        for step in plan:            if step.requires_web_access:                result = self.web_tool.fetch_data(step.url)            elif step.requires_database:                result = self.db_tool.query(step.sql)            elif step.requires_file_operation:                result = self.file_tool.process(step.path)                            # LLM INTERPRETS results and plans next steps            next_actions = self.llm.analyze_results(result)

This misconception is particularly important because it helps explain:

*   Why tool integration is crucial for practical agent systems
*   Why agents need careful permission and capability management
*   The importance of proper tool abstraction and safety measures
*   Why LLM responses alone can't perform real-world actions

This diagram illustrate several key points:

1.  **Separation of Responsibilities**
    
    *   LLM handles planning, reasoning, and decision-making
    *   Tools perform actual real-world actions
    *   Clear boundaries between thinking and doing
2.  **Flow of Control**
    
    *   User requests flow through the LLM first
    *   LLM determines which tools to use
    *   Tools execute actions and return results
    *   LLM interprets results and plans next steps
3.  **Real World Impact**
    
    *   Only tools can affect the external world
    *   LLM provides intelligence but not execution
    *   Actions are constrained by available tools

This helps explain why:

*   Tool integration is crucial for practical agent systems
*   Security and permissions must be implemented at the tool level
*   LLM capabilities alone don't enable real-world actions
*   System design must carefully consider tool access and limitations

#### Misconception 7:"My AI Assistant/AI chatbot is an AI Agent"[â€‹](#misconception-7my-ai-assistantai-chatbot--is-an-ai-agent "Direct link to Misconception 7:\"My AI Assistant/AI chatbot  is an AI Agent\"")

    # This is NOT an agent - it's an AI Assistantclass BasicAIAssistant:    def chat(self, user_input):        response = llm.generate_response(user_input)        return response# This is CLOSER to an agentclass AIAgent:    def __init__(self):        self.tools = load_available_tools()        self.memory = AgentMemory()        self.planner = ActionPlanner()            def handle_task(self, task):        # Autonomous decision making        goal = self.planner.define_goal(task)        plan = self.planner.create_plan(goal)                # Dynamic tool selection and execution        while not goal.is_achieved():            next_action = self.planner.next_action(plan)            tool = self.select_tool(next_action)            result = tool.execute(next_action.parameters)                        # Adaptive behavior            if not result.is_successful():                plan = self.planner.revise_plan(result)                        self.memory.update(result)

However, sometimes as we discussed in chapter 1, AI assistants could have certain level of Agentic behavior depending on how they are implemented.

### Key Differences:[â€‹](#key-differences "Direct link to Key Differences:")

1.  **Autonomy Level**
    
    *   Assistant: Responds to direct commands and questions
    *   Agent: Makes autonomous decisions about how to achieve goals
2.  **Tool Usage**
    
    *   Assistant: May have access to tools but uses them as instructed
    *   Agent: Autonomously decides which tools to use and when
3.  **Goal Orientation**
    
    *   Assistant: Focuses on responding to immediate requests
    *   Agent: Maintains and works toward longer-term goals
4.  **Memory Usage**
    
    *   Assistant: May maintain conversation history
    *   Agent: Uses memory strategically for goal achievement
5.  **Decision Making**
    
    *   Assistant: Makes limited decisions within conversation scope
    *   Agent: Makes complex decisions about actions, strategy, and resource use

### Example Task Comparison:[â€‹](#example-task-comparison "Direct link to Example Task Comparison:")

    # Research Task Example# AI Assistant Approach:async def assistant_research(query):    """Responds to direct questions with available information"""    response = await llm.generate(        f"Please research about {query}"    )    return response# AI Agent Approach:async def agent_research(query):    """Autonomously conducts comprehensive research"""    plan = await self.create_research_plan(query)    sources = []        for step in plan:        if step.type == "web_search":            results = await self.tools.search(step.query)            sources.extend(results)        elif step.type == "verify_information":            verified_data = await self.tools.fact_check(results)        elif step.type == "synthesize":            synthesis = await self.tools.analyze(verified_data)                    # Adaptive planning        if self.evaluate_progress() < self.quality_threshold:            plan = await self.revise_research_plan()        return self.compile_research(sources, synthesis)

This misconception is particularly important because:

1.  It affects system design expectations
2.  It influences how we evaluate AI system capabilities
3.  It impacts how we implement security and permissions
4.  It shapes user expectations and interaction patterns

These diagrams highlight the key differences between AI Assistants and AI Agents:

1.  **Architecture Complexity**
    
    *   Assistant: Simple, linear flow with reactive tool usage
    *   Agent: Complex system with multiple interacting components
2.  **Processing Flow**
    
    *   Assistant: Direct input â†’ response pattern
    *   Agent: Multi-step process with planning and feedback loops
3.  **Tool Integration**
    
    *   Assistant: Passive, explicitly requested tool usage
    *   Agent: Active, autonomous tool selection and execution
4.  **Memory Usage**
    
    *   Assistant: Basic conversation tracking
    *   Agent: Sophisticated memory system for context and learning
5.  **Decision Making**
    
    *   Assistant: Reactive decisions based on immediate input
    *   Agent: Proactive decisions based on goals and strategy</content>
</page>

<page>
  <title>Microservices and AI Agents | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/concepts-ai-agents/agent-comparison</url>
  <content>Fetch.ai Microservice Agent and AI Agent
----------------------------------------

Before diving into the world of AI agents, letâ€™s first explore the key differences between microservices and AI agents.

*   **Microservice**: If you want to fetch specific information or perform an isolated action, you typically use tools or APIsâ€”this is known as a microservice. Imagine you want to check available flights from London to New York. Youâ€™d use an API or tool to fetch the available options. Thatâ€™s a microservice in actionâ€”focused, specific, and task-oriented.
    
*   **AI Agent**: If you have a larger objective that requires coordination between multiple microservices or tools, thatâ€™s where an AI agent steps in. Now, letâ€™s say you want to book the best possible flight. This involves more than just checking flight options: youâ€™ll compare prices across different days, check your personal schedule, and evaluate additional services like baggage policies or meal preferences. To achieve this goal, you need reasoning and decision-making capabilitiesâ€”this is where an AI agent comes into play.
    

The Fetch.ai ecosystem offers a variety of tools to build both microservices and AI agents, enabling developers to cater to different use cases.

### Microservices with uAgents[â€‹](#microservices-with-uagents "Direct link to Microservices with uAgents")

uAgents is Fetch.ai's lightweight Python framework for creating agents (microservices). Itâ€™s designed to be simple and efficient, making it a great choice for building basic microservices.

### AI Agents with the Fetch.ai SDK[â€‹](#ai-agents-with-the-fetchai-sdk "Direct link to AI Agents with the Fetch.ai SDK")

Fetch.ai's SDK is built on top of the uAgents framework. It simplifies the creation of intelligent, dynamic AI agents that can interact with other agents and microservices seamlessly. The SDK allows for more flexibility, reasoning, and adaptability compared to uAgents.

Youâ€™ll learn in the upcoming sections how to decide between these tools based on the problem youâ€™re solving.

How Fetch.aiâ€™s Ecosystem Works[â€‹](#how-fetchais-ecosystem-works "Direct link to How Fetch.aiâ€™s Ecosystem Works")
----------------------------------------------------------------------------------------------------------------

Fetch.ai provides an integrated ecosystem to host and manage your agents, ensuring connectivity and scalability.

1.  **Agentverse Platform**:
    
    *   Agents can be hosted on the Agentverse platform, allowing them to stay active and accessible.
    *   Due to security constraints, Agentverse only supports a limited set of packages.
2.  **Mailbox Service**:
    
    *   For scenarios requiring custom environments (like virtual machines), you can use the Mailbox Service. This lets you host agents outside Agentverse while maintaining connectivity.
    *   The mailbox ensures that messages sent to an agent are queued, so the agent can process them once it comes online.
3.  **Agent Registration with the Almanac Contract**:
    
    *   When agents are registered on Agentverse, they are also listed in the Almanac Contract on Fetch.aiâ€™s blockchain. This registry allows other agents to discover your agent using its unique address.

Why Use the Fetch.ai SDK?[â€‹](#why-use-the-fetchai-sdk "Direct link to Why Use the Fetch.ai SDK?")
-------------------------------------------------------------------------------------------------

While the uAgents framework is sufficient for basic microservices, the Fetch.ai SDK offers additional capabilities suited for more complex AI agents. Hereâ€™s why the SDK exists:

*   Dynamic Messaging:
    *   uAgents require precise Data Models to identify and process messages. In contrast, the SDK enables agents to handle dynamic messages, making it more adaptable.
*   Handler Differences:
    *   uAgents rely on predefined handlers (e.g., interval-based or message-based handlers).
    *   SDK-based agents leverage Flask webhooks, enabling more flexible and scalable communication.

With these distinctions, the SDK empowers developers to build agents capable of reasoning, adaptability, and complex decision-making.</content>
</page>

<page>
  <title>Team Based Artchitectures | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/concepts-ai-agents/team-based-architectures</url>
  <content>### Multi-agent Systems[â€‹](#multi-agent-systems "Direct link to Multi-agent Systems")

An agent uses an LLM to control application flow. However, as systems grow complex, using a single agent can become challenging. This is where multi-agent systems come in.

### Why Use Multiple Agents?[â€‹](#why-use-multiple-agents "Direct link to Why Use Multiple Agents?")

*   **Simplicity**: Break complex tasks into manageable pieces
*   **Expertise**: Create specialized agents for specific tasks
*   **Better Control**: Manage how agents work together

### Common Multi-agent Patterns[â€‹](#common-multi-agent-patterns "Direct link to Common Multi-agent Patterns")

1.  **Network Pattern**
    
    *   Agents can communicate freely with every other agent
    *   Any agent can decide which other agent to call next
    *   Flexible but potentially complex to manage
2.  **Supervisor Pattern**
    
    *   Central supervisor coordinates other agents
    *   Clear control flow through supervisor
    *   Better oversight and management
    *   Can be implemented through tool-calling
3.  **Hierarchical Pattern**
    
    *   Supervisors of supervisors
    *   Allows for more complex control flows
    *   Suitable for large-scale systems
4.  **Custom Workflow Pattern**
    
    *   Agents communicate with specific subset of agents
    *   Parts of flow are deterministic
    *   Limited decision-making about which agents to call next

We will build our agent using a supervisor based multi-agent pattern.

### Supervisor-Based Agent Architecture[â€‹](#supervisor-based-agent-architecture "Direct link to Supervisor-Based Agent Architecture")

A team-based architecture typically consists of three main components:

1.  **Supervisor Agent**
    
    *   Coordinates team activities
    *   Makes routing decisions
    *   Ensures task completion
2.  **Specialist Agents**
    
    *   Handle domain-specific tasks
    *   Maintain focused expertise
    *   Provide detailed analysis
3.  **State Management System**
    
    *   Maintains conversation context
    *   Tracks team progress
    *   Manages shared resources

### Implementation Pattern[â€‹](#implementation-pattern "Direct link to Implementation Pattern")

The code implements a team-based architecture using langgraph with three essential components that work together to process complex tasks:

#### 1\. Supervisor Agent[â€‹](#1-supervisor-agent "Direct link to 1. Supervisor Agent")

    def create_supervisor_agent(llm: ChatOpenAI):    """Creates a supervisor agent that orchestrates the team's activities.        The supervisor agent is the core decision-maker that:    1. Analyzes incoming queries to understand requirements    2. Determines which specialist agent is best suited for each subtask    3. Routes tasks to appropriate specialists    4. Monitors the overall progress of the task    5. Decides when enough information has been gathered        Args:        llm: Language model for decision making            Returns:        A supervisor agent configured with team coordination capabilities    """    supervisor_prompt = """    Core responsibilities:    1. Analyze incoming queries to break down complex tasks    2. Determine what specific information is needed    3. Select the most appropriate specialist for each subtask    4. Monitor progress and ensure task completion    5. Decide when sufficient information has been gathered    """        return create_team_supervisor(        llm=llm,        system_prompt=supervisor_prompt,        members=["SpecialistA", "SpecialistB"]    )

#### 2\. Specialist Agents[â€‹](#2-specialist-agents "Direct link to 2. Specialist Agents")

    def create_specialist_agent(    llm: ChatOpenAI,    domain: str,    tools: List[Tool]):    """Creates a specialist agent with domain-specific expertise.        Specialist agents are focused experts that:    1. Handle specific types of tasks within their domain    2. Use specialized tools for their domain    3. Provide structured analysis and insights    4. Request clarification when needed        Args:        llm: Language model for domain-specific processing        domain: Area of expertise (e.g., "financial analysis", "market research")        tools: List of domain-specific tools available to this specialist            Returns:        A specialist agent configured for its specific domain    """    system_prompt = f"""    You are specialized in {domain}.    When responding:    1. Use your domain-specific tools effectively    2. Provide clearly structured outputs    3. Explicitly request any missing information needed    """        return create_agent(        llm=llm,        tools=tools,        system_prompt=system_prompt    )

#### 3\. State Management[â€‹](#3-state-management "Direct link to 3. State Management")

    class TeamState(TypedDict):    """Manages the shared state and context for the entire team.        Attributes:        messages: List of all messages in the conversation history        team_members: List of available specialist agents        next: Identifier of the next agent to act        information_needed: List of missing information to be gathered        reasoning: Explanation for the current decision or action    """    messages: List[BaseMessage]    team_members: List[str]    next: str    information_needed: List[str]    reasoning: str

### Team Graph Implementation[â€‹](#team-graph-implementation "Direct link to Team Graph Implementation")

The team graph orchestrates how all components work together:

    def create_team_graph():    """Creates a coordinated team of agents with defined interaction patterns.        The graph defines:    1. How agents communicate with each other    2. The flow of information between agents    3. Decision points for task routing    4. Conditions for task completion        Process Flow:    1. Supervisor receives task and analyzes requirements    2. Tasks are routed to appropriate specialists    3. Specialists process tasks and return results    4. Supervisor evaluates results and decides next steps    5. Process continues until task is complete        Returns:        A compiled graph ready for task processing    """    # Initialize team members    specialist_a = create_specialist_agent(...)    specialist_b = create_specialist_agent(...)    supervisor = create_supervisor_agent(...)        # Create the coordination graph    graph = StateGraph(TeamState)        # Define team structure    graph.add_node("SpecialistA", specialist_a)    graph.add_node("SpecialistB", specialist_b)    graph.add_node("supervisor", supervisor)        # Define information flow    graph.add_edge("SpecialistA", "supervisor")    graph.add_edge("SpecialistB", "supervisor")        # Set up decision routing    graph.add_conditional_edges(        "supervisor",        lambda x: x["next"],        {            "SpecialistA": "SpecialistA",            "SpecialistB": "SpecialistB",            "FINISH": END        }    )        return graph.compile()</content>
</page>

<page>
  <title>uAgents Adapters | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/agent-creation/uagents-adapter-guide</url>
  <content>Version: 1.0.3

uAgents Adapters: Connecting AI Framework Ecosystems
----------------------------------------------------

uAgents Adapters provide a bridge between the uAgents ecosystem and various agentic frameworks, enabling seamless communication between different AI agent architectures.

Why Use Adapters?[â€‹](#why-use-adapters "Direct link to Why Use Adapters?")
--------------------------------------------------------------------------

AI development landscapes often involve multiple frameworks and technologies, each with their own strengths:

*   **LangChain**: Powerful for composing LLMs with tools and chains
*   **LangGraph**: Excellent for complex orchestration and stateful workflows
*   **CrewAI**: Specialized for multi-agent collaborative systems

The uAgents Adapter package allows you to leverage these specialized frameworks while still benefiting from the uAgents ecosystem for communication, discovery, and deployment.

Available Adapters[â€‹](#available-adapters "Direct link to Available Adapters")
------------------------------------------------------------------------------

The uAgents Adapter package currently supports several major AI frameworks:

### 1\. LangChain Adapter[â€‹](#1-langchain-adapter "Direct link to 1. LangChain Adapter")

Connect LangChain agents, chains, and tools to the uAgents ecosystem.

    from uagents_adapter import LangchainRegisterTool# Register a LangChain agent as a uAgenttool = LangchainRegisterTool()agent_info = tool.invoke({    "agent_obj": langchain_agent,    "name": "my_langchain_agent",    "port": 8000,    "description": "A LangChain agent powered by GPT-4",    "api_token": AGENTVERSE_API_KEY})

### 2\. LangGraph Adapter[â€‹](#2-langgraph-adapter "Direct link to 2. LangGraph Adapter")

Integrate LangGraph's powerful orchestration with uAgents.

    from uagents_adapter import LangchainRegisterTool# Wrap LangGraph agent function for uAgent integrationdef langgraph_agent_func(query):    # Process with LangGraph    result = langgraph_app.invoke(query)    return result# Register the LangGraph function as a uAgenttool = LangchainRegisterTool()agent_info = tool.invoke({    "agent_obj": langgraph_agent_func,    "name": "my_langgraph_agent",    "port": 8080,    "description": "A LangGraph orchestration agent",    "api_token": AGENTVERSE_API_KEY})

### 3\. CrewAI Adapter[â€‹](#3-crewai-adapter "Direct link to 3. CrewAI Adapter")

Expose CrewAI's collaborative agent teams as uAgents.

    from uagents_adapter import CrewaiRegisterTool# Create a function to handle CrewAI operationsdef crew_handler(query):    # Process with CrewAI    result = my_crew.kickoff(inputs={"query": query})    return result# Register the CrewAI function as a uAgenttool = CrewaiRegisterTool()agent_info = tool.invoke({    "agent_obj": crew_handler,    "name": "my_crew_agent",    "port": 8081,    "description": "A CrewAI team of specialized agents",    "api_token": AGENTVERSE_API_KEY})

Common Parameters[â€‹](#common-parameters "Direct link to Common Parameters")
---------------------------------------------------------------------------

All adapters accept the following parameters:

| Parameter | Type | Description |
| --- | --- | --- |
| `agent_obj` | object | The framework-specific agent or function to wrap |
| `name` | string | Name for your agent in the uAgents ecosystem |
| `port` | int | Port for the agent's HTTP server |
| `description` | string | Human-readable description of agent capabilities |
| `api_token` | string | Your Agentverse API key for registration |
| `mailbox` | bool | Whether to use Agentverse mailbox for persistence (optional) |
| `ai_agent_address` | string | AI Agent address to conver Natural language into structured query prompt (optional) |

Communication Protocol[â€‹](#communication-protocol "Direct link to Communication Protocol")
------------------------------------------------------------------------------------------

Once registered, adapter agents communicate using the uAgents chat protocol:

    from uagents_core.contrib.protocols.chat import (    ChatMessage, TextContent)# Send a message to an adapter-wrapped agentmessage = ChatMessage(    timestamp=datetime.utcnow(),    msg_id=uuid4(),    content=[TextContent(type="text", text="Your query here")])await ctx.send(adapter_agent_address, message)

Cleanup and Management[â€‹](#cleanup-and-management "Direct link to Cleanup and Management")
------------------------------------------------------------------------------------------

Always clean up your agents when shutting down to ensure proper deregistration:

    from uagents_adapter import cleanup_uagenttry:    # Your agent code here    while True:        time.sleep(1)except KeyboardInterrupt:    # Clean up the agent    cleanup_uagent("your_agent_name")    print("Agent stopped.")

Next Steps[â€‹](#next-steps "Direct link to Next Steps")
------------------------------------------------------

To explore concrete examples of adapter usage, refer to the [uAgents Adapter Examples](https://innovationlab.fetch.ai/resources/docs/examples/adapters/crewai-adapter-example) section.</content>
</page>

<page>
  <title>AI Agent Creation | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/agent-creation/sdk-creation</url>
  <content>Creation and Registration of AI Agent using SDK
-----------------------------------------------

Why SDK Agents?[â€‹](#why-sdk-agents "Direct link to Why SDK Agents?")
--------------------------------------------------------------------

While the uAgents framework is great for basic microservices, the Fetch.ai SDK provides enhanced capabilities for building AI agents with greater flexibility and adaptability. Key differences include:

*   **Dynamic Messaging**: Processes flexible or unstructured data, unlike uAgents that require strict data models.
    
*   **Flexible Handlers**: Relies on Flask endpoints instead of predefined handlers, enabling more scalable and customizable communication.
    

These features are especially helpful if your AI agent needs advanced reasoning or can benefit from rapid decision-making and adaptability.

Installing fetchai SDK[â€‹](#installing-fetchai-sdk "Direct link to Installing fetchai SDK")
------------------------------------------------------------------------------------------

Fetch.ai's [SDK](https://pypi.org/project/fetchai/) package is a Python library running on Ubuntu/Debian, macOS, and Windows systems.

On your computer, you may need to install:

*   [Python 3.8+](https://www.python.org/downloads/)
*   [PIP](https://pypi.org/project/pip/) - the Python package manager.
*   [fetchai](https://pypi.org/project/fetchai/) library

### Install with Pip[â€‹](#install-with-pip "Direct link to Install with Pip")

*   Install Fetch.ai uagents library:

*   Check if installation was successful:

### Creating and Registering an AI Agent[â€‹](#creating-and-registering-an-ai-agent "Direct link to Creating and Registering an AI Agent")

In this example, you will create an AI agent identity and register it on Agentverse.

1.  **Create a python script** named `register_agent.py`.

2.  **Add the following code** to `register_agent.py`.

register\_ai\_agent.py

    import osfrom uagents_core.crypto import Identityfrom fetchai.registration import register_with_agentverse# Store your Agentverse API Key in the environment variables. AGENTVERSE_KEY = os.getenv("AGENTVERSE_KEY")# Your Agent's unique key for generating an address on Agentverseai_identity = Identity.from_seed(os.getenv("AGENT_SECRET_KEY"), 0)# Give your Agent a name. This allows you to easily identify one# of your Agents from other agents on Agentverse.name = "My AI's Name"# This is how you optimize your Agent's search engine performancereadme = """![domain:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)domain:domain-of-your-agent<description>My AI's description of capabilities and offerings</description><use_cases>    <use_case>An example of one of your AI's use cases.</use_case></use_cases><payload_requirements><description>The requirements your AI has for requests</description><payload>    <requirement>        <parameter>question</parameter>        <description>The question that you would like this AI work with you to solve</description>    </requirement></payload></payload_requirements>"""# The webhook that your AI receives messages on.ai_webhook = "https://api.sampleurl.com/webhook"register_with_agentverse(    ai_identity,    ai_webhook,    AGENTVERSE_KEY,    name,    readme,)

3.  Set up environment variables in a `.env` file (or similar approach):

    AGENTVERSE_KEY='YOUR_AGENTVERSE_API_KEY'AGENT_SECRET_KEY='YOUR_RANDOM_SECRET_SEED'

*   The `AGENTVERSE_KEY` is obtained from your Agentverse account.
*   The `AGENT_SECRET_KEY` should be a unique, random phrase to ensure your agent consistently generates the same address.

4.  Run the Script

    python3 regsiter_agent.py

*   **Sample Output:**

    INFO:fetchai:Registering with Almanac APIINFO:fetchai:Successfully registered as custom agent in AgentverseINFO:fetchai:Completed registering agent with Agentverse

### Checking Your Agent on Agentverse[â€‹](#checking-your-agent-on-agentverse "Direct link to Checking Your Agent on Agentverse")

1.  **Go to the Agents tab** in the Agentverse interface.
    
2.  **Select â€œLocal Agentsâ€** and look for the name used in your script (e.g., â€œMy AIâ€™s Nameâ€).
    

3.  **Click on the agent name** to view its details (address, registration data, etc.).

You have successfully created and registered an AI agent with the Fetch.ai SDK. You can now integrate this agent into your applications, allowing it to interact with other agents or services via Agentverse.

#### Next Steps:[â€‹](#next-steps "Direct link to Next Steps:")

*   Further develop your agentâ€™s code and logic.
*   Integrate custom Flask endpoints to handle advanced communication.
*   Explore additional SDK features (transaction handling, smart contracts, etc.) to expand the agentâ€™s capabilities.</content>
</page>

<page>
  <title>AI Agent to AI Agent Communication | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/agent-communication/sdk-sdk-communication</url>
  <content>In this guide, we will create two AI agents using the fetchai SDK and then see how to send and handle requests received by these agents. You can find all our supporting code files in our dedicated [github repo](https://github.com/fetchai/fetchai).

AI Agent creation[â€‹](#ai-agent-creation "Direct link to AI Agent creation")
---------------------------------------------------------------------------

Let's start by creating an AI Agent using the [fetchai SDK](https://github.com/fetchai/fetchai).

Please remember to have the **fetchai** package installed in the terminal in order to create and run the agents.

### Creating our first agent to receive the message[â€‹](#creating-our-first-agent-to-receive-the-message "Direct link to Creating our first agent to receive the message")

Create a new Python script:

    touch my_first_sdk_agent.py

Open `my_first_sdk_agent.py` in your text editor and add the following code which helps you register your agent and handle messages:

my\_first\_sdk\_agent.py

    from flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai import fetchfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osfrom dotenv import load_dotenv# Configure logginglogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app)# Initialising client identity to get registered on agentverseclient_identity = None # Function to register agentdef init_client():    """Initialize and register the client agent."""    global client_identity    try:        # Load the agent secret key from environment variables        client_identity = Identity.from_seed(os.getenv("AGENT_SECRET_KEY_1"), 0)        logger.info(f"Client agent started with address: {client_identity.address}")        readme = """            ![domain:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)            domain:domain-of-your-agent            <description>This Agent can only receive a message from another agent in string format.</description>            <use_cases>                <use_case>To receive a message from another agent.</use_case>            </use_cases>            <payload_requirements>            <description>This agent only requires a message in the text format.</description>            <payload>                <requirement>                    <parameter>message</parameter>                    <description>The agent can receive any kind of message.</description>                </requirement>            </payload>            </payload_requirements>        """                # Register the agent with Agentverse        register_with_agentverse(            identity=client_identity,            url="http://localhost:5002/api/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="Quickstart Agent 1",            readme=readme        )        logger.info("Quickstart agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise# app route to recieve the messages from other agents@app.route('/api/webhook', methods=['POST'])def webhook():    """Handle incoming messages"""    global agent_response    try:        # Parse the incoming webhook message        data = request.get_data().decode("utf-8")        logger.info("Received response")        message = parse_message_from_agent(data)        agent_response = message.payload        logger.info(f"Processed response: {agent_response}")        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500if __name__ == "__main__":    load_dotenv()       # Load environment variables    init_client()       #Register your agent on Agentverse    app.run(host="0.0.0.0", port=5002)      

Save the `.env` file with your `AGENTVERSE_KEY` and `AGENT_SECRET_KEY_1`, get your `AGENTVERSE_KEY` by referring to this [guide](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key) and the AGENT\_SECRET\_KEY\_1 is a random unique key just for your AI Agent. Â 

    AGENT_SECRET_KEY_1='Your random secret seed phrase for the agent'AGENTVERSE_API_KEY='YOUR AGENTVERSE API KEY FROM AGENTVERSE'

Start the first agent as follows:

#### Expected Output[â€‹](#expected-output "Direct link to Expected Output")

    (my_venv) abhi@Fetchs-MacBook-Pro chorot 3 session % python3 my_sdk_agent_1.pyINFO:__main__:Client agent started with address: agent1q2d7qdxxpc2emmph6wze47p0g7vnfxwuzk3h6yfnaem0px0hc3acu3yzz8kINFO:fetchai:Registering with Almanac APIINFO:fetchai:Successfully registered as custom agent in AgentverseINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Quickstart agent registration complete! * Serving Flask app 'my_sdk_agent_1' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5002 * Running on http://192.168.0.105:5002INFO:werkzeug:Press CTRL+C to quit

You can verify that your agent is registered by checking in your [Agentverse Local Agents](https://agentverse.ai/agents/local) â†—ï¸.

Webhook will help you receive and handle messages, we will learn more about communication here.

### Creating our second agent to send the message[â€‹](#creating-our-second-agent-to-send-the-message "Direct link to Creating our second agent to send the message")

Open `my_second_sdk_agent.py` in your text editor and add the following code which helps you register agent and handle messages:

my\_second\_sdk\_agent.py

    from flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai import fetchfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osfrom dotenv import load_dotenv# Configure logginglogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app)# Initialising client identity to get registered on agentverseclient_identity = None agent_response = None# Function to register agentdef init_client():    """Initialize and register the client agent."""    global client_identity    try:        # Load the agent secret key from environment variables        client_identity = Identity.from_seed(os.getenv("AGENT_SECRET_KEY_2"), 0)        logger.info(f"Client agent started with address: {client_identity.address}")        readme = """            ![domain:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)            domain:domain-of-your-agent            <description>This Agent can only send a message to another agent in string format.</description>            <use_cases>                <use_case>To send a message to another agent.</use_case>            </use_cases>            <payload_requirements>            <description>This agent can only send a message in the text format.</description>            <payload>                <requirement>                    <parameter>message</parameter>                    <description>The agent can send a message to another agent.</description>                </requirement>            </payload>            </payload_requirements>        """        # Register the agent with Agentverse        register_with_agentverse(            identity=client_identity,            url="http://localhost:5005/api/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="Quickstart Agent 2",            readme=readme        )        logger.info("Quickstart agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise@app.route('/api/send-data', methods=['POST'])def send_data():   """Send payload to the selected agent based on provided address."""   global agent_response   agent_response = None   try:       # Parse the request payload       data = request.json       payload = data.get('payload')  # Extract the payload dictionary       agent_address = data.get('agentAddress')  # Extract the agent address       # Validate the input data       if not payload or not agent_address:           return jsonify({"error": "Missing payload or agent address"}), 400       logger.info(f"Sending payload to agent: {agent_address}")       logger.info(f"Payload: {payload}")       # Send the payload to the specified agent       send_message_to_agent(           client_identity,  # Frontend client identity           agent_address,    # Agent address where we have to send the data           payload           # Payload containing the data       )       return jsonify({"status": "request_sent", "agent_address": agent_address, "payload": payload})   except Exception as e:       logger.error(f"Error sending data to agent: {e}")       return jsonify({"error": str(e)}), 500if __name__ == "__main__":    load_dotenv()   # Load environment variables    init_client()   #Register your Agent on Agentverse    app.run(host="0.0.0.0", port=5005)

Start the second agent as follows.

    python my_second_agent.py

#### Expected Output[â€‹](#expected-output-1 "Direct link to Expected Output")

    (my_venv) abhi@Fetchs-MacBook-Pro chorot 3 session % python3 my_sdk_agent_2.pyINFO:__main__:Client agent started with address: agent1qvgxzj40657wh4ftk65scvqqwv7sw5lf7z5uf3mdnc8d6zmrugumu4g78wsINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Quickstart agent registration complete! * Serving Flask app 'my_sdk_agent_2' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5005 * Running on http://192.168.0.105:5005INFO:werkzeug:Press CTRL+C to quit

### Sending message from Agent2 to Agent1[â€‹](#sending-message-from-agent2-to-agent1 "Direct link to Sending message from Agent2 to Agent1")

We will use curl command to send message from agent2 to agent1. We will send a payload with Hello Message.

    curl -X POST http://localhost:5005/api/send-data \-H "Content-Type: application/json" \-d '{  "payload": {"message":"Hello This is message from agent2"},  "agentAddress": <Your agent 1 address>}'

#### Curl Command Output[â€‹](#curl-command-output "Direct link to Curl Command Output")

    {"agent_address":"agent1q2d7qdxxpc2emmph6wze47p0g7vnfxwuzk3h6yfnaem0px0hc3acu3yzz8k","payload":{"message":"Hello This is message from agent2"},"status":"request_sent"}

#### Agent2 Logs[â€‹](#agent2-logs "Direct link to Agent2 Logs")

    INFO:__main__:Sending payload to agent: agent1q2d7qdxxpc2emmph6wze47p0g7vnfxwuzk3h6yfnaem0px0hc3acu3yzz8kINFO:__main__:Payload: {'message': 'Hello This is message from agent2'}{"version":1,"sender":"agent1qvgxzj40657wh4ftk65scvqqwv7sw5lf7z5uf3mdnc8d6zmrugumu4g78ws","target":"agent1q2d7qdxxpc2emmph6wze47p0g7vnfxwuzk3h6yfnaem0px0hc3acu3yzz8k","session":"e025c919-2c6d-4e9c-b562-a879cbb35aec","schema_digest":"model:708d789bb90924328daa69a47f7a8f3483980f16a1142c24b12972a2e4174bc6","protocol_digest":"proto:a03398ea81d7aaaf67e72940937676eae0d019f8e1d8b5efbadfef9fd2e98bb2","payload":"eyJtZXNzYWdlIjoiSGVsbG8gVGhpcyBpcyBtZXNzYWdlIGZyb20gYWdlbnQyIn0=","expires":null,"nonce":null,"signature":"sig10phxfu9s9lsrm3ux4pffufw6yxs37z0ag82hhlw0mkkwap74av7cjwk86dr9nnmgpgxwhyqdmnl90um8wu77emafutfw77jdvlswg9spmc899"}INFO:fetchai:Got response looking up agent endpointhttp://localhost:5002/api/webhookINFO:fetchai:Sent message to agentINFO:werkzeug:127.0.0.1 - - [29/Jan/2025 11:17:59] "POST /api/send-data HTTP/1.1" 200 -

#### Agent1 Logs[â€‹](#agent1-logs "Direct link to Agent1 Logs")

    INFO:__main__:Received responseINFO:__main__:Processed response: {'message': 'Hello This is message from agent2'}INFO:werkzeug:127.0.0.1 - - [29/Jan/2025 11:17:59] "POST /api/webhook HTTP/1.1" 200 -

This is how messages can be sent and handled between agents.

### Parse and Send Message functions.[â€‹](#parse-and-send-message-functions "Direct link to Parse and Send Message functions.")

The `send_message_to_agent` function is used to send a message from one agent to the other, while the `parse_message_from_agent` function is used to handle incoming messages.

#### Sending a Message[â€‹](#sending-a-message "Direct link to Sending a Message")

The `send_message_to_agent` function constructs an Envelope containing the sender's identity, target agent address, message payload, and other metadata.

    from uagents_core.crypto import Identityfrom fetchai.communication import send_message_to_agentsend_message_to_agent(    sender=sender_identity,             #sender_address    target="receiver_agent_address",    #target agent address    payload={"message": "Hello, Agent!"})# payload is in {str : str} format

#### Parsing an Incoming Message[â€‹](#parsing-an-incoming-message "Direct link to Parsing an Incoming Message")

The `parse_message_from_agent` function extracts messages received from other agents by decoding the payload.

    from fetchai.communication import parse_message_from_agenttry:    message = parse_message_from_agent(data)except ValueError as e:    print(f"Error parsing message: {e}")    return# Extract sender and payload detailssender_address = message.sendermessage_payload = message.payload</content>
</page>

<page>
  <title>Agent Chat Protocol | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/agent-communication/agent-chat-protocol</url>
  <content>The Agent Chat Protocol is a standardized communication framework that enables agents to exchange messages in a structured and reliable manner. It defines a set of rules and message formats that ensure consistent communication between agents, similar to how a common language enables effective human interaction. This guide demonstrates how to implement and utilize this protocol in your agents.

Understanding the Chat Protocol[â€‹](#understanding-the-chat-protocol "Direct link to Understanding the Chat Protocol")
---------------------------------------------------------------------------------------------------------------------

The chat protocol consists of several key components that work together to enable reliable communication between agents. Let's explore each component:

### 1\. Core Models[â€‹](#1-core-models "Direct link to 1. Core Models")

#### TextContent[â€‹](#textcontent "Direct link to TextContent")

    class TextContent(Model):    type: Literal['text']    text: str

*   Basic content type for text messages
*   Uses `Literal['text']` to ensure type safety
*   `text` field stores the actual message content

#### Resource and ResourceContent[â€‹](#resource-and-resourcecontent "Direct link to Resource and ResourceContent")

    class Resource(Model):    uri: str    metadata: dict[str, str]class ResourceContent(Model):    type: Literal['resource']    resource_id: UUID4    resource: Resource | list[Resource]

*   `Resource`: Represents external resources (files, images, etc.)
    *   `uri`: Location of the resource
    *   `metadata`: Additional resource information
*   `ResourceContent`: Wraps resources in messages
    *   `resource_id`: Unique identifier for tracking
    *   `resource`: Single or multiple resources

#### Metadata Types[â€‹](#metadata-types "Direct link to Metadata Types")

    class Metadata(TypedDict):    mime_type: str    role: strclass MetadataContent(Model):    type: Literal['metadata']    metadata: dict[str, str]

*   `Metadata`: Defines resource metadata structure
    *   `mime_type`: Resource type (e.g., "text/plain")
    *   `role`: Resource's purpose in communication
*   `MetadataContent`: For sending metadata-only messages

### 2\. Session and Stream Management[â€‹](#2-session-and-stream-management "Direct link to 2. Session and Stream Management")

#### Session Control[â€‹](#session-control "Direct link to Session Control")

    class StartSessionContent(Model):    type: Literal['start-session']class EndSessionContent(Model):    type: Literal['end-session']

*   Manages chat session lifecycle
*   `StartSessionContent`: Initiates new sessions
*   `EndSessionContent`: Properly terminates sessions

#### Stream Control[â€‹](#stream-control "Direct link to Stream Control")

    class StartStreamContent(Model):    type: Literal['start-stream']    stream_id: UUID4class EndStreamContent(Model):    type: Literal['start-stream']    stream_id: UUID4

*   Handles continuous data streams
*   `stream_id`: Unique identifier for stream tracking

### 3\. Agent Content Type[â€‹](#3-agent-content-type "Direct link to 3. Agent Content Type")

    AgentContent = (    TextContent    | ResourceContent    | MetadataContent    | StartSessionContent    | EndSessionContent    | StartStreamContent    | EndStreamContent)

*   Combines all possible content types
*   Ensures type safety in message content

### 4\. Message Types[â€‹](#4-message-types "Direct link to 4. Message Types")

#### ChatMessage[â€‹](#chatmessage "Direct link to ChatMessage")

    class ChatMessage(Model):    timestamp: datetime    msg_id: UUID4    content: list[AgentContent]

*   Primary message type for communication
*   `timestamp`: When message was sent (UTC)
*   `msg_id`: Unique message identifier
*   `content`: List of content elements discussed above.

#### ChatAcknowledgement[â€‹](#chatacknowledgement "Direct link to ChatAcknowledgement")

    class ChatAcknowledgement(Model):    timestamp: datetime    acknowledged_msg_id: UUID4    metadata: dict[str, str] | None = None

*   Confirms message receipt
*   `acknowledged_msg_id`: References original message
*   Optional metadata for additional information

### 5\. Message Handlers[â€‹](#5-message-handlers "Direct link to 5. Message Handlers")

    @protocol.on_message(ChatMessage)async def handle_message(_ctx: Context, sender: str, msg: ChatMessage):    print('I got a chat message', sender, msg)@protocol.on_message(ChatAcknowledgement)async def handle_ack(_ctx: Context, sender: str, msg: ChatAcknowledgement):    print('I got a chat acknowledgement', sender, msg)

*   Process incoming messages
*   Handle acknowledgments

Using the Chat Protocol[â€‹](#using-the-chat-protocol "Direct link to Using the Chat Protocol")
---------------------------------------------------------------------------------------------

To implement the chat protocol in your agents, you can import all the above components from the `uagents_core` package:

    from uagents_core.contrib.protocols.chat import (    ChatMessage,    ChatAcknowledgement,    TextContent,    chat_protocol_spec)

Basic Message Flow[â€‹](#basic-message-flow "Direct link to Basic Message Flow")
------------------------------------------------------------------------------

The protocol follows a simple request-response pattern with acknowledgments:

1.  Agent A sends a `ChatMessage` to Agent B
2.  Agent B sends a `ChatAcknowledgement` back to Agent A
3.  Agent B can then send a `ChatMessage` response to Agent A
4.  Agent A sends a `ChatAcknowledgement` back to Agent B

Let's create two agents on [Agentverse](https://agentverse.ai/) that communicate using the chat protocol. Refer to the [Hosted Agents](https://innovationlab.fetch.ai/resources/docs/agent-creation/uagent-creation#hosted-agents) section to understand the detailed steps for agent creation on Agentverse.

### Agent1 Script[â€‹](#agent1-script "Direct link to Agent1 Script")

agent1.py

    from datetime import datetimefrom uuid import uuid4from uagents import Agent, Protocol, Context, Modelfrom time import sleep#import the necessary components from the chat protocolfrom uagents_core.contrib.protocols.chat import (    ChatAcknowledgement,    ChatMessage,    TextContent,    chat_protocol_spec,)# Intialise agent1agent1 = Agent()# Store agent2's address (you'll need to replace this with actual address)agent2_address = "agent1qf8n9q8ndlfvphmnwjzj9p077yq0m6kqc22se9g89y5en22sc38ck4p4e8d"# Initialize the chat protocolchat_proto = Protocol(spec=chat_protocol_spec)#Startup Handler - Print agent details and send initial message@agent1.on_event("startup")async def startup_handler(ctx: Context):    # Print agent details    ctx.logger.info(f"My name is {ctx.agent.name} and my address is {ctx.agent.address}")        # Send initial message to agent2    initial_message = ChatMessage(        timestamp=datetime.utcnow(),        msg_id=uuid4(),        content=[TextContent(type="text", text="Hello from Agent1!")]    )            await ctx.send(agent2_address, initial_message)# Message Handler - Process received messages and send acknowledgements@chat_proto.on_message(ChatMessage)async def handle_message(ctx: Context, sender: str, msg: ChatMessage):    for item in msg.content:        if isinstance(item, TextContent):            # Log received message            ctx.logger.info(f"Received message from {sender}: {item.text}")                        # Send acknowledgment            ack = ChatAcknowledgement(                timestamp=datetime.utcnow(),                acknowledged_msg_id=msg.msg_id            )            await ctx.send(sender, ack)                        # Send response message            response = ChatMessage(                timestamp=datetime.utcnow(),                msg_id=uuid4(),                content=[TextContent(type="text", text="Hello from Agent1!")]            )            await ctx.send(sender, response)# Acknowledgement Handler - Process received acknowledgements@chat_proto.on_message(ChatAcknowledgement)async def handle_acknowledgement(ctx: Context, sender: str, msg: ChatAcknowledgement):    ctx.logger.info(f"Received acknowledgement from {sender} for message: {msg.acknowledged_msg_id}")# Include the protocol in the agent to enable the chat functionality# This allows the agent to send/receive messages and handle acknowledgements using the chat protocolagent1.include(chat_proto, publish_manifest=True)if __name__ == '__main__':    agent1.run()

### Agent2 Script[â€‹](#agent2-script "Direct link to Agent2 Script")

agent2.py

    from datetime import datetimefrom uuid import uuid4from uagents import Agent, Protocol, Context#import the necessary components from the chat protocolfrom uagents_core.contrib.protocols.chat import (    ChatAcknowledgement,    ChatMessage,    TextContent,    chat_protocol_spec,)# Initialise agent2agent2 = Agent()# Initialize the chat protocolchat_proto = Protocol(spec=chat_protocol_spec)# Startup Handler - Print agent details@agent2.on_event("startup")async def startup_handler(ctx: Context):    # Print agent details    ctx.logger.info(f"My name is {ctx.agent.name} and my address is {ctx.agent.address}")# Message Handler - Process received messages and send acknowledgements@chat_proto.on_message(ChatMessage)async def handle_message(ctx: Context, sender: str, msg: ChatMessage):    for item in msg.content:        if isinstance(item, TextContent):            # Log received message            ctx.logger.info(f"Received message from {sender}: {item.text}")                        # Send acknowledgment            ack = ChatAcknowledgement(                timestamp=datetime.utcnow(),                acknowledged_msg_id=msg.msg_id            )            await ctx.send(sender, ack)                        # Send response message            response = ChatMessage(                timestamp=datetime.utcnow(),                msg_id=uuid4(),                content=[TextContent(type="text", text="Hello from Agent2!")]            )            await ctx.send(sender, response)# Acknowledgement Handler - Process received acknowledgements@chat_proto.on_message(ChatAcknowledgement)async def handle_acknowledgement(ctx: Context, sender: str, msg: ChatAcknowledgement):    ctx.logger.info(f"Received acknowledgement from {sender} for message: {msg.acknowledged_msg_id}")# Include the protocol in the agent to enable the chat functionality# This allows the agent to send/receive messages and handle acknowledgements using the chat protocolagent2.include(chat_proto, publish_manifest=True)if __name__ == '__main__':    agent2.run()

### Running the Agents[â€‹](#running-the-agents "Direct link to Running the Agents")

To run the example, you'll need to:

1.  Start Agent2 first:
    
2.  Copy Agent2's address from the startup logs and update it in Agent1's script
    
3.  Start Agent1:
    

### Expected Output[â€‹](#expected-output "Direct link to Expected Output")

When running both agents, you should see output similar to:

Agent2 Logs

Agent1 Logs

This guide demonstrates the communication via chat protocol between two agents hosted on [Agentverse](https://agentverse.ai/). If you wish to run these agents on your local machine instead, you'll need to initialize the agents with specific ports and endpoints:

    # For agent1agent1 = Agent(    name="agent1",    port=8000,    endpoint=["http://localhost:8000/submit"])# For agent2agent2 = Agent(    name="agent2",    port=8001,    endpoint=["http://localhost:8001/submit"])

To learn more about setting up and running agents locally, refer to the [Local Agents](https://innovationlab.fetch.ai/resources/docs/agent-creation/uagent-creation) section of our documentation.</content>
</page>

<page>
  <title>Agentverse API Key | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/agentverse/agentverse-api-key</url>
  <content>Getting Agentverse API Key
--------------------------

To get [Agentverse](https://agentverse.ai/) API Key, just login to Agentverse using your gmail address and go to profile section as shown in picture below:

Click on **\+ New API Key** button, give name to your API Key and with write permission to Access to all resources in Agentverse. Your permission can be for 30 days now. Check below image for more clarity on the process :Â 

Click on **Generate API Key**, it will take you through the authentication process again. Once you do the authentication and Allow Access the API key will pop-up on your screen like the image below and save it somewhere as it cannot be regenerated.

Keep your API Key stored somewhere safe. You wonâ€™t be able to access it again.</content>
</page>

<page>
  <title>AI Agent to uAgent Communication | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/agent-communication/sdk-uagent-communication</url>
  <content>uAgent â†” AI Agent Communication Using Fetch.ai SDK and uAgents
--------------------------------------------------------------

This guide demonstrates how to enable communication between a Microservice Agent created using the uagents framework and an AI Agent created using the Fetch.ai SDK.

### uAgent Script (uagent.py)[â€‹](#uagent-script-uagentpy "Direct link to uAgent Script (uagent.py)")

Please remember to have the **uagents** and the **fetchai** package installed in the terminal in order to create and run the agents.

This uAgent will receive a message from the AI Agent and send a response back.

uagent.py

    from uagents import Agent, Context, Model# Define the request model the uAgent will handleclass Request(Model):    message: str# Define the response model the uAgent will send backclass Response(Model):    response: str# Initialize the uAgentuagent = Agent(    name="Sample uAgent",    port=8000,    endpoint=["http://localhost:8000/submit"])# Handle incoming messages with the Request model@uagent.on_message(model=Request)async def message_handler(ctx: Context, sender: str, msg: Request):    ctx.logger.info(f"Received message from {sender}: {msg.message}")    # Generate a response message    response = Response(response=f'Hello, AI Agent! I received your message:{msg.message}')        # Send the response back to the AI Agent    await ctx.send(sender, response)if __name__ == "__main__":    uagent.run()

### Explanation[â€‹](#explanation "Direct link to Explanation")

*   **Agent Initialization**: The uAgent listens on port 8000 for incoming messages.
*   **Message Handling**: When a message matching the `Request` model is received, the agent logs it and responds with a predefined message using the `Response` model.

Setting Up the AI Agent[â€‹](#setting-up-the-ai-agent "Direct link to Setting Up the AI Agent")
---------------------------------------------------------------------------------------------

The AI Agent will send messages to the uAgent and handle the response received from the uAgent.

### AI Agent Script (ai\_agent.py)[â€‹](#ai-agent-script-ai_agentpy "Direct link to AI Agent Script (ai_agent.py)")

ai\_agent.py

    import osfrom flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai.communication import send_message_to_agent, parse_message_from_agentimport loggingfrom dotenv import load_dotenv# Configure logginglogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)# Initialize Flask appapp = Flask(__name__)CORS(app)client_identity = Noneagent_response = Noneclass Request(Model):    message: str# Load environment variables from .env fileload_dotenv()def init_client():    """Initialize and register the client agent."""    global client_identity    try:        # Load the client identity from environment variables        client_identity = Identity.from_seed("Sample AI AGENT SEED PHRASE for communication", 0)        logger.info(f"Client agent started with address: {client_identity.address}")        readme = """        ![domain:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)        domain:domain-of-your-agent        <description>This Agent can send a message to a uAgent and receive a message from a uAgent in string format.</description>        <use_cases>        <use_case>Send and receive messages with another uAgent.</use_case>        </use_cases>        <payload_requirements>            <description>This agent can only send and receive messages in text format.</description>            <payload>                <requirement>                    <parameter>message</parameter>                    <description>The agent sends and receives messages in text format.</description>                </requirement>            </payload>        </payload_requirements>        """        # Register the agent with Agentverse        register_with_agentverse(            identity=client_identity,            url="http://localhost:5002/api/webhook",            agentverse_token = os.getenv("AGENTVERSE_API_KEY"),            agent_title="Sample AI Agent communication"            readme=readme        )        logger.info("Client agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise@app.route('/request', methods=['POST'])def send_data():    """Send payload to the selected agent based on provided address."""    global agent_response    agent_response = None    try:        # Parse the request payload        data = request.json        payload = data.get('payload')  # Extract the payload dictionary        uagent_address = "agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6" #run the uagent.py copy the address and paste here                # Build the Data Model digest for the Request model to ensure message format consistency between the uAgent and AI Agent        model_digest = Model.build_schema_digest(Request)        # Send the payload to the specified agent        send_message_to_agent(            client_identity,  # Frontend client identity            uagent_address,  # Agent address where we have to send the data            payload,  # Payload containing the data            model_digest=model_digest        )        return jsonify({"status": "request_sent", "payload": payload})    except Exception as e:        logger.error(f"Error sending data to agent: {e}")        return jsonify({"error": str(e)}), 500# app route to get recieve the messages on the agent@app.route('/api/webhook', methods=['POST'])def webhook():    """Handle incoming messages from the dashboard agent."""    global agent_response    try:        # Parse the incoming webhook message        data = request.get_data().decode("utf-8")        logger.info("Received response")        message = parse_message_from_agent(data)        agent_response = message.payload        logger.info(f"Processed response: {agent_response}")        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500if __name__ == "__main__":    load_dotenv()    init_client()    app.run(host="0.0.0.0", port=5002)

### Explanation[â€‹](#explanation-1 "Direct link to Explanation")

*   **Data Model**: The Request and Response models define the structure of messages exchanged between the AI Agent and the uAgent. A correctly defined data model is essential for sending a message to the uAgent from an SDK-based AI Agent.
*   **Schema Validation**: The model digest ensures that messages conform to the expected schema before transmission, preventing format mismatches.
*   **Sending Data**: The AI Agent sends a message to the uAgent using the /request endpoint.
*   **Handling Response**: The AI Agent listens for responses using the /api/webhook endpoint.

Environment Variables[â€‹](#environment-variables "Direct link to Environment Variables")
---------------------------------------------------------------------------------------

Create a .env file and add the following environment variables:

    AGENTVERSE_API_KEY="YOUR_AGENTVERSE_API_KEY"AGENT_SECRET_KEY="YOUR_SECRET_KEY_FOR_AI_AGENT"

Replace the placeholders with your actual API keys and agent secrets. Refer to this [guide](https://innovationlab.fetch.ai/resources/docs/1.0.3/agentverse/agentverse-api-key) to get your Agentverse API Key.

Testing the Communication[â€‹](#testing-the-communication "Direct link to Testing the Communication")
---------------------------------------------------------------------------------------------------

### Step 1: Running the uAgent[â€‹](#step-1-running-the-uagent "Direct link to Step 1: Running the uAgent")

Start the uAgent by running the following command in your terminal:

#### uAgent Logs[â€‹](#uagent-logs "Direct link to uAgent Logs")

    (venv) abhi@Fetchs-MacBook-Pro ILAgents % python3 uagent.pyINFO:     [Sample uAgent]: Starting agent with address: agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: My name is Sample uAgent and my address is agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8000&address=agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: Starting server on http://0.0.0.0:8000 (Press CTRL+C to quit)INFO:     [uagents.registration]: Registration on Almanac API successfulINFO:     [uagents.registration]: Registering on almanac contract...INFO:     [uagents.registration]: Registering on almanac contract...complete

#### Copy the uAgent Address[â€‹](#copy-the-uagent-address "Direct link to Copy the uAgent Address")

Look for the uAgent address in the logs, which appears in this format:

    agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6

Copy this uAgent address and paste it into the AI Agent script at the following line:

    uagent_address="PASTE YOUR UAGENT ADDRESS HERE"

### Step 2: Running the AI Agent[â€‹](#step-2-running-the-ai-agent "Direct link to Step 2: Running the AI Agent")

Start the AI Agent by running the following command in your terminal:

#### AI Agent Logs[â€‹](#ai-agent-logs "Direct link to AI Agent Logs")

    (venv) abhi@Fetchs-MacBook-Pro ILAgents % python3 ai_agent.pyINFO:__main__:Client agent started with address: agent1qw7u5sw63a88kmcn5j5kxf7q326u5hgmppvy2vpxlh3re6y0yp8253ec7xlINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Client agent registration complete! * Serving Flask app 'ai_agent' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5002 * Running on http://172.20.10.2:5002INFO:werkzeug:Press CTRL+C to quit

### Step 3: Sending a message from Agent2 to Agent1[â€‹](#step-3-sending-a-message-from-agent2-to-agent1 "Direct link to Step 3: Sending a message from Agent2 to Agent1")

We will use the following curl command to send a message from the AI Agent to the uAgent:

    curl -X POST http://localhost:5002/request \-H "Content-Type: application/json" \-d '{  "payload": {"message": "Hello uAgent!"}}'

#### Expected Logs on the uAgent Terminal[â€‹](#expected-logs-on-the-uagent-terminal "Direct link to Expected Logs on the uAgent Terminal")

    (venv) abhi@Fetchs-MacBook-Pro ILAgents % python3 uagent.pyINFO:     [Sample uAgent]: Starting agent with address: agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: My name is Sample uAgent and my address is agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8000&address=agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: Starting server on http://0.0.0.0:8000 (Press CTRL+C to quit)INFO:     [uagents.registration]: Registration on Almanac API successfulINFO:     [uagents.registration]: Registering on almanac contract...INFO:     [uagents.registration]: Registering on almanac contract...completeINFO:     [Sample uAgent]: Received message from agent1qw7u5sw63a88kmcn5j5kxf7q326u5hgmppvy2vpxlh3re6y0yp8253ec7xl: Hello uAgent!

#### Expected Logs on the AI Agent Terminal[â€‹](#expected-logs-on-the-ai-agent-terminal "Direct link to Expected Logs on the AI Agent Terminal")

    (venv) abhi@Fetchs-MacBook-Pro ILAgents % python3 ai_agent.pyINFO:__main__:Client agent started with address: agent1qw7u5sw63a88kmcn5j5kxf7q326u5hgmppvy2vpxlh3re6y0yp8253ec7xlINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Client agent registration complete! * Serving Flask app 'ai_agent' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5002 * Running on http://172.20.10.2:5002INFO:werkzeug:Press CTRL+C to quit{"version":1,"sender":"agent1qw7u5sw63a88kmcn5j5kxf7q326u5hgmppvy2vpxlh3re6y0yp8253ec7xl","target":"agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6","session":"c28d03fd-ad42-4c09-80b8-2bb8df9d7c3e","schema_digest":"model:14d760ab9a6127711e530c0f1bd84d5caa48c6bc6566ca489581d6918e6dff85","protocol_digest":"proto:a03398ea81d7aaaf67e72940937676eae0d019f8e1d8b5efbadfef9fd2e98bb2","payload":"eyJtZXNzYWdlIjoiaGV5In0=","expires":null,"nonce":null,"signature":"sig14ukfdrc98924wvx66xa2c32pk2wqyn69cepkvcpwahlygv3tc2t8hrzfkuzc9earscpnasdt8txpu99cyw24gmyspfdhqkxsn7a3lnq60mpaq"}INFO:fetchai:Got response looking up agent endpointhttp://localhost:8000/submitINFO:fetchai:Sent message to agentINFO:werkzeug:127.0.0.1 - - [30/Jan/2025 14:21:58] "POST /request HTTP/1.1" 200 -INFO:__main__:Received responseINFO:__main__:Processed response: {'response': 'Hello, AI Agent! I received your message: Hello uAgent!'}INFO:werkzeug:127.0.0.1 - - [30/Jan/2025 14:21:58] "POST /api/webhook HTTP/1.1" 200 -

Explanation of Communication Flow[â€‹](#explanation-of-communication-flow "Direct link to Explanation of Communication Flow")
---------------------------------------------------------------------------------------------------------------------------

*   The AI Agent sends a message using the **send\_message\_to\_agent** function.
*   The uAgent receives the message via the **@uagent.on\_message** handler.
*   The uAgent processes the message and responds using **await ctx.send()**.
*   The AI Agent receives the response through the **/api/webhook** endpoint.

Key Takeaways[â€‹](#key-takeaways "Direct link to Key Takeaways")
---------------------------------------------------------------

*   The **uAgent** handles structured messages using Fetch.aiâ€™s uAgents framework.
*   The **AI Agent** utilizes Fetch.aiâ€™s SDK for message transmission and parsing.
*   Communication between the two agents follows a request-response pattern using their respective handlers.

This setup can be extended to build more complex agent interactions involving dynamic data exchange, service orchestration, and autonomous decision-making.</content>
</page>

<page>
  <title>Search Agent and MicroServices | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/agentverse/searching</url>
  <content>Searching microservices and AI Agents on Agentverse
---------------------------------------------------

When you want to discover or connect with microservices or AI agents dynamically on Agentverse, you can use the **Agentverse Search API**. Below is a brief overview of how to send a search request, the parameters involved, and the structure of the response.

Making a Search Request[â€‹](#making-a-search-request "Direct link to Making a Search Request")
---------------------------------------------------------------------------------------------

*   Python
*   Curl
*   JavaScript

    body = {    "filters": {        "state": [],        "category": [],        "agent_type": [],        "protocol_digest": []    },    "sort": "relevancy",    "direction": "asc",    "search_text": "<string>",    "offset": 0,    "limit": 1,}await fetch("https://agentverse.ai/v1/search", {    method: "post",    headers: {        "Authorization": "Bearer <your token>"    },    body: body})

Response[â€‹](#response "Direct link to Response")
------------------------------------------------

You will receive a list of JSON objects with details about each agent:

    [  {    "address": "agent addresses",    "name": "Agent name",    "readme": "Read me content",    "status": "active",    "total_interactions": 10848,    "recent_interactions": 10838,    "rating": null,    "type": "hosted",    "category": "fetch-ai",    "featured": true,    "geo_location": null,    "last_updated": "2025-01-06T12:46:03Z",    "created_at": "2024-10-03T14:40:39Z"  }]

### Available Filters[â€‹](#available-filters "Direct link to Available Filters")

*   **state** : `active`, `inactive`.
*   **category** : `fetch-ai`, `community`.
*   **agent\_type** : `hosted`, `local`, `mailbox`, `proxy`, `custom`.
*   **protocol\_digest** : The protocol in which agent is included into.
*   **model\_digest** : Model digest in which agent is included into.

### Importance of Good Readme[â€‹](#importance-of-good-readme "Direct link to Importance of Good Readme")

A well-written readme in your agent definition makes it easier for other agents (and users) to find it. Make sure you:

*   Include descriptive names, tags, or domains. You can mention `[tags : ]` and `[domain : ]` in your agent.
    
*   Describe the main functions or services the agent provides.
    
*   Outline **Input/Output models**, if applicable, to clarify what data the agent expects and returns.
    
    *   For SDK AI Agent
    
        ![tag:innovationlab](https://img.shields.io/badge/innovationlab-3D8BD3)![tag:domain/tag-of-your-agent](https://img.shields.io/badge/domain-colorcode)<description>My AI's description of capabilities and offerings</description><use_cases>    <use_case>An example of one of your AI's use cases.</use_case></use_cases><payload_requirements><description>The requirements your AI has for requests</description><payload>    <requirement>        <parameter>question</parameter>        <description>The question that you would like this AI work with you to solve</description>    </requirement></payload></payload_requirements>
    
    *   For uAgents
    
        ![tag:innovationlab](https://img.shields.io/badge/innovationlab-3D8BD3)![tag:domain/tag-of-your-agent](https://img.shields.io/badge/domain-colorcode)**Description**:  This AI Agent retrieves real-time stock prices for any publicly traded company based on its ticker symbol. It provides  share prices, stock quotes, and stock prices to users. Simply input a stock ticker (e.g., AAPL, TSLA)  to get the latest stock price.**Input Data Model**class StockPriceRequest(Model):    ticker: str**Output Data Model**class StockPriceResponse(Model):    price: float
    

By following these guidelines, you can improve your agentâ€™s visibility in search results and help others understand its capabilities and usage requirements.

note

**Note:** If you are creating your agents in **`Hackathon`**, do remember to include the innovation labs tags.

    ![tag:innovationlab](https://img.shields.io/badge/innovationlab-3D8BD3)

Please include domain tag to your agent like below,</content>
</page>

<page>
  <title>End to End Application with Agentverse Architecture | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/agentverse/agentverse-based-application</url>
  <content>This diagram depicts an agent-based architecture, where the Client Application interacts with a Prime Agent to discover and utilize various specialized Agents registered within the AgentVerse. The Prime Agent orchestrates the communication between the Client and the Agents to fulfill the client's requests.

The flow of the system is as follows:

1.  The client application searches the AgentVerse to discover available agents.
2.  The client application sends a request to the Prime Agent.
3.  The Prime Agent communicates with the AgentVerse to find the appropriate Agent.
4.  The Prime Agent sends the request to the Agent.
5.  The Agent processes the request and sends the response back to the Prime Agent.
6.  The Prime Agent returns the final response to the client application.

This architecture allows for a modular and extensible system, where new agents can be easily integrated, and the Prime Agent can orchestrate the interactions between agents to fulfill complex client requests.

Core Components and Implementation[â€‹](#core-components-and-implementation "Direct link to Core Components and Implementation")
------------------------------------------------------------------------------------------------------------------------------

The system enables intelligent financial analysis through a team of specialized agents, coordinated via Agentverse. It combines frontend user interaction with backend processing through multiple agent layers.

### 1\. React Frontend (Client Application)[â€‹](#1-react-frontend-client-application "Direct link to 1. React Frontend (Client Application)")

*   React-based user interface (OptimusPrime component)
*   Two main API endpoints:
    *   `/api/send-request`: Sends analysis queries
    *   `/api/get-response`: Polls for results
*   Handles message display and user interactions

### 2\. Primary Agent (Query Router)[â€‹](#2-primary-agent-query-router "Direct link to 2. Primary Agent (Query Router)")

*   **Port**: 5001
*   **Role**: Routes queries to Financial Analysis Agent
*   **Key Functions**:
    *   Discovers Financial Analysis Agent through Agentverse
    *   Forwards user queries
    *   Manages response polling
*   **Endpoints**:
    *   `/webhook`: Receives responses from Financial Agent

### 3\. Financial Analysis Agent[â€‹](#3-financial-analysis-agent "Direct link to 3. Financial Analysis Agent")

*   **Port**: 5008
*   **Role**: Processes financial analysis requests
*   **Components**:
    *   Supervisor Agent: Coordinates analysis
    *   Search Agent: Handles market research
    *   SEC Analyst: Processes SEC filings
*   **Tools**:
    *   RAG System for document analysis
    *   Tavily Search for market data

### 4\. Agentverse Integration[â€‹](#4-agentverse-integration "Direct link to 4. Agentverse Integration")

*   **Agent Discovery**: Allows finding agents by capability
*   **Agent Registry**: Manages agent registration and lookup
*   **Message Routing**: Handles inter-agent communication

Communication Flow[â€‹](#communication-flow "Direct link to Communication Flow")
------------------------------------------------------------------------------

1.  User submits query through React UI
2.  Primary Agent searches for Financial Analysis Agent
3.  Primary Agent forwards query to Financial Agent
4.  Financial Agent processes query using specialist team
5.  Response returns through Agentverse to Primary Agent
6.  Frontend polls Primary Agent for results
7.  Results displayed to user

Key Implementation Details[â€‹](#key-implementation-details "Direct link to Key Implementation Details")
------------------------------------------------------------------------------------------------------

### Frontend[â€‹](#frontend "Direct link to Frontend")

    // Sends request and starts pollingconst handleSendMessage = async () => {    await fetch('/api/send-request', {...});    startPolling();  // Begin checking for response};

### Primary Agent[â€‹](#primary-agent "Direct link to Primary Agent")

    # Agent discovery and routingdef find_financial_agent():    available_ais = fetch.ai("Financial Analysis Agent")    return available_ais.get('ais', [0])@app.route('/webhook', methods=['POST'])def webhook():    message = parse_message_from_agent(data)    primary_agent.latest_response = message.payload

### Financial Agent registration with Agentverse[â€‹](#financial-agent-registration-with-agentverse "Direct link to Financial Agent registration with Agentverse")

    # Agent registrationregister_with_agentverse(    identity=financial_identity,    url="http://localhost:5008/webhook",    agent_title="Financial Analysis Agent",    readme="..."  # Capabilities description)

Detailed Implementation Details
-------------------------------

1\. Frontend Implementation (React)[â€‹](#1-frontend-implementation-react "Direct link to 1. Frontend Implementation (React)")
----------------------------------------------------------------------------------------------------------------------------

### Message Handling and UI State[â€‹](#message-handling-and-ui-state "Direct link to Message Handling and UI State")

    const OptimusPrime = () => {    const [messages, setMessages] = useState([]);    const [inputText, setInputText] = useState('');    const [isProcessing, setIsProcessing] = useState(false);    // Handles submitting new messages    const handleSendMessage = async () => {        if (!inputText.trim() || isProcessing) return;        // Add user message to UI        const userMessage = {            type: 'user',            content: inputText,            timestamp: new Date().toLocaleTimeString()        };        setMessages(prev => [...prev, userMessage]);        setInputText('');        setIsProcessing(true);        try {            // Send request to primary agent                        await fetch('/api/send-request', {                method: 'POST',                headers: { 'Content-Type': 'application/json' },                body: JSON.stringify({ input: inputText }),            });            // Start polling for response            startPollingForResponse();        } catch (error) {            handleError(error);        }    };

### Response Polling System[â€‹](#response-polling-system "Direct link to Response Polling System")

        // Polls for agent response    const startPollingForResponse = () => {        const pollInterval = setInterval(async () => {            try {                const responseData = await fetch('/api/get-response');                const data = await responseData.json();                if (data.status !== 'waiting' && data.analysis_result) {                    clearInterval(pollInterval);                    setIsProcessing(false);                    // Process agent responses                    data.analysis_result.analysis.forEach(response => {                        setMessages(prev => [...prev, {                            type: 'agent',                            agentName: response.name || 'Agent',                            content: response.content,                            timestamp: new Date().toLocaleTimeString()                        }]);                    });                }            } catch (error) {                clearInterval(pollInterval);                setIsProcessing(false);                handleError(error);            }        }, 1000);    };

2\. Primary Agent Implementation[â€‹](#2-primary-agent-implementation "Direct link to 2. Primary Agent Implementation")
---------------------------------------------------------------------------------------------------------------------

### Agent Setup and Initialization[â€‹](#agent-setup-and-initialization "Direct link to Agent Setup and Initialization")

    class PrimaryAgent:    def __init__(self):        self.identity = None        self.latest_response = None        def initialize(self):        try:            # Initialize agent identity            self.identity = Identity.from_seed(                os.getenv("PRIMARY_AGENT_KEY"),                 0            )                        # Register with Agentverse            register_with_agentverse(                identity=self.identity,                url="http://localhost:5001/webhook",                agentverse_token=os.getenv("AGENTVERSE_API_KEY"),                agent_title="Financial Query Router",                readme="<description>Routes queries to Financial Analysis Agent</description>"            )        except Exception as e:            logger.error(f"Initialization error: {e}")            raise

### Financial Agent Discovery and Communication[â€‹](#financial-agent-discovery-and-communication "Direct link to Financial Agent Discovery and Communication")

        def find_financial_agent(self):        """Find registered financial analysis agent"""        try:            # Search for financial agent in Agentverse            available_ais = fetch.ai("Financial Analysis Agent")            agents = available_ais.get('ais', [])                        if agents:                logger.info(f"Found financial agent at address: {agents[0]['address']}")                return agents[0]            return None                    except Exception as e:            logger.error(f"Error finding financial agent: {e}")            return None    @app.route('/api/send-request', methods=['POST'])    def send_request():        try:            # Extract user query            data = request.json            user_input = data.get('input')                        # Find and validate financial agent            agent = primary_agent.find_financial_agent()            if not agent:                return jsonify({"error": "Financial analysis agent not available"}), 404                        # Forward request to financial agent            send_message_to_agent(                primary_agent.identity,                agent['address'],                {"request": user_input}            )                        return jsonify({"status": "request_sent"})                    except Exception as e:            logger.error(f"Error processing request: {e}")            return jsonify({"error": str(e)}), 500

### Response Management[â€‹](#response-management "Direct link to Response Management")

        @app.route('/webhook', methods=['POST'])    def webhook():        try:            # Parse incoming agent message            data = request.get_data().decode("utf-8")            message = parse_message_from_agent(data)                        # Store response for polling            primary_agent.latest_response = message.payload                        return jsonify({"status": "success"})                    except Exception as e:            logger.error(f"Error in webhook: {e}")            return jsonify({"error": str(e)}), 500    @app.route('/api/get-response', methods=['GET'])    def get_response():        try:            if primary_agent.latest_response:                response = primary_agent.latest_response                primary_agent.latest_response = None                return jsonify(response)            return jsonify({"status": "waiting"})        except Exception as e:            logger.error(f"Error getting response: {e}")            return jsonify({"error": str(e)}), 500

3\. Financial Analysis Agent Registration[â€‹](#3-financial-analysis-agent-registration "Direct link to 3. Financial Analysis Agent Registration")
------------------------------------------------------------------------------------------------------------------------------------------------

### Agent Registration and Setup[â€‹](#agent-registration-and-setup "Direct link to Agent Registration and Setup")

    def init_agent():    """Initialize and register the agent with agentverse"""    global financial_identity, research_chain    try:        # Initialize the research chain        research_chain = init_financial_system()                # Initialize identity and register with Agentverse        financial_identity = Identity.from_seed(            os.getenv("FINANCIAL_AGENT_KEY"),             0        )                # Register with detailed capabilities description        register_with_agentverse(            identity=financial_identity,            url="http://localhost:5008/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="Financial Analysis Agent",            readme = """                <description>A comprehensive financial analysis agent that combines                 SEC filing analysis with real-time market data for Apple Inc.</description>                <use_cases>                    <use_case>Get detailed revenue analysis from SEC filings</use_case>                    <use_case>Analyze risk factors from latest 10-K</use_case>                    <use_case>Track financial metrics and trends</use_case>                </use_cases>                <payload_requirements>                    <payload>                        <requirement>                            <parameter>query</parameter>                            <description>What would you like to know about Apple's financials?</description>                        </requirement>                    </payload>                </payload_requirements>            """        )

### Query Processing and Response[â€‹](#query-processing-and-response "Direct link to Query Processing and Response")

    @app.route('/webhook', methods=['POST'])def webhook():    try:        # Parse incoming message        data = request.get_data().decode('utf-8')        message = parse_message_from_agent(data)        query = message.payload.get("request", "")        agent_address = message.sender        # Validate query        if not query:            return jsonify({"status": "error", "message": "No query provided"}), 400        # Process query using research chain        result = research_chain.invoke({            "messages": [HumanMessage(content=query)],            "team_members": ["Search", "SECAnalyst"]        })        # Format response for client        formatted_result = {            "analysis": [                {                    "role": msg.type if hasattr(msg, 'type') else "message",                    "content": msg.content,                    "name": msg.name if hasattr(msg, 'name') else None                }                for msg in result.get('messages', [])            ]        }        # Send response back through Agentverse        send_message_to_agent(            financial_identity,            agent_address,            {'analysis_result': formatted_result}        )        return jsonify({"status": "analysis_sent"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"status": "error", "message": str(e)}), 500</content>
</page>

<page>
  <title>TransactAI Payment Example | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/transactAI/</url>
  <content>This example demonstrates how to use TransactAI for agent-to-agent payments on Agentverse with [TransactAI](https://agentverse.ai/agents/details/agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrq/profile). The example shows a complete payment flow between two agents: Alice (sender) and Bob (receiver).

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

The example demonstrates the following:

1.  Registering agents with TransactAI
2.  Linking on-chain wallet addresses
3.  Making on-chain deposits to fund the TransactAI account
4.  Sending payments between agents
5.  Receiving payments
6.  Withdrawing funds to on-chain wallets

Workflow Diagram[â€‹](#workflow-diagram "Direct link to Workflow Diagram")
------------------------------------------------------------------------

1.  **Registration Process**: Before using TransactAI, agents must register themselves and their wallet addresses.
    
2.  **Denominations**: All amounts are expressed in "atestfet" (smallest unit), where 1 TESTFET = 10^18 atestfet.
    
3.  **Deposit Process**:
    
    *   Make an on-chain deposit to TransactAI's wallet
    *   Notify TransactAI with the transaction hash
    *   Wait for TransactAI to confirm the deposit (may require several blockchain confirmations)
4.  **Payment Flow**:
    
    *   Sender sends payment command to TransactAI
    *   TransactAI updates internal balances
    *   TransactAI sends confirmation to sender
    *   TransactAI sends notification to recipient
5.  **Withdrawal Process**:
    
    *   Send withdrawal request to TransactAI with amount and wallet address
    *   TransactAI executes on-chain transaction
    *   TransactAI sends confirmation with transaction hash
6.  **Acknowledgements**: All messages must be acknowledged to confirm receipt.
    

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

*   [Agnetverse](https://agentverse.ai/) account.
*   Access to Fetch.ai Dorado testnet.
*   Testnet tokens (can be obtained from [Fetch.ai Dorado Faucet](https://companion.fetch.ai/dorado-1/accounts/fetch1n9vnna29c8us5pk9fxqhzltmgpeyn3hl4le28h#Transactions))

Example Code[â€‹](#example-code "Direct link to Example Code")
------------------------------------------------------------

### agent\_protocol.py[â€‹](#agent_protocolpy "Direct link to agent_protocol.py")

First, we need the custom protocol that enables communication with TransactAI:

agent\_protocol.py

    """Custom Agent Protocol for TransactAIThis protocol allows agents to communicate with the TransactAI payment agentby defining message structures and content types."""from uagents import Protocolfrom datetime import datetimefrom typing import Literal, TypedDict, Dict, List, Unionimport uuidfrom pydantic.v1 import UUID4, Fieldfrom uagents_core.models import Modelfrom uagents_core.protocol import ProtocolSpecification# --- Content Model Definitions ---class Metadata(TypedDict, total=False):    mime_type: str    role: strclass TextContent(Model):    type: Literal["text"] = "text"    text: strclass MetadataContent(Model):    type: Literal["metadata"] = "metadata"    metadata: dict[str, str]# Combined content typesAgentContent = Union[TextContent, MetadataContent]# --- Main Protocol Message Models ---class AgentMessage(Model):    timestamp: datetime = Field(default_factory=datetime.utcnow)    msg_id: UUID4 = Field(default_factory=uuid.uuid4)    content: list[AgentContent]class AgentAcknowledgement(Model):    timestamp: datetime = Field(default_factory=datetime.utcnow)    acknowledged_msg_id: UUID4    metadata: dict[str, str] | None = None# --- Protocol Specification ---agent_protocol_spec = ProtocolSpecification(    name="AgentProtocol",    version="1.0.0",    interactions={        AgentMessage: {AgentAcknowledgement},        AgentAcknowledgement: set(),    },)# --- Protocol Instance ---agent_proto = Protocol(spec=agent_protocol_spec)# --- Helper Functions ---def create_metadata_message(metadata: Dict[str, str]) -> AgentMessage:    """Create an agent message with metadata content"""    return AgentMessage(        content=[MetadataContent(metadata=metadata)]    )def create_text_message(text: str) -> AgentMessage:    """Create an agent message with text content"""    return AgentMessage(        content=[TextContent(text=text)]    )

### Alice (Sender) Agent[â€‹](#alice-sender-agent "Direct link to Alice (Sender) Agent")

alice\_agent.py

    """Alice Agent - Sends payments via TransactAI"""import asynciofrom uagents import Agent, Contextfrom uagents.network import get_ledgerfrom datetime import datetime# Import the custom agent protocol# Ensure agent_protocol.py is in the same directory or accessible in PYTHONPATHtry:    from agent_protocol import (        agent_proto,        AgentMessage,        AgentAcknowledgement,        create_metadata_message    )except ImportError:    print("Error: agent_protocol.py not found. Please ensure it's in the correct path.")    # Define minimal models if import fails, to allow basic understanding    from uagents import Model, Protocol    from typing import List, Dict, Union, Literal, Optional    from pydantic.v1 import Field, UUID4    import uuid    class MetadataContent(Model):        type: Literal["metadata"] = "metadata"        metadata: dict[str, str]    AgentContent = Union[MetadataContent]    class AgentMessage(Model):        timestamp: datetime = Field(default_factory=datetime.utcnow)        msg_id: UUID4 = Field(default_factory=uuid.uuid4)        content: list[AgentContent]    class AgentAcknowledgement(Model):        timestamp: datetime = Field(default_factory=datetime.utcnow)        acknowledged_msg_id: UUID4        metadata: Optional[dict[str, str]] = None    def create_metadata_message(metadata: Dict[str, str]) -> AgentMessage:        return AgentMessage(content=[MetadataContent(metadata=metadata)])    # Define a dummy protocol if needed for basic structure    agent_proto = Protocol("DummyAgentProto", version="1.0")# TransactAI agent address (ensure this is correct)TRANSACTAI_ADDRESS = "agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrq"TRANSACTAI_WALLET = "fetch1uyxsdlejg7axp4dzmqpq54g0uwde5nv6fflhkv" # TransactAI's on-chain wallet# Bob's agent addressBOB_ADDRESS = "agent1q2v8ck694z7h6qww90kvv4qafrnvlaquwpeuqtarsyhdmsve4v6k7r0a8l0" # Replace it with your bob's address # Alice agent setup# CRITICAL: Replace this seed phrase with your own secure one for actual use!alice = Agent()DEPOSIT_CONFIRMED_FLAG = "deposit_confirmed"PAYMENT_ATTEMPTED_FLAG = "payment_attempted"@alice.on_event("startup")async def startup(ctx: Context):    ctx.logger.info(f"Alice started. Address: {alice.address}")    ctx.logger.info(f"Wallet address: {alice.wallet.address()}")    # Initialize flags    ctx.storage.set(DEPOSIT_CONFIRMED_FLAG, False)    ctx.storage.set(PAYMENT_ATTEMPTED_FLAG, False)    # 1. Register Agent with TransactAI    ctx.logger.info("Registering agent with TransactAI...")    register_msg = create_metadata_message({'command': 'register'})    await ctx.send(TRANSACTAI_ADDRESS, register_msg)    await asyncio.sleep(2.0) # Wait for agent registration    # 2. Register Wallet with TransactAI    ctx.logger.info("Registering wallet with TransactAI...")    wallet_address = str(alice.wallet.address()) # Get Alice's wallet address    register_wallet_msg = create_metadata_message({        'command': 'register_wallet',        'wallet_address': wallet_address    })    await ctx.send(TRANSACTAI_ADDRESS, register_wallet_msg)    await asyncio.sleep(5.0) # Wait for wallet registration    # 3. On-chain deposit to TransactAI wallet    ctx.logger.info("Sending on-chain deposit to TransactAI wallet...")    deposit_amount = 100000000000000000 # 0.1 testfet (needs enough for payment)    tx_hash = None    try:        ledger = get_ledger("dorado") # Get ledger instance right before use        # Ensure wallet has funds from faucet: https://companion.fetch.ai/dorado-1/accounts        ctx.logger.info(f"Attempting to send {deposit_amount} atestfet to {TRANSACTAI_WALLET}")        tx = ledger.send_tokens(TRANSACTAI_WALLET, deposit_amount, "atestfet", alice.wallet)        result = tx.wait_to_complete()        tx_hash = result.tx_hash        ctx.logger.info(f"Deposit transaction hash: {tx_hash}")    except Exception as e:        ctx.logger.error(f"Error sending on-chain deposit: {e}")        ctx.logger.error("Ensure Alice's wallet has sufficient 'atestfet' from the faucet.")        return    # 4. Send deposit confirmation command to TransactAI    if tx_hash:        ctx.logger.info(f"Sending deposit confirmation command for tx_hash: {tx_hash}")        deposit_confirm_msg = create_metadata_message({            'command': 'deposit',            'tx_hash': tx_hash,            'amount': str(deposit_amount),            'denom': "atestfet"        })        await ctx.send(TRANSACTAI_ADDRESS, deposit_confirm_msg)        # Wait for deposit confirmation state change        MAX_WAIT_TIME = 60 # seconds        WAIT_INTERVAL = 5 # seconds        time_waited = 0        deposit_confirmed = False        while time_waited < MAX_WAIT_TIME:            if ctx.storage.get(DEPOSIT_CONFIRMED_FLAG) is True:                deposit_confirmed = True                ctx.logger.info("Deposit confirmed by TransactAI.")                break            ctx.logger.info(f"Waiting for deposit confirmation... ({time_waited}/{MAX_WAIT_TIME}s)")            await asyncio.sleep(WAIT_INTERVAL)            time_waited += WAIT_INTERVAL        if not deposit_confirmed:            ctx.logger.error("Timed out waiting for deposit confirmation from TransactAI.")            return # Stop if deposit not confirmed    else:        ctx.logger.error("On-chain deposit failed, cannot proceed.")        return # Stop if deposit failed    # 5. Send payment to Bob via TransactAI (only if deposit confirmed and payment not already attempted)    # This part is triggered by handle_transactai_response upon successful deposit confirmation    # We call maybe_send_payment here just in case the confirmation message arrived *during* the wait loop    await maybe_send_payment(ctx)# Handle responses from TransactAI@agent_proto.on_message(model=AgentMessage)async def handle_transactai_response(ctx: Context, sender: str, msg: AgentMessage):    ctx.logger.info(f"Received message from {sender}")    response_handled = False # Flag to ensure ack is sent even if no specific handler matches    for content in msg.content:        if content.type == "metadata":            metadata = content.metadata            ctx.logger.info(f"Metadata: {metadata}")                        command = metadata.get('command')            status = metadata.get('status')            if command == 'register_response':                 ctx.logger.info(f"Registration response: {status}")                 response_handled = True            elif command == 'register_wallet_response':                 ctx.logger.info(f"Wallet registration response: {status}")                 response_handled = True            elif command == 'deposit_response':                 ctx.logger.info(f"Deposit response received: {metadata}")                 if status == 'success':                     ctx.storage.set(DEPOSIT_CONFIRMED_FLAG, True)                     # If payment hasn't been attempted yet, trigger it now                     asyncio.create_task(maybe_send_payment(ctx))                 elif status == 'pending_confirmation':                     ctx.logger.info("Deposit is still pending confirmation.")                 else: # Failed                     ctx.logger.error(f"Deposit failed: {metadata.get('reason')}")                     # Consider setting a 'deposit_failed' flag if needed                 response_handled = True            elif command == 'payment_confirmation':                 if status == 'success':                     ctx.logger.info(f"Payment successful! New balance: {metadata.get('balance')}")                 else:                     ctx.logger.error(f"Payment failed! Reason: {metadata.get('reason')}, Balance: {metadata.get('balance')}")                 response_handled = True            # Handle other responses if needed (e.g., balance_response)                elif content.type == "text":            ctx.logger.info(f"Text: {content.text}")            response_handled = True # Acknowledge text messages too    # Send acknowledgement back to TransactAI if message was processed    if response_handled:        await ctx.send(sender, AgentAcknowledgement(            timestamp=datetime.utcnow(),            acknowledged_msg_id=msg.msg_id        ))    else:        ctx.logger.warning(f"Received unhandled message content types from {sender}: {[c.type for c in msg.content]}")# Separate function to attempt payment after deposit confirmationasync def maybe_send_payment(ctx: Context):    # Ensure deposit is confirmed and payment hasn't been attempted    if ctx.storage.get(DEPOSIT_CONFIRMED_FLAG) is True and not ctx.storage.get(PAYMENT_ATTEMPTED_FLAG):        payment_amount = 100000000000000000 # Example amount (0.1 atestfet)        ctx.logger.info(f"Deposit confirmed, now attempting to pay {payment_amount} to Bob ({BOB_ADDRESS})...")        ctx.storage.set(PAYMENT_ATTEMPTED_FLAG, True) # Mark as attempted        payment_msg = create_metadata_message({            'command': 'payment',            'recipient': BOB_ADDRESS,            'amount': str(payment_amount),            'reference': f"payment-{datetime.utcnow().isoformat()}"        })        await ctx.send(TRANSACTAI_ADDRESS, payment_msg)# Handle acknowledgements (optional)@agent_proto.on_message(model=AgentAcknowledgement)async def handle_ack(ctx: Context, sender: str, msg: AgentAcknowledgement):    ctx.logger.info(f"Received acknowledgement from {sender} for message {msg.acknowledged_msg_id}")# Include the protocolalice.include(agent_proto)if __name__ == "__main__":    print(f"Alice starting. Address: {alice.address}")    print("Ensure agent_protocol.py is accessible.")    print("CRITICAL: Replace the example ALICE_SEED in the code if using for anything beyond this demo.")    alice.run()

### Bob (Receiver) Agent[â€‹](#bob-receiver-agent "Direct link to Bob (Receiver) Agent")

bob\_agent.py

    """Bob Agent - Receives payments via TransactAI"""import asynciofrom uagents import Agent, Contextfrom datetime import datetime# Import the custom agent protocolfrom agent_protocol import (    agent_proto,    AgentMessage,    AgentAcknowledgement,    create_metadata_message)# TransactAI agent address (ensure this is correct)TRANSACTAI_ADDRESS = "agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrq"# Bob agent setupbob = Agent()@bob.on_event("startup")async def startup(ctx: Context):    ctx.logger.info(f"Bob started. Address: {bob.address}")    ctx.logger.info(f"Wallet address: {bob.wallet.address()}")    # Give agents time to register etc.    await asyncio.sleep(5.0)    # 1. Register Agent with TransactAI    ctx.logger.info("Registering agent with TransactAI...")    register_msg = create_metadata_message({'command': 'register'})    await ctx.send(TRANSACTAI_ADDRESS, register_msg)    await asyncio.sleep(2.0) # Wait briefly    # 2. Register Wallet with TransactAI    ctx.logger.info("Registering wallet with TransactAI...")    wallet_address = str(bob.wallet.address()) # Get Bob's wallet address    register_wallet_msg = create_metadata_message({        'command': 'register_wallet',        'wallet_address': wallet_address    })    await ctx.send(TRANSACTAI_ADDRESS, register_wallet_msg)    await asyncio.sleep(2.0) # Wait briefly# Handle responses/notifications from TransactAI@agent_proto.on_message(model=AgentMessage)async def handle_transactai_message(ctx: Context, sender: str, msg: AgentMessage):    ctx.logger.info(f"Received message from {sender}")    for content in msg.content:        if content.type == "metadata":            metadata = content.metadata            ctx.logger.info(f"Metadata: {metadata}")                        command = metadata.get('command')            status = metadata.get('status')            if command == 'register_response':                 ctx.logger.info(f"Registration response: {status}")            elif command == 'payment_received':                 ctx.logger.info(f"Payment received from {metadata.get('from')}!")                 amount_received_str = metadata.get('amount')                 ctx.logger.info(f"Amount: {amount_received_str}, Reference: {metadata.get('reference')}")                 ctx.logger.info(f"New balance: {metadata.get('balance')}")                 # Attempt to withdraw the received amount                 try:                     amount_to_withdraw = int(amount_received_str)                     if amount_to_withdraw > 0:                         ctx.logger.info(f"Attempting to withdraw received amount: {amount_to_withdraw}")                         withdraw_msg = create_metadata_message({                             'command': 'withdraw',                             'amount': str(amount_to_withdraw),                             'wallet_address': str(bob.wallet.address()), # Bob's own wallet                             'denom': "atestfet"                         })                         # Send withdrawal request asynchronously                         asyncio.create_task(ctx.send(TRANSACTAI_ADDRESS, withdraw_msg))                     else:                         ctx.logger.info("Received payment amount is zero or invalid, not withdrawing.")                 except (ValueError, TypeError) as e:                     ctx.logger.error(f"Could not parse amount for withdrawal: {amount_received_str}, Error: {e}")            elif command == 'withdraw_confirmation':                 ctx.logger.info(f"Withdrawal confirmation received: {metadata}")                 if status == 'success':                     ctx.logger.info(f"Withdrawal successful! Tx: {metadata.get('tx_hash')}, New balance: {metadata.get('balance')}")                 else:                     ctx.logger.error(f"Withdrawal failed! Reason: {metadata.get('reason')}")            # Handle other relevant messages like escrow notifications if needed                elif content.type == "text":            ctx.logger.info(f"Text: {content.text}")    # Send acknowledgement back to TransactAI    await ctx.send(sender, AgentAcknowledgement(        timestamp=datetime.utcnow(),        acknowledged_msg_id=msg.msg_id    ))# Handle acknowledgements (optional)@agent_proto.on_message(model=AgentAcknowledgement)async def handle_ack(ctx: Context, sender: str, msg: AgentAcknowledgement):    ctx.logger.info(f"Received acknowledgement from {sender} for message {msg.acknowledged_msg_id}")# Include the protocolbob.include(agent_proto)if __name__ == "__main__":    print(f"Bob starting. Address: {bob.address}")    bob.run()

Running the Example[â€‹](#running-the-example "Direct link to Running the Example")
---------------------------------------------------------------------------------

To run this example:

1.  Create two agents Alice and Bob on Agentverse.
2.  Create `agent_protocol.py` in both the agents and put in the code from above.
3.  Get some test tokens for the sender's wallet from the [Fetch.ai Dorado Faucet](https://companion.fetch.ai/dorado-1/accounts)
4.  Start the receiver agent (bob) first from agentverse.
5.  Copy the bob's address and update `RECIPIENT_ADDRESS` in alice's agent.py
6.  Start the alice agent as well.

Expected Output[â€‹](#expected-output "Direct link to Expected Output")
---------------------------------------------------------------------

### Receiver Output (Bob)[â€‹](#receiver-output-bob "Direct link to Receiver Output (Bob)")

    Receiver agent started. Address: agent1q2v8ck694z7h6qww90kvv4qafrnvlaquwpeuqtarsyhdmsve4v6k7r0a8l0Wallet address: fetch1ljd6tn7kr27wv0jwn6g63a7x38zj5ltfvwm8dwRegistering with TransactAI...Registering wallet fetch1ljd6tn7kr27wv0jwn6g63a7x38zj5ltfvwm8dw...âœ… Setup complete - Ready to receive paymentsReceived message from agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrqMetadata: {'command': 'register_response', 'status': 'success', 'balance': '0'}Registration response: successReceived message from agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrqMetadata: {'command': 'register_wallet_response', 'status': 'success', 'wallet_address': 'fetch1ljd6tn7kr27wv0jwn6g63a7x38zj5ltfvwm8dw'}Wallet registration response: successReceived message from agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrqMetadata: {'command': 'payment_received', 'from': 'agent1q2gxyqnwyr85v6ek7rplk09etnqgn4cfllph6gx3gtsuqzzn5p856gway5s', 'amount': '50000000000000000', 'reference': 'Example payment 2025-04-29T13:18:27.441861', 'balance': '5.0E+16'}ğŸ’° Payment received!  From: agent1q2gxyqnwyr85v6ek7rplk09etnqgn4cfllph6gx3gtsuqzzn5p856gway5s  Amount: 50000000000000000 atestfet  Reference: Example payment 2025-04-29T13:18:27.441861  New balance: 5.0E+16Withdrawing 50000000000000000 atestfet to fetch1ljd6tn7kr27wv0jwn6g63a7x38zj5ltfvwm8dw...Received message from agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrqMetadata: {'command': 'withdraw_confirmation', 'status': 'success', 'amount': '50000000000000000', 'wallet_address': 'fetch1ljd6tn7kr27wv0jwn6g63a7x38zj5ltfvwm8dw', 'tx_hash': 'A44295DCE532825CE257D63C79A63C01180FCC8DEF44258271C5E5FB9AE0F561', 'balance': '0', 'message': 'Withdrawal processed. Funds sent to your wallet.'}âœ… Withdrawal successful!  Transaction hash: A44295DCE532825CE257D63C79A63C01180FCC8DEF44258271C5E5FB9AE0F561  New balance: 0

### Sender Output[â€‹](#sender-output "Direct link to Sender Output")

    Sender agent started. Address: agent1q2gxyqnwyr85v6ek7rplk09etnqgn4cfllph6gx3gtsuqzzn5p856gway5sWallet address: fetch1n9vnna29c8us5pk9fxqhzltmgpeyn3hl4le28hRegistering with TransactAI...Registering wallet fetch1n9vnna29c8us5pk9fxqhzltmgpeyn3hl4le28h...Making on-chain deposit of 100000000000000000 atestfet...Attempting to send 100000000000000000 atestfet to fetch1uyxsdlejg7axp4dzmqpq54g0uwde5nv6fflhkvDeposit transaction hash: AE4A482EFE4215D6A54C63D1F4EE9EFA50437572BDECBD347FAA752BD9B3E0FAWaiting for deposit confirmation... (0/60s)Received message from agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrqMetadata: {'command': 'register_response', 'status': 'success', 'balance': '0'}Registration response: successReceived message from agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrqMetadata: {'command': 'register_wallet_response', 'status': 'success', 'wallet_address': 'fetch1n9vnna29c8us5pk9fxqhzltmgpeyn3hl4le28h'}Wallet registration response: successReceived message from agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrqMetadata: {'command': 'deposit_response', 'status': 'pending_confirmation', 'reason': 'Awaiting confirmations (2/6)'}Deposit pending: Awaiting confirmations (2/6)Waiting for deposit confirmation... (5/60s)Waiting for deposit confirmation... (10/60s)Received message from agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrqMetadata: {'command': 'deposit_response', 'status': 'success', 'amount': '100000000000000000', 'denom': 'atestfet', 'balance': '1.0E+17', 'tx_hash': 'AE4A482EFE4215D6A54C63D1F4EE9EFA50437572BDECBD347FAA752BD9B3E0FA'}Deposit confirmed! New balance: 1.0E+17âœ… Deposit confirmed!Sending payment of 50000000000000000 atestfet to agent1q2v8ck694z7h6qww90kvv4qafrnvlaquwpeuqtarsyhdmsve4v6k7r0a8l0...Received message from agent1qtdvskm3g5ngmvfuqek6shrpjz6ed8jc84s6phmark05z5a8naxawu5jsrqMetadata: {'command': 'payment_confirmation', 'status': 'success', 'recipient': 'agent1q2v8ck694z7h6qww90kvv4qafrnvlaquwpeuqtarsyhdmsve4v6k7r0a8l0', 'amount': '50000000000000000', 'balance': '5.0E+16'}Payment successful! New balance: 5.0E+16

Additional Features[â€‹](#additional-features "Direct link to Additional Features")
---------------------------------------------------------------------------------

In addition to basic payments, TransactAI also supports:

*   **Escrow Services**: Hold funds conditionally until released by the sender
*   **Balance Queries**: Check your current balance at any time
*   **Automated Withdrawals**: Set up automatic withdrawals for received payments

Refer to the complete [TransactAI documentation](https://innovationlab.fetch.ai/resources/docs/1.0.3/agent-transaction/agent-transaction) for more details on these advanced features.</content>
</page>

<page>
  <title>Getting Started with ASI-1 Mini | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/asione/asi1-mini-getting-started</url>
  <content>This guide will walk you through the process of setting up and making your first API call to ASI:One, the Web3-native Large Language Model designed for agentic AI.

How to Get an API Key[â€‹](#how-to-get-an-api-key "Direct link to How to Get an API Key")
---------------------------------------------------------------------------------------

Before you can start using ASI:One, you'll need to obtain an API key. Follow these steps:

1.  Visit [https://asi1.ai/chat](https://asi1.ai/chat) and log in to your account.

2.  Once logged in, look for the link in the top right corner to create an API Key.

3.  Click on "Create new" or "Create API Key" to generate a new key.

4.  Give your API key a descriptive name that helps you identify its purpose or the project it's associated with.

5.  Click "Create" to generate the key.
    
6.  Your API key will be displayed. Make sure to copy this key and store it securely, as you won't be able to view it again.
    

7.  You can manage your API keys at any time by visiting [https://asi1.ai/dashboard/api-keys](https://asi1.ai/dashboard/api-keys).

You can get your API key directly by visiting [https://asi1.ai/dashboard/api-keys](https://asi1.ai/dashboard/api-keys) after creating an account.

Making Your First API Call[â€‹](#making-your-first-api-call "Direct link to Making Your First API Call")
------------------------------------------------------------------------------------------------------

Once you have your API key, you can start making requests to the ASI:One API. Here's a simple example using Python:

    import requestsimport jsonurl = "https://api.asi1.ai/v1/chat/completions"payload = json.dumps({  "model": "asi1-mini",  "messages": [    {      "role": "user",      "content": "Hello, tell me about agentic AI"    }  ],  "temperature": 0.7,  "stream": False,  "max_tokens": 500})headers = {  'Content-Type': 'application/json',  'Accept': 'application/json',  'Authorization': 'Bearer YOUR_API_KEY'  # Replace with your actual API key}response = requests.request("POST", url, headers=headers, data=payload)print(response.text)

Replace `YOUR_API_KEY` with the API key you obtained in the previous steps.

Understanding the Response[â€‹](#understanding-the-response "Direct link to Understanding the Response")
------------------------------------------------------------------------------------------------------

The API will return a JSON response containing the model's reply. If you set `stream` to `True`, the response will be streamed as it's generated, which is useful for creating more responsive applications.

A typical response will include:

*   The completion ID
*   The model name
*   The generated text
*   Usage information (token counts)

Streaming Responses[â€‹](#streaming-responses "Direct link to Streaming Responses")
---------------------------------------------------------------------------------

For a more interactive experience, you can set the `stream` parameter to `True` to receive the response as it's being generated:

    import requestsimport jsonurl = "https://api.asi1.ai/v1/chat/completions"payload = json.dumps({  "model": "asi1-mini",  "messages": [    {      "role": "user",      "content": "Explain the concept of decentralized AI"    }  ],  "temperature": 0.7,  "stream": True,  "max_tokens": 500})headers = {  'Content-Type': 'application/json',  'Accept': 'application/json',  'Authorization': 'Bearer YOUR_API_KEY'  # Replace with your actual API key}response = requests.request("POST", url, headers=headers, data=payload, stream=True)for line in response.iter_lines():    if line:        line_text = line.decode('utf-8')        if line_text.startswith('data: '):            data_str = line_text[6:]  # Remove 'data: ' prefix            if data_str == '[DONE]':                break            try:                data = json.loads(data_str)                if 'choices' in data and len(data['choices']) > 0:                    delta = data['choices'][0].get('delta', {})                    if 'content' in delta:                        print(delta['content'], end='', flush=True)            except json.JSONDecodeError:                pass

Next Steps[â€‹](#next-steps "Direct link to Next Steps")
------------------------------------------------------

Now that you've made your first API call to ASI:One, you can explore more advanced features:

*   Check out the [API Reference](https://innovationlab.fetch.ai/resources/docs/1.0.3/asione/asi1-mini-api-reference) for detailed information on all available endpoints and parameters
*   Learn about [Chat Completion](https://innovationlab.fetch.ai/resources/docs/1.0.3/asione/asi1-mini-chat-completion) for more details on generating conversational responses
*   Explore [example](https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/asione/asi1-mini-language-tutor) applications and use cases.

By leveraging ASI-1 Mini's agentic capabilities, you can build sophisticated AI applications that can reason, plan, and execute complex tasks autonomously within the Web3 ecosystem.</content>
</page>

<page>
  <title>ASI-1 Mini API Reference | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/asione/asi1-mini-api-reference</url>
  <content>This API Reference describes the RESTful and streaming interfaces of the ASI:One platform.

Introduction[â€‹](#introduction "Direct link to Introduction")
------------------------------------------------------------

ASI:One provides a powerful API that allows developers to integrate advanced agentic AI capabilities into their applications. The API is designed to be easy to use while providing access to the full range of ASI:One's capabilities.

OpenAI Compatibility[â€‹](#openai-compatibility "Direct link to OpenAI Compatibility")
------------------------------------------------------------------------------------

Where possible, the ASI:One API conforms to the OpenAI API specification. This means that users can often plug the ASI:One API into existing code that uses the OpenAI API with minimal changes.

API keys can be created from your account when you log into ASI:One. Authorization is done by adding the following header into your requests:

    Authorization: Bearer <api token>

Remember your API key is a secret, do not share it with anyone. If you need to revoke access for a particular key, simply log into your account and delete it from your profile.

Base URL[â€‹](#base-url "Direct link to Base URL")
------------------------------------------------

All API requests should be made to the following base URL:

Endpoints[â€‹](#endpoints "Direct link to Endpoints")
---------------------------------------------------

### Chat Completions[â€‹](#chat-completions "Direct link to Chat Completions")

Creates a model response for the given chat conversation.

#### Request Parameters[â€‹](#request-parameters "Direct link to Request Parameters")

| Parameter | Type | Required | Description |
| --- | --- | --- | --- |
| model | string | Yes | ID of the model to use. Currently, only "asi1-mini" is available. |
| messages | array | Yes | An array of message objects representing the conversation history. |
| temperature | number | No | What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. Default is 1.0. |
| stream | boolean | No | If set to true, partial message deltas will be sent as they become available. Default is false. |
| max\_tokens | integer | No | The maximum number of tokens that can be generated in the chat completion. This can be used to control costs for text generated via API. |

#### Message Object[â€‹](#message-object "Direct link to Message Object")

Each message in the `messages` array should have the following structure:

| Field | Type | Description |
| --- | --- | --- |
| role | string | The role of the message author. Must be one of "system", "user", or "assistant". |
| content | string | The content of the message. |

#### Example Request[â€‹](#example-request "Direct link to Example Request")

    {  "model": "asi1-mini",  "messages": [    {      "role": "system",      "content": "You are a helpful assistant specialized in Web3 technologies."    },    {      "role": "user",      "content": "Explain the concept of decentralized AI."    }  ],  "temperature": 0.7,  "stream": false,  "max_tokens": 500}

#### Response Format[â€‹](#response-format "Direct link to Response Format")

The API returns a JSON object with the following structure:

| Field | Type | Description |
| --- | --- | --- |
| id | string | The completion request ID. |
| model | string | The name of the model being used. |
| thought | string | (Optional) The thoughts that were generated as part of the chat completion request. Only present when streaming is enabled. |
| choices | array | An array of completion choices. |
| usage | object | (Optional) Information about token usage. |

#### Choice Object[â€‹](#choice-object "Direct link to Choice Object")

Each choice in the `choices` array has the following structure:

| Field | Type | Description |
| --- | --- | --- |
| index | integer | The index of the choice in the array. |
| delta | object | (When streaming) Contains the incremental content being streamed. |
| finish\_reason | string | The reason the model stopped generating text. Can be "stop", "length", etc. |
| stop\_reason | string | Additional information about why generation stopped. |

#### Example Response (Non-streaming)[â€‹](#example-response-non-streaming "Direct link to Example Response (Non-streaming)")

    {  "id": "id_comqjiusZjAoyuXlh",  "object": "chat.completion",  "model": "asi1-mini",  "choices": [    {      "index": 0,      "message": {        "role": "assistant",        "content": "Decentralized AI refers to artificial intelligence systems that operate on distributed networks rather than centralized servers. This approach aligns with Web3 principles by removing central points of control and enabling more democratic access to AI capabilities..."      },      "finish_reason": "stop",      "stop_reason": "stop"    }  ],  "usage": {    "prompt_tokens": 435,    "completion_tokens": 391,    "total_tokens": 826  }}

#### Streaming Responses[â€‹](#streaming-responses "Direct link to Streaming Responses")

When `stream` is set to `true`, the API will send a series of server-sent events (SSE) with partial completions as they become available. Each event is prefixed with `data:` and contains a JSON object with incremental updates.

The stream is terminated with a `data: [DONE]` message.

Error Handling[â€‹](#error-handling "Direct link to Error Handling")
------------------------------------------------------------------

The API uses standard HTTP status codes to indicate the success or failure of requests:

*   200: Success
*   400: Bad Request (invalid parameters)
*   401: Unauthorized (invalid API key)
*   422: Unprocessable Entity (valid parameters but request cannot be processed)
*   429: Too Many Requests (rate limit exceeded)
*   500: Internal Server Error

Error responses include a JSON object with an `error` field containing details about the error.

Rate Limits[â€‹](#rate-limits "Direct link to Rate Limits")
---------------------------------------------------------

API usage is subject to rate limits based on your account tier. If you exceed these limits, you'll receive a 429 status code. The response headers include information about your current rate limit status.

Best Practices[â€‹](#best-practices "Direct link to Best Practices")
------------------------------------------------------------------

*   Store your API key securely and never expose it in client-side code.
*   Implement proper error handling to gracefully handle API errors.
*   For long-running conversations, consider maintaining context on your side to reduce token usage.
*   Use streaming for more responsive user interfaces when generating longer responses.

For more detailed examples of how to use the API, see the [Chat Completion](https://innovationlab.fetch.ai/resources/docs/1.0.3/asione/asi1-mini-chat-completion) guide.</content>
</page>

<page>
  <title>Chat Completion with ASI-1 Mini | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/asione/asi1-mini-chat-completion</url>
  <content>The Chat Completion API is the primary way to interact with ASI:One. This guide provides detailed information on how to use the API effectively, with examples and best practices.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

The Chat Completion API allows you to have conversational interactions with ASI:One. You provide a series of messages representing a conversation, and the model generates a response that continues the conversation in a natural way.

What sets ASI:One apart from other LLMs is its agentic capabilities - it can reason through complex problems, maintain context across long conversations, and execute multi-step tasks autonomously.

Endpoint[â€‹](#endpoint "Direct link to Endpoint")
------------------------------------------------

    POST https://api.asi1.ai/v1/chat/completions

Request Format[â€‹](#request-format "Direct link to Request Format")
------------------------------------------------------------------

A basic chat completion request includes the model name, a list of messages, and optional parameters to control the generation:

    {  "model": "asi1-mini",  "messages": [    {      "role": "user",      "content": "Your message here"    }  ],  "temperature": 0,  "stream": false,  "max_tokens": 500}

### Required Parameters[â€‹](#required-parameters "Direct link to Required Parameters")

*   `model`: Currently, only "asi1-mini" is available.
*   `messages`: An array of message objects representing the conversation history.

### Optional Parameters[â€‹](#optional-parameters "Direct link to Optional Parameters")

*   `temperature`: Controls randomness in the response. Range is 0-2, with lower values producing more deterministic outputs. Default is 1.0.
*   `stream`: When set to `true`, the API will stream the response as it's generated. Default is `false`.
*   `max_tokens`: The maximum number of tokens to generate. Default varies based on the model.

Message Roles[â€‹](#message-roles "Direct link to Message Roles")
---------------------------------------------------------------

Each message in the conversation has a `role` and `content`. The available roles are:

*   `system`: Used to set the behavior or context for the assistant. System messages help guide the model's behavior.
*   `user`: Represents messages from the user.
*   `assistant`: Represents previous responses from the assistant.

Basic Example[â€‹](#basic-example "Direct link to Basic Example")
---------------------------------------------------------------

Here's a simple example of a chat completion request and response:

### Request[â€‹](#request "Direct link to Request")

    import requestsimport jsonurl = "https://api.asi1.ai/v1/chat/completions"payload = json.dumps({  "model": "asi1-mini",  "messages": [    {      "role": "user",      "content": "Hi, tell me about giraffes"    }  ],  "temperature": 0,  "stream": False,  "max_tokens": 500})headers = {  'Content-Type': 'application/json',  'Accept': 'application/json',  'Authorization': 'Bearer YOUR_API_KEY'  # Replace with your actual API key}response = requests.request("POST", url, headers=headers, data=payload)print(response.text)

### Response[â€‹](#response "Direct link to Response")

    {  "id": "id_comqjiusZjAoyuXlh",  "object": "chat.completion",  "model": "asi1-mini",  "choices": [    {      "index": 0,      "message": {        "role": "assistant",        "content": "Giraffes are fascinating creatures known for their towering height and distinctive long necks. They are the tallest land animals, with heights ranging from 14 to 19 feet (4.3 to 5.8 meters). Males, called bulls, are typically taller and heavier than females (cows). Their long necks allow them to reach leaves, flowers, and fruits high up in trees, especially from their preferred food source, the acacia tree.\n\nThese animals are native to Africa and are commonly found in savannas, grasslands, and open woodlands. Giraffes live in loose social groups, often mingling with other herbivores like zebras and antelopes. They are generally peaceful but can defend themselves with powerful kicks if threatened by predators such as lions or hyenas.\n\nOne of the most unique traits of giraffes is their spotted coat pattern, which is unique to each individual, much like a human fingerprint. They also have a specialized cardiovascular system to manage blood flow due to their heightâ€”especially when lowering their heads to drink water. Giraffes only need to drink water occasionally, as they obtain most of their moisture from the plants they consume.\n\nTheir social behavior and communication are also intriguing. Giraffes communicate using low-frequency sounds that are often inaudible to human ears. Additionally, they engage in a behavior called \"necking,\" where males swing their necks to compete for dominance or mates."      },      "finish_reason": "stop",      "stop_reason": "stop"    }  ],  "usage": {    "prompt_tokens": 435,    "completion_tokens": 391,    "total_tokens": 826  }}

Multi-turn Conversations[â€‹](#multi-turn-conversations "Direct link to Multi-turn Conversations")
------------------------------------------------------------------------------------------------

ASI:One excels at maintaining context across multiple turns of conversation. To create a multi-turn conversation, include previous messages in the `messages` array:

    payload = json.dumps({  "model": "asi1-mini",  "messages": [    {      "role": "system",      "content": "You are an expert in Web3 technologies and decentralized systems."    },    {      "role": "user",      "content": "What is a blockchain?"    },    {      "role": "assistant",      "content": "A blockchain is a distributed, immutable ledger that records transactions across many computers. It's the underlying technology behind cryptocurrencies like Bitcoin, but its applications extend far beyond digital currencies. Each 'block' contains a set of transactions, and once verified, it's linked to the previous block, forming a chain. This design makes the data extremely difficult to alter retroactively, providing security and transparency."    },    {      "role": "user",      "content": "How does this relate to Web3?"    }  ],  "temperature": 0.7,  "max_tokens": 500})

Streaming Responses[â€‹](#streaming-responses "Direct link to Streaming Responses")
---------------------------------------------------------------------------------

For more interactive applications, you can stream the response as it's being generated:

    import requestsimport jsonurl = "https://api.asi1.ai/v1/chat/completions"payload = json.dumps({  "model": "asi1-mini",  "messages": [    {      "role": "user",      "content": "Explain the concept of decentralized AI"    }  ],  "temperature": 0.7,  "stream": True,  "max_tokens": 500})headers = {  'Content-Type': 'application/json',  'Accept': 'application/json',  'Authorization': 'Bearer YOUR_API_KEY'}response = requests.request("POST", url, headers=headers, data=payload, stream=True)for line in response.iter_lines():    if line:        line_text = line.decode('utf-8')        if line_text.startswith('data: '):            data_str = line_text[6:]  # Remove 'data: ' prefix            if data_str == '[DONE]':                break            try:                data = json.loads(data_str)                if 'choices' in data and len(data['choices']) > 0:                    delta = data['choices'][0].get('delta', {})                    if 'content' in delta:                        print(delta['content'], end='', flush=True)            except json.JSONDecodeError:                pass

When streaming is enabled, you'll receive a series of server-sent events (SSE), each containing a small piece of the response. The stream is terminated with a `data: [DONE]` message.

Accessing Model Thoughts[â€‹](#accessing-model-thoughts "Direct link to Accessing Model Thoughts")
------------------------------------------------------------------------------------------------

A unique feature of ASI:One is the ability to access the model's "thoughts" during generation when streaming is enabled. These thoughts provide insight into the model's reasoning process:

    for line in response.iter_lines():    if line:        line_text = line.decode('utf-8')        if line_text.startswith('data: '):            data_str = line_text[6:]  # Remove 'data: ' prefix            if data_str == '[DONE]':                break            try:                data = json.loads(data_str)                if 'thought' in data:                    print(f"Thought: {data['thought']}")                elif 'choices' in data and len(data['choices']) > 0:                    delta = data['choices'][0].get('delta', {})                    if 'content' in delta:                        print(f"Content: {delta['content']}", end='', flush=True)            except json.JSONDecodeError:                pass

Best Practices[â€‹](#best-practices "Direct link to Best Practices")
------------------------------------------------------------------

### System Messages[â€‹](#system-messages "Direct link to System Messages")

Use system messages to guide the model's behavior. For example:

    {  "role": "system",  "content": "You are an AI assistant specialized in blockchain technology. Provide concise, accurate information and use examples where appropriate."}

### Managing Context Length[â€‹](#managing-context-length "Direct link to Managing Context Length")

ASI:One has a limited context window. To manage long conversations:

1.  Summarize previous turns when necessary
2.  Remove less relevant messages from the history
3.  Focus on the most recent and relevant context

### Controlling Response Style[â€‹](#controlling-response-style "Direct link to Controlling Response Style")

Use the temperature parameter to control the creativity of responses:

*   Lower temperature (0.2-0.5): More deterministic, factual responses
*   Medium temperature (0.5-0.8): Balanced creativity and coherence
*   Higher temperature (0.8-1.0): More creative, diverse responses

### Error Handling[â€‹](#error-handling "Direct link to Error Handling")

Implement robust error handling in your application:

    try:    response = requests.request("POST", url, headers=headers, data=payload)    response.raise_for_status()  # Raise an exception for 4XX/5XX responses    result = response.json()    # Process the resultexcept requests.exceptions.HTTPError as http_err:    print(f"HTTP error occurred: {http_err}")    # Handle specific status codes if neededexcept requests.exceptions.ConnectionError as conn_err:    print(f"Connection error occurred: {conn_err}")except requests.exceptions.Timeout as timeout_err:    print(f"Timeout error occurred: {timeout_err}")except requests.exceptions.RequestException as req_err:    print(f"An error occurred: {req_err}")except json.JSONDecodeError as json_err:    print(f"JSON decode error: {json_err}")

Advanced Use Cases[â€‹](#advanced-use-cases "Direct link to Advanced Use Cases")
------------------------------------------------------------------------------

ASI:One's agentic capabilities make it particularly well-suited for:

1.  **Multi-step reasoning tasks**: Problems that require breaking down into steps
2.  **Autonomous agents**: Creating AI assistants that can plan and execute tasks
3.  **Web3 integrations**: Interacting with blockchain data and smart contracts
4.  **Context-aware applications**: Systems that need to maintain state and adapt to changing information

Conclusion[â€‹](#conclusion "Direct link to Conclusion")
------------------------------------------------------

The Chat Completion API provides a powerful interface to ASI-1 Mini's capabilities. By understanding how to structure your requests and leverage the model's agentic reasoning, you can build sophisticated applications that go beyond simple text generation.

For more information, refer to the [API Reference](https://innovationlab.fetch.ai/resources/docs/1.0.3/asione/asi1-mini-api-reference) documentation.</content>
</page>

<page>
  <title>LangGraph Agent with MCP adapter | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/mcp-integration/langgraph-mcp-agent-example</url>
  <content>    import osimport timefrom dotenv import load_dotenvfrom langchain_openai import ChatOpenAIfrom langchain_core.messages import HumanMessagefrom mcp import ClientSession, StdioServerParametersfrom mcp.client.stdio import stdio_clientimport asynciofrom langchain_mcp_adapters.tools import load_mcp_toolsfrom langgraph.prebuilt import create_react_agentfrom uagents_adapter import LangchainRegisterTool, cleanup_uagentfrom uagents_adapter.langchain import AgentManager# Load environment variablesload_dotenv()# Set your API keysOPENAI_API_KEY = os.getenv("OPENAI_API_KEY")API_TOKEN = os.getenv("AGENTVERSE_API_KEY")# Initialize the modelmodel = ChatOpenAI(model="gpt-4o")# Store the agent globally so it can be accessed by the wrapper functionagent = Noneasync def setup_math_agent():    global agent        print("Setting up math agent...")    server_params = StdioServerParameters(        command="python",        args=["math_server.py"],    )        async with stdio_client(server_params) as (read, write):        async with ClientSession(read, write) as session:            await session.initialize()            tools = await load_mcp_tools(session)            agent = create_react_agent(model, tools)                        # Test the agent            test_response = await agent.ainvoke({                "messages": [HumanMessage(content="what's (3 + 5) x 12?")]            })            print(f"Test response: {test_response['messages'][-1].content}")                        # Keep the connection alive            while True:                await asyncio.sleep(1)def main():    # Initialize agent manager    manager = AgentManager()        # Create agent wrapper    async def agent_func(x):        response = await agent.ainvoke({"messages": [HumanMessage(content=x)]})        return response["messages"][-1].content        agent_wrapper = manager.create_agent_wrapper(agent_func)        # Start the agent in background    manager.start_agent(setup_math_agent)        # Register with uAgents    print("Registering math agent...")    tool = LangchainRegisterTool()    agent_info = tool.invoke(        {            "agent_obj": agent_wrapper,            "name": "math_agent_langchain_mcp",            "port": 8080,            "description": "A math calculation agent",            "api_token": API_TOKEN,            "mailbox": True        }    )        print(f"âœ… Registered math agent: {agent_info}")        try:        manager.run_forever()    except KeyboardInterrupt:        print("ğŸ›‘ Shutting down...")        cleanup_uagent("math_agent")        print("âœ… Agent stopped.")if __name__ == "__main__":    main()</content>
</page>

<page>
  <title>Function Calling with ASI-1 Mini | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/asione/asi1-mini-function-calling</url>
  <content>Function calling in ASI:One allows models to go beyond text generation by invoking external functions with the right parameters. This enables integration with APIs, tools, or your own code to retrieve live data, perform tasks, or trigger actions based on user input.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

Function calling enables you to integrate your custom code with the [Chat Completion API](https://innovationlab.fetch.ai/resources/docs/1.0.3/asione/asi1-mini-chat-completion) in ASI:One. When given access to defined tools, the model can choose to call them based on the conversation context. After the function is called, you execute the corresponding code, return the results, and the model incorporates the output into its final reply. This guide provides instructions for connecting ASI:One models to your custom functions to retrieve data or perform specific actions.

ASI:One offers three model variants to suit different needs:

*   **asi1-mini**: The standard model with balanced performance and speed
*   **asi1-extended**: Enhanced capabilities for more complex tasks
*   **asi1-fast**: Optimized for quicker response times

Here's a basic example of how function calling works with ASI1:

*   Python
*   cURL

    import requestsimport json# ASI1 API settingsAPI_KEY = "your_api_key"BASE_URL = "https://api.asi1.ai/v1"headers = {    "Authorization": f"Bearer {API_KEY}",    "Content-Type": "application/json"}# Define the get_weather toolget_weather_tool = {    "type": "function",    "function": {        "name": "get_weather",        "description": "Get current temperature for a given location (latitude and longitude).",        "parameters": {            "type": "object",            "properties": {                "latitude": {"type": "number"},                "longitude": {"type": "number"}            },            "required": ["latitude", "longitude"]        }    }}# Initial message setupinitial_message = [    {        "role": "system",        "content": "You are a weather assistant. When a user asks for the weather in a location, use the get_weather tool with the appropriate latitude and longitude for that location."    },    {        "role": "user",        "content": "What's the current weather like in New York right now?"    }]# First call to modelpayload = {    "model": "asi1-mini",    "messages": initial_message,    "tools": [get_weather_tool],    "temperature": 0.7,    "max_tokens": 1024}response = requests.post(    f"{BASE_URL}/chat/completions",    headers=headers,    json=payload)

### Example Response[â€‹](#example-response "Direct link to Example Response")

When you make a function call request, the model will respond with a structured output that includes the function call details. Here's an example response:

    {  "id": "id_t0m4Pzm0KDbMg0WaX",  "model": "asi1-mini",  "executable_data": [],  "conversation_id": null,  "thought": [],  "choices": [    {      "index": 0,      "finish_reason": "tool_calls",      "message": {        "role": "assistant",        "content": "",        "reasoning": null,        "tool_calls": [          {            "id": "call_WzB5g",            "index": 0,            "type": "function",            "function": {              "name": "get_weather",              "arguments": "{\"latitude\":40.7128,\"longitude\":-74.006}"            }          }        ]      }    }  ],  "usage": {    "prompt_tokens": 36,    "completion_tokens": 8,    "total_tokens": 44  }}

### Sample function[â€‹](#sample-function "Direct link to Sample function")

Let's look at the steps to allow a model to use a real `get_weather` function defined below:

Sample get\_weather function implemented in your codebase

    import requestsdef get_weather(latitude, longitude):    response = requests.get(f"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m")    data = response.json()    return data['current']['temperature_2m']

Let's walk through the complete cycle of executing a function call with ASI1, using the weather example:

### Step 1: Initial Request with Tools[â€‹](#step-1-initial-request-with-tools "Direct link to Step 1: Initial Request with Tools")

First, we make a request to the model with the function definition and user message:

    # Define the get_weather toolget_weather_tool = {    "type": "function",    "function": {        "name": "get_weather",        "description": "Get current temperature for a given location (latitude and longitude).",        "parameters": {            "type": "object",            "properties": {                "latitude": {"type": "number"},                "longitude": {"type": "number"}            },            "required": ["latitude", "longitude"]        }    }}# User's questioninitial_message = {    "role": "user",    "content": "What's the current weather like in London right now?"}# First call to modelpayload = {    "model": "asi1-mini",    "messages": [initial_message],    "tools": [get_weather_tool],    "temperature": 0.7,    "max_tokens": 1024}first_response = requests.post(    f"{BASE_URL}/chat/completions",    headers=headers,    json=payload)

### Step 2: Parse Tool Calls from Response[â€‹](#step-2-parse-tool-calls-from-response "Direct link to Step 2: Parse Tool Calls from Response")

The model responds with a function call that we need to parse:

    first_response.raise_for_status()first_response_json = first_response.json()tool_calls = first_response_json["choices"][0]["message"].get("tool_calls", [])messages_history = [    initial_message,    first_response_json["choices"][0]["message"]]

### Step 3: Execute Tools and Format Results[â€‹](#step-3-execute-tools-and-format-results "Direct link to Step 3: Execute Tools and Format Results")

Next, we execute the function and format the results:

    # Simulate execution of get_weather tooldef get_weather(lat, lon):    response = requests.get(        f"https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={lon}&current=temperature_2m,wind_speed_10m"    )    data = response.json()    return data['current']['temperature_2m']# Process tool callfor tool_call in tool_calls:    function_name = tool_call["function"]["name"]    arguments = json.loads(tool_call["function"]["arguments"])    if function_name == "get_weather":        latitude = arguments["latitude"]        longitude = arguments["longitude"]        temperature = get_weather(latitude, longitude)        result = {            "temperature_celsius": temperature,            "location": f"lat: {latitude}, lon: {longitude}"        }    else:        result = {"error": f"Unknown tool: {function_name}"}    # Tool result message    tool_result_message = {        "role": "tool",        "tool_call_id": tool_call["id"],        "content": json.dumps(result)    }    messages_history.append(tool_result_message)

### Step 4: Send Results Back to Model[â€‹](#step-4-send-results-back-to-model "Direct link to Step 4: Send Results Back to Model")

Now we send the function results back to the model:

    # Final call to model with tool resultsfinal_payload = {    "model": "asi1-mini",    "messages": messages_history,    "temperature": 0.7,    "max_tokens": 1024}final_response = requests.post(    f"{BASE_URL}/chat/completions",    headers=headers,    json=final_payload)

### Step 5: Receive Final Answer[â€‹](#step-5-receive-final-answer "Direct link to Step 5: Receive Final Answer")

Finally, we get the model's response incorporating the function results:

    final_response.raise_for_status()final_response_json = final_response.json()# Final resultprint(final_response_json["choices"][0]["message"]["content"])

### Tool Result Handling in ASI1[â€‹](#tool-result-handling-in-asi1 "Direct link to Tool Result Handling in ASI1")

Here are key guidelines to ensure correct behavior and prevent common errors.

* * *

**Preserving Tool Call IDs**

Each tool call comes with a unique `id` that **must** be preserved when sending results back.

    # Correcttool_result_message = {  "role": "tool",  "tool_call_id": tool_call.id,  # Use the exact ID from the tool call  "content": json.dumps(result)}# Incorrect - Don't make up IDstool_result_message = {  "role": "tool",  "tool_call_id": "my_custom_id",  # This will cause an error  "content": json.dumps(result)}

**Message History Order**

The message history **must** maintain this exact order:

*   Original user message
*   Assistant message with `tool_calls` (content should be null or empty)
*   Tool result messages (one for each tool\_call, identified by `tool_call_id`)

**Content Formatting**

Tool results **must** be JSON-stringified within the `content` field.

    # Correct"content": json.dumps({"key": "value"})# Incorrect - Don't send raw objects"content": {"key": "value"}  # This will cause an error

**Error Handling**

If a tool fails, send back a result message indicating the error.

    try:  # Execute tool  result = execute_tool(function_name, arguments)  content_to_send = json.dumps(result)except Exception as e:  # Send error as tool result content  error_content = {      "error": f"Tool execution failed: {str(e)}",      "status": "failed"  }  content_to_send = json.dumps(error_content)tool_result_message = {  "role": "tool",  "tool_call_id": tool_call.id, # Still use the original tool_call.id  "content": content_to_send}messages_history.append(tool_result_message)

Function Definition[â€‹](#function-definition "Direct link to Function Definition")
---------------------------------------------------------------------------------

Functions are specified using the tools parameter in each API request, where each tool is described as a function object.

Each function is defined using a schema that tells the model what the function does and what input arguments it requires. The schema includes the following key fields:

*   **`name`** (string) : A unique, descriptive identifier for the function (e.g., `get_weather_forecast`, `send_email`). Use underscores or camelCase formatting. Avoid spaces or special characters.
    
*   **`description`** (string) : A detailed explanation of what the function does and when it should be used. Clear, specific descriptions improve the model's ability to use the function correctly.
    
*   **`parameters`** (object) : Defines the input parameters the function expects.
    
    *   **`type`** (string) : Usually set to `"object"` to represent structured input.
        
    *   **`properties`** (object) : Lists each input parameter and its details:
        
        *   **`type`** (string): The data type (e.g., `string`, `integer`, `boolean`, `array`).
        *   **`description`** (string): A clear explanation of the parameter's purpose and expected format.  
            _Example:_ `"City and country, e.g., 'Paris, France'"`
        *   **`enum`** _(optional)_: An array of allowed values, useful when inputs must be restricted.  
            _Example:_ `"enum": ["celsius", "fahrenheit"]`
    *   **`required`** (array of strings) : Lists the parameter names that must be included when calling the function.
        

Example Function Schema

    {    "type": "function",    "function": {        "name": "get_weather",        "description": "Retrieves current weather for the given location.",        "parameters": {            "type": "object",            "properties": {                "location": {                    "type": "string",                    "description": "City and country e.g. BogotÃ¡, Colombia"                },                "units": {                    "type": "string",                    "enum": [                        "celsius",                        "fahrenheit"                    ],                    "description": "Units the temperature will be returned in."                }            },            "required": [                "location",                "units"            ],            "additionalProperties": False        },        "strict": True    }}

Additional Configurations[â€‹](#additional-configurations "Direct link to Additional Configurations")
---------------------------------------------------------------------------------------------------

ASI1 provides several options to control how and when tools are called, as well as how strictly the model adheres to your function schemas.

### Tool Choice[â€‹](#tool-choice "Direct link to Tool Choice")

By default, the model determines when and how many tools to use. You can control this behavior with the `tool_choice` parameter:

*   **Auto (default):** The model may call zero, one, or multiple functions.
*   **Required:** The model must call at least one function.
    
        "tool_choice": "required"
    
*   **Forced Function:** Force the model to call a specific function.
    
        "tool_choice": {  "type": "function",  "function": { "name": "get_weather" }}
    
*   **None:** Prevent the model from calling any functions.

### Parallel Function Calling[â€‹](#parallel-function-calling "Direct link to Parallel Function Calling")

By default, the model may call multiple functions in a single turn. To restrict this and ensure only one (or zero) tool is called per turn, set:

    "parallel_tool_calls": False

> **Note:** If parallel tool calls are enabled, strict mode may be disabled for those calls.

### Strict Mode[â€‹](#strict-mode "Direct link to Strict Mode")

Setting `strict` to `True` ensures the model strictly follows your function schema. This is recommended for most use cases.

**Requirements for strict mode:**

1.  `additionalProperties` must be set to `False` for each object in the `parameters`.
2.  All fields in `properties` must be listed in `required`.

**Example with strict mode enabled:**

    {  "type": "function",  "function": {    "name": "get_weather",    "description": "Retrieves current weather for the given location.",    "strict": True,    "parameters": {      "type": "object",      "properties": {        "location": {          "type": "string",          "description": "City and country e.g. BogotÃ¡, Colombia"        },        "units": {          "type": ["string", "null"],          "enum": ["celsius", "fahrenheit"],          "description": "Units the temperature will be returned in."        }      },      "required": ["location", "units"],      "additionalProperties": False    }  }}

**Example with strict mode disabled:**

    {  "type": "function",  "function": {    "name": "get_weather",    "description": "Retrieves current weather for the given location.",    "parameters": {      "type": "object",      "properties": {        "location": {          "type": "string",          "description": "City and country e.g. BogotÃ¡, Colombia"        },        "units": {          "type": "string",          "enum": ["celsius", "fahrenheit"],          "description": "Units the temperature will be returned in."        }      },      "required": ["location"]    }  }}

> **Tip:** We recommend enabling strict mode for reliable function calling.</content>
</page>

<page>
  <title>Multi-Server MCP Langgraph Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/mcp-integration/multi-server-agent-example</url>
  <content>This guide demonstrates two approaches for building LangGraph agents that connect to multiple MCP servers, then wrap them as uAgents and register them on [Agentverse](https://innovationlab.fetch.ai/resources/docs/agentverse) for discovery and use by ASI:One LLM.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

Both examples in this guide:

*   Connect to multiple MCP servers (math and weather) using `langchain_mcp_adapters.MultiServerMCPClient`
*   Support multiple transport methods (stdio and SSE) for different MCP servers
*   Wrap the LangGraph agent using `uagents_adapter` to become a uAgent
*   Register the uAgent on Agentverse, making it discoverable and callable by ASI:One

The key difference is in the agent architecture:

*   **Basic Multi-Server Agent**: Uses a simple LangGraph agent with ReAct framework
*   **Advanced State Graph Agent**: Uses a LangGraph state graph for more complex workflows and state management

Transport Methods[â€‹](#transport-methods "Direct link to Transport Methods")
---------------------------------------------------------------------------

In both examples, we use two different transport methods for the MCP servers:

1.  **stdio Transport** (Math Server):
    
    *   Used for local MCP servers that run as subprocesses
    *   Communication happens through standard input/output
    *   Good for local development and testing
    *   Example: `mcp.run(transport="stdio")`
2.  **SSE Transport** (Weather Server):
    
    *   Server-Sent Events (SSE) for real-time communication
    *   Used for remote or web-based MCP servers
    *   Supports long-lived connections
    *   Example: `mcp.run(transport="sse", port=8000)`

The `MultiServerMCPClient` handles both transport types seamlessly, allowing you to mix and match different transport methods based on your needs.

Common Server Setup[â€‹](#common-server-setup "Direct link to Common Server Setup")
---------------------------------------------------------------------------------

Both examples use the same MCP servers. Let's set those up first:

### 1\. Create the Math MCP Server[â€‹](#1-create-the-math-mcp-server "Direct link to 1. Create the Math MCP Server")

math\_server.py

    from mcp.server.fastmcp import FastMCPmcp = FastMCP("Math")@mcp.tool()def add(a: int, b: int) -> int:    """Add two numbers"""    return a + b@mcp.tool()def multiply(a: int, b: int) -> int:    """Multiply two numbers"""    return a * bif __name__ == "__main__":    mcp.run(transport="stdio")

### 2\. Create the Weather MCP Server[â€‹](#2-create-the-weather-mcp-server "Direct link to 2. Create the Weather MCP Server")

weather\_server.py

    from mcp.server.fastmcp import FastMCPmcp = FastMCP("Weather")@mcp.tool()def get_weather(city: str) -> str:    """Get the current weather for a city"""    # This is a mock implementation    return f"The weather in {city} is sunny and 25Â°C"if __name__ == "__main__":    mcp.run(transport="sse", port=8000)

Approach 1: Basic Multi-Server Agent[â€‹](#approach-1-basic-multi-server-agent "Direct link to Approach 1: Basic Multi-Server Agent")
-----------------------------------------------------------------------------------------------------------------------------------

This approach uses LangGraph's `create_react_agent` to create a simple agent that can access tools from multiple MCP servers.

### Create and Register the Basic Multi-Server Agent[â€‹](#create-and-register-the-basic-multi-server-agent "Direct link to Create and Register the Basic Multi-Server Agent")

basic\_agent.py

    import osimport timefrom dotenv import load_dotenvfrom langchain_openai import ChatOpenAIfrom langchain_core.messages import HumanMessagefrom langchain_mcp_adapters.client import MultiServerMCPClientfrom langgraph.prebuilt import create_react_agentfrom uagents_adapter import LangchainRegisterTool, cleanup_uagentfrom uagents_adapter.langchain import AgentManager# Load environment variablesload_dotenv()# Set your API keysOPENAI_API_KEY = os.getenv("OPENAI_API_KEY")API_TOKEN = os.getenv("AGENTVERSE_API_KEY")# Initialize the modelmodel = ChatOpenAI(model="gpt-4o")# Store the agent globally so it can be accessed by the wrapper functionagent = Noneasync def setup_multi_server_agent():    global agent        print("Setting up multi-server agent...")    async with MultiServerMCPClient(        {            "math": {                "command": "python",                "args": ["math_server.py"],                "transport": "stdio",            },            "weather": {                "url": "http://localhost:8000/sse",                "transport": "sse",            }        }    ) as client:        tools = client.get_tools()        agent = create_react_agent(model, tools)                # Test the agent with both services        print("Testing math capabilities...")        math_response = await agent.ainvoke({            "messages": [HumanMessage(content="what's (3 + 5) x 12?")]        })        print(f"Math test response: {math_response['messages'][-1].content}")                print("Testing weather capabilities...")        weather_response = await agent.ainvoke({            "messages": [HumanMessage(content="what's the weather in NYC?")]        })        print(f"Weather test response: {weather_response['messages'][-1].content}")                # Keep the connection alive        while True:            await asyncio.sleep(1)def main():    # Initialize agent manager    manager = AgentManager()        # Create agent wrapper    async def agent_func(x):        response = await agent.ainvoke({"messages": [HumanMessage(content=x)]})        return response["messages"][-1].content        agent_wrapper = manager.create_agent_wrapper(agent_func)        # Start the agent in background    manager.start_agent(setup_multi_server_agent)        # Register with uAgents    print("Registering multi-server agent...")    tool = LangchainRegisterTool()    agent_info = tool.invoke(        {            "agent_obj": agent_wrapper,            "name": "multi_server_agent_math_langchain_mcp",            "port": 8080,            "description": "A multi-service agent that can handle math calculations and weather queries",            "api_token": API_TOKEN,            "mailbox": True        }    )        print(f"âœ… Registered multi-server agent: {agent_info}")        try:        manager.run_forever()    except KeyboardInterrupt:        print("ğŸ›‘ Shutting down...")        cleanup_uagent("multi_server_agent")        print("âœ… Agent stopped.")if __name__ == "__main__":    import asyncio    main()

Approach 2: Advanced State Graph Agent[â€‹](#approach-2-advanced-state-graph-agent "Direct link to Approach 2: Advanced State Graph Agent")
-----------------------------------------------------------------------------------------------------------------------------------------

This approach uses LangGraph's `StateGraph` to create a more sophisticated agent with explicit state management and conditional workflow branching.

### Create and Register the Multi-Server Graph Agent[â€‹](#create-and-register-the-multi-server-graph-agent "Direct link to Create and Register the Multi-Server Graph Agent")

graph\_agent.py

    import osimport timeimport asynciofrom dotenv import load_dotenvfrom langchain_openai import ChatOpenAIfrom langchain_core.messages import HumanMessagefrom langchain_mcp_adapters.client import MultiServerMCPClientfrom langgraph.graph import StateGraph, MessagesState, STARTfrom langgraph.prebuilt import ToolNode, tools_conditionfrom uagents_adapter import LangchainRegisterTool, cleanup_uagentfrom uagents_adapter.langchain import AgentManager# Load environment variablesload_dotenv()# Set your API keysOPENAI_API_KEY = os.getenv("OPENAI_API_KEY")API_TOKEN = os.getenv("AGENTVERSE_API_KEY")# Initialize the modelmodel = ChatOpenAI(model="gpt-4o")# Store the graph globally so it can be accessed by the wrapper function_global_graph = None# Add an event to signal when the graph is readygraph_ready = asyncio.Event()async def setup_multi_server_graph_agent():    global _global_graph        print("Setting up multi-server graph agent...")    try:        # Create the client without async with        client = MultiServerMCPClient(            {                "math": {                    "command": "python",                    "args": ["./math_server.py"],                    "transport": "stdio",                },                "weather": {                    "url": "http://localhost:8000/sse",                    "transport": "sse",                }            }        )                # Get tools directly        tools = await client.get_tools()        print(f"Successfully loaded {len(tools)} tools")                # Define call_model function        def call_model(state: MessagesState):            response = model.bind_tools(tools).invoke(state["messages"])            return {"messages": response}        # Build the graph        builder = StateGraph(MessagesState)        builder.add_node(call_model)        builder.add_node(ToolNode(tools))        builder.add_edge(START, "call_model")        builder.add_conditional_edges(            "call_model",            tools_condition,        )        builder.add_edge("tools", "call_model")        _global_graph = builder.compile()        print("Graph successfully compiled")                # Test the graph        try:            print("Testing math capabilities...")            math_response = await _global_graph.ainvoke({"messages": "what's (3 + 5) x 12?"})            print(f"Math test response: {math_response['messages'][-1].content}")                        print("Testing weather capabilities...")            weather_response = await _global_graph.ainvoke({"messages": "what's the weather in NYC?"})            print(f"Weather test response: {weather_response['messages'][-1].content}")        except Exception as e:            print(f"Error during testing: {e}")                # Signal that the graph is ready        graph_ready.set()        # Keep the connection alive        while True:            await asyncio.sleep(1)    except Exception as e:        print(f"Error setting up graph: {e}")        # Set the event even in case of error to avoid deadlock        graph_ready.set()def main():    print("Initializing agent...")    # Initialize agent manager    manager = AgentManager()        # Create graph wrapper with proper error handling    async def graph_func(x):        # Wait for the graph to be ready before trying to use it        await graph_ready.wait()                if _global_graph is None:            error_msg = "Error: Graph not initialized properly. Please try again later."            print(f"Response: {error_msg}")            return error_msg                try:            # Print the incoming message            print(f"\nReceived query: {x}")                        # Process the message            if isinstance(x, str):                response = await _global_graph.ainvoke({"messages": x})            else:                response = await _global_graph.ainvoke({"messages": x})                        # Extract and print the response            result = response["messages"][-1].content            print(f"\nâœ… Response: {result}\n")            return result        except Exception as e:            error_msg = f"Error processing request: {str(e)}"            print(f"\nâŒ {error_msg}\n")            return error_msg        agent_wrapper = manager.create_agent_wrapper(graph_func)        # Start the graph in background    manager.start_agent(setup_multi_server_graph_agent)        # Register with uAgents    print("Registering multi-server graph agent...")    tool = LangchainRegisterTool()    try:        agent_info = tool.invoke(            {                "agent_obj": agent_wrapper,                "name": "multi_server_graph_agent_math_langchain_mcp",                "port": 8080,                "description": "A multi-service graph agent that can handle math calculations and weather queries",                "api_token": API_TOKEN,                "mailbox": True            }        )        print(f"âœ… Registered multi-server graph agent: {agent_info}")    except Exception as e:        print(f"âš ï¸ Error registering agent: {e}")        print("Continuing with local agent only...")    try:        manager.run_forever()    except KeyboardInterrupt:        print("ğŸ›‘ Shutting down...")        cleanup_uagent("multi_server_graph_agent")        print("âœ… Graph stopped.")if __name__ == "__main__":    main()

Key Differences Between the Two Approaches[â€‹](#key-differences-between-the-two-approaches "Direct link to Key Differences Between the Two Approaches")
------------------------------------------------------------------------------------------------------------------------------------------------------

1.  **Architecture**:
    
    *   **Basic Agent**: Uses LangGraph's `create_react_agent` for a simple ReAct-style agent.
    *   **Graph Agent**: Uses LangGraph's `StateGraph` for explicit state management and workflow control.
2.  **Control Flow**:
    
    *   **Basic Agent**: The control flow is managed internally by the ReAct framework.
    *   **Graph Agent**: The control flow is explicitly defined with nodes, edges, and conditional branching.
3.  **State Management**:
    
    *   **Basic Agent**: State is managed implicitly within the ReAct agent.
    *   **Graph Agent**: State is explicitly managed and can be more easily inspected and modified.
4.  **Extensibility**:
    
    *   **Basic Agent**: Simpler to set up but less flexible for complex workflows.
    *   **Graph Agent**: More complex setup but offers greater flexibility for sophisticated agent behaviors.

Getting Started[â€‹](#getting-started "Direct link to Getting Started")
---------------------------------------------------------------------

1.  **Get your Agentverse API Key**:
    
    *   Follow the [Agentverse API Key guide](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key) to obtain your API key
    *   Make sure to save your API key securely as it cannot be regenerated
2.  **Set up environment variables** in a `.env` file:
    
        OPENAI_API_KEY=your_openai_api_keyAGENTVERSE_API_KEY=your_agentverse_api_key
    
3.  **Install dependencies**:
    
        pip install langchain-openai mcp langchain-mcp-adapters uagents-adapter python-dotenv
    
4.  **Create the files**:
    
    *   Save the math server code as `math_server.py`
    *   Save the weather server code as `weather_server.py`
    *   Save the basic agent code as `basic_agent.py`
    *   Save the graph agent code as `graph_agent.py`
5.  **Start the servers and agents**:
    
        # Terminal 1: Start the weather serverpython weather_server.py# Terminal 2: Start the basic agentpython basic_agent.py# OR to run the graph agent insteadpython graph_agent.py
    
6.  **Test your agent** by querying it from Agentverse chat UI.
    

*   Open your Agentverse account and goto [local agents](https://agentverse.ai/agents/local).

*   Click on your agent and Use the "Chat with Agent" button on your Agentverse agent page

When to Use Each Approach[â€‹](#when-to-use-each-approach "Direct link to When to Use Each Approach")
---------------------------------------------------------------------------------------------------

*   **Use the Basic Agent** when:
    
    *   You need a simple agent that can access multiple tools
    *   You want a quick setup with minimal boilerplate
    *   The agent's decision-making process is relatively straightforward
*   **Use the Graph Agent** when:
    
    *   You need more control over the agent's workflow
    *   You want explicit state management
    *   You need complex conditional branching in your agent's behavior
    *   You're building an agent with multiple specialized steps or phases

note

**Note:** These examples demonstrate how to connect to multiple MCP servers using different transport methods and agent architectures. You can extend these patterns to include any number of MCP servers with different capabilities and create more sophisticated agent behaviors.</content>
</page>

<page>
  <title>Connect an Agent to Multiple Remote MCP Servers | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/mcp-integration/connect-an-agent-to-multiple-remote-mcp-servers</url>
  <content>This example demonstrates how to build a uAgent client that connects to multiple remote MCP servers hosted on [Smithery.ai](https://smithery.ai/) (such as PubMed, clinical trials, medical calculators, and web search), handles user queries using Claude for intelligent tool selection, and registers itself on [Agentverse](https://innovationlab.fetch.ai/resources/docs/agentverse) for discovery and use by ASI:One LLM.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

*   **uAgent client** connects to multiple remote MCP servers via HTTP using [Smithery.ai](https://smithery.ai/)'s platform
*   Uses Claude to intelligently select and call the appropriate tools based on user queries
*   Formats responses using Claude for better readability and user experience
*   The agent is registered on [Agentverse](https://innovationlab.fetch.ai/resources/docs/agentverse), making it discoverable and callable by ASI:One LLM
*   Supports multiple specialized MCP servers for different domains (medical research, web search, etc.)

MCP Servers Used[â€‹](#mcp-servers-used "Direct link to MCP Servers Used")
------------------------------------------------------------------------

This example connects to several MCP servers hosted on Smithery.ai:

1.  **Medical Calculators** (`@vitaldb/medcalc`):
    
    *   BMI calculation
    *   HOMA-IR (insulin resistance)
    *   Other medical formulas
2.  **Clinical Trials** (`@JackKuo666/clinicaltrials-mcp-server`):
    
    *   Search clinical trial databases
    *   Get trial details and status
3.  **PubMed** (`@JackKuo666/pubmed-mcp-server`):
    
    *   Search biomedical literature
    *   Get article metadata
4.  **Paper Search** (`@openags/paper-search-mcp`):
    
    *   Search scientific research metadata
    *   Get paper details
5.  **DuckDuckGo** (`@nickclyde/duckduckgo-mcp-server`):
    
    *   Perform DuckDuckGo web searches
    *   Get real-time web results

Example: Medical Research Agent[â€‹](#example-medical-research-agent "Direct link to Example: Medical Research Agent")
--------------------------------------------------------------------------------------------------------------------

### Configure the uAgent Client[â€‹](#configure-the-uagent-client "Direct link to Configure the uAgent Client")

mcp\_agent.py

    from anthropic import Anthropicfrom dotenv import load_dotenvfrom uagents_core.contrib.protocols.chat import (    chat_protocol_spec,    ChatMessage,    ChatAcknowledgement,    TextContent,    StartSessionContent,)from uagents import Agent, Context, Protocolfrom uagents.setup import fund_agent_if_lowfrom datetime import datetime, timezone, timedeltafrom uuid import uuid4import mcpfrom mcp.client.streamable_http import streamablehttp_clientimport jsonimport base64import asynciofrom typing import Dict, List, Optional, Anyfrom contextlib import AsyncExitStackimport os# Load environment variablesload_dotenv()# Get API keys from environment variablesANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")if not ANTHROPIC_API_KEY:    raise ValueError("Please set the ANTHROPIC_API_KEY environment variable in your .env file")SMITHERY_API_KEY = os.getenv("SMITHERY_API_KEY")if not SMITHERY_API_KEY:    raise ValueError("Please set the SMITHERY_API_KEY environment variable in your .env file")class MedicalResearchMCPClient:    def __init__(self):        self.sessions: Dict[str, mcp.ClientSession] = {}        self.exit_stack = AsyncExitStack()        self.anthropic = Anthropic(api_key=ANTHROPIC_API_KEY)        self.all_tools = []        self.tool_server_map = {}        self.server_configs = {}        self.default_timeout = timedelta(seconds=30)    def get_server_config(self, server_path: str) -> dict:        """Get or create server configuration"""        if server_path not in self.server_configs:            config_templates = {                "@JackKuo666/pubmed-mcp-server": {},                "@openags/paper-search-mcp": {},                "@JackKuo666/clinicaltrials-mcp-server": {},                "@vitaldb/medcalc": {},            }            self.server_configs[server_path] = config_templates.get(server_path, {})        return self.server_configs[server_path]    async def connect_to_servers(self, ctx: Context):        """Connect to all MCP servers and collect their tools"""        base_config = {            "ignoreRobotsTxt": True        }        servers = [            "@JackKuo666/pubmed-mcp-server",            "@openags/paper-search-mcp",            "@JackKuo666/clinicaltrials-mcp-server",            "@vitaldb/medcalc",        ]        for server_path in servers:            try:                ctx.logger.info(f"Connecting to server: {server_path}")                server_config = self.get_server_config(server_path)                config = {**base_config, **server_config}                config_b64 = base64.b64encode(json.dumps(config).encode()).decode()                url = f"https://server.smithery.ai/{server_path}/mcp?config={config_b64}&api_key={SMITHERY_API_KEY}"                try:                    read_stream, write_stream, _ = await self.exit_stack.enter_async_context(                        streamablehttp_client(url)                    )                    session = await self.exit_stack.enter_async_context(                        mcp.ClientSession(read_stream, write_stream)                    )                    await session.initialize()                    tools_result = await session.list_tools()                    tools = tools_result.tools                    self.sessions[server_path] = session                    for tool in tools:                        tool_info = {                            "name": tool.name,                            "description": f"[{server_path}] {tool.description}",                            "input_schema": tool.inputSchema,                            "server": server_path,                            "tool_name": tool.name                        }                        self.all_tools.append(tool_info)                        self.tool_server_map[tool.name] = server_path                    ctx.logger.info(f"Successfully connected to {server_path}")                    ctx.logger.info(f"Available tools: {', '.join([t.name for t in tools])}")                except Exception as e:                    ctx.logger.error(f"Error during connection setup: {str(e)}")                    raise            except Exception as e:                ctx.logger.error(f"Error connecting to {server_path}: {str(e)}")                continue    async def process_query(self, query: str, ctx: Context) -> str:        try:            messages = [{"role": "user", "content": query}]            claude_tools = [{                "name": tool["name"],                "description": tool["description"],                "input_schema": tool["input_schema"]            } for tool in self.all_tools]            response = self.anthropic.messages.create(                model="claude-3-5-sonnet-20241022",                max_tokens=1000,                messages=messages,                tools=claude_tools            )            tool_response = None            for content in response.content:                if content.type == 'tool_use':                    tool_name = content.name                    tool_args = content.input                    server_path = self.tool_server_map.get(tool_name)                    if server_path and server_path in self.sessions:                        ctx.logger.info(f"Calling tool {tool_name} from {server_path}")                        try:                            result = await asyncio.wait_for(                                self.sessions[server_path].call_tool(tool_name, tool_args),                                timeout=self.default_timeout.total_seconds()                            )                            if isinstance(result.content, str):                                tool_response = result.content                            elif isinstance(result.content, list):                                tool_response = "\n".join([str(item) for item in result.content])                            else:                                tool_response = str(result.content)                        except asyncio.TimeoutError:                            return f"Error: The MCP server did not respond. Please try again later."                        except Exception as e:                            return f"Error calling tool {tool_name}: {str(e)}"            if tool_response:                format_prompt = f"""Please format the following response in a clear, user-friendly way. Do not add any additional information or knowledge, just format what is provided: {tool_response} Instructions: 1. If the response contains multiple records (like clinical trials), present ALL records in a clear format, do not say something like "Saved to a CSV file" or anything similar. 2. Use appropriate headings and sections 3. Maintain all the original information 4. Do not add any external knowledge or commentary 5. Do not summarize or modify the content 6. Keep the formatting simple and clean 7. If the response mentions a CSV file, do not include that information in the response. 9. For long responses, ensure all records are shown, not just a subset"""                format_response = self.anthropic.messages.create(                    model="claude-3-5-sonnet-20241022",                    max_tokens=2000,                    messages=[{"role": "user", "content": format_prompt}]                )                if format_response.content and len(format_response.content) > 0:                    return format_response.content[0].text                else:                    return tool_response            else:                return "No response received from the tool."        except Exception as e:            ctx.logger.error(f"Error processing query: {str(e)}")            return f"An error occurred while processing your query: {str(e)}"    async def cleanup(self):        await self.exit_stack.aclose()# Initialize chat protocol and agentchat_proto = Protocol(spec=chat_protocol_spec)mcp_agent = Agent(    name='MedicalResearchMCPAgent',    port=8001,    mailbox=True)client = MedicalResearchMCPClient()@chat_proto.on_message(model=ChatMessage)async def handle_chat_message(ctx: Context, sender: str, msg: ChatMessage):    try:        ack = ChatAcknowledgement(            timestamp=datetime.now(timezone.utc),            acknowledged_msg_id=msg.msg_id        )        await ctx.send(sender, ack)        if not client.sessions:            await client.connect_to_servers(ctx)        for item in msg.content:            if isinstance(item, StartSessionContent):                ctx.logger.info(f"Got a start session message from {sender}")                continue            elif isinstance(item, TextContent):                ctx.logger.info(f"Got a message from {sender}: {item.text}")                response_text = await client.process_query(item.text, ctx)                ctx.logger.info(f"Response text: {response_text}")                response = ChatMessage(                    timestamp=datetime.now(timezone.utc),                    msg_id=uuid4(),                    content=[TextContent(type="text", text=response_text)]                )                await ctx.send(sender, response)            else:                ctx.logger.info(f"Got unexpected content from {sender}")    except Exception as e:        ctx.logger.error(f"Error handling chat message: {str(e)}")        error_response = ChatMessage(            timestamp=datetime.now(timezone.utc),            msg_id=uuid4(),            content=[TextContent(type="text", text=f"An error occurred: {str(e)}")]        )        await ctx.send(sender, error_response)@chat_proto.on_message(model=ChatAcknowledgement)async def handle_chat_acknowledgement(ctx: Context, sender: str, msg: ChatAcknowledgement):    ctx.logger.info(f"Received acknowledgement from {sender} for message {msg.acknowledged_msg_id}")    if msg.metadata:        ctx.logger.info(f"Metadata: {msg.metadata}")mcp_agent.include(chat_proto)if __name__ == "__main__":    try:        mcp_agent.run()    except Exception as e:        print(f"Error running agent: {str(e)}")    finally:        asyncio.run(client.cleanup())

Script Breakdown[â€‹](#script-breakdown "Direct link to Script Breakdown")
------------------------------------------------------------------------

This section walks through the main components of the `mcp_agent.py` script to help you understand how each part contributes to building a Claude-powered uAgent that connects to multiple remote MCP servers.

### 1\. Loading Configuration and Dependencies[â€‹](#1-loading-configuration-and-dependencies "Direct link to 1. Loading Configuration and Dependencies")

    ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")SMITHERY_API_KEY = os.getenv("SMITHERY_API_KEY")

Environment variables are loaded using `dotenv` to securely manage API keys for Anthropic and [Smithery.ai](https://smithery.ai/). These credentials are required to authenticate and connect with the MCP Servers and process natural language queries.

### 2\. Creating the MCP Client[â€‹](#2-creating-the-mcp-client "Direct link to 2. Creating the MCP Client")

    class MedicalResearchMCPClient:    def __init__(self):        ...

The `MedicalResearchMCPClient` class handles all interactions with MCP servers. It manages session lifecycle, tool discovery, and query execution. The `exit_stack` allows multiple async context managers to be managed together.

### 3\. MCP Server Configuration[â€‹](#3-mcp-server-configuration "Direct link to 3. MCP Server Configuration")

    def get_server_config(self, server_path: str) -> dict:    ...

First, identify the MCP servers you want to integrate from the [Smithery.ai](https://smithery.ai/) platform. Once selected, navigate to the serverâ€™s API tab to locate its configuration schema. This schema outlines required parameters, authentication methods, and any necessary API keys for establishing a connection.

note

**Note:** For this example, we've selected MCP Servers that do not require extra authentication or server parameters.

### 4\. Connecting to Multiple Remote MCP Servers[â€‹](#4-connecting-to-multiple-remote-mcp-servers "Direct link to 4. Connecting to Multiple Remote MCP Servers")

    async def connect_to_servers(self, ctx: Context):    ...

Establishes connections to several predefined MCP servers using their Smithery-hosted URLs. The `config` is encoded and passed as a base64 parameter to the server endpoint. Each tool exposed by the server is retrieved and stored for later use.

    config_b64 = base64.b64encode(json.dumps(config).encode()).decode()url = f"https://server.smithery.ai/{server_path}/mcp?config={config_b64}&api_key={SMITHERY_API_KEY}"

MCP sessions and tool metadata are stored in local dictionaries, enabling tool invocation at runtime.

### 5\. Processing User Queries[â€‹](#5-processing-user-queries "Direct link to 5. Processing User Queries")

    async def process_query(self, query: str, ctx: Context) -> str:    ...

To process the user's query, it fetches all the tools of the MCP Servers that the client is connected to and then uses Claude to select the appropriate tool and provide input arguments for invocation.

    claude_tools = [{    "name": tool["name"],    "description": tool["description"],    "input_schema": tool["input_schema"]} for tool in self.all_tools]

Once Claude returns a tool selection, the agent locates the matching MCP session using the tool name, then calls the tool with the arguments returned by Claude. The output is parsed and passed back for formatting.

    if content.type == 'tool_use':    tool_name = content.name    tool_args = content.input    server_path = self.tool_server_map.get(tool_name)    if server_path and server_path in self.sessions:        ctx.logger.info(f"Calling tool {tool_name} from {server_path}")        try:            result = await asyncio.wait_for(                self.sessions[server_path].call_tool(tool_name, tool_args),                timeout=self.default_timeout.total_seconds()            )            if isinstance(result.content, str):                tool_response = result.content            elif isinstance(result.content, list):                tool_response = "\n".join([str(item) for item in result.content])            else:                tool_response = str(result.content)        except asyncio.TimeoutError:            return f"Error: The MCP server did not respond. Please try again later."        except Exception as e:            return f"Error calling tool {tool_name}: {str(e)}"

The tool is invoked via the relevant MCP session. If the response is successful, it's passed back to Claude with specific formatting instructions.

    format_response = self.anthropic.messages.create(...)

This second call ensures the output is easy to read while preserving all original data.

### 6\. Initializing the uAgent[â€‹](#6-initializing-the-uagent "Direct link to 6. Initializing the uAgent")

    mcp_agent = Agent(    name='MedicalResearchMCPAgent',    port=8001,    mailbox=True)

Since we are creating this Agent locally, we will enable `mailbox` to connect it to Agentverse so that it is discoverable and callable by ASI:One.

### 7\. Implementing the Chat Protocol[â€‹](#7-implementing-the-chat-protocol "Direct link to 7. Implementing the Chat Protocol")

    chat_proto = Protocol(spec=chat_protocol_spec)

The agent includes the standardized `chat_protocol_spec` to communicate with other agents and ASI:One.

    @chat_proto.on_message(model=ChatMessage)async def handle_chat_message(ctx: Context, sender: str, msg: ChatMessage):    ...

When ASI:One selects this agent to handle a user query, it sends the request as a ChatMessage to this handler. Upon receiving the message, the agent invokes the `process_query()` to connect with the remote MCP Servers to address the user's question and send back the formatted response as a `ChatMessage` to ASI:One. You can refer the [Chat Protocol documentation](https://innovationlab.fetch.ai/resources/docs/agent-communication/agent-chat-protocol) for more information.

### 8\. Cleanup on Shutdown[â€‹](#8-cleanup-on-shutdown "Direct link to 8. Cleanup on Shutdown")

    async def cleanup(self):    await self.exit_stack.aclose()

Gracefully closes all active MCP sessions to prevent resource leaks when the agent is stopped or restarted.

### 9\. Running the Agent[â€‹](#9-running-the-agent "Direct link to 9. Running the Agent")

    if __name__ == "__main__":    mcp_agent.run()

Runs the agent on the defined port. On termination, all active connections to remote MCP servers are closed via the `cleanup()` method.s

### Register the Agent on Agentverse[â€‹](#register-the-agent-on-agentverse "Direct link to Register the Agent on Agentverse")

*   Use the Agent inspector link upon agent startup to register your agent on Agentverse, making it discoverable by ASI:One LLM and other agents.
*   Add a comprehensive README.md in the Overview tab of your agent to improve discoverability.

Getting Started[â€‹](#getting-started "Direct link to Getting Started")
---------------------------------------------------------------------

1.  **Get your API Keys**:
    
    *   Follow the [Agentverse API Key guide](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key) to obtain your API key
    *   Get your [Smithery.ai API key](https://smithery.ai/account/api-keys) from their platform
    *   Get your [Anthropic API key](https://console.anthropic.com/settings/keys) from their platform
    *   Make sure to save your API keys securely
2.  **Set up environment variables** in a `.env` file:
    
        ANTHROPIC_API_KEY=your_anthropic_api_keySMITHERY_API_KEY=your_smithery_api_keyAGENTVERSE_API_KEY=your_agentverse_api_key
    
3.  **Install dependencies**:
    
        pip install uagents anthropic mcp python-dotenv
    
4.  **Create the agent file**:
    
    *   Save the code above as `mcp_agent.py`
    *   Create a `README.md` file in the same directory
    *   Update the API keys in your `.env` file
5.  **Run the agent**:
    
6.  **Test your agent**:
    
    *   Open your Agentverse account and goto [local agents](https://agentverse.ai/agents/local).
    *   Click on your agent and Use the "Chat with Agent" button on your Agentverse agent page

*   Query your agent through ASI:One LLM (make sure to enable the "Agents" switch)

note

**Note:** The ASI:One LLM uses an Agent Ranking mechanism to select the most appropriate agent for each query. To test your agent directly, use the "Chat with Agent" button on your Agentverse agent page. For more information about the Chat Protocol and ASI:One integration, check out the [Chat Protocol documentation](https://innovationlab.fetch.ai/resources/docs/agent-communication/agent-chat-protocol).</content>
</page>

<page>
  <title>Creating a MCP Server on Agentverse | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/mcp-integration/mcp-adapter-example</url>
  <content>This example demonstrates how to deploy a **Model Control Protocol (MCP) Server** on [Agentverse](https://agentverse.ai/). The MCP Server Adapter allows MCP servers to be easily discoverable by other agents on Agentverse and [ASI:One](https://asi1.ai/).

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

The MCP Server Adapter makes it easy to bring your tools into the Agentverse ecosystem by:

*   Wrapping MCP servers as uAgents for seamless, decentralized communication
*   Exposing MCP tools to other agents on Agentverse for easy discovery and reuse
*   Enabling the Chat Protocol, allowing natural language conversations with the MCP Server directly or through ASI:One

Example Implementation[â€‹](#example-implementation "Direct link to Example Implementation")
------------------------------------------------------------------------------------------

In this example, the MCP Server provides weather-related tools, including:

*   `get_alerts`: Returns weather alerts for a given US state.
    
*   `get_forecast`: Returns a weather forecast for a specific latitude and longitude. You can define your own tools by following this pattern, making it easy to bring any Python-based service into the Agentverse network.
    
*   Create a FastMCP Server that implements the MCP Server logic.
    
*   Create an Agent that uses the `MCPServerAdapter` from the `uagents-adapter` package to wrap the MCP Server as a uAgent.
    

To get started,

1.  Navigate to [Agentverse](https://agentverse.ai/) â†’ Agents tab â†’ + New Agent.
2.  Choose Blank Agent
3.  Provide a name for your new Agent and click on Create.

Refer to the [Hosted Agents](https://innovationlab.fetch.ai/resources/docs/agent-creation/uagent-creation#hosted-agents) section to understand the detailed steps for agent creation on Agentverse.

### Step 1: Create a FastMCP Server[â€‹](#step-1-create-a-fastmcp-server "Direct link to Step 1: Create a FastMCP Server")

Create a `server.py` file implementing your MCP server.

1.  Click on New File.

2.  Rename the file to `server.py`

3.  Directory Structure

3.  Copy the following MCP Server Implementation and paste in your `server.py` file on Agentverse.

    from typing import Anyimport httpxfrom mcp.server.fastmcp import FastMCP# Create a FastMCP server instancemcp = FastMCP("weather")NWS_API_BASE = "https://api.weather.gov"USER_AGENT = "weather-app/1.0"async def make_nws_request(url: str) -> dict[str, Any] | None:    headers = {        "User-Agent": USER_AGENT,        "Accept": "application/geo+json"    }    async with httpx.AsyncClient() as client:        try:            response = await client.get(url, headers=headers, timeout=30.0)            response.raise_for_status()            return response.json()        except Exception:            return Nonedef format_alert(feature: dict) -> str:    props = feature["properties"]    return f""" Event: {props.get('event', 'Unknown')} Area: {props.get('areaDesc', 'Unknown')} Severity: {props.get('severity', 'Unknown')} Description: {props.get('description', 'No description available')} Instructions: {props.get('instruction', 'No specific instructions provided')}"""@mcp.tool()async def get_alerts(state: str) -> str:    """Get weather alerts for a US state."""    url = f"{NWS_API_BASE}/alerts/active/area/{state}"    data = await make_nws_request(url)    if not data or "features" not in data:        return "Unable to fetch alerts or no alerts found."    if not data["features"]:        return "No active alerts for this state."    alerts = [format_alert(feature) for feature in data["features"]]    return "\n---\n".join(alerts)@mcp.tool()async def get_forecast(latitude: float, longitude: float) -> str:    """Get weather forecast for a location."""        points_url = f"{NWS_API_BASE}/points/{latitude},{longitude}"    points_data = await make_nws_request(points_url)    if not points_data:        return "Unable to fetch forecast data for this location."    forecast_url = points_data["properties"]["forecast"]    forecast_data = await make_nws_request(forecast_url)    if not forecast_data:        return "Unable to fetch detailed forecast."    periods = forecast_data["properties"]["periods"]    forecasts = []    for period in periods[:5]:        forecast = f"""{period['name']}: Temperature: {period['temperature']}Â°{period['temperatureUnit']} Wind: {period['windSpeed']} {period['windDirection']} Forecast: {period['detailedForecast']}"""        forecasts.append(forecast)    return "\n---\n".join(forecasts)if __name__ == "__main__":    # Initialize and run the server    mcp.run(transport='stdio')

note

**Important:** When creating MCP tools, always include detailed docstrings using triple quotes (""") to describe what each tool in the MCP Server as these descriptions play a critical role in selecting the right MCP Tool based on the user's query.

### Step 2: Create an Agent for Your FastMCP Server[â€‹](#step-2-create-an-agent-for-your-fastmcp-server "Direct link to Step 2: Create an Agent for Your FastMCP Server")

To create an Agent for your Fast MCP Server, we will import the `MCPServerAdapter` from the `uagents-adapter` package. We will also import the MCP server instance `mcp` from `server.py`.

    from uagents_adapter import MCPServerAdapterfrom server import mcp  # This is your FastMCP server instance from server.py

To enable intelligent tool selection, the adapter leverages the ASI:One LLM. You'll need an ASI:One API key, which you can obtain by logging into [ASI:One](https://asi1.ai/) and navigating to the "API Keys" tab.

**Instantiate the MCPServerAdapter:**

    mcp_adapter = MCPServerAdapter(    mcp_server=mcp,                # (FastMCP) Your MCP server instance    asi1_api_key="your-asi1-api-key",  # (str) Your ASI:One API key    model="asi1-mini"              # (str) Model to use: "asi1-mini", "asi1-extended", or "asi1-fast")

#### MCPServerAdapter Parameters[â€‹](#mcpserveradapter-parameters "Direct link to MCPServerAdapter Parameters")

| Parameter | Type | Description | Required |
| --- | --- | --- | --- |
| `mcp_server` | FastMCP | The FastMCP server instance exposing your tools. | Yes |
| `asi1_api_key` | str | Your ASI:One API key for LLM-powered tool selection. | Yes |
| `model` | str | The ASI:One model to use (`"asi1-mini"`, `"asi1-extended"`, or `"asi1-fast"`). | Yes |

**Whole Script:**

    from uagents_adapter import MCPServerAdapterfrom server import mcp# Create an MCP adapter with your MCP servermcp_adapter = MCPServerAdapter(    mcp_server=mcp,                     # (FastMCP) Your MCP server instance    asi1_api_key="your-asi1-api-key",  # (str) Your ASI:One API key    model="asi1-mini"              # (str) Model to use: "asi1-mini", "asi1-extended", or "asi1-fast")# Create a uAgentagent = Agent()# Include protocols from the adapterfor protocol in mcp_adapter.protocols:    agent.include(protocol, publish_manifest=True)if __name__ == "__main__":    # Run the MCP adapter with the agent    mcp_adapter.run(agent)

note

The MCPServerAdapter only supports FastMCP Servers at the moment.

This setup ensures your agent can intelligently select and execute the right tool from the MCP Server based on the user queries.

#### Adding a README to your Agent[â€‹](#adding-a-readme-to-your-agent "Direct link to Adding a README to your Agent")

1.  Go to the Overview section in the Editor.
    
2.  Click on Edit and add a good description for your Agent so that it can be easily searchable by the ASI1 LLM. Please refer the [Importance of Good Readme](https://innovationlab.fetch.ai/resources/docs/agentverse/searching#importance-of-good-readme) section for more details.
    

### Step 3: Test Your Agent[â€‹](#step-3-test-your-agent "Direct link to Step 3: Test Your Agent")

1.  Start your Agent.
    
2.  Switch to the Overview Tab and use the "Chat with Agent" button to start talking to your agent.
    

#### Query your Agent from ASI1 LLM[â€‹](#query-your-agent-from-asi1-llm "Direct link to Query your Agent from ASI1 LLM")

1.  Login to the [ASI1 LLM](https://asi1.ai/), either using your Google Account or the ASI1 Wallet and Start a New Chat.
    
2.  Toggle the "Agents" switch to enable ASI1 to connect with Agents on Agentverse.
    

3.  To query your specific agent, you can copy the agent's address and mention in your query to explicitly connect with your agent. For instance, "Please ask the agent1qgggh8wy6ux2xwkc267cfpxk390c4ve0ts23yz5d9l6qsnckyvs2zpx08gq for weather alerts San Diego"

You can click on the Agent URL to check the agent that answered your question.

note

**Note:** If you ask about the weather without mentioning the address of your specific agent, ASI:One LLM might select another agent as it uses the Agent Ranking mechanism to select the most appropriate agent for each query. To test your agent directly, use the "Chat with Agent" button on your Agentverse agent page.</content>
</page>

<page>
  <title>Mettalex - A Practical Implementation of AI Agents in the Web3 Ecosystem | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/on-chain-examples/mettalex-agents</url>
  <content>Mettalex: A Practical Implementation of AI Agents in the Web3 Ecosystem - Powered by Fetch.ai Agent Tech
--------------------------------------------------------------------------------------------------------

Introduction[â€‹](#introduction "Direct link to Introduction")
------------------------------------------------------------

[Mettalex](https://www.mettalex.ai/) stands out as the **first P2P orderbook** and **agent-based DEX** for commodity and digital (tokenized) assets trading. It harnesses **Fetch.aiâ€™s uAgents** to power **autonomous order matching**, **secure on-chain escrow**, and **cross-chain operations**. By eliminating reliance on centralized order books or liquidity pools, Mettalex aims to offer **slippage-free**, trustless trades with **maximum transparency**.

Key uAgents features in this use case include their **wallet- and chain-agnostic** capabilities. Additionally, the agents provide a robust **communication** and **execution** layer, making the trading process seamless and efficient.

Mettalex: Agent-Based Commodity Trading in a Nutshell[â€‹](#mettalex-agent-based-commodity-trading-in-a-nutshell "Direct link to Mettalex: Agent-Based Commodity Trading in a Nutshell")
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

1.  Direct P2P Order Matching
    
    *   **No Liquidity Pools:** Mettalex uses uAgents to match buyers and sellers directly, eliminating AMMs and reducing slippage.
    *   **Zero Slippage Execution:** Final prices match exactly what each party has agreed toâ€”crucial for volatile or low-liquidity commodity markets.
2.  Escrow-Backed Settlement
    
    *   **On-Chain Escrow:** Traders lock funds in a smart contract. The agent only completes a trade if both sides have met the exact terms.
    *   **Fail-Safe Mechanism:** If either side fails to finalize the transaction, agents revert the escrow to protect user funds.
3.  Multi-Wallet & Cross-Chain Support
    
    *   **Wallet-Agnostic:** Users may use MetaMask, Ledger, or any Web3-compatible wallet. The agent logic remains the same, ensuring a uniform trading experience.
    *   **Chain-Agnostic:** Mettalex agents can run on multiple blockchains in parallel (e.g., Ethereum, BNB Chain, Cosmos). They coordinate escrow locks and trades across networks if bridging solutions exist.
4.  On-Chain Registration & Discovery
    
    *   **Almanac Registry:** Agents register on a Cosmos-based on-chain Almanac contract, allowing other agents/dApps to verify their identity.
5.  Transparent Execution & Governance
    
    *   **Public Transaction Logs:** Every escrow creation, signature, and fund release is recorded on-chain. Users can view agent logs and track progress in real time.
    *   **MTLX Governance:** Mettalexâ€™s governance token (MTLX) can let agents automate fee changes or protocol upgrades, broadening the platformâ€™s agent-driven ecosystem.

Putting It All Together[â€‹](#putting-it-all-together "Direct link to Putting It All Together")
---------------------------------------------------------------------------------------------

*   User places a trade -> Funds locked in escrow.
*   Agent checks buyer/seller positions -> Confirms each sideâ€™s escrow.
*   Agents sign trade parameters on-chain -> Escrows release funds upon successful match.
*   Settlement -> Both parties receive respective assets with no slippage and full transparency.

If you wish to learn more about Mettalex, please visit [Mettalex Docs](https://www.mettalex.com/docs).</content>
</page>

<page>
  <title>Solana Agent Integration with Fetch.ai uAgents | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/on-chain-examples/solana-agents</url>
  <content>This example shows how to integrate Solana wallets within **Fetch.aiâ€™s uAgents** framework. Weâ€™ll walk through the **EscrowAgent, PlayerAgent, and ChallengerAgent** scripts, detailing how each agent:

1.  Registers with the **Almanac** contract (for discoverability)
2.  Loads Solana **private keys** from environment variables
3.  Executes transfers, checks balances, and handles bet-based business logic via **Solana Devnet**
4.  Communicates with other agents through **uAgents** messaging

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

*   **Solana CLI** (configured to Devnet)
*   **Poetry** (for dependency management)
*   **Python 3.8+**
*   Fetch.aiâ€™s **uagents** library
*   **Solders, requests**, etc. (handled by `poetry install`)
*   `.env` with your Solana private keys (Base64 arrays) for each agent

note

**Note:** Each agent script runs on a different portâ€”EscrowAgent uses `:8000`, PlayerAgent uses `:8001`, and ChallengerAgent uses `:8002` by default.

High-Level Architecture[â€‹](#high-level-architecture "Direct link to High-Level Architecture")
---------------------------------------------------------------------------------------------

1.  **PlayerAgent** & **ChallengerAgent** each place a bet by transferring SOL to the **Escrow** wallet (managed by the EscrowAgent).
2.  **EscrowAgent** collects two bets, checks the BTC price via an external API, and decides a winner. 90% of the total stake is transferred to the winnerâ€™s Solana wallet; the loser forfeits.

Escrow Agent[â€‹](#escrow-agent "Direct link to Escrow Agent")
------------------------------------------------------------

### Overview[â€‹](#overview "Direct link to Overview")

The **EscrowAgent**:

*   Registers on **Almanac** so other agents (Player/Challenger) can discover it
*   Waits for two **escrowRequest** messages
*   Fetches **BTC price from Binance**
*   **Transfers** the correct portion of **SOL** to the winner

### Script Breakdown[â€‹](#script-breakdown "Direct link to Script Breakdown")

**Required Libraries**

    import osimport base58import astfrom uagents import Agent, Context, Modelfrom solders.keypair import Keypairfrom functions import get_latest_btc_price, transfer_solimport time

*   os & ast for environment handling
*   uagents for agent creation
*   solders.keypair for Solana KeyPair
*   functions for helper utilities (price fetch & SOL transfer)

**Key Classes & Models**

    class escrowRequest(Model):    amount: float    price: float    public_key: strclass escrowResponse(Model):    result: str

*   **escrowRequest** holds the userâ€™s desired bet: `amount`, `price`, and the userâ€™s **Solana public key**.
*   **escrowResponse** returns the result to the user: either `"You Won"` or `"You Lost"`.

**Initialization & Identity**

    # Retrieve ESCROW_SECRET_LIST from the .envescrow_secret_key_str = os.getenv('ESCROW_SECRET_LIST')escrow_secret_key_list = ast.literal_eval(escrow_secret_key_str)escrow_secret_key_bytes = bytes(escrow_secret_key_list)escrow_keypair = Keypair.from_bytes(escrow_secret_key_bytes)escrow_pubkey_base58 = base58.b58encode(bytes(escrow_keypair.pubkey())).decode('utf-8')agent = Agent(    name="EscrowAgent",    port=8000,    seed="Escrow Wallet",    endpoint=["http://127.0.0.1:8000/submit"],)

*   We decode the **Solana private key** from `.env`.
*   Create a `Keypair` for the Escrowâ€™s wallet.
*   Instantiate a `uagents.Agent` with the name â€œEscrowAgentâ€ listening on port `8000`.

**On Startup**

    @agent.on_event('startup')async def saf(ctx: Context):    ctx.logger.info("Escrow agent initialized, ready for bids.")    ctx.logger.info(f"Escrow agent address: {agent.address}")    ctx.storage.set("bids_count", 0)

*   Logs that the Escrow is online.
*   Initializes a storage key bids\_count=0 to track how many requests have come in.

**Message Handling**

Receiving Bets

    @agent.on_message(model=escrowRequest, replies={escrowResponse})async def escrow_request_handler(ctx: Context, sender: str, msg: escrowRequest):    current_count = ctx.storage.get("bids_count") or 0    ...    if current_count == 0:        # Store first bet    elif current_count == 1:        # Store second bet        # Compare        # Transfer to winner        # Respond with escrowResponse        # Reset storage

*   **First Bet:** If `bids_count=0`, store the details (amount, price, userâ€™s pubkey).
*   **Second Bet:** If `bids_count=1`, store the second userâ€™s bet, then call `get_latest_btc_price()`.
*   Calculate each userâ€™s distance from the real BTC price.
*   Transfer 90% of the total stake to the winnerâ€™s public key with `transfer_sol()`.
*   Send escrowResponse messages to both the winner `(You Won)` and loser `(You Lost)`.
*   Reset internal storage.

**Running the EscrowAgent**

    if __name__ == "__main__":    agent.run()

*   Save the script as `escrow_agent.py`
*   Simply run `poetry run python escrow_agent.py`.

Player Agent[â€‹](#player-agent "Direct link to Player Agent")
------------------------------------------------------------

### Overview[â€‹](#overview-1 "Direct link to Overview")

The **PlayerAgent** simulates a user placing a bet on BTCâ€™s future price.

### Script Breakdown[â€‹](#script-breakdown-1 "Direct link to Script Breakdown")

**Required Libraries**

    import osimport astimport base58from uagents import Agent, Context, Modelfrom solders.keypair import Keypairfrom functions import check_balance, transfer_solfrom uagents.setup import fund_agent_if_lowcheck_balance and transfer_sol from functions.py to manage SOL balances/transfersfund_agent_if_low from uagents.setup can top up the agentâ€™s fetch-side address if needed

**Key Classes**

    class escrowRequest(Model):    amount: float    price: float    public_key: strclass escrowResponse(Model):    result: str

*   Re-used from the Escrow flow: escrowRequest is how we send the userâ€™s bet to the EscrowAgent.

**Initialization & Identity**

    secret_key_str = os.getenv('PLAYER_SECRET_LIST')secret_key_list = ast.literal_eval(secret_key_str)secret_key_bytes = bytes(secret_key_list)agent_keypair = Keypair.from_bytes(secret_key_bytes)agent_pubkey_base58 = base58.b58encode(bytes(agent_keypair.pubkey())).decode('utf-8')agent = Agent(    name="PlayerAgent",    port=8001,    seed="Player Escrow Wallet 1",    endpoint=["http://127.0.0.1:8001/submit"],)

*   Decodes the **PLAYER\_SECRET\_LIST** from `.env`.
*   Assigns the Playerâ€™s Solana wallet keypair.
*   Sets up a uAgent on port `8001`.

**Startup Sequence**

    @agent.on_event('startup')async def starter_function(ctx: Context):    initial_balance = check_balance(agent_keypair.pubkey())    ctx.logger.info(f"Initial agent balance: {initial_balance} SOL")    amount = float(input('What is the amount of SOL you want to deposit? '))    price = float(input('What is the price of Bitcoin you want to bid at? '))    # Send bet to Escrow    await ctx.send(        'agent1qd6ts50kuy3vqq36s5yg2dkzujq60x0l0sr2acfafnp5zea749yvvvq2qm7',        escrowRequest(amount=amount, price=price, public_key=agent_pubkey_base58)    )    # Transfer SOL to escrowâ€™s base58 key    transfer_result = transfer_sol(agent_keypair, '8WMWFo13At1REkwy5t7ck6sLgCUrJ9dn66mbaccPiJ26', amount)    ctx.logger.info(f"Transfer result: {transfer_result}")    final_balance = check_balance(agent_keypair.pubkey())    ctx.logger.info(f"Final agent balance: {final_balance} SOL")

*   **Check initial SOL** in the userâ€™s wallet.
*   **Ask** for deposit & BTC price guess.
*   **Send** an `escrowRequest` message to the known Escrow agent address.
*   `transfer_sol` to the Escrow agentâ€™s public key.
*   Log final SOL.

**Receiving Escrow Responses**

    @agent.on_message(model=escrowResponse)async def escrow_request_handler(ctx: Context, sender: str, msg: escrowResponse):    balance = check_balance(agent_keypair.pubkey())    ctx.logger.info(f'{msg.result}. Updated account balance: {balance} SOL')

*   When the **EscrowAgent** decides the outcome, it sends an `escrowResponse`.
*   This handler logs either â€œYou Wonâ€ or â€œYou Lostâ€ plus the updated balance.

**Running PlayerAgent**

    if __name__ == "__main__":    agent.run()

*   Save the script as `player_agent.py`
*   Run with poetry `run python player_agent.py`.

Challenger Agent[â€‹](#challenger-agent "Direct link to Challenger Agent")
------------------------------------------------------------------------

### Overview[â€‹](#overview-2 "Direct link to Overview")

Nearly identical to `player_agent.py`, but simulates another user (Challenger) placing a competing bet.

### Script Breakdown[â€‹](#script-breakdown-2 "Direct link to Script Breakdown")

**Required Libraries**

    import osimport astimport base58from uagents import Agent, Context, Modelfrom solders.keypair import Keypairfrom functions import check_balance, transfer_solfrom uagents.setup import fund_agent_if_low

**Key Classes**

    class escrowRequest(Model):    amount: float    price: float    public_key: strclass escrowResponse(Model):    result: str

**Initialization & Startup**

    secret_key_str = os.getenv('CHALLENGER_SECRET_LIST')secret_key_list = ast.literal_eval(secret_key_str)secret_key_bytes = bytes(secret_key_list)agent_keypair = Keypair.from_bytes(secret_key_bytes)agent_pubkey_base58 = base58.b58encode(bytes(agent_keypair.pubkey())).decode('utf-8')agent = Agent(    name="Challenger",    port=8002,    seed="Challenger Escrow Wallet 2",    endpoint=["http://127.0.0.1:8002/submit"],)fund_agent_if_low(agent.wallet.address())@agent.on_event('startup')async def starter_function(ctx: Context):    initial_balance = check_balance(agent_keypair.pubkey())    ctx.logger.info(f"Initial agent balance: {initial_balance} SOL")    amount = float(input('What is the amount of SOL you want to deposit? '))    price = float(input('What is the price of Bitcoin you want to bid at? '))    # Send the bet    await ctx.send(        'agent1qd6ts50kuy3vqq36s5yg2dkzujq60x0l0sr2acfafnp5zea749yvvvq2qm7',        escrowRequest(amount=amount, price=price, public_key=agent_pubkey_base58)    )    # Transfer SOL    transfer_result = transfer_sol(agent_keypair, '8WMWFo13At1REkwy5t7ck6sLgCUrJ9dn66mbaccPiJ26', amount)    ctx.logger.info(f"Transfer result: {transfer_result}")    final_balance = check_balance(agent_keypair.pubkey())    ctx.logger.info(f"Final agent balance: {final_balance} SOL")@agent.on_message(model=escrowResponse)async def escrow_request_handler(ctx: Context, sender: str, msg: escrowResponse):    balance = check_balance(agent_keypair.pubkey())    ctx.logger.info(f'{msg.result}. Updated account balance: {balance} SOL')if __name__ == "__main__":    agent.run()

*   Exactly the same flow: read user input, transfer SOL, wait for a response from the Escrow agent.

Utility Script : functions.py[â€‹](#utility-script--functionspy "Direct link to Utility Script : functions.py")
-------------------------------------------------------------------------------------------------------------

Below is a brief summary of the key utility functions. For full code, see the repository.

    import base58from solders.keypair import Keypairfrom solders.pubkey import Pubkeyfrom solders.transaction import Transactionfrom solders.system_program import TransferParams, transferfrom solana.rpc.api import Clientfrom solana.rpc.types import TxOptsimport requestsdef get_keypair_details(secret_key_list):    """    Given a list of secret key bytes (integers), returns a dictionary with the keypair, public key,    private key in bytes, and private key in Base58 encoding.    """    # Convert the list of integers into a bytes object (private key)    secret_key_bytes = bytes(secret_key_list)    # Restore the Keypair using the secret key    keypair = Keypair.from_bytes(secret_key_bytes)    # Public key (from keypair)    public_key = keypair.pubkey()    # Private key in Base58 encoding (for readability)    private_key_base58 = base58.b58encode(secret_key_bytes).decode()    # Return all necessary details in a dictionary    return {        "keypair": keypair,        "public_key": public_key,  # Solders Pubkey object        "private_key_bytes": secret_key_bytes,  # Private key in bytes        "private_key_base58": private_key_base58  # Private key in Base58    }client = Client("https://api.devnet.solana.com")# Function to check balancedef check_balance(pubkey):    balance_resp = client.get_balance(Pubkey.from_bytes(bytes(pubkey)))    print(f'balance_resp :{balance_resp}')    balance = balance_resp.value  # Extract balance in lamports    return balance / 1_000_000_000  # Convert lamports to SOLdef transfer_sol(from_keypair, to_pubkey_base58, amount_sol):    # Convert SOL to lamports (1 SOL = 1 billion lamports)    lamports = int(amount_sol * 1_000_000_000)    # Convert the recipient's Base58 public key string to a Pubkey object    to_pubkey = Pubkey.from_string(to_pubkey_base58)    # Get latest blockhash    blockhash_resp = client.get_latest_blockhash()    recent_blockhash = blockhash_resp.value.blockhash    # Create a transfer instruction    transfer_instruction = transfer(        TransferParams(from_pubkey=from_keypair.pubkey(), to_pubkey=to_pubkey, lamports=lamports)    )    # Create the transaction with the instruction directly    transaction = Transaction.new_signed_with_payer(        [transfer_instruction],  # Pass the list of instructions directly        from_keypair.pubkey(),  # Fee-payer (challenger)        [from_keypair],  # Signers (challenger)        recent_blockhash  # Use recent blockhash directly    )    # Send the transaction    result = client.send_raw_transaction(bytes(transaction), opts=TxOpts(skip_confirmation=False))    return resultdef get_latest_btc_price():    try:        # Binance API endpoint for fetching the latest BTC price in USDT        url = 'https://api.binance.com/api/v3/ticker/price?symbol=BTCUSDT'        response = requests.get(url)        response.raise_for_status()  # Raise an error for bad responses        data = response.json()        return float(data['price'])  # Return the latest BTC price as a float    except requests.exceptions.RequestException as e:        print(f"Error fetching BTC price: {e}")        return None

.env File Example[â€‹](#env-file-example "Direct link to .env File Example")
--------------------------------------------------------------------------

    # .envAGENTVERSE_API_KEY="<Your_FetchAI_Agentverse_Token>"PLAYER_SECRET_LIST="[79,79,237,8,87,104,75,156,47,204,53,127,171,9,114,244,...]"CHALLENGER_SECRET_LIST="[134,53,148,91,88,30,254,53,171,183,219,91,33,67,24,9,65,...]"ESCROW_SECRET_LIST="[251,164,58,0,121,167,133,83,114,82,162,22,88,214,195,91,82,...]"

Ensure each secret list matches the integer arrays from your `player-wallet.json`, `challenger-wallet.json`, and `escrow-wallet.json`. Also, do not commit your `.env` to source control.

Steps to Run the Agents[â€‹](#steps-to-run-the-agents "Direct link to Steps to Run the Agents")
---------------------------------------------------------------------------------------------

1.  Set Up Virtual Environment & Dependencies

2.  Fund Each Wallet on Devnet

    solana airdrop 5 <PLAYER_PUBKEY> --url devnetsolana airdrop 5 <CHALLENGER_PUBKEY> --url devnetsolana airdrop 5 <ESCROW_PUBKEY> --url devnet

3.  Start EscrowAgent

    poetry run python escrow_agent.py

*   Wait for it to display Escrow agent initialized, ready for bids.

4.  Start PlayerAgent

    poetry run python player_agent.py

*   Input the deposit amount & BTC guess when prompted.

5.  Start ChallengerAgent

    poetry run python challenger_agent.py

*   Similarly input deposit & guess. Once the Escrow receives the second bet, it decides a winner.

### Sample Output[â€‹](#sample-output "Direct link to Sample Output")

**EscrowAgent:**

    INFO: [EscrowAgent]: Escrow agent initialized, ready for bids.INFO: [EscrowAgent]: Received escrowRequest messageINFO: [EscrowAgent]: Storing first request ...INFO: [EscrowAgent]: Received escrowRequest messageINFO: [EscrowAgent]: Storing second request ...INFO: [EscrowAgent]: Processing bids to determine the winner.INFO: [EscrowAgent]: First difference: 1820.0, Second difference: 586820.0INFO: [EscrowAgent]: Transferring 0.9 SOL to winner ...INFO: [EscrowAgent]: Notifying winner and loser.

**PlayerAgent:**

    What is the amount of SOL you want to deposit? 0.5What is the price of Bitcoin you want to bid at? 65000INFO: [PlayerAgent]: Transfer result: ...INFO: [PlayerAgent]: Final agent balance: 4.31998 SOLINFO: [PlayerAgent]: You Won. Updated account balance: 5.21998 SOL

Debugging Common Issues[â€‹](#debugging-common-issues "Direct link to Debugging Common Issues")
---------------------------------------------------------------------------------------------

1.  Key Decoding Errors
    
    *   Ensure your `.env` secret lists are valid JSON arrays of integers.
    *   If you see `ValueError` or `Cannot decode secret key`, confirm you have no trailing commas.
2.  Faucet or Balance Issues
    
    *   Double-check `solana balance <PUBKEY> --url devnet`. If less then 1 SOL, some transactions might fail due to insufficient lamports for fees.
3.  Agent Registration Problems
    
    *   Confirm **AGENTVERSE\_API\_KEY** is correct in your `.env`.
    *   Make sure each agent can reach the default Almanac endpoint (requires internet connection).
4.  BTC Price Fetch Errors
    
    *   If Binance is unreachable or rate-limits your IP, consider adding retry logic or a fallback endpoint.

By following this **Solana + uAgents** guide, youâ€™ve set up three distinct agents that:

*   Load private keys from .env
*   Register on Fetch.aiâ€™s Almanac for discovery
*   Communicate using @agent.on\_message and typed models (escrowRequest, escrowResponse)
*   Interact with Solana Devnet for safe, low-cost experimentation

This architecture can be extended for **NFT auctions, DeFi ops, cross-chain bridging**, or any scenario where you need **agent-driven** logic plus on-chain Solana transactions. Enjoy building!

note

**Note:** GitHub repository for this example is available [here](https://github.com/abhifetch/solana-fetch-uagents-integration).</content>
</page>

<page>
  <title>BNB Chain Agents | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/on-chain-examples/bnb-chain-agents</url>
  <content>Version: 1.0.3

This guide demonstrates how to create and use AI agents for interacting with the BNB Chain using uAgents. We'll build a system of three agents that work together to send transactions, validate them, and monitor wallet activity.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

The system consists of three main agents:

1.  **Transaction Sender Agent (Agent1)**: Handles BNB transfer requests and initiates transactions
2.  **Transaction Validator Agent (Agent2)**: Verifies transaction status using BscScan API
3.  **Wallet Monitor Agent**: Monitors specified wallet addresses for any transaction activity

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

Before getting started, you'll need:

*   Python 3.11 or higher
*   [A BNB Testnet account with some test BNB](https://github.com/abhifetch/BNBChainUAgents?tab=readme-ov-file#wallet-creation).
*   [BscScan API Key](https://docs.bscscan.com/getting-started/viewing-api-usage-statistics)
*   Basic understanding of Web3 and blockchain concepts

Architecture Diagram[â€‹](#architecture-diagram "Direct link to Architecture Diagram")
------------------------------------------------------------------------------------

Installation[â€‹](#installation "Direct link to Installation")
------------------------------------------------------------

1.  Create a new directory and set up your virtual environment:

    mkdir bnb-chain-agentscd bnb-chain-agentspython -m venv venvsource venv/bin/activate  # On macOS/Linuxvenv\Scripts\activate     # On Windows

2.  Install the required packages:

    pip install uagents web3 python-dotenv requests

3.  Create a `.env` file with your credentials:

    USER_WALLET=your_wallet_addressUSER_KEY=your_private_keyBSCSCAN_API_KEY=your_bscscan_api_key

Implementation[â€‹](#implementation "Direct link to Implementation")
------------------------------------------------------------------

### 1\. Transaction Sender Agent (agent1.py)[â€‹](#1-transaction-sender-agent-agent1py "Direct link to 1. Transaction Sender Agent (agent1.py)")

This agent handles BNB transfer requests and communicates with the validator agent:

    from uagents import Agent, Context, Modelfrom web3 import Web3from web3.middleware import geth_poa_middlewareimport osfrom dotenv import load_dotenv# Load environment variablesload_dotenv()agent_user = Agent(    name='User BNB Agent to make transactions',    port=8000,    endpoint=['http://localhost:8000/submit'])class RequestTransfer(Model):    to_address: str    amount: float    agent_to_address: strclass RequestDetails(Model):    tx_hash: strclass ResponseTransfer(Model):    response: str# Connect to BNB Chain Testnetprovider_url = "https://data-seed-prebsc-1-s1.binance.org:8545/"web3 = Web3(Web3.HTTPProvider(provider_url))web3.middleware_onion.inject(geth_poa_middleware, layer=0)@agent_user.on_rest_post("/send/bnb", RequestTransfer, ResponseTransfer)async def handle_post(ctx: Context, req: RequestTransfer) -> ResponseTransfer:    try:        # Get wallet credentials        user_wallet = os.getenv("USER_WALLET")        user_key = os.getenv("USER_KEY")                # Prepare transaction        to_address = web3.to_checksum_address(req.to_address)        nonce = web3.eth.get_transaction_count(user_wallet)                tx = {            'to': to_address,            'value': web3.to_wei(req.amount, 'ether'),            'gas': 21000,            'gasPrice': web3.eth.gas_price,            'nonce': nonce,            'chainId': 97  # BNB Testnet        }        # Sign and send transaction        signed_tx = web3.eth.account.sign_transaction(tx, user_key)        tx_hash = web3.eth.send_raw_transaction(signed_tx.raw_transaction)                # Send to validator agent        message, status = await ctx.send_and_receive(            req.agent_to_address,            RequestDetails(tx_hash=web3.to_hex(tx_hash)),            response_type=ResponseTransfer        )                return ResponseTransfer(response=message.response)        except Exception as e:        return ResponseTransfer(response=f"Transaction Failed: {str(e)}")if __name__ == "__main__":    agent_user.run()

### 2\. Transaction Validator Agent (agent2.py)[â€‹](#2-transaction-validator-agent-agent2py "Direct link to 2. Transaction Validator Agent (agent2.py)")

This agent verifies transaction status using the BscScan API:

    from uagents import Agent, Context, Modelfrom web3 import Web3import osfrom dotenv import load_dotenvimport asyncioimport requestsagent_dummy = Agent(    name='Dummy BNB Agent to make transactions',    port=8001,    endpoint=['http://localhost:8001/submit'])load_dotenv()BSCSCAN_API_KEY = os.getenv("BSCSCAN_API_KEY")class RequestDetails(Model):    tx_hash: strclass ResponseTransfer(Model):    response: strasync def get_transaction_status(tx_hash: str) -> dict:    base_url = "https://api.bscscan.com/api"    params = {        "module": "transaction",        "action": "getstatus",        "txhash": tx_hash,        "apikey": BSCSCAN_API_KEY    }        response = await asyncio.to_thread(requests.get, base_url, params=params)    return response.json() if response.status_code == 200 else {"error": f"HTTP error {response.status_code}"}@agent_dummy.on_message(model=RequestDetails, replies={ResponseTransfer})async def startup_handler(ctx: Context, sender: str, msg: RequestDetails):    tx_status = await get_transaction_status(msg.tx_hash)        if "error" in tx_status:        reply = f"API Error: {tx_status['error']}"    else:        if tx_status.get("status") == "1" and tx_status.get("message") == "OK":            if tx_status["result"].get("isError") == "0":                reply = f"Successful transfer confirmed by receiver.âœ… tx_hash: {msg.tx_hash}"            else:                err_desc = tx_status["result"].get("errDescription", "Unknown error")                reply = f"Transfer rejected: {err_desc}"        else:            reply = "Transfer status unknown or API response error."    await ctx.send(sender, ResponseTransfer(response=reply))if __name__ == "__main__":    agent_dummy.run()

### 3\. Wallet Monitor Agent (monitor\_wallet.py)[â€‹](#3-wallet-monitor-agent-monitor_walletpy "Direct link to 3. Wallet Monitor Agent (monitor_wallet.py)")

This agent monitors wallet activity in real-time:

    from uagents import Agent, Contextfrom web3 import Web3from web3.middleware import geth_poa_middlewareimport jsonimport osfrom dotenv import load_dotenvload_dotenv()user_wallet = os.getenv("USER_WALLET")monitor_agent = Agent(    name='Monitor BNB Agent to monitor address',    port=8003,    endpoint=['http://localhost:8003/submit'])# Connect to BNB Chain Testnetprovider_url = "https://data-seed-prebsc-1-s1.binance.org:8545/"web3 = Web3(Web3.HTTPProvider(provider_url))web3.middleware_onion.inject(geth_poa_middleware, layer=0)monitored_address = web3.to_checksum_address(user_wallet)last_scanned_block = web3.eth.block_number@monitor_agent.on_interval(period=10)async def monitor_handler(ctx: Context):    global last_scanned_block    try:        current_block = web3.eth.block_number        if current_block <= last_scanned_block:            return        for block_num in range(last_scanned_block + 1, current_block + 1):            block = web3.eth.get_block(block_num, full_transactions=True)            for tx in block['transactions']:                tx_from = tx['from']                tx_to = tx['to']                                if (tx_from and tx_from.lower() == monitored_address.lower()) or \                   (tx_to and tx_to.lower() == monitored_address.lower()):                    details = {                        "blockNumber": block_num,                        "hash": tx['hash'].hex(),                        "from": tx_from,                        "to": tx_to if tx_to else "Contract Creation",                        "value": str(web3.from_wei(tx['value'], 'ether')) + " BNB"                    }                    ctx.logger.info(f"Recorded transaction: {json.dumps(details, indent=2)}")        last_scanned_block = current_block    except Exception as e:        ctx.logger.error(f"Error during monitoring: {e}")if __name__ == "__main__":    monitor_agent.run()

Usage[â€‹](#usage "Direct link to Usage")
---------------------------------------

1.  Create a new wallet (if needed):

    # wallet_creation.pyfrom eth_account import Accountimport secrets# Generate a random private keyprivate_key = secrets.token_hex(32)account = Account.from_key(private_key)print(f"Address: {account.address}")print(f"Private Key: {private_key}")

Save this as `wallet_creation.py` and run:

    python wallet_creation.py

2.  Get test BNB:
    
    *   Visit the [BNB Chain Testnet Faucet](https://testnet.bnbchain.org/faucet-smart)
    *   Follow Steps provided [here](https://github.com/abhifetch/BNBChainUAgents?tab=readme-ov-file#wallet-creation).
3.  Start all three agents in separate terminal windows:
    

    # Terminal 1python agent1.py# Terminal 2python agent2.py# Terminal 3python monitor_wallet.py

2.  Send a BNB transfer request using curl:

    curl -d '{    "to_address": "RECIPIENT_ADDRESS",    "amount": 0.01,    "agent_to_address": "AGENT2_ADDRESS"}' \-H "Content-Type: application/json" \-X POST http://localhost:8000/send/bnb

Replace `RECIPIENT_ADDRESS` with the destination wallet address and `AGENT2_ADDRESS` with the address of your validator agent.

System Flow[â€‹](#system-flow "Direct link to System Flow")
---------------------------------------------------------

1.  The Transaction Sender Agent receives a transfer request via HTTP POST
2.  It creates and signs the transaction using the private key
3.  The transaction is sent to the BNB Chain Testnet
4.  The transaction hash is forwarded to the Validator Agent
5.  The Validator Agent checks the transaction status using BscScan API
6.  The Wallet Monitor Agent continuously scans for new transactions
7.  All agents log their activities and transaction details

Best Practices[â€‹](#best-practices "Direct link to Best Practices")
------------------------------------------------------------------

1.  **Security**:
    
    *   Never commit your `.env` file or expose private keys
    *   Use environment variables for sensitive data
    *   Validate input data before processing
2.  **Error Handling**:
    
    *   Implement proper error handling for API calls
    *   Log errors and transaction details
    *   Provide meaningful error messages
3.  **Monitoring**:
    
    *   Use the monitoring agent to track transactions
    *   Implement alerts for suspicious activities
    *   Keep logs for auditing purposes

Example Outputs[â€‹](#example-outputs "Direct link to Example Outputs")
---------------------------------------------------------------------

Here are example outputs from running the system:

### Wallet Creation Output[â€‹](#wallet-creation-output "Direct link to Wallet Creation Output")

    Address: 0x742d35Cc6634C0532925a3b844Bc454e4438f44ePrivate Key: 123e4567e89b12d3a456426614174000c29f34862b7b2c2b7b2c2b7b2c2b7b2c

### Transaction Sender Agent Output (agent1.py)[â€‹](#transaction-sender-agent-output-agent1py "Direct link to Transaction Sender Agent Output (agent1.py)")

    INFO:     [User BNB Agent to make transactions]: Starting agent with address: agent1qgrw5n9ttqtwxvzdwet9jjsgttqx3q4pnt5f9f5d5x2dyv476acrgx6gg79INFO:     [User BNB Agent to make transactions]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8000INFO:     [User BNB Agent to make transactions]: Starting server on http://0.0.0.0:8000INFO:     [User BNB Agent to make transactions]: signed_tx: SignedTransaction(...)INFO:     [User BNB Agent to make transactions]: âœ… Transaction Sent! Tx Hash: 0x1d1ae447fc7230f40b1770c78ad77a0c8de9fdf373c0f911f270912981a5f461INFO:     [User BNB Agent to make transactions]: Message received: Successful transfer confirmed by receiver.âœ…

### Transaction Validator Agent Output (agent2.py)[â€‹](#transaction-validator-agent-output-agent2py "Direct link to Transaction Validator Agent Output (agent2.py)")

    INFO:     [Dummy BNB Agent to make transactions]: Starting agent with address: agent1qv75qva6vhn54zasp6svczkwzxn3qugjrngx6zepq3xqtaquhuayy5s2v4eINFO:     [Dummy BNB Agent to make transactions]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8001INFO:     [Dummy BNB Agent to make transactions]: Starting server on http://0.0.0.0:8001INFO:     [Dummy BNB Agent to make transactions]: Received transaction status request for: 0x1d1ae447fc7230f40b1770c78ad77a0c8de9fdf373c0f911f270912981a5f461INFO:     [Dummy BNB Agent to make transactions]: Transaction status: {'status': '1', 'message': 'OK', 'result': {'isError': '0'}}INFO:     [Dummy BNB Agent to make transactions]: Reply: Successful transfer confirmed by receiver.âœ…

### Wallet Monitor Agent Output (monitor\_wallet.py)[â€‹](#wallet-monitor-agent-output-monitor_walletpy "Direct link to Wallet Monitor Agent Output (monitor_wallet.py)")

    INFO:     [Monitor BNB Agent to monitor address]: Starting agent with address: agent1q2l4mrp9pnqgyhqk2x6j30qgg4uccdgct2hpkheyyld27qjdqfxf7t6jn6jINFO:     [Monitor BNB Agent to monitor address]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8003INFO:     [Monitor BNB Agent to monitor address]: Starting server on http://0.0.0.0:8003INFO:     [Monitor BNB Agent to monitor address]: Scanning blocks from 48790417 to 48790420...INFO:     [Monitor BNB Agent to monitor address]: Recorded transaction: {  "blockNumber": 48790420,  "hash": "0x1d1ae447fc7230f40b1770c78ad77a0c8de9fdf373c0f911f270912981a5f461",  "from": "0xAfEe66D9404bC93D73ae367cCE4e0ca8280462B6",  "to": "0x129e995c5F229fB7677B663fb83Bc427C1F276cf",  "value": "0.01 BNB"}

### Example CURL Request and Response[â€‹](#example-curl-request-and-response "Direct link to Example CURL Request and Response")

    # Requestcurl -d '{    "to_address": "0x129e995c5F229fB7677B663fb83Bc427C1F276cf",    "amount": 0.01,    "agent_to_address": "agent1qv75qva6vhn54zasp6svczkwzxn3qugjrngx6zepq3xqtaquhuayy5s2v4e"}' \-H "Content-Type: application/json" \-X POST http://localhost:8000/send/bnb# Response{    "response": "Successful transfer confirmed by receiver.âœ… tx_hash: 0x1d1ae447fc7230f40b1770c78ad77a0c8de9fdf373c0f911f270912981a5f461"}</content>
</page>

<page>
  <title>Langchain Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/other-frameworks/langchain</url>
  <content>Getting a langchain tool on Agentverse Marketplace
--------------------------------------------------

This document is a comprehensive guide to help users integrate **LangChain** tools into **Fetch.ai's** ecosystem using the provided **Alphavantage Agent** and **User Agent scripts**. These agents work together to fetch stock prices and demonstrate how LangChain tools can interact seamlessly with Agentverse agents.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

### Purpose[â€‹](#purpose "Direct link to Purpose")

The purpose of this script is to:

*   **Use LangChain integration** to fetch stock prices from the Alpha Vantage API.
*   **Demonstrate communication** between agents within the Fetch.ai Agentverse.
*   **Provide** a clear example of webhook-based communication.

### Key Features[â€‹](#key-features "Direct link to Key Features")

*   The\_\_ Alphavantage Agent\_\_ handles requests for stock prices using the Alpha Vantage API.
*   The **User Agent** acts as a client to forward requests and retrieve responses.
*   Communication is done via **HTTP webhooks** registered with Fetch.aiâ€™s Agentverse.

### Architecture Diagram[â€‹](#architecture-diagram "Direct link to Architecture Diagram")

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

### Environment Setup[â€‹](#environment-setup "Direct link to Environment Setup")

*   **Python 3.8** or higher.
*   A **virtual environment** (recommended).
*   **Flask** and related libraries installed.

### Required Libraries[â€‹](#required-libraries "Direct link to Required Libraries")

Install the necessary libraries using:

    pip install flask flask-cors fetchai langchain â€œflask[async]â€

note

**Note:** For more advanced dependency management, consider creating a `requirements.txt` and using `pip install -r requirements.txt`.

### Environment Variables[â€‹](#environment-variables "Direct link to Environment Variables")

Create a `.env` file in the project directory with the following content:

    AV_STOCKPRICE_AI_KEY=<your_alpha_vantage_ai_key>AGENTVERSE_API_KEY=<your_agentverse_api_key>USER_STOCKPRICE_AI_KEY=<your_user_stockprice_ai_key>ALPHAVANTAGE_API_KEY=<your_agentverse_api_key>

Replace the placeholders with your respective API keys. Sign up for a free API key at [Alpha Vantage](https://www.alphavantage.co/).

Alphvantage Agent Script[â€‹](#alphvantage-agent-script "Direct link to Alphvantage Agent Script")
------------------------------------------------------------------------------------------------

The **Alphavantage Agent** Script focuses on:

*   **Registering** with Fetch.aiâ€™s Agentverse.
*   **Handling requests** to fetch stock prices.
*   **Responding** to the User Agent with stock price data.

### Script Breakdown (stockprice.py)[â€‹](#script-breakdown-stockpricepy "Direct link to Script Breakdown (stockprice.py)")

**Required Libraries**

The script imports the required modules for:

*   **Identity Management**: Fetch.aiâ€™s Identity and registration modules.
*   **Communication**: Flask for HTTP handling and Fetch.aiâ€™s communication utilities.
*   **Alpha Vantage API**: LangChainâ€™s `AlphaVantageAPIWrapper`.

    import osimport loggingimport asynciofrom uagents_core.crypto import Identityfrom flask import Flask, request, jsonifyfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentfrom langchain_community.utilities.alpha_vantage import AlphaVantageAPIWrapper

**Setting Up Logging**

Logging is configured to monitor the agentâ€™s activities:

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)

**Flask Webhook**

The Flask app is created to handle incoming webhook requests:

    flask_app = Flask(__name__)

**Alpha Vantage Integration**

An instance of the Alpha Vantage wrapper is created:

    alpha_vantage = AlphaVantageAPIWrapper()

**Fetch Stock Price Function**

The get\_stock\_price function fetches stock prices asynchronously from the Alpha Vantage API:

    async def get_stock_price(symbol):    response = alpha_vantage._get_quote_endpoint(symbol)    return response['Global Quote']['05. price']

**Webhook Endpoint**

The /webhook route handles requests sent by other agents:

    @flask_app.route('/webhook', methods=['POST'])async def webhook():    global response    global av_identity    try:        # Parse incoming message        data = request.get_data().decode('utf-8')        message = parse_message_from_agent(data)        symbol = message.payload.get("request", "")        agent_address = message.sender        if not symbol:            return jsonify({"status": "error", "message": "No data path provided"}), 400        price = await get_stock_price(symbol)        print(price)        payload = {'price': price}        send_message_to_agent(av_identity, agent_address, payload)        return jsonify({"status": "wiki_content_sent"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"status": "error", "message": str(e)}), 500

**Registering the Agent**

The init\_agent function registers the Alphavantage Agent with Fetch.aiâ€™s Agentverse:

    def init_agent():   global av_identity   try:       av_identity = Identity.from_seed(os.getenv("AV_STOCKPRICE_AI_KEY"), 0)       register_with_agentverse(           identity=av_identity,           url="http://localhost:5008/webhook",           agentverse_token=os.getenv("AGENTVERSE_API_KEY"),           agent_title="Alphavantage Stock Price Langchain tool",           # Define the client agent's metadata           readme = """               <description>This is langchain based alphavantage tool to get the latest stock price for the given symbol.</description>               <use_cases>                   <use_case>Gives you the stock price of given symbol.</use_case>               </use_cases>               <payload_requirements>               <description>Expects the symbol for which you want stock price for.</description>                   <payload>                       <requirement>                           <parameter>symbol</parameter>                           <description>Symbol for which you want to check the stock price?</description>                       </requirement>                   </payload>               </payload_requirements>           """       )       logger.info("Langchain Alphavantage tool agent registered successfully!")   except Exception as e:       logger.error(f"Error initializing agent: {e}")       raise

**Running the Agent**

The Flask server runs on port 5008 to handle requests:

    if __name__ == "__main__":    init_agent()    flask_app.run(host="0.0.0.0", port=5008, debug=True)

User Agent Script[â€‹](#user-agent-script "Direct link to User Agent Script")
---------------------------------------------------------------------------

The User Agent script acts as the client that forwards requests to the Alphavantage Agent and retrieves responses.

### Script Breakdown (user.py)[â€‹](#script-breakdown-userpy "Direct link to Script Breakdown (user.py)")

**Importing Libraries**

The script imports required libraries for Flask, Fetch.ai, and communication:

    from flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai import fetchfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osfrom dotenv import load_dotenvfrom flask import Flask, jsonify, request

**Setting Up Logging and Flask**

Logging and Flask are configured similarly to the Alphavantage Agent:

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app)

**Initializing the User Agent**

The init\_client function initializes and registers the User Agent with Agentverse:

    def init_client():   """Initialize and register the client agent."""   global client_identity   try:       # Load the client identity from environment variables       client_identity = Identity.from_seed(os.getenv("USER_STOCKPRICE_AI_KEY"), 0)       logger.info(f"Client agent started with address: {client_identity.address}")       # Define the client agent's metadata       readme = """       <description>Frontend client that tests with Stocks price using alphavantage.</description>       <use_cases>           <use_case>Sends Request to the AI Agent to check stock price</use_case>       </use_cases>       <payload_requirements>           <description>Expects request which is to be sent to another agent.</description>           <payload>               <requirement>                   <parameter>request</parameter>                   <description>This is the request which is to be sent to other agent</description>               </requirement>           </payload>       </payload_requirements>       """       # Register the agent with Agentverse       register_with_agentverse(           identity=client_identity,           url="http://localhost:5002/api/webhook",           agentverse_token=os.getenv("AGENTVERSE_API_KEY"),           agent_title="User agent for Stock Price check",           readme=readme       )       logger.info("Client agent registration complete!")   except Exception as e:       logger.error(f"Initialization error: {e}")       raise

**Searching Agents**

The /api/search-agents endpoint allows users to search for available agents:

    # searching the agents which can create dashboard on agentverse@app.route('/api/search-agents', methods=['GET'])def search_agents():   """Search for available dashboard agents based on user input."""   try:       # Extract user input from query parameters       user_query = request.args.get('query', '')       if not user_query:           return jsonify({"error": "Query parameter 'query' is required."}), 400       # Fetch available agents based on user query       available_ais = fetch.ai(user_query)  # Pass the user query to the fetch.ai function       print(f'---------------------{available_ais}----------------------')       # Access the 'ais' list within 'agents' (assuming fetch.ai returns the correct structure)       agents = available_ais.get('ais', [])       print(f'----------------------------------{agents}------------------------------------')       extracted_data = []       for agent in agents:           name = agent.get('name')  # Extract agent name           address = agent.get('address')           # Append formatted data to extracted_data list           extracted_data.append({               'name': name,               'address': address,           })       # Format the response with indentation for readability       response = jsonify(extracted_data)       response.headers.add('Content-Type', 'application/json; charset=utf-8')       response.headers.add('Access-Control-Allow-Origin', '*')       response.headers.add('Access-Control-Allow-Headers', 'Content-Type')       return response, 200   except Exception as e:       logger.error(f"Error finding agents: {e}")       return jsonify({"error": str(e)}), 500

**Sending Data to Alphavantage Agent**

The /api/send-data endpoint forwards stock price requests to the Alphavantage Agent:

    @app.route('/api/send-data', methods=['POST'])def send_data():   """Send payload to the selected agent based on provided address."""   global agent_response   agent_response = None   try:       # Parse the request payload       data = request.json       payload = data.get('payload')  # Extract the payload dictionary       agent_address = data.get('agentAddress')  # Extract the agent address       # Validate the input data       if not payload or not agent_address:           return jsonify({"error": "Missing payload or agent address"}), 400       logger.info(f"Sending payload to agent: {agent_address}")       logger.info(f"Payload: {payload}")       # Send the payload to the specified agent       send_message_to_agent(           client_identity,  # Frontend client identity           agent_address,    # Agent address where we have to send the data           payload           # Payload containing the data       )       return jsonify({"status": "request_sent", "agent_address": agent_address, "payload": payload})   except Exception as e:       logger.error(f"Error sending data to agent: {e}")       return jsonify({"error": str(e)}), 500

**Webhook Endpoint**

The /webhook route handles requests sent by other agents:

    # app route to get recieve the messages on the agent@app.route('/api/webhook', methods=['POST'])def webhook():    """Handle incoming messages from the dashboard agent."""    global agent_response    try:        # Parse the incoming webhook message        data = request.get_data().decode("utf-8")        logger.info("Received response")        message = parse_message_from_agent(data)        agent_response = message.payload        logger.info(f"Processed response: {agent_response}")        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500

**Retrieving the Agent's Response**

The /api/get-response endpoint retrieves the response (stock price) from the stocks price Agent and saves it :

    @app.route('/api/get-response', methods=['GET'])def get_response():    global agent_response    try:        if agent_response:            response = agent_response            print(f'stock_price : {response}')            agent_response = None  # Clear the response after sending                        keys = list(response.keys())  # Convert dict_keys to a list            first_key = keys[0]           # Get the first key            response_final = response.get(first_key, "")             logger.info(f"Got response for the stock price {response_final}")            return jsonify({"stock_price": response_final})        else:            return jsonify({"error": "No storm response available"}), 404    except Exception as e:        logger.error(f"Error creating markdown file: {e}")        return jsonify({"error": str(e)}), 500

**Running the agent**

    # function to start the flask serverdef start_server():    """Start the Flask server."""    try:        # Load environment variables        load_dotenv()        init_client()        app.run(host="0.0.0.0", port=5002)    except Exception as e:        logger.error(f"Server error: {e}")        raiseif __name__ == "__main__":    start_server()

Steps to run Scripts[â€‹](#steps-to-run-scripts "Direct link to Steps to run Scripts")
------------------------------------------------------------------------------------

Follow these steps to get both the Alphavantage Agent and User Agent scripts up and running.

**1\. Start the Alphavantage Agent**

Ensure the `AV_STOCKPRICE_AI_KEY` and `AGENTVERSE_API_KEY` values are correctly set in the `.env` file.

#### Run the Alphavantage Agent script[â€‹](#run-the-alphavantage-agent-script "Direct link to Run the Alphavantage Agent script")

#### Output[â€‹](#output "Direct link to Output")

    (my_env) abhi@Fetchs-MacBook-Pro Langchain Tool Example % python3 av_agent.pyINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Langchain Alphavantage tool agent registered successfully! * Serving Flask app 'av_agent' * Debug mode: onINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5008INFO:werkzeug:Press CTRL+C to quitINFO:werkzeug: * Restarting with statINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Langchain Alphavantage tool agent registered successfully!

The agent listens on **port 5008** for requests.

**2\. Start the User Agent**

Ensure the `USER_STOCKPRICE_AI_KEY` and `AGENTVERSE_API_KEY` values are correctly set in the `.env` file.

#### Run the User Agent script[â€‹](#run-the-user-agent-script "Direct link to Run the User Agent script")

#### Output[â€‹](#output-1 "Direct link to Output")

    (my_env) abhi@Fetchs-MacBook-Pro Langchain Tool Example % python3 user_agent.pyINFO:__main__:Client agent started with address: agent1qtyqvwlqgd96jzdcd9yj9nem09j48xel2envjdcnvxpx2nfq0rygwxfg3kvINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Client agent registration complete! * Serving Flask app 'user_agent' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5002 * Running on http://192.168.0.106:5002

### Interacting with the Agents[â€‹](#interacting-with-the-agents "Direct link to Interacting with the Agents")

Once both agents are running, use the following endpoints from another terminal:

**1\. Search for Agents**

    curl -X GET "http://localhost:5002/api/search-agents?query=I%20want%20to%20check%20the%20stock%20price"

**Sample Output**

    [{"address":"agent1qwkzy0twyd27egqfrcw8m6vdtsz7hrm9leu7jfezzzlgckywj7shytz5kf6","name":"Alphavantage Stock Price Langchain tool"},{"address":"agent1qtzp03dh92dldvsmr7v0xxvwcfwnpctftjqwdrqrmfszex0dal7pzapsy8d","name":"User agent for Stock Price check"},{"address":"agent1qtnc7ruc63g3n84qczekqc6qwp0e5ayvs3v8e5pae563qrqcf852xlqz2pk","name":"Alphavantage Stock Price Langchain tool"},{"address":"agent1qtyqvwlqgd96jzdcd9yj9nem09j48xel2envjdcnvxpx2nfq0rygwxfg3kv","name":"User agent for Stock Price check"},{"address":"agent1q05k0g4f90zka3m34zj52sdyqtvvvzyry0lc0sdzcdm2tjllkqeruqhue4z","name":"IL Testing user"},{"address":"agent1qtmkpt8lhyx4u2ndzcgr7jkjuuade4ay78yld2ngaghs03z6qve3senwrsa","name":"Penny"},{"address":"agent1qtxx79e689zwjlh7m8dsfuv7lyn45d2xapr4p9w2u9axvzm487tzjfwqzdp","name":"Penny"},{"address":"agent1qvc0em6vrlr80txaz3ksy7tspzaxl564jwqtaana7exehg0hz7gl6zg0wl0","name":"Penny"},{"address":"agent1qw5s6e7mxcd8mp6f2vvunk9rklwnv8ynft43unx77syea4nnskxyj4yk5xq","name":"Penny"},{"address":"agent1qwlyun6slk2gy8uxnnmvy35tuw2ftp44wza6yfgw06ca9axlllzaxgj5jut","name":"Penny"}]

**2\. Send a Stock Price Request**

    curl -X POST "http://localhost:5002/api/send-data" \-H "Content-Type: application/json" \-d '{    "payload": {"request": "AAPL"},    "agentAddress": "agent1qwkzy0twyd27egqfrcw8m6vdtsz7hrm9leu7jfezzzlgckywj7shytz5kf6"}'

**Sample Output**

    {"agent_address":"agent1qwkzy0twyd27egqfrcw8m6vdtsz7hrm9leu7jfezzzlgckywj7shytz5kf6","payload":{"request":"AAPL"},"status":"request_sent"}

**3\. Retrieve the Stock Price Response**

Fetch the stock price response from the User Agent:

    curl -X GET "http://localhost:5002/api/get-response"

**Sample Output**

    {"stock_price":"234.4000"}

Debugging Common Issues[â€‹](#debugging-common-issues "Direct link to Debugging Common Issues")
---------------------------------------------------------------------------------------------

Agent Registration Fails:

a. Check the .env file for correct API keys.

b. Ensure the AGENTVERSE\_API\_KEY is valid.

404 Errors:

a. Verify that both agents are running on their respective ports (5008 and 5002 in this case).

b. Double-check the agentAddress in requests.

You now have an **Alphavantage Agent** integrated with the **Fetch.ai Agentverse** for fetching stock prices via **LangChain**. Feel free to adapt the example to include other APIs, advanced AI features, or additional endpoints.</content>
</page>

<page>
  <title>AI Language Tutor with ASI-1 Mini | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/asione/asi1-mini-language-tutor</url>
  <content>Building an AI Language Tutor uAgent with ASI-1 Mini
----------------------------------------------------

This guide explains how to create a simple uAgent that serves as an AI language tutor using the ASI-1 Mini API. The agent accepts a user query and returns language learning assistanceâ€”such as translation, grammar correction, or pronunciation tipsâ€”based on the provided prompt.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

### Purpose[â€‹](#purpose "Direct link to Purpose")

This project demonstrates how to integrate ASI-1 Mini, Fetch.ai's Web3-native large language model, into a uAgent. The agent, named **AI\_Language\_Tutor**, is designed to:

*   Call the **ASI-1 Mini API** with a prompt tailored for language learning.
*   Provide translation, grammar correction, or pronunciation tips based on user input.
*   Log the response upon agent startup.

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

Before running this project, ensure you have:

*   Python 3.11 installed.
*   uAgents library installed. If not already installed, use:

*   Requests library installed:

*   A valid API key for ASI-1 Mini. Get your API Key [here](https://innovationlab.fetch.ai/resources/docs/1.0.3/asione/asi1-mini-getting-started#how-to-get-an-api-key).

Script Breakdown[â€‹](#script-breakdown "Direct link to Script Breakdown")
------------------------------------------------------------------------

**1\. Importing Required Libraries**

The script begins by importing the necessary modules:

*   **requests:** To perform HTTP requests to the ASI-1 Mini API.
*   **json:** For handling JSON responses and potential JSON decode errors.
*   **uagents:** To create the uAgent and manage events.
*   **Context:** Provides access to the agent's runtime context, including logging.

    import requestsimport jsonfrom uagents import Agent, Context

**2\. Initializing the uAgent**

An instance of the uAgent is created with a name, port, and endpoint.

    agent = Agent(    name="AI_Language_Tutor",    port=8000,  # You can change this to any available port    endpoint="http://localhost:8000/submit")

**3\. Defining the ASI-1 Mini API Helper Function**

The function `get_language_help` constructs a custom prompt and sends a POST request to the ASI-1 Mini API. Based on the user's query and target language, it asks the API to either translate, correct grammar, or offer pronunciation tips.

    def get_language_help(query: str, target_language: str = "Spanish") -> str:    url = "https://api.asi1.ai/v1/chat/completions"    headers = {        'Content-Type': 'application/json',        'Authorization': f'Bearer <Your_ASI1_Mini_API_Key>'  # Replace with your API Key    }    prompt = f"""You are an AI language tutor. Help the user with their language learning request:    - If they ask for a **translation**, provide it in {target_language}.    - If they provide a sentence, **correct any grammar mistakes**.    - If they ask for pronunciation tips, explain how to say it.    User request: "{query}"    """    payload = {        "model": "asi1-mini",        "messages": [{"role": "user", "content": prompt}],        "temperature": 0.7,        "max_tokens": 0    }    try:        response = requests.post(url, headers=headers, json=payload)        response.raise_for_status()        return response.json()['choices'][0]['message']['content']    except requests.exceptions.RequestException as e:        return f"API Request Error: {str(e)}"    except json.JSONDecodeError:        return "API Error: Unable to parse JSON response"

**4\. Startup Event Handler**

The agent registers an `on_event("startup")` handler. When the agent starts, it:

*   Logs a sample query.
*   Calls the `get_language_help` function with an example query.
*   Logs the returned response from the ASI-1 Mini API.

    @agent.on_event("startup")async def language_tutor_demo(ctx: Context):    query = "How do you say 'Good morning' in French?"  # Example query    ctx.logger.info(f"User query: {query}")    response = get_language_help(query, target_language="French")    ctx.logger.info(f"ğŸŒ AI Tutor Response: {response}")

**5\. Running the Agent**

The script concludes by running the agent, which starts the server and registers the startup handler.

    if __name__ == "__main__":    agent.run()

Running the Agent[â€‹](#running-the-agent "Direct link to Running the Agent")
---------------------------------------------------------------------------

**1\. Activate Your Virtual Environment:**

    source venv/bin/activate   # On Windows: venv\Scripts\activate

**2\. Run the Script:**

    python my_language_tutor.py

**3\. Expected Output:**

Upon startup, you should see log messages similar to:

    INFO: [AI_Language_Tutor]: Starting agent with address: agent1...INFO: [AI_Language_Tutor]: User query: How do you say 'Good morning' in French?INFO: [AI_Language_Tutor]: ğŸŒ AI Tutor Response: Bonjour (Good morning/Good day).

GitHub Repository[â€‹](#github-repository "Direct link to GitHub Repository")
---------------------------------------------------------------------------

For the complete code and additional examples, visit the [ASI-1-Mini-simple-Examples](https://github.com/abhifetch/ASI-1-Mini-simple-Examples/blob/main/language_tutor.py) repository.

note

**Note:** You can learn more about ASI-1 Mini APIs [**here**](https://docs.asi1.ai/docs/).</content>
</page>

<page>
  <title>Creating ASI1 Compatible uAgent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/chat-protocol/asi1-compatible-uagents</url>
  <content>This guide demonstrates how to make your agents accessible via ASI1 LLM by integrating the chat protocol. We'll use a Football Team Agent example to show how the chat protocol enables seamless communication between agents and the LLM.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

In this example, you'll learn how to build a uAgent compatible with Fetch.ai's ASI1 Large Language Model (LLM). Using a Football Team Agent as an example, the guide shows how you can enable your agent to understand and respond to natural language queries.

Message Flow[â€‹](#message-flow "Direct link to Message Flow")
------------------------------------------------------------

The communication flow between ASI1 LLM, the Football Team Agent, and OpenAI Agent follows this sequence:

1.  **Query Initiation (1.1)**
    
    *   ASI1 LLM sends a natural language query (e.g., "Give me the list of players in Manchester United Football Team") as a `ChatMessage` to the Football Team Agent on the `ChatMessage handler`.
2.  **Parameter Extraction (2, 3)**
    
    *   The Football Team Agent forwards the query to OpenAI Agent for parameter extraction
    *   OpenAI Agent processes the natural language and extracts structured parameters (e.g., team\_name="Manchester United")
    *   The parameters are returned in a Pydantic Model format as `StructuredOutputResponse` on the `StructuredOutputResponse handler`.
3.  **Team Data Processing (4, 5)**
    
    *   The Football Team Agent calls the `get_team_info` function with the extracted parameters
    *   The function returns the team details.
4.  **Football Team Agent Response (6.1)**
    
    *   The Football Team Agent sends the formatted response back as a `ChatMessage` to ASI1 LLM.
5.  **Message Acknowledgements (1.2, 6.2)**
    
    *   Each message exchanged using the chat protocol is automatically acknowledged by the receiving agent using `ChatAcknowledgement`.

Here's a visual representation of the flow:

Implementation[â€‹](#implementation "Direct link to Implementation")
------------------------------------------------------------------

In this example we will create an agent and its associated files on [Agentverse](https://agentverse.ai/) that communicate using the chat protocol with the ASI1 LLM. Refer to the [Hosted Agents](https://innovationlab.fetch.ai/resources/docs/agent-creation/uagent-creation#hosted-agents) section to understand the detailed steps for agent creation on Agentverse.

Create a new agent named "FootballTeamAgent" on Agentverse and create the following files:

    agent.py        # Main agent file football_team.py   # Football team service implementation and API integrationchat_proto.py   # Chat protocol implementation for enabling text based communication 

To create a new file on Agentverse:

1.  Click on the New File icon

2.  Assign a name to the File

3.  Directory Structure

### 1\. Football Team Function and Data Models[â€‹](#1-football-team-function-and-data-models "Direct link to 1. Football Team Function and Data Models")

Let's start by defining our data models and the function to retrieve the list of players in a football team. These models will define how we request team information and receive responses. We'll use the AllSports API to fetch team and player information. You can obtain your API key by signing up at [AllSports API](https://allsportsapi.com/), which provides comprehensive sports data feeds including football (soccer) team and player information.

To implement the football team service add the following chat protocol in the `football_team.py` file created on Agentverse:

football\_team.py

    import requestsfrom uagents import Model, FieldAPI_KEY = "YOUR_ALLSPORTS_API_KEY"BASE_URL = "https://apiv2.allsportsapi.com/football/"class FootballTeamRequest(Model):    team_name: strclass FootballTeamResponse(Model):    results: strasync def get_team_info(team_name: str) -> str:    """    Fetch team information from AllSportsAPI and return as plain text    """    try:        params = {            "met": "Teams",            "teamName": team_name,            "APIkey": API_KEY        }        response = requests.get(BASE_URL, params=params)        data = response.json()        if data.get("success") == 1 and data.get("result"):            team_info = data["result"][0]            result = f"\nTeam Name: {team_info['team_name']}\n"            result += f"Team Logo: {team_info['team_logo']}\n\n"            result += "Players:\n"                        for player in team_info.get("players", []):                result += f"- Name: {player['player_name']}\n"                result += f"  Type: {player['player_type']}\n"                result += f"  Image: {player['player_image']}\n\n"                        return result        else:            return "Team not found or invalid API key."                except Exception as e:        return f"Error fetching team information: {str(e)}"

### 2\. Chat Protocol Integration[â€‹](#2-chat-protocol-integration "Direct link to 2. Chat Protocol Integration")

The `chat_proto.py` file is essential for enabling natural language communication between your agent and ASI1 LLM.

#### LLM Integration[â€‹](#llm-integration "Direct link to LLM Integration")

To extract the important parameters from a user query passed by the LLM, you can use any of the following LLMs that support structured output:

*   OpenAI Agent: `agent1q0h70caed8ax769shpemapzkyk65uscw4xwk6dc4t3emvp5jdcvqs9xs32y`
*   Claude.ai Agent: `agent1qvk7q2av3e2y5gf5s90nfzkc8a48q3wdqeevwrtgqfdl0k78rspd6f2l4dx`

> **Note**: To ensure fair usage, each agent is limited to 6 requests per hour. Please implement appropriate rate limiting in your application.

#### Message Flow[â€‹](#message-flow-1 "Direct link to Message Flow")

1.  When a user sends a query (e.g., "Show me Manchester United players"):
    
    *   The `ChatMessage` handler receives the message
    *   Acknowledges receipt to the sender
    *   Forwards the query to the chosen LLM for processing
2.  The LLM processes the query and returns a `StructuredOutputResponse`:
    
    *   Extracts relevant parameters (e.g., team\_name="Manchester United")
    *   Returns data in the format specified by your agent's schema
3.  Your agent processes the structured data:
    
    *   Calls appropriate functions (e.g., `get_team_info`)
    *   Formats the response and sends it back.

#### Customizing the Handlers[â€‹](#customizing-the-handlers "Direct link to Customizing the Handlers")

When implementing a new agent for a different use case, you'll need to:

*   Update the schema in `StructuredOutputPrompt` to match your agent's data model
*   Modify the response handling in `handle_structured_output_response` function for your use case.

To enable natural language communication with your agent add the following chat protocol in the `chat_proto.py` file created on Agentverse:

chat\_proto.py

    from datetime import datetimefrom uuid import uuid4from typing import Anyfrom uagents import Context, Model, Protocol#Import the necessary components of the chat protocolfrom uagents_core.contrib.protocols.chat import (    ChatAcknowledgement,    ChatMessage,    EndSessionContent,    StartSessionContent,    TextContent,    chat_protocol_spec,)from football_team import get_team_info, FootballTeamRequest#Replace the AI Agent Address with anyone of the following LLMs as they support StructuredOutput required for the processing of this agent. AI_AGENT_ADDRESS = 'agent1q0h70caed8ax769shpemapzkyk65uscw4xwk6dc4t3emvp5jdcvqs9xs32y'if not AI_AGENT_ADDRESS:    raise ValueError("AI_AGENT_ADDRESS not set")def create_text_chat(text: str, end_session: bool = True) -> ChatMessage:    content = [TextContent(type="text", text=text)]    if end_session:        content.append(EndSessionContent(type="end-session"))    return ChatMessage(        timestamp=datetime.utcnow(),        msg_id=uuid4(),        content=content,    )chat_proto = Protocol(spec=chat_protocol_spec)struct_output_client_proto = Protocol(    name="StructuredOutputClientProtocol", version="0.1.0")class StructuredOutputPrompt(Model):    prompt: str    output_schema: dict[str, Any]class StructuredOutputResponse(Model):    output: dict[str, Any]@chat_proto.on_message(ChatMessage)async def handle_message(ctx: Context, sender: str, msg: ChatMessage):    ctx.logger.info(f"Got a message from {sender}: {msg.content[0].text}")    ctx.storage.set(str(ctx.session), sender)    await ctx.send(        sender,        ChatAcknowledgement(timestamp=datetime.utcnow(), acknowledged_msg_id=msg.msg_id),    )    for item in msg.content:        if isinstance(item, StartSessionContent):            ctx.logger.info(f"Got a start session message from {sender}")            continue        elif isinstance(item, TextContent):            ctx.logger.info(f"Got a message from {sender}: {item.text}")            ctx.storage.set(str(ctx.session), sender)            await ctx.send(                AI_AGENT_ADDRESS,                StructuredOutputPrompt(                    prompt=item.text, output_schema=FootballTeamRequest.schema()                ),            )        else:            ctx.logger.info(f"Got unexpected content from {sender}")@chat_proto.on_message(ChatAcknowledgement)async def handle_ack(ctx: Context, sender: str, msg: ChatAcknowledgement):    ctx.logger.info(        f"Got an acknowledgement from {sender} for {msg.acknowledged_msg_id}"    )@struct_output_client_proto.on_message(StructuredOutputResponse)async def handle_structured_output_response(    ctx: Context, sender: str, msg: StructuredOutputResponse):    session_sender = ctx.storage.get(str(ctx.session))    if session_sender is None:        ctx.logger.error(            "Discarding message because no session sender found in storage"        )        return    if "<UNKNOWN>" in str(msg.output):        await ctx.send(            session_sender,            create_text_chat(                "Sorry, I couldn't process your location request. Please try again later."            ),        )        return    prompt = FootballTeamRequest.parse_obj(msg.output)    try:        team_info = await get_team_info(prompt.team_name)    except Exception as err:        ctx.logger.error(err)        await ctx.send(            session_sender,            create_text_chat(                "Sorry, I couldn't process your request. Please try again later."            ),        )        return    if "error" in team_info:        await ctx.send(session_sender, create_text_chat(str(team_info["error"])))        return    chat_message = create_text_chat(team_info)    await ctx.send(session_sender, chat_message)

### 3\. Football Team Agent Setup[â€‹](#3-football-team-agent-setup "Direct link to 3. Football Team Agent Setup")

The `agent.py` file is the core of your application. Think of it as the main control center that:

*   Sets up your agent
*   Handles incoming requests
*   Manages rate limiting
*   Monitors the agent's health

Let's break down each component:

#### Basic Setup[â€‹](#basic-setup "Direct link to Basic Setup")

    from uagents import Agent, Context, Modelfrom uagents.experimental.quota import QuotaProtocol, RateLimitagent = Agent()  # Create a new agent instance

Import the necessary packages and initialise your agent.

#### Rate Limiting Setup[â€‹](#rate-limiting-setup "Direct link to Rate Limiting Setup")

    proto = QuotaProtocol(    storage_reference=agent.storage,    name="Football-Team-Protocol",    version="0.1.0",    default_rate_limit=RateLimit(window_size_minutes=60, max_requests=30),)

To ensure fair usage and prevent API abuse, we recommend implementing the QuotaProtocol. While optional, this protocol helps manage service usage by limiting requests. In this example, we've set a limit of 30 requests per hour per user, but you can adjust this threshold based on your agent's functionality. This helps maintain service reliability and protects your agent from excessive requests.

#### Request Handler[â€‹](#request-handler "Direct link to Request Handler")

    @proto.on_message(    FootballTeamRequest, replies={FootballTeamResponse, ErrorMessage})async def handle_request(ctx: Context, sender: str, msg: FootballTeamRequest):    ctx.logger.info("Received team info request")    try:        results = await get_team_info(msg.team_name)        await ctx.send(sender, FootballTeamResponse(results=results))    except Exception as err:        await ctx.send(sender, ErrorMessage(error=str(err)))

This message handler allows your agent to receive direct requests from other agents in a structured format. While we're using it with ASI1 LLM in this example, it's versatile enough to work with agents that don't have the chat protocol enabled. This makes it particularly useful in multi-agent systems, where other agents can directly request information directly.

#### Health Monitoring[â€‹](#health-monitoring "Direct link to Health Monitoring")

    ### Health check related codedef agent_is_healthy() -> bool:    """    Implement the actual health check logic here.    For example, check if the agent can connect to the AllSports API.    """    try:        import asyncio        asyncio.run(get_team_info("Manchester United"))        return True    except Exception:        return Falseclass HealthCheck(Model):    passclass HealthStatus(str, Enum):    HEALTHY = "healthy"    UNHEALTHY = "unhealthy"class AgentHealth(Model):    agent_name: str    status: HealthStatushealth_protocol = QuotaProtocol(    storage_reference=agent.storage, name="HealthProtocol", version="0.1.0")@health_protocol.on_message(HealthCheck, replies={AgentHealth})async def handle_health_check(ctx: Context, sender: str, msg: HealthCheck):    status = HealthStatus.UNHEALTHY    try:        if agent_is_healthy():            status = HealthStatus.HEALTHY    except Exception as err:        ctx.logger.error(err)    finally:        await ctx.send(sender, AgentHealth(agent_name="football_agent", status=status))

The HealthProtocol, while optional, provides a periodic health check system to monitor your agent's performance and reliability. It tests the agent's ability to fetch information about a team and verifies that all components are working correctly. This monitoring helps quickly identify and address any issues that might affect your agent's service, ensuring reliable operation for your users.

#### Protocol Registration[â€‹](#protocol-registration "Direct link to Protocol Registration")

    agent.include(proto, publish_manifest=True)agent.include(health_protocol, publish_manifest=True)agent.include(chat_proto, publish_manifest=True)agent.include(struct_output_client_proto, publish_manifest=True)

This registers all the necessary protocols with your agent:

*   Main protocol for handling team requests
*   Health monitoring protocol
*   Chat protocol for natural language communication
*   Structured output protocol for formatted responses

Here's the complete implementation:

agent.py

    import osfrom enum import Enumfrom uagents import Agent, Context, Modelfrom uagents.experimental.quota import QuotaProtocol, RateLimitfrom uagents_core.models import ErrorMessagefrom chat_proto import chat_proto, struct_output_client_protofrom football_team import get_team_info, FootballTeamRequest, FootballTeamResponseagent = Agent()proto = QuotaProtocol(    storage_reference=agent.storage,    name="Football-Team-Protocol",    version="0.1.0",    default_rate_limit=RateLimit(window_size_minutes=60, max_requests=30),)@proto.on_message(    FootballTeamRequest, replies={FootballTeamResponse, ErrorMessage})async def handle_request(ctx: Context, sender: str, msg: FootballTeamRequest):    ctx.logger.info("Received team info request")    try:        results = await get_team_info(msg.team_name)        ctx.logger.info(f'printing results in function {results}')        ctx.logger.info("Successfully fetched team information")        await ctx.send(sender, FootballTeamResponse(results=results))    except Exception as err:        ctx.logger.error(err)        await ctx.send(sender, ErrorMessage(error=str(err)))agent.include(proto, publish_manifest=True)### Health check related codedef agent_is_healthy() -> bool:    """    Implement the actual health check logic here.    For example, check if the agent can connect to the AllSports API.    """    try:        import asyncio        asyncio.run(get_team_info("Manchester United"))        return True    except Exception:        return Falseclass HealthCheck(Model):    passclass HealthStatus(str, Enum):    HEALTHY = "healthy"    UNHEALTHY = "unhealthy"class AgentHealth(Model):    agent_name: str    status: HealthStatushealth_protocol = QuotaProtocol(    storage_reference=agent.storage, name="HealthProtocol", version="0.1.0")@health_protocol.on_message(HealthCheck, replies={AgentHealth})async def handle_health_check(ctx: Context, sender: str, msg: HealthCheck):    status = HealthStatus.UNHEALTHY    try:        if agent_is_healthy():            status = HealthStatus.HEALTHY    except Exception as err:        ctx.logger.error(err)    finally:        await ctx.send(sender, AgentHealth(agent_name="football_agent", status=status))agent.include(health_protocol, publish_manifest=True)agent.include(chat_proto, publish_manifest=True)agent.include(struct_output_client_proto, publish_manifest=True)if __name__ == "__main__":    agent.run() 

Adding a README to your Agent[â€‹](#adding-a-readme-to-your-agent "Direct link to Adding a README to your Agent")
---------------------------------------------------------------------------------------------------------------

1.  Go to the Overview section in the Editor.
    
2.  Click on Edit and add a good description for your Agent so that it can be easily searchable by the ASI1 LLM. Please refer the [Importance of Good Readme](https://innovationlab.fetch.ai/resources/docs/agentverse/searching#importance-of-good-readme) section for more details.
    
3.  Make sure the Agent has the right `AgentChatProtocol`.
    

Query your Agent from ASI1 LLM[â€‹](#query-your-agent-from-asi1-llm "Direct link to Query your Agent from ASI1 LLM")
------------------------------------------------------------------------------------------------------------------

1.  Login to the [ASI1 LLM](https://asi1.ai/), either using your Google Account or the ASI1 Wallet and Start a New Chat.
    
2.  Toggle the "Agents" switch to enable ASI1 to connect with Agents on Agentverse.
    

3.  Type in a query to ask your Agent for instance 'I want to get the player details for the Manchester City Football Team'.

> **Note**: The ASI1 LLM may not always select your agent for answering the query as it is designed to pick the best agent for a task based on a number of parameters.</content>
</page>

<page>
  <title>CrewAI Adapter Example | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/adapters/crewai-adapter-example</url>
  <content>CrewAI Adapter for uAgents
--------------------------

This example demonstrates how to integrate a **CrewAI multi-agent system** with the **uAgents ecosystem** using the uAgents Adapter package. CrewAI allows you to create collaborative teams of AI agents working together to accomplish complex tasks.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

The CrewAI adapter enables:

*   Creating specialized agent teams with distinct roles and responsibilities
*   Orchestrating complex workflows between different AI agents
*   Exposing CrewAI teams as uAgents for seamless communication with the broader agent ecosystem
*   Deploying CrewAI applications to the Agentverse network

Trip Planner Example[â€‹](#trip-planner-example "Direct link to Trip Planner Example")
------------------------------------------------------------------------------------

Let's look at a real-world example of a trip planning system with multiple specialized agents working together to create a complete travel itinerary. We'll compare the standard CrewAI implementation with the uAgents-integrated version.

### Standard CrewAI Implementation[â€‹](#standard-crewai-implementation "Direct link to Standard CrewAI Implementation")

First, let's look at how a standard CrewAI system is implemented without uAgents integration:

    # Standard main.pyfrom textwrap import dedentfrom crewai import Crewfrom dotenv import load_dotenvfrom trip_agents import TripAgentsfrom trip_tasks import TripTasksload_dotenv()class TripCrew:    def __init__(self, origin, cities, date_range, interests):        self.cities = cities        self.origin = origin        self.interests = interests        self.date_range = date_range    def run(self):        agents = TripAgents()        tasks = TripTasks()        city_selector_agent = agents.city_selection_agent()        local_expert_agent = agents.local_expert()        travel_concierge_agent = agents.travel_concierge()        identify_task = tasks.identify_task(            city_selector_agent,            self.origin,            self.cities,            self.interests,            self.date_range,        )        gather_task = tasks.gather_task(local_expert_agent, self.origin, self.interests, self.date_range)        plan_task = tasks.plan_task(travel_concierge_agent, self.origin, self.interests, self.date_range)        crew = Crew(            agents=[city_selector_agent, local_expert_agent, travel_concierge_agent],            tasks=[identify_task, gather_task, plan_task],            verbose=True,        )        result = crew.kickoff()        return resultif __name__ == "__main__":    print("## Welcome to Trip Planner Crew")    print("-------------------------------")    location = input(        dedent("""      From where will you be traveling from?    """)    )    cities = input(        dedent("""      What are the cities options you are interested in visiting?    """)    )    date_range = input(        dedent("""      What is the date range you are interested in traveling?    """)    )    interests = input(        dedent("""      What are some of your high level interests and hobbies?    """)    )    trip_crew = TripCrew(location, cities, date_range, interests)    result = trip_crew.run()    print("\n\n########################")    print("## Here is you Trip Plan")    print("########################\n")    print(result)

### uAgents Integration[â€‹](#uagents-integration "Direct link to uAgents Integration")

Now, let's see how we can integrate this same CrewAI system with uAgents to enable network communication:

    #!/usr/bin/env python3"""Trip Planner script using CrewAI adapter for uAgents."""import osfrom crewai import Crewfrom dotenv import load_dotenvfrom uagents_adapter import CrewaiRegisterToolfrom trip_agents import TripAgentsfrom trip_tasks import TripTasksclass TripCrew:    def __init__(self, origin, cities, date_range, interests):        self.cities = cities        self.origin = origin        self.interests = interests        self.date_range = date_range    def run(self):        agents = TripAgents()        tasks = TripTasks()        city_selector_agent = agents.city_selection_agent()        local_expert_agent = agents.local_expert()        travel_concierge_agent = agents.travel_concierge()        identify_task = tasks.identify_task(            city_selector_agent,            self.origin,            self.cities,            self.interests,            self.date_range,        )        gather_task = tasks.gather_task(local_expert_agent, self.origin, self.interests, self.date_range)        plan_task = tasks.plan_task(travel_concierge_agent, self.origin, self.interests, self.date_range)        crew = Crew(            agents=[city_selector_agent, local_expert_agent, travel_concierge_agent],            tasks=[identify_task, gather_task, plan_task],            verbose=True,        )        result = crew.kickoff()        return result    def kickoff(self, inputs=None):        """        Compatibility method for uAgents integration.        Accepts a dictionary of inputs and calls run() with them.        """        if inputs:            self.origin = inputs.get("origin", self.origin)            self.cities = inputs.get("cities", self.cities)            self.date_range = inputs.get("date_range", self.date_range)            self.interests = inputs.get("interests", self.interests)        return self.run()def main():    """Main function to demonstrate Trip Planner with CrewAI adapter."""    # Load API key from environment    load_dotenv()    api_key = os.getenv("AGENTVERSE_API_KEY")    openai_api_key = os.getenv("OPENAI_API_KEY")    if not api_key:        print("Error: AGENTVERSE_API_KEY not found in environment")        return    if not openai_api_key:        print("Error: OPENAI_API_KEY not found in environment")        return    # Set OpenAI API key in environment    os.environ["OPENAI_API_KEY"] = openai_api_key    # Create an instance of TripCrew with default empty values    trip_crew = TripCrew("", "", "", "")    # Create tool for registering the crew with Agentverse    register_tool = CrewaiRegisterTool()    # Define parameters schema for the trip planner    query_params = {        "origin": {"type": "str", "required": True},        "cities": {"type": "str", "required": True},        "date_range": {"type": "str", "required": True},        "interests": {"type": "str", "required": True},    }    # Register the crew with parameter schema    result = register_tool.run(        tool_input={            "crew_obj": trip_crew,            "name": "Trip Planner Crew AI Agent adapters",            "port": 8080,            "description": "A CrewAI agent that helps plan trips based on preferences",            "api_token": api_key,            "mailbox": True,            "query_params": query_params,            "example_query": "Plan a trip from New York to Paris in June, I'm interested in art and history other than museums.",        }    )    # Get the agent address from the result    if isinstance(result, dict) and "address" in result:        result["address"]    print(f"\nCrewAI agent registration result: {result}")    # Keep the program running    try:        while True:            import time            time.sleep(1)    except KeyboardInterrupt:        print("\nExiting...")if __name__ == "__main__":    main()

Key Differences in uAgents Integration[â€‹](#key-differences-in-uagents-integration "Direct link to Key Differences in uAgents Integration")
------------------------------------------------------------------------------------------------------------------------------------------

When integrating a CrewAI system with uAgents, there are several important differences:

1.  **CrewaiRegisterTool**:
    
    *   Uses the specialized `CrewaiRegisterTool` instead of the generic `UAgentRegisterTool`.
    *   This tool is specifically designed to handle CrewAI's collaborative agent structure.
2.  **Kickoff Method**:
    
    *   The `TripCrew` class has an additional `kickoff` method that serves as an adapter between uAgents messages and the CrewAI system.
    *   It extracts parameters from the input dictionary and passes them to the actual execution method.
3.  **Parameter Schema**:
    
    *   A `query_params` schema is defined to validate and structure inputs to the CrewAI system.
    *   This allows for better error handling and client guidance when using the agent.
4.  **Example Query**:
    
    *   An example query is provided to help users understand the expected input format.
    *   This improves usability when interacting with the agent through chat protocols.

Specialized Agents in the Trip Planner[â€‹](#specialized-agents-in-the-trip-planner "Direct link to Specialized Agents in the Trip Planner")
------------------------------------------------------------------------------------------------------------------------------------------

The trip planning system uses three specialized agents, defined in `trip_agents.py`:

1.  **City Selection Agent**: Analyzes client preferences to select the optimal city to visit
2.  **Local Expert**: Identifies authentic local experiences and hidden gems
3.  **Travel Concierge**: Creates detailed itineraries and plans logistics

Each agent is assigned specific tasks through the `trip_tasks.py` file:

1.  **Identify Task**: Determines the best city based on client preferences
2.  **Gather Task**: Collects detailed information about activities and attractions
3.  **Plan Task**: Creates a comprehensive itinerary with transportation details

Interacting with the Trip Planner[â€‹](#interacting-with-the-trip-planner "Direct link to Interacting with the Trip Planner")
---------------------------------------------------------------------------------------------------------------------------

Once registered as a uAgent, you can interact with the CrewAI trip planner using any uAgent client:

    from datetime import datetime, timezonefrom uuid import uuid4from uagents import Agent, Protocol, Context#import the necessary components from the chat protocolfrom uagents_core.contrib.protocols.chat import (    ChatAcknowledgement,    ChatMessage,    TextContent,    chat_protocol_spec,)# Initialise agent2agent2 = Agent(name="client_agent",               port = 8082,               mailbox=True,               seed="client agent testing seed"               )# Initialize the chat protocolchat_proto = Protocol(spec=chat_protocol_spec)langgraph_agent_address = "agent1q0zyxrneyaury3f5c7aj67hfa5w65cykzplxkst5f5mnyf4y3em3kplxn4t"# Startup Handler - Print agent details@agent2.on_event("startup")async def startup_handler(ctx: Context):    # Print agent details    ctx.logger.info(f"My name is {ctx.agent.name} and my address is {ctx.agent.address}")    # Send initial message to agent2    initial_message = ChatMessage(        timestamp=datetime.now(timezone.utc),        msg_id=uuid4(),        content=[TextContent(type="text", text="Plan a trip for me from london to paris starting on 22nd of April 2025 and I am interested in a mountains beaches and history")]    )    await ctx.send(langgraph_agent_address, initial_message)# Message Handler - Process received messages and send acknowledgements@chat_proto.on_message(ChatMessage)async def handle_message(ctx: Context, sender: str, msg: ChatMessage):    for item in msg.content:        if isinstance(item, TextContent):            # Log received message            ctx.logger.info(f"Received message from {sender}: {item.text}")                        # Send acknowledgment            ack = ChatAcknowledgement(                timestamp=datetime.now(timezone.utc),                acknowledged_msg_id=msg.msg_id            )            await ctx.send(sender, ack)            # Acknowledgement Handler - Process received acknowledgements@chat_proto.on_message(ChatAcknowledgement)async def handle_acknowledgement(ctx: Context, sender: str, msg: ChatAcknowledgement):    ctx.logger.info(f"Received acknowledgement from {sender} for message: {msg.acknowledged_msg_id}")# Include the protocol in the agent to enable the chat functionality# This allows the agent to send/receive messages and handle acknowledgements using the chat protocolagent2.include(chat_proto, publish_manifest=True)if __name__ == '__main__':    agent2.run()

Benefits of the uAgents Integration[â€‹](#benefits-of-the-uagents-integration "Direct link to Benefits of the uAgents Integration")
---------------------------------------------------------------------------------------------------------------------------------

Integrating CrewAI with uAgents provides several significant advantages:

*   **Network Communication**: Enables remote access to your CrewAI system over networks
*   **Structured Inputs**: Validates inputs through a defined parameter schema
*   **Persistent Mailbox**: Allows asynchronous communication with message storage
*   **Agentverse Integration**: Makes your CrewAI system discoverable in the agent ecosystem
*   **NL Processing**: Optional AI agent integration for processing natural language queries

Getting Started[â€‹](#getting-started "Direct link to Getting Started")
---------------------------------------------------------------------

1.  Clone the [Trip Planner repository](https://github.com/abhifetch/crewai-example/tree/main/trip_planner)
    
2.  Install dependencies:
    
        pip install uagents==0.22.3 "crewai[tools]"==0.105.0 uagents-adapter==0.2.1 python-dotenv==1.0.0 langchain_openai==0.2.13
    
    Or use the provided requirements.txt:
    
        pip install -r requirements.txt
    
3.  Set up your environment variables:
    
        OPENAI_API_KEY=your_openai_keyAGENTVERSE_API_KEY=your_agentverse_keyAGENT_SEED=your_agent_seed_phrase
    
4.  Run the CrewAI trip planner with uAgents adapter:
    
        cd crewai-example/trip_plannerpython main_uagents.py
    
5.  In a separate terminal, run a client agent to interact with it:
    
        cd crewai-examplepython client_agent.py
    

Expected Outputs[â€‹](#expected-outputs "Direct link to Expected Outputs")
------------------------------------------------------------------------

When running the examples, you should expect to see outputs similar to these:

### Standard CrewAI (`main.py`)[â€‹](#standard-crewai-mainpy "Direct link to standard-crewai-mainpy")

    ## Welcome to Trip Planner Crew-------------------------------From where will you be traveling from?> New YorkWhat are the cities options you are interested in visiting?> Paris, Rome, BarcelonaWhat is the date range you are interested in traveling?> June 10-20, 2023What are some of your high level interests and hobbies?> Food, art, architecture, and history[City Selection Specialist] I'll analyze which city would be the best fit based on the traveler's preferences...Working on: Analyze the traveler's preferences and determine which city from the options would be the best fit...[... search and reasoning details ...]########################## Here is you Trip Plan######################### PARIS: 3-DAY FOOD & ART JOURNEY*A curated itinerary for experiencing the best of Parisian cuisine and artistic treasures*## RECOMMENDED ACCOMMODATIONSLe Marais district or Saint-Germain-des-PrÃ©s would be ideal locations, offering central positioning with charming atmosphere and proximity to key attractions.[... detailed itinerary continues ...]

### uAgents Integration (`main_uagents.py`)[â€‹](#uagents-integration-main_uagentspy "Direct link to uagents-integration-main_uagentspy")

First terminal:

    (venv) abhi@Fetchs-MacBook-Pro test examples % python3 trip_planner/main_uagents.pyINFO:     [Trip Planner Crew AI Agent adapters]: Starting agent with address: agent1q2sgs58jzw70e8vvsrlx8k3yukdqc9gwkhp8p7q6tslcxhy0eqtxyq4fv07INFO:     [Trip Planner Crew AI Agent adapters]: Agent 'Trip Planner Crew AI Agent adapters' started with address: agent1q2sgs58jzw70e8vvsrlx8k3yukdqc9gwkhp8p7q6tslcxhy0eqtxyq4fv07INFO:     [Trip Planner Crew AI Agent adapters]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8080&address=agent1q2sgs58jzw70e8vvsrlx8k3yukdqc9gwkhp8p7q6tslcxhy0eqtxyq4fv07INFO:     [Trip Planner Crew AI Agent adapters]: Starting server on http://0.0.0.0:8080 (Press CTRL+C to quit)INFO:     [Trip Planner Crew AI Agent adapters]: Starting mailbox client for https://agentverse.aiINFO:     [Trip Planner Crew AI Agent adapters]: Mailbox access token acquiredConnecting agent 'Trip Planner Crew AI Agent adapters' to Agentverse...INFO:     [mailbox]: Successfully registered as mailbox agent in AgentverseSuccessfully connected agent 'Trip Planner Crew AI Agent adapters' to AgentverseUpdating agent 'Trip Planner Crew AI Agent adapters' README on Agentverse...Successfully updated agent 'Trip Planner Crew AI Agent adapters' README on AgentverseCrewAI agent registration result: Agent 'Trip Planner Crew AI Agent adapters' registered with address: agent1q2sgs58jzw70e8vvsrlx8k3yukdqc9gwkhp8p7q6tslcxhy0eqtxyq4fv07 with mailbox (Parameters: origin, cities, date_range, interests)INFO:     [mailbox]: Successfully registered as mailbox agent in AgentverseINFO:     [Trip Planner Crew AI Agent adapters]: Got a message from agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49INFO:     [Trip Planner Crew AI Agent adapters]: Received message model digest: timestamp=datetime.datetime(2025, 4, 21, 10, 13, 39, 989489, tzinfo=datetime.timezone.utc) msg_id=UUID('7930acf1-b16e-4b20-896b-7d801763eaa6') content=[TextContent(type='text', text='Plan a trip for me from london to paris starting on 22nd of April 2025 and I am interested in a mountains beaches and history')]INFO:     [Trip Planner Crew AI Agent adapters]: Got a text message from agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49: Plan a trip for me from london to paris starting on 22nd of April 2025 and I am interested in a mountains beaches and historyINFO:     [Trip Planner Crew AI Agent adapters]: Using crew object: <__main__.TripCrew object at 0x12c1f79d0>INFO:     [Trip Planner Crew AI Agent adapters]: Extracting parameters using keys: ['origin', 'cities', 'date_range', 'interests']INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"INFO:     [Trip Planner Crew AI Agent adapters]: Extracted parameters: {'origin': 'london', 'cities': 'paris', 'date_range': '22nd of April 2025', 'interests': 'mountains beaches and history'}INFO:     [Trip Planner Crew AI Agent adapters]: Running crew with extracted parametersâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Crew Execution Started â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®â”‚                                                                                                                                      â”‚â”‚  Crew Execution Started                                                                                                              â”‚â”‚  Name: crew                                                                                                                          â”‚â”‚  ID: 1462f3ae-5ce4-4ea3-b1af-5639aac04dd2                                                                                            â”‚â”‚                                                                                                                                      â”‚â”‚                                                                                                                                      â”‚â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ğŸš€ Crew: crewâ””â”€â”€ ğŸ“‹ Task: c181e31b-6b7f-4471-ab8f-fa5f06078365       Status: Executing Task...[... crew execution continues ...]

### Client Agent (`client_agent.py`)[â€‹](#client-agent-client_agentpy "Direct link to client-agent-client_agentpy")

Second terminal:

    (venv) abhi@Fetchs-MacBook-Pro crewai-example % python3 trip_planner/client_agent.py INFO:     [client_agent]: Starting agent with address: agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49INFO:     [client_agent]: My name is client_agent and my address is agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49INFO:     [client_agent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8082&address=agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49INFO:     [client_agent]: Starting server on http://0.0.0.0:8082 (Press CTRL+C to quit)INFO:     [client_agent]: Starting mailbox client for https://agentverse.aiINFO:     [client_agent]: Manifest published successfully: AgentChatProtocolINFO:     [client_agent]: Mailbox access token acquiredINFO:     [client_agent]: Received acknowledgement from agent1q2sgs58jzw70e8vvsrlx8k3yukdqc9gwkhp8p7q6tslcxhy0eqtxyq4fv07 for message: 7930acf1-b16e-4b20-896b-7d801763eaa6INFO:     [uagents.registration]: Registration on Almanac API successfulINFO:     [uagents.registration]: Almanac contract registration is up to date![... detailed itinerary continues ...]

This example demonstrates how uAgents adapters can bring collaborative AI agent systems into a networked environment, making complex workflows accessible through standardized messaging protocols.</content>
</page>

<page>
  <title>Stripe Payment Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/integrations/stripe-integration</url>
  <content>Build a Stripe Payment Agent with ASI:One and uAgents
-----------------------------------------------------

This guide demonstrates how to create an intelligent Stripe payment agent using ASI:One for natural language processing and the official Stripe API, deployed via uAgents.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

The Stripe ASI:One Agent provides:

*   **AI-powered payment link creation** with natural-language requests
*   **Direct Stripe API** integration (products, prices, payment links)
*   **ASI:One reasoning** for natural language understanding
*   **uAgents deployment** for 24/7 availability

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

Before you begin, ensure you have:

*   **Python 3.11+** installed
*   **Stripe account** with API keys ([Get sandbox keys](https://dashboard.stripe.com/test/apikeys))
*   **ASI:One API key** ([Get API key](https://asi1.ai/dashboard/api-keys))

Installation[â€‹](#installation "Direct link to Installation")
------------------------------------------------------------

### 1\. Create Project Directory[â€‹](#1-create-project-directory "Direct link to 1. Create Project Directory")

    mkdir stripe_asi_agentcd stripe_asi_agent

### 2\. Set Up Virtual Environment[â€‹](#2-set-up-virtual-environment "Direct link to 2. Set Up Virtual Environment")

    python -m venv venvsource venv/bin/activate  # On Windows: venv\Scripts\activate

### 3\. Install Dependencies[â€‹](#3-install-dependencies "Direct link to 3. Install Dependencies")

Create a `requirements.txt` file:

    # Core uAgentsuagents==0.22.5# Stripestripe==12.2.0# Environment and utilitiespython-dotenv>=1.0.0requests>=2.31.0

Install the packages:

    pip install -r requirements.txt

Environment Setup[â€‹](#environment-setup "Direct link to Environment Setup")
---------------------------------------------------------------------------

### 1\. Create Environment File[â€‹](#1-create-environment-file "Direct link to 1. Create Environment File")

Create a `.env` file in your project directory:

    # Stripe ConfigurationSTRIPE_SECRET_KEY=sk_test_your_stripe_secret_key_here# ASI:One ConfigurationASI_API_KEY=your_asi_api_key_here

### 2\. Get Your API Keys[â€‹](#2-get-your-api-keys "Direct link to 2. Get Your API Keys")

**Stripe Keys:**

1.  Go to [Stripe Dashboard](https://dashboard.stripe.com/test/apikeys)
2.  Copy your **Secret key** (starts with `sk_test_`)

**ASI:One Key:**

1.  Go to [ASI:One Platform](https://innovationlab.fetch.ai/resources/docs/asione/asi1-mini-getting-started)
2.  Create a new API key

Create the Agent[â€‹](#create-the-agent "Direct link to Create the Agent")
------------------------------------------------------------------------

Create `stripe_llm_agent.py`:

    #!/usr/bin/env python3"""Stripe Payment-Link Agent using ASI:One function-calling and uAgents.Send any message like:    "Create a payment link for a $1200 laptop"The LLM extracts {product, amount} via structured output, we callStripe to create Product, Price, and PaymentLink, then return the URL."""import os, json, stripefrom uuid import uuid4from datetime import datetime, timezonefrom dotenv import load_dotenvimport requestsfrom uagents import Agent, Context, Protocolfrom uagents_core.contrib.protocols.chat import (    ChatAcknowledgement,    ChatMessage,    TextContent,    chat_protocol_spec,)# ---------------------------------------------------------------------------# Environment# ---------------------------------------------------------------------------load_dotenv()stripe.api_key = os.getenv("STRIPE_SECRET_KEY")ASI_API_KEY = os.getenv("ASI_API_KEY")if not stripe.api_key or not ASI_API_KEY:    raise RuntimeError("STRIPE_SECRET_KEY and ASI_API_KEY must be set in environment")# ASI:One API settingsBASE_URL = "https://api.asi1.ai/v1"headers = {    "Authorization": f"Bearer {ASI_API_KEY}",    "Content-Type": "application/json"}# ---------------------------------------------------------------------------# ASI:One tool / function definition# ---------------------------------------------------------------------------tool_def = {    "type": "function",    "function": {        "name": "create_payment_link",        "description": "Create a one-off Stripe payment link",        "parameters": {            "type": "object",            "properties": {                "product": {                    "type": "string",                    "description": "Product or service name"                },                "amount": {                    "type": "number",                    "description": "Price as a decimal number"                },                "currency": {                    "type": "string",                    "description": "ISO currency code (e.g. usd, inr, eur). Default usd",                    "enum": ["usd", "inr", "eur", "gbp", "aud", "cad"]                }            },            "required": ["product", "amount"],            "additionalProperties": False        },        "strict": True    }}def is_payment_related(text: str) -> bool:    """Check if the message is related to creating a payment link."""    prompt = f"""Analyze if the following message is about creating a payment link or processing a payment.    Return only 'true' if it's payment-related, 'false' otherwise.        Message: {text}        Examples of payment-related messages:    - "Create a payment link for a $100 product"    - "I want to sell my laptop for $500"    - "Generate a payment link for consulting services"        Examples of non-payment messages:    - "Hello, how are you?"    - "What's the weather like?"    - "Tell me a joke"    """        payload = {        "model": "asi1-mini",        "messages": [{"role": "user", "content": prompt}],        "temperature": 0    }        response = requests.post(        f"{BASE_URL}/chat/completions",        headers=headers,        json=payload    )    response.raise_for_status()    return response.json()["choices"][0]["message"]["content"].strip().lower() == 'true'def parse_with_llm(user_text: str):    """Returns dict with product & amount extracted by LLM."""    payload = {        "model": "asi1-mini",        "messages": [{"role": "user", "content": user_text}],        "tools": [tool_def],        "tool_choice": {            "type": "function",            "function": {"name": "create_payment_link"}        },        "temperature": 0    }        response = requests.post(        f"{BASE_URL}/chat/completions",        headers=headers,        json=payload    )    response.raise_for_status()    response_json = response.json()        # Handle tool calls from ASI:One response    tool_calls = response_json["choices"][0]["message"].get("tool_calls", [])    if not tool_calls:        raise ValueError("No payment details found in the message")            tool_call = tool_calls[0]    return json.loads(tool_call["function"]["arguments"])# ---------------------------------------------------------------------------# Stripe helper# ---------------------------------------------------------------------------def create_payment_link(product: str, amount_usd: float, currency: str):    prod = stripe.Product.create(name=product)    price = stripe.Price.create(unit_amount=int(amount_usd * 100), currency=currency, product=prod.id)    link = stripe.PaymentLink.create(line_items=[{"price": price.id, "quantity": 1}])    return link# ---------------------------------------------------------------------------# uAgents setup# ---------------------------------------------------------------------------agent = Agent(name="stripe_llm_agent", port=<"ANY OPEN PORT ON YOUR MACHINE">, mailbox=True, seed=<"RANDOM STRING SECRERT SEED PHASE">)chat_proto = Protocol(spec=chat_protocol_spec)@chat_proto.on_message(ChatMessage)async def handler(ctx: Context, sender: str, msg: ChatMessage):    # ack    await ctx.send(sender, ChatAcknowledgement(timestamp=datetime.now(timezone.utc), acknowledged_msg_id=msg.msg_id))    text = " ".join(c.text for c in msg.content if hasattr(c, "text")).strip()    if not text:        return    try:        # First check if the message is payment-related        if not is_payment_related(text):            reply = (                "âŒ I can only help with creating payment links. "                "Please ask me to create a payment link for a product or service.\n\n"                "Example: 'Create a payment link for a $100 laptop'"            )        else:            args = parse_with_llm(text)            currency = args.get("currency", "usd").lower()            link = create_payment_link(args["product"], args["amount"], currency)            reply = (                f"âœ… Product Created: {args['product']} - {args['amount']} {currency.upper()}\n"                f"âœ… Payment Link Generated: {link.url}\n\n"                "---\n*Powered by Stripe + ASI:One + uAgents*"            )    except Exception as e:        reply = f"âŒ Error: {e}"    await ctx.send(sender, ChatMessage(timestamp=datetime.now(timezone.utc), msg_id=uuid4(), content=[TextContent(type="text", text=reply)]))# Additional handler required by AgentChatProtocol spec@chat_proto.on_message(ChatAcknowledgement)async def handle_ack(ctx: Context, sender: str, ack: ChatAcknowledgement):    ctx.logger.debug(f"Ack from {sender} for {ack.acknowledged_msg_id}")# include protocols after all handlers are registeredagent.include(chat_proto, publish_manifest=True)if __name__ == "__main__":    agent.run()

Understanding the Code[â€‹](#understanding-the-code "Direct link to Understanding the Code")
------------------------------------------------------------------------------------------

### Key Components[â€‹](#key-components "Direct link to Key Components")

**1\. ASI:One Integration**

    tool_def = {    "type": "function",    "function": {        "name": "create_payment_link",        "description": "Create a one-off Stripe payment link",        "parameters": {            "type": "object",            "properties": {                "product": {"type": "string"},                "amount": {"type": "number"},                "currency": {"type": "string"}            },            "required": ["product", "amount"]        }    }}

*   Uses ASI:One for natural language understanding
*   Defines structured function calling
*   Handles payment-related message detection

**2\. Stripe Integration**

    def create_payment_link(product: str, amount_usd: float, currency: str):    prod = stripe.Product.create(name=product)    price = stripe.Price.create(unit_amount=int(amount_usd * 100), currency=currency, product=prod.id)    link = stripe.PaymentLink.create(line_items=[{"price": price.id, "quantity": 1}])    return link

*   Direct Stripe API integration
*   Creates products, prices, and payment links
*   Handles currency conversion

**3\. uAgents Setup**

    agent = Agent(name="stripe_llm_agent", port=8032, mailbox=True)chat_proto = Protocol(spec=chat_protocol_spec)

*   Sets up the agent with mailbox functionality
*   Handles message routing and acknowledgments
*   Provides 24/7 availability

Running the Agent[â€‹](#running-the-agent "Direct link to Running the Agent")
---------------------------------------------------------------------------

### 1\. Start the Agent[â€‹](#1-start-the-agent "Direct link to 1. Start the Agent")

With your virtual environment activated and `.env` file configured:

    python stripe_llm_agent.py

### 2\. Expected Output[â€‹](#2-expected-output "Direct link to 2. Expected Output")

    ğŸƒ Agent running â€” press Ctrl+C to stop

Usage Examples[â€‹](#usage-examples "Direct link to Usage Examples")
------------------------------------------------------------------

### Example 1: Basic Payment Request[â€‹](#example-1-basic-payment-request "Direct link to Example 1: Basic Payment Request")

**Input:**

    "I want to buy a laptop for $1200"

**Expected Response:**

    âœ… Product Created: Laptop - 1200 USDâœ… Payment Link Generated: https://buy.stripe.com/test_XXXXXXXXX---*Powered by Stripe + ASI:One + uAgents*

### Example 2: Membership Payment Link[â€‹](#example-2-membership-payment-link "Direct link to Example 2: Membership Payment Link")

**Input:**

    "Generate a payment link for $49 annual membership"

**Expected Response:**

    âœ… Product Created: Annual Membership - 49 USDâœ… Payment Link Generated: https://buy.stripe.com/test_XXXXXXXXX---*Powered by Stripe + ASI:One + uAgents*

Agent Capabilities[â€‹](#agent-capabilities "Direct link to Agent Capabilities")
------------------------------------------------------------------------------

### Payment Operations[â€‹](#payment-operations "Direct link to Payment Operations")

*   âœ… **Create Products** - Dynamic product creation
*   âœ… **Generate Payment Links** - Secure Stripe checkout URLs
*   âœ… **Currency Support** - Multiple currency options (USD, INR, EUR, GBP, AUD, CAD)

### AI Features[â€‹](#ai-features "Direct link to AI Features")

*   ğŸ¤– **Natural Language Processing** - Understands complex payment requests
*   ğŸ§  **Intelligent Reasoning** - Uses ASI:One for context understanding
*   ğŸ¯ **Message Classification** - Detects payment-related queries
*   ğŸ”„ **Error Recovery** - Handles API failures gracefully

### Integration Features[â€‹](#integration-features "Direct link to Integration Features")

*   ğŸŒ **uAgents Deployment** - 24/7 availability
*   ğŸ“¬ **Mailbox Communication** - Agent-to-agent messaging
*   ğŸ”— **API Integration** - Real Stripe API calls with sandbox safety
*   ğŸ“Š **Logging & Monitoring** - Built-in error tracking

This guide demonstrates the power of combining ASI:One's natural language processing with Stripe's Payment Links for autonomous agent commerce.

A live instance of this payment-link agent is running on Agentverse: [STRIPE TESTING AGENT](https://agentverse.ai/agents/details/agent1qttcp5xanrqv2u304ds2azwdsj5z8eh7h0jtdhl85qz3su49et7869c2v33/profile)

### Chat screen[â€‹](#chat-screen "Direct link to Chat screen")

### Payment Window[â€‹](#payment-window "Direct link to Payment Window")

note

**Note:** If you want to test it type in the card number as `4242 4242 4242 4242`.

### You can even test this on ASI:One LLM.[â€‹](#you-can-even-test-this-on-asi-llm "Direct link to you-can-even-test-this-on-asi-llm")

Open the [ASI:One](https://asi1.ai/chat) and enable agents to ask a question.

    can you check for an stripe agent which can generate a payment link for 1400 indian rupees for iphone 15 cover

note

**Note:** These are the test sandbox version, no money will be deducted and dont put in your original card details.</content>
</page>

<page>
  <title>Understanding Agentic Systems | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.2/concepts-ai-agents/understanding-agentic-systems</url>
  <content>### The Spectrum of LLM Applications[â€‹](#the-spectrum-of-llm-applications "Direct link to The Spectrum of LLM Applications")

It's crucial to understand where different types of LLM-based systems fall on the complexity spectrum:

1.  **Simple LLM Applications**
    
    *   Single LLM calls with prompt engineering
    *   Basic RAG (Retrieval-Augmented Generation) implementations
    *   Structured output parsing
    *   These are NOT agents, just applications using LLM capabilities
2.  **Multi-Step LLM Applications**
    
    *   Multiple LLM calls in sequence
    *   API integrations and external tool usage
    *   Basic state management
    *   While more complex, these still aren't necessarily agents
3.  **True AI Agents**
    
    *   Dynamic decision-making about process flow
    *   Autonomous tool selection and usage
    *   Complex state management and memory
    *   Goal-oriented behavior with adaptive strategies

### Common Misconceptions[â€‹](#common-misconceptions "Direct link to Common Misconceptions")

#### Misconception 1: "My application uses multiple LLM calls, so it's an agent"[â€‹](#misconception-1-my-application-uses-multiple-llm-calls-so-its-an-agent "Direct link to Misconception 1: \"My application uses multiple LLM calls, so it's an agent\"")

    # This is NOT an agent - it's a multi-step LLM applicationdef process_document(doc):    summary = llm.generate_summary(doc)    keywords = llm.extract_keywords(summary)    sentiment = llm.analyze_sentiment(summary)    return {        "summary": summary,        "keywords": keywords,        "sentiment": sentiment    }

#### Misconception 2: "I'm using tools and APIs, so it's an agent"[â€‹](#misconception-2-im-using-tools-and-apis-so-its-an-agent "Direct link to Misconception 2: \"I'm using tools and APIs, so it's an agent\"")

    # This is NOT an agent - it's an automated workflowdef analyze_stock(symbol):    price_data = stock_api.get_price(symbol)    news = news_api.get_recent_news(symbol)    analysis = llm.analyze(f"Price: {price_data}, News: {news}")    return analysis

### Understanding Agentic Systems[â€‹](#understanding-agentic-systems "Direct link to Understanding Agentic Systems")

#### 1\. Workflows vs Agents[â€‹](#1-workflows-vs-agents "Direct link to 1. Workflows vs Agents")

**Workflows:**

    # Predefined workflow exampleclass FinancialAnalysisWorkflow:    def execute(self, query):        # Fixed sequence of steps        market_data = self.get_market_data()        financial_statements = self.get_financial_statements()        competitor_analysis = self.get_competitor_data()                return self.generate_analysis(            market_data,            financial_statements,            competitor_analysis        )

**Agents:**

    # True agent exampleclass FinancialAnalysisAgent:    def analyze(self, query):        # Dynamic planning        analysis_plan = self.plan_analysis_strategy(query)                # Flexible execution        for step in analysis_plan:            if self.needs_revision(step, self.current_state):                analysis_plan = self.revise_plan()                        tool = self.select_appropriate_tool(step)            result = self.execute_tool(tool, step)            self.update_state(result)                        if self.goal_achieved():                break                return self.synthesize_findings()

### When to Use Each Approach[â€‹](#when-to-use-each-approach "Direct link to When to Use Each Approach")

1.  **Use Simple LLM Applications When:**
    
    *   Tasks are straightforward and well-defined
    *   Minimal or no tool interaction is needed
    *   Quick response times are critical
    *   Cost efficiency is a primary concern
2.  **Use Workflows When:**
    
    *   Process steps are well-understood and consistent
    *   Predictability is more important than flexibility
    *   You need tight control over execution
    *   Performance and reliability are crucial
3.  **Use Agents When:**
    
    *   Tasks require dynamic decision-making
    *   Problems have multiple valid solution paths
    *   Flexibility and adaptation are crucial
    *   Complex tool interactions are needed
    *   Tasks benefit from maintaining context

### Implementation Best Practices[â€‹](#implementation-best-practices "Direct link to Implementation Best Practices")

1.  **Start Simple**
    
    *   Begin with the simplest solution that could work
    *   Only add complexity when clearly needed
    *   Validate the need for agency before implementing it
2.  **Consider Trade-offs**
    
    *   Agents typically have higher latency
    *   Cost increases with complexity
    *   Maintenance burden grows with sophistication
3.  **Design for Observability**
    
    *   Implement comprehensive logging
    *   Track decision-making processes
    *   Monitor tool usage patterns
4.  **Build in Safeguards**
    
    *   Implement rate limiting
    *   Add timeout mechanisms
    *   Create fallback strategies

### Evaluation Criteria for True Agency[â€‹](#evaluation-criteria-for-true-agency "Direct link to Evaluation Criteria for True Agency")

Before implementing an agent, evaluate if your system really needs true agency by checking these criteria:

1.  **Decision Autonomy**
    
    *   Can the system choose different paths based on context?
    *   Does it make meaningful decisions about tool usage?
    *   Can it adapt its strategy during execution?
2.  **State Management**
    
    *   Does it maintain meaningful state?
    *   Can it use past interactions to inform decisions?
    *   Does it track progress toward goals?
3.  **Tool Integration**
    
    *   Can it choose tools dynamically?
    *   Does it understand tool capabilities?
    *   Can it combine tools in novel ways?
4.  **Goal Orientation**
    
    *   Does it understand and work toward specific objectives?
    *   Can it recognize when goals are achieved?
    *   Can it adjust goals based on new information?</content>
</page>

<page>
  <title>Team Based Artchitectures | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.2/concepts-ai-agents/team-based-architectures</url>
  <content>### Multi-Agent Architectures[â€‹](#multi-agent-architectures "Direct link to Multi-Agent Architectures")

Multi-agent systems distribute complex tasks across specialized agents, each with distinct roles and capabilities. This architecture provides several advantages:

1.  **Specialization**
    
    *   Agents can focus on specific domains
    *   Reduced prompt complexity per agent
    *   Clear separation of concerns
2.  **Scalability**
    
    *   Parallel processing capabilities
    *   Distributed decision making
    *   Independent scaling of components
3.  **Robustness**
    
    *   Graceful degradation
    *   Fault isolation
    *   Easier maintenance

### Team-Based Agent Architecture[â€‹](#team-based-agent-architecture "Direct link to Team-Based Agent Architecture")

A team-based architecture typically consists of three main components:

1.  **Supervisor Agent**
    
    *   Coordinates team activities
    *   Makes routing decisions
    *   Ensures task completion
2.  **Specialist Agents**
    
    *   Handle domain-specific tasks
    *   Maintain focused expertise
    *   Provide detailed analysis
3.  **State Management System**
    
    *   Maintains conversation context
    *   Tracks team progress
    *   Manages shared resources

### Implementing a Team-Based System[â€‹](#implementing-a-team-based-system "Direct link to Implementing a Team-Based System")

#### 1\. Supervisor Agent Implementation[â€‹](#1-supervisor-agent-implementation "Direct link to 1. Supervisor Agent Implementation")

    def create_supervisor_agent(llm: ChatOpenAI):    """    Create a supervisor agent that coordinates team activities.        Key responsibilities:    - Task decomposition    - Agent selection    - Progress monitoring    - Final response synthesis    """    supervisor_prompt = """    Core responsibilities:    1. Analyze incoming queries    2. Determine required information    3. Select appropriate specialist    4. Monitor progress    5. Decide when to conclude    """        return create_team_supervisor(        llm=llm,        system_prompt=supervisor_prompt,        members=["SpecialistA", "SpecialistB"]    )

#### 2\. Specialist Agent Implementation[â€‹](#2-specialist-agent-implementation "Direct link to 2. Specialist Agent Implementation")

    def create_specialist_agent(    llm: ChatOpenAI,    domain: str,    tools: List[Tool]):    """    Create a specialist agent with domain-specific capabilities.        Key features:    - Focused expertise    - Specific tool access    - Clear output format    """    system_prompt = f"""    You are specialized in {domain}.    When responding:    1. Use domain-specific tools    2. Provide structured output    3. Request missing information    """        return create_agent(        llm=llm,        tools=tools,        system_prompt=system_prompt    )

#### 3\. State Management[â€‹](#3-state-management "Direct link to 3. State Management")

    class TeamState(TypedDict):    """    Manage team-wide state and context.        Components:    - Message history    - Team composition    - Current status    - Required information    """    messages: List[BaseMessage]    team_members: List[str]    next: str    information_needed: List[str]    reasoning: str

### Building the Team Graph[â€‹](#building-the-team-graph "Direct link to Building the Team Graph")

The team graph defines how agents interact and how information flows:

    def create_team_graph():    """    Create a coordinated team of agents with defined interactions.        Structure:    1. Initialize components    2. Define connections    3. Set workflow rules    4. Establish entry points    """    # Initialize agents    specialist_a = create_specialist_agent(...)    specialist_b = create_specialist_agent(...)    supervisor = create_supervisor_agent(...)        # Create graph    graph = StateGraph(TeamState)        # Add nodes    graph.add_node("SpecialistA", specialist_a)    graph.add_node("SpecialistB", specialist_b)    graph.add_node("supervisor", supervisor)        # Define workflows    graph.add_edge("SpecialistA", "supervisor")    graph.add_edge("SpecialistB", "supervisor")        # Add conditional routing    graph.add_conditional_edges(        "supervisor",        lambda x: x["next"],        {            "SpecialistA": "SpecialistA",            "SpecialistB": "SpecialistB",            "FINISH": END        }    )        return graph.compile()</content>
</page>

<page>
  <title>Microservices and AI Agents | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.2/concepts-ai-agents/agent-comparison</url>
  <content>Fetch.ai Microservice Agent and AI Agent
----------------------------------------

Before diving into the world of AI agents, letâ€™s first explore the key differences between microservices and AI agents.

*   **Microservice**: If you want to fetch specific information or perform an isolated action, you typically use tools or APIsâ€”this is known as a microservice. Imagine you want to check available flights from London to New York. Youâ€™d use an API or tool to fetch the available options. Thatâ€™s a microservice in actionâ€”focused, specific, and task-oriented.
    
*   **AI Agent**: If you have a larger objective that requires coordination between multiple microservices or tools, thatâ€™s where an AI agent steps in. Now, letâ€™s say you want to book the best possible flight. This involves more than just checking flight options: youâ€™ll compare prices across different days, check your personal schedule, and evaluate additional services like baggage policies or meal preferences. To achieve this goal, you need reasoning and decision-making capabilitiesâ€”this is where an AI agent comes into play.
    

The Fetch.ai ecosystem offers a variety of tools to build both microservices and AI agents, enabling developers to cater to different use cases.

### Microservices with uAgents[â€‹](#microservices-with-uagents "Direct link to Microservices with uAgents")

uAgents is Fetch.ai's lightweight Python framework for creating agents (microservices). Itâ€™s designed to be simple and efficient, making it a great choice for building basic microservices.

### AI Agents with the Fetch.ai SDK[â€‹](#ai-agents-with-the-fetchai-sdk "Direct link to AI Agents with the Fetch.ai SDK")

Fetch.ai's SDK is built on top of the uAgents framework. It simplifies the creation of intelligent, dynamic AI agents that can interact with other agents and microservices seamlessly. The SDK allows for more flexibility, reasoning, and adaptability compared to uAgents.

Youâ€™ll learn in the upcoming sections how to decide between these tools based on the problem youâ€™re solving.

How Fetch.aiâ€™s Ecosystem Works[â€‹](#how-fetchais-ecosystem-works "Direct link to How Fetch.aiâ€™s Ecosystem Works")
----------------------------------------------------------------------------------------------------------------

Fetch.ai provides an integrated ecosystem to host and manage your agents, ensuring connectivity and scalability.

1.  **Agentverse Platform**:
    
    *   Agents can be hosted on the Agentverse platform, allowing them to stay active and accessible.
    *   Due to security constraints, Agentverse only supports a limited set of packages.
2.  **Mailbox Service**:
    
    *   For scenarios requiring custom environments (like virtual machines), you can use the Mailbox Service. This lets you host agents outside Agentverse while maintaining connectivity.
    *   The mailbox ensures that messages sent to an agent are queued, so the agent can process them once it comes online.
3.  **Agent Registration with the Almanac Contract**:
    
    *   When agents are registered on Agentverse, they are also listed in the Almanac Contract on Fetch.aiâ€™s blockchain. This registry allows other agents to discover your agent using its unique address.

Why Use the Fetch.ai SDK?[â€‹](#why-use-the-fetchai-sdk "Direct link to Why Use the Fetch.ai SDK?")
-------------------------------------------------------------------------------------------------

While the uAgents framework is sufficient for basic microservices, the Fetch.ai SDK offers additional capabilities suited for more complex AI agents. Hereâ€™s why the SDK exists:

*   Dynamic Messaging:
    *   uAgents require precise Data Models to identify and process messages. In contrast, the SDK enables agents to handle dynamic messages, making it more adaptable.
*   Handler Differences:
    *   uAgents rely on predefined handlers (e.g., interval-based or message-based handlers).
    *   SDK-based agents leverage Flask webhooks, enabling more flexible and scalable communication.

With these distinctions, the SDK empowers developers to build agents capable of reasoning, adaptability, and complex decision-making.</content>
</page>

<page>
  <title>AI Agent Creation | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.2/agent-creation/sdk-creation</url>
  <content>Creation and Registration of AI Agent using SDK
-----------------------------------------------

Why SDK Agents?[â€‹](#why-sdk-agents "Direct link to Why SDK Agents?")
--------------------------------------------------------------------

While the uAgents framework is great for basic microservices, the Fetch.ai SDK provides enhanced capabilities for building AI agents with greater flexibility and adaptability. Key differences include:

*   **Dynamic Messaging**: Processes flexible or unstructured data, unlike uAgents that require strict data models.
    
*   **Flexible Handlers**: Relies on Flask endpoints instead of predefined handlers, enabling more scalable and customizable communication.
    

These features are especially helpful if your AI agent needs advanced reasoning or can benefit from rapid decision-making and adaptability.

Installing fetchai SDK[â€‹](#installing-fetchai-sdk "Direct link to Installing fetchai SDK")
------------------------------------------------------------------------------------------

Fetch.ai's [SDK](https://pypi.org/project/fetchai/) package is a Python library running on Ubuntu/Debian, macOS, and Windows systems.

On your computer, you may need to install:

*   [Python 3.8+](https://www.python.org/downloads/)
*   [PIP](https://pypi.org/project/pip/) - the Python package manager.
*   [fetchai](https://pypi.org/project/fetchai/) library

### Install with Pip[â€‹](#install-with-pip "Direct link to Install with Pip")

*   Install Fetch.ai uagents library:

*   Check if installation was successful:

### Creating and Registering an AI Agent[â€‹](#creating-and-registering-an-ai-agent "Direct link to Creating and Registering an AI Agent")

In this example, you will create an AI agent identity and register it on Agentverse.

1.  **Create a python script** named `register_agent.py`.

2.  **Add the following code** to `register_agent.py`.

register\_ai\_agent.py

    import osfrom uagents_core.crypto import Identityfrom fetchai.registration import register_with_agentverse# Store your Agentverse API Key in the environment variables. AGENTVERSE_KEY = os.getenv("AGENTVERSE_KEY")# Your Agent's unique key for generating an address on Agentverseai_identity = Identity.from_seed(os.getenv("AGENT_SECRET_KEY"), 0)# Give your Agent a name. This allows you to easily identify one# of your Agents from other agents on Agentverse.name = "My AI's Name"# This is how you optimize your Agent's search engine performancereadme = """![domain:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)domain:domain-of-your-agent<description>My AI's description of capabilities and offerings</description><use_cases>    <use_case>An example of one of your AI's use cases.</use_case></use_cases><payload_requirements><description>The requirements your AI has for requests</description><payload>    <requirement>        <parameter>question</parameter>        <description>The question that you would like this AI work with you to solve</description>    </requirement></payload></payload_requirements>"""# The webhook that your AI receives messages on.ai_webhook = "https://api.sampleurl.com/webhook"register_with_agentverse(    ai_identity,    ai_webhook,    AGENTVERSE_KEY,    name,    readme,)

3.  Set up environment variables in a `.env` file (or similar approach):

    AGENTVERSE_KEY='YOUR_AGENTVERSE_API_KEY'AGENT_SECRET_KEY='YOUR_RANDOM_SECRET_SEED'

*   The `AGENTVERSE_KEY` is obtained from your Agentverse account.
*   The `AGENT_SECRET_KEY` should be a unique, random phrase to ensure your agent consistently generates the same address.

4.  Run the Script

    python3 regsiter_agent.py

*   **Sample Output:**

    INFO:fetchai:Registering with Almanac APIINFO:fetchai:Successfully registered as custom agent in AgentverseINFO:fetchai:Completed registering agent with Agentverse

### Checking Your Agent on Agentverse[â€‹](#checking-your-agent-on-agentverse "Direct link to Checking Your Agent on Agentverse")

1.  **Go to the Agents tab** in the Agentverse interface.
    
2.  **Select â€œLocal Agentsâ€** and look for the name used in your script (e.g., â€œMy AIâ€™s Nameâ€).
    

3.  **Click on the agent name** to view its details (address, registration data, etc.).

You have successfully created and registered an AI agent with the Fetch.ai SDK. You can now integrate this agent into your applications, allowing it to interact with other agents or services via Agentverse.

#### Next Steps:[â€‹](#next-steps "Direct link to Next Steps:")

*   Further develop your agentâ€™s code and logic.
*   Integrate custom Flask endpoints to handle advanced communication.
*   Explore additional SDK features (transaction handling, smart contracts, etc.) to expand the agentâ€™s capabilities.</content>
</page>

<page>
  <title>Agentverse API Key | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.2/agentverse/agentverse-api-key</url>
  <content>Getting Agentverse API Key
--------------------------

To get [Agentverse](https://agentverse.ai/) API Key, just login to Agentverse using your gmail address and go to profile section as shown in picture below:

Click on **\+ New API Key** button, give name to your API Key and with write permission to Access to all resources in Agentverse. Your permission can be for 30 days now. Check below image for more clarity on the process :Â 

Click on **Generate API Key**, it will take you through the authentication process again. Once you do the authentication and Allow Access the API key will pop-up on your screen like the image below and save it somewhere as it cannot be regenerated.

Keep your API Key stored somewhere safe. You wonâ€™t be able to access it again.</content>
</page>

<page>
  <title>AI Agent to AI Agent Communication | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.2/agent-communication/sdk-sdk-communication</url>
  <content>In this guide, we will create two AI agents using the fetchai SDK and then see how to send and handle requests received by these agents. You can find all our supporting code files in our dedicated [github repo](https://github.com/fetchai/fetchai).

AI Agent creation[â€‹](#ai-agent-creation "Direct link to AI Agent creation")
---------------------------------------------------------------------------

Let's start by creating an AI Agent using the [fetchai SDK](https://github.com/fetchai/fetchai).

Please remember to have the **fetchai** package installed in the terminal in order to create and run the agents.

### Creating our first agent to receive the message[â€‹](#creating-our-first-agent-to-receive-the-message "Direct link to Creating our first agent to receive the message")

Create a new Python script:

    touch my_first_sdk_agent.py

Open `my_first_sdk_agent.py` in your text editor and add the following code which helps you register your agent and handle messages:

my\_first\_sdk\_agent.py

    from flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai import fetchfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osfrom dotenv import load_dotenv# Configure logginglogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app)# Initialising client identity to get registered on agentverseclient_identity = None # Function to register agentdef init_client():    """Initialize and register the client agent."""    global client_identity    try:        # Load the agent secret key from environment variables        client_identity = Identity.from_seed(os.getenv("AGENT_SECRET_KEY_1"), 0)        logger.info(f"Client agent started with address: {client_identity.address}")        readme = """            ![domain:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)            domain:domain-of-your-agent            <description>This Agent can only receive a message from another agent in string format.</description>            <use_cases>                <use_case>To receive a message from another agent.</use_case>            </use_cases>            <payload_requirements>            <description>This agent only requires a message in the text format.</description>            <payload>                <requirement>                    <parameter>message</parameter>                    <description>The agent can receive any kind of message.</description>                </requirement>            </payload>            </payload_requirements>        """                # Register the agent with Agentverse        register_with_agentverse(            identity=client_identity,            url="http://localhost:5002/api/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="Quickstart Agent 1",            readme=readme        )        logger.info("Quickstart agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise# app route to recieve the messages from other agents@app.route('/api/webhook', methods=['POST'])def webhook():    """Handle incoming messages"""    global agent_response    try:        # Parse the incoming webhook message        data = request.get_data().decode("utf-8")        logger.info("Received response")        message = parse_message_from_agent(data)        agent_response = message.payload        logger.info(f"Processed response: {agent_response}")        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500if __name__ == "__main__":    load_dotenv()       # Load environment variables    init_client()       #Register your agent on Agentverse    app.run(host="0.0.0.0", port=5002)      

Save the `.env` file with your `AGENTVERSE_KEY` and `AGENT_SECRET_KEY_1`, get your `AGENTVERSE_KEY` by referring to this [guide](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key) and the AGENT\_SECRET\_KEY\_1 is a random unique key just for your AI Agent. Â 

    AGENT_SECRET_KEY_1='Your random secret seed phrase for the agent'AGENTVERSE_API_KEY='YOUR AGENTVERSE API KEY FROM AGENTVERSE'

Start the first agent as follows:

#### Expected Output[â€‹](#expected-output "Direct link to Expected Output")

    (my_venv) abhi@Fetchs-MacBook-Pro chorot 3 session % python3 my_sdk_agent_1.pyINFO:__main__:Client agent started with address: agent1q2d7qdxxpc2emmph6wze47p0g7vnfxwuzk3h6yfnaem0px0hc3acu3yzz8kINFO:fetchai:Registering with Almanac APIINFO:fetchai:Successfully registered as custom agent in AgentverseINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Quickstart agent registration complete! * Serving Flask app 'my_sdk_agent_1' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5002 * Running on http://192.168.0.105:5002INFO:werkzeug:Press CTRL+C to quit

You can verify that your agent is registered by checking in your [Agentverse Local Agents](https://agentverse.ai/agents/local) â†—ï¸.

Webhook will help you receive and handle messages, we will learn more about communication here.

### Creating our second agent to send the message[â€‹](#creating-our-second-agent-to-send-the-message "Direct link to Creating our second agent to send the message")

Open `my_second_sdk_agent.py` in your text editor and add the following code which helps you register agent and handle messages:

my\_second\_sdk\_agent.py

    from flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai import fetchfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osfrom dotenv import load_dotenv# Configure logginglogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app)# Initialising client identity to get registered on agentverseclient_identity = None agent_response = None# Function to register agentdef init_client():    """Initialize and register the client agent."""    global client_identity    try:        # Load the agent secret key from environment variables        client_identity = Identity.from_seed(os.getenv("AGENT_SECRET_KEY_2"), 0)        logger.info(f"Client agent started with address: {client_identity.address}")        readme = """            ![domain:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)            domain:domain-of-your-agent            <description>This Agent can only send a message to another agent in string format.</description>            <use_cases>                <use_case>To send a message to another agent.</use_case>            </use_cases>            <payload_requirements>            <description>This agent can only send a message in the text format.</description>            <payload>                <requirement>                    <parameter>message</parameter>                    <description>The agent can send a message to another agent.</description>                </requirement>            </payload>            </payload_requirements>        """        # Register the agent with Agentverse        register_with_agentverse(            identity=client_identity,            url="http://localhost:5005/api/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="Quickstart Agent 2",            readme=readme        )        logger.info("Quickstart agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise@app.route('/api/send-data', methods=['POST'])def send_data():   """Send payload to the selected agent based on provided address."""   global agent_response   agent_response = None   try:       # Parse the request payload       data = request.json       payload = data.get('payload')  # Extract the payload dictionary       agent_address = data.get('agentAddress')  # Extract the agent address       # Validate the input data       if not payload or not agent_address:           return jsonify({"error": "Missing payload or agent address"}), 400       logger.info(f"Sending payload to agent: {agent_address}")       logger.info(f"Payload: {payload}")       # Send the payload to the specified agent       send_message_to_agent(           client_identity,  # Frontend client identity           agent_address,    # Agent address where we have to send the data           payload           # Payload containing the data       )       return jsonify({"status": "request_sent", "agent_address": agent_address, "payload": payload})   except Exception as e:       logger.error(f"Error sending data to agent: {e}")       return jsonify({"error": str(e)}), 500if __name__ == "__main__":    load_dotenv()   # Load environment variables    init_client()   #Register your Agent on Agentverse    app.run(host="0.0.0.0", port=5005)

Start the second agent as follows.

    python my_second_agent.py

#### Expected Output[â€‹](#expected-output-1 "Direct link to Expected Output")

    (my_venv) abhi@Fetchs-MacBook-Pro chorot 3 session % python3 my_sdk_agent_2.pyINFO:__main__:Client agent started with address: agent1qvgxzj40657wh4ftk65scvqqwv7sw5lf7z5uf3mdnc8d6zmrugumu4g78wsINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Quickstart agent registration complete! * Serving Flask app 'my_sdk_agent_2' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5005 * Running on http://192.168.0.105:5005INFO:werkzeug:Press CTRL+C to quit

### Sending message from Agent2 to Agent1[â€‹](#sending-message-from-agent2-to-agent1 "Direct link to Sending message from Agent2 to Agent1")

We will use curl command to send message from agent2 to agent1. We will send a payload with Hello Message.

    curl -X POST http://localhost:5005/api/send-data \-H "Content-Type: application/json" \-d '{  "payload": {"message":"Hello This is message from agent2"},  "agentAddress": <Your agent 1 address>}'

#### Curl Command Output[â€‹](#curl-command-output "Direct link to Curl Command Output")

    {"agent_address":"agent1q2d7qdxxpc2emmph6wze47p0g7vnfxwuzk3h6yfnaem0px0hc3acu3yzz8k","payload":{"message":"Hello This is message from agent2"},"status":"request_sent"}

#### Agent2 Logs[â€‹](#agent2-logs "Direct link to Agent2 Logs")

    INFO:__main__:Sending payload to agent: agent1q2d7qdxxpc2emmph6wze47p0g7vnfxwuzk3h6yfnaem0px0hc3acu3yzz8kINFO:__main__:Payload: {'message': 'Hello This is message from agent2'}{"version":1,"sender":"agent1qvgxzj40657wh4ftk65scvqqwv7sw5lf7z5uf3mdnc8d6zmrugumu4g78ws","target":"agent1q2d7qdxxpc2emmph6wze47p0g7vnfxwuzk3h6yfnaem0px0hc3acu3yzz8k","session":"e025c919-2c6d-4e9c-b562-a879cbb35aec","schema_digest":"model:708d789bb90924328daa69a47f7a8f3483980f16a1142c24b12972a2e4174bc6","protocol_digest":"proto:a03398ea81d7aaaf67e72940937676eae0d019f8e1d8b5efbadfef9fd2e98bb2","payload":"eyJtZXNzYWdlIjoiSGVsbG8gVGhpcyBpcyBtZXNzYWdlIGZyb20gYWdlbnQyIn0=","expires":null,"nonce":null,"signature":"sig10phxfu9s9lsrm3ux4pffufw6yxs37z0ag82hhlw0mkkwap74av7cjwk86dr9nnmgpgxwhyqdmnl90um8wu77emafutfw77jdvlswg9spmc899"}INFO:fetchai:Got response looking up agent endpointhttp://localhost:5002/api/webhookINFO:fetchai:Sent message to agentINFO:werkzeug:127.0.0.1 - - [29/Jan/2025 11:17:59] "POST /api/send-data HTTP/1.1" 200 -

#### Agent1 Logs[â€‹](#agent1-logs "Direct link to Agent1 Logs")

    INFO:__main__:Received responseINFO:__main__:Processed response: {'message': 'Hello This is message from agent2'}INFO:werkzeug:127.0.0.1 - - [29/Jan/2025 11:17:59] "POST /api/webhook HTTP/1.1" 200 -

This is how messages can be sent and handled between agents.

### Parse and Send Message functions.[â€‹](#parse-and-send-message-functions "Direct link to Parse and Send Message functions.")

The `send_message_to_agent` function is used to send a message from one agent to the other, while the `parse_message_from_agent` function is used to handle incoming messages.

#### Sending a Message[â€‹](#sending-a-message "Direct link to Sending a Message")

The `send_message_to_agent` function constructs an Envelope containing the sender's identity, target agent address, message payload, and other metadata.

    from uagents_core.crypto import Identityfrom fetchai.communication import send_message_to_agentsend_message_to_agent(    sender=sender_identity,             #sender_address    target="receiver_agent_address",    #target agent address    payload={"message": "Hello, Agent!"})# payload is in {str : str} format

#### Parsing an Incoming Message[â€‹](#parsing-an-incoming-message "Direct link to Parsing an Incoming Message")

The `parse_message_from_agent` function extracts messages received from other agents by decoding the payload.

    from fetchai.communication import parse_message_from_agenttry:    message = parse_message_from_agent(data)except ValueError as e:    print(f"Error parsing message: {e}")    return# Extract sender and payload detailssender_address = message.sendermessage_payload = message.payload</content>
</page>

<page>
  <title>AI Agent to uAgent Communication | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.2/agent-communication/sdk-uagent-communication</url>
  <content>uAgent â†” AI Agent Communication Using Fetch.ai SDK and uAgents
--------------------------------------------------------------

This guide demonstrates how to enable communication between a Microservice Agent created using the uagents framework and an AI Agent created using the Fetch.ai SDK.

### uAgent Script (uagent.py)[â€‹](#uagent-script-uagentpy "Direct link to uAgent Script (uagent.py)")

Please remember to have the **uagents** and the **fetchai** package installed in the terminal in order to create and run the agents.

This uAgent will receive a message from the AI Agent and send a response back.

uagent.py

    from uagents import Agent, Context, Model# Define the request model the uAgent will handleclass Request(Model):    message: str# Define the response model the uAgent will send backclass Response(Model):    response: str# Initialize the uAgentuagent = Agent(    name="Sample uAgent",    port=8000,    endpoint=["http://localhost:8000/submit"])# Handle incoming messages with the Request model@uagent.on_message(model=Request)async def message_handler(ctx: Context, sender: str, msg: Request):    ctx.logger.info(f"Received message from {sender}: {msg.message}")    # Generate a response message    response = Response(response=f'Hello, AI Agent! I received your message:{msg.message}')        # Send the response back to the AI Agent    await ctx.send(sender, response)if __name__ == "__main__":    uagent.run()

### Explanation[â€‹](#explanation "Direct link to Explanation")

*   **Agent Initialization**: The uAgent listens on port 8000 for incoming messages.
*   **Message Handling**: When a message matching the `Request` model is received, the agent logs it and responds with a predefined message using the `Response` model.

Setting Up the AI Agent[â€‹](#setting-up-the-ai-agent "Direct link to Setting Up the AI Agent")
---------------------------------------------------------------------------------------------

The AI Agent will send messages to the uAgent and handle the response received from the uAgent.

### AI Agent Script (ai\_agent.py)[â€‹](#ai-agent-script-ai_agentpy "Direct link to AI Agent Script (ai_agent.py)")

ai\_agent.py

    import osfrom flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai.communication import send_message_to_agent, parse_message_from_agentimport loggingfrom dotenv import load_dotenv# Configure logginglogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)# Initialize Flask appapp = Flask(__name__)CORS(app)client_identity = Noneagent_response = Noneclass Request(Model):    message: str# Load environment variables from .env fileload_dotenv()def init_client():    """Initialize and register the client agent."""    global client_identity    try:        # Load the client identity from environment variables        client_identity = Identity.from_seed("Sample AI AGENT SEED PHRASE for communication", 0)        logger.info(f"Client agent started with address: {client_identity.address}")        readme = """        ![domain:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)        domain:domain-of-your-agent        <description>This Agent can send a message to a uAgent and receive a message from a uAgent in string format.</description>        <use_cases>        <use_case>Send and receive messages with another uAgent.</use_case>        </use_cases>        <payload_requirements>            <description>This agent can only send and receive messages in text format.</description>            <payload>                <requirement>                    <parameter>message</parameter>                    <description>The agent sends and receives messages in text format.</description>                </requirement>            </payload>        </payload_requirements>        """        # Register the agent with Agentverse        register_with_agentverse(            identity=client_identity,            url="http://localhost:5002/api/webhook",            agentverse_token = os.getenv("AGENTVERSE_API_KEY"),            agent_title="Sample AI Agent communication"            readme=readme        )        logger.info("Client agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise@app.route('/request', methods=['POST'])def send_data():    """Send payload to the selected agent based on provided address."""    global agent_response    agent_response = None    try:        # Parse the request payload        data = request.json        payload = data.get('payload')  # Extract the payload dictionary        uagent_address = "agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6" #run the uagent.py copy the address and paste here                # Build the Data Model digest for the Request model to ensure message format consistency between the uAgent and AI Agent        model_digest = Model.build_schema_digest(Request)        # Send the payload to the specified agent        send_message_to_agent(            client_identity,  # Frontend client identity            uagent_address,  # Agent address where we have to send the data            payload,  # Payload containing the data            model_digest=model_digest        )        return jsonify({"status": "request_sent", "payload": payload})    except Exception as e:        logger.error(f"Error sending data to agent: {e}")        return jsonify({"error": str(e)}), 500# app route to get recieve the messages on the agent@app.route('/api/webhook', methods=['POST'])def webhook():    """Handle incoming messages from the dashboard agent."""    global agent_response    try:        # Parse the incoming webhook message        data = request.get_data().decode("utf-8")        logger.info("Received response")        message = parse_message_from_agent(data)        agent_response = message.payload        logger.info(f"Processed response: {agent_response}")        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500if __name__ == "__main__":    load_dotenv()    init_client()    app.run(host="0.0.0.0", port=5002)

### Explanation[â€‹](#explanation-1 "Direct link to Explanation")

*   **Data Model**: The Request and Response models define the structure of messages exchanged between the AI Agent and the uAgent. A correctly defined data model is essential for sending a message to the uAgent from an SDK-based AI Agent.
*   **Schema Validation**: The model digest ensures that messages conform to the expected schema before transmission, preventing format mismatches.
*   **Sending Data**: The AI Agent sends a message to the uAgent using the /request endpoint.
*   **Handling Response**: The AI Agent listens for responses using the /api/webhook endpoint.

Environment Variables[â€‹](#environment-variables "Direct link to Environment Variables")
---------------------------------------------------------------------------------------

Create a .env file and add the following environment variables:

    AGENTVERSE_API_KEY="YOUR_AGENTVERSE_API_KEY"AGENT_SECRET_KEY="YOUR_SECRET_KEY_FOR_AI_AGENT"

Replace the placeholders with your actual API keys and agent secrets. Refer to this [guide](https://innovationlab.fetch.ai/resources/docs/1.0.2/agentverse/agentverse-api-key) to get your Agentverse API Key.

Testing the Communication[â€‹](#testing-the-communication "Direct link to Testing the Communication")
---------------------------------------------------------------------------------------------------

### Step 1: Running the uAgent[â€‹](#step-1-running-the-uagent "Direct link to Step 1: Running the uAgent")

Start the uAgent by running the following command in your terminal:

#### uAgent Logs[â€‹](#uagent-logs "Direct link to uAgent Logs")

    (venv) abhi@Fetchs-MacBook-Pro ILAgents % python3 uagent.pyINFO:     [Sample uAgent]: Starting agent with address: agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: My name is Sample uAgent and my address is agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8000&address=agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: Starting server on http://0.0.0.0:8000 (Press CTRL+C to quit)INFO:     [uagents.registration]: Registration on Almanac API successfulINFO:     [uagents.registration]: Registering on almanac contract...INFO:     [uagents.registration]: Registering on almanac contract...complete

#### Copy the uAgent Address[â€‹](#copy-the-uagent-address "Direct link to Copy the uAgent Address")

Look for the uAgent address in the logs, which appears in this format:

    agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6

Copy this uAgent address and paste it into the AI Agent script at the following line:

    uagent_address="PASTE YOUR UAGENT ADDRESS HERE"

### Step 2: Running the AI Agent[â€‹](#step-2-running-the-ai-agent "Direct link to Step 2: Running the AI Agent")

Start the AI Agent by running the following command in your terminal:

#### AI Agent Logs[â€‹](#ai-agent-logs "Direct link to AI Agent Logs")

    (venv) abhi@Fetchs-MacBook-Pro ILAgents % python3 ai_agent.pyINFO:__main__:Client agent started with address: agent1qw7u5sw63a88kmcn5j5kxf7q326u5hgmppvy2vpxlh3re6y0yp8253ec7xlINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Client agent registration complete! * Serving Flask app 'ai_agent' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5002 * Running on http://172.20.10.2:5002INFO:werkzeug:Press CTRL+C to quit

### Step 3: Sending a message from Agent2 to Agent1[â€‹](#step-3-sending-a-message-from-agent2-to-agent1 "Direct link to Step 3: Sending a message from Agent2 to Agent1")

We will use the following curl command to send a message from the AI Agent to the uAgent:

    curl -X POST http://localhost:5002/request \-H "Content-Type: application/json" \-d '{  "payload": {"message": "Hello uAgent!"}}'

#### Expected Logs on the uAgent Terminal[â€‹](#expected-logs-on-the-uagent-terminal "Direct link to Expected Logs on the uAgent Terminal")

    (venv) abhi@Fetchs-MacBook-Pro ILAgents % python3 uagent.pyINFO:     [Sample uAgent]: Starting agent with address: agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: My name is Sample uAgent and my address is agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8000&address=agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: Starting server on http://0.0.0.0:8000 (Press CTRL+C to quit)INFO:     [uagents.registration]: Registration on Almanac API successfulINFO:     [uagents.registration]: Registering on almanac contract...INFO:     [uagents.registration]: Registering on almanac contract...completeINFO:     [Sample uAgent]: Received message from agent1qw7u5sw63a88kmcn5j5kxf7q326u5hgmppvy2vpxlh3re6y0yp8253ec7xl: Hello uAgent!

#### Expected Logs on the AI Agent Terminal[â€‹](#expected-logs-on-the-ai-agent-terminal "Direct link to Expected Logs on the AI Agent Terminal")

    (venv) abhi@Fetchs-MacBook-Pro ILAgents % python3 ai_agent.pyINFO:__main__:Client agent started with address: agent1qw7u5sw63a88kmcn5j5kxf7q326u5hgmppvy2vpxlh3re6y0yp8253ec7xlINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Client agent registration complete! * Serving Flask app 'ai_agent' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5002 * Running on http://172.20.10.2:5002INFO:werkzeug:Press CTRL+C to quit{"version":1,"sender":"agent1qw7u5sw63a88kmcn5j5kxf7q326u5hgmppvy2vpxlh3re6y0yp8253ec7xl","target":"agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6","session":"c28d03fd-ad42-4c09-80b8-2bb8df9d7c3e","schema_digest":"model:14d760ab9a6127711e530c0f1bd84d5caa48c6bc6566ca489581d6918e6dff85","protocol_digest":"proto:a03398ea81d7aaaf67e72940937676eae0d019f8e1d8b5efbadfef9fd2e98bb2","payload":"eyJtZXNzYWdlIjoiaGV5In0=","expires":null,"nonce":null,"signature":"sig14ukfdrc98924wvx66xa2c32pk2wqyn69cepkvcpwahlygv3tc2t8hrzfkuzc9earscpnasdt8txpu99cyw24gmyspfdhqkxsn7a3lnq60mpaq"}INFO:fetchai:Got response looking up agent endpointhttp://localhost:8000/submitINFO:fetchai:Sent message to agentINFO:werkzeug:127.0.0.1 - - [30/Jan/2025 14:21:58] "POST /request HTTP/1.1" 200 -INFO:__main__:Received responseINFO:__main__:Processed response: {'response': 'Hello, AI Agent! I received your message: Hello uAgent!'}INFO:werkzeug:127.0.0.1 - - [30/Jan/2025 14:21:58] "POST /api/webhook HTTP/1.1" 200 -

Explanation of Communication Flow[â€‹](#explanation-of-communication-flow "Direct link to Explanation of Communication Flow")
---------------------------------------------------------------------------------------------------------------------------

*   The AI Agent sends a message using the **send\_message\_to\_agent** function.
*   The uAgent receives the message via the **@uagent.on\_message** handler.
*   The uAgent processes the message and responds using **await ctx.send()**.
*   The AI Agent receives the response through the **/api/webhook** endpoint.

Key Takeaways[â€‹](#key-takeaways "Direct link to Key Takeaways")
---------------------------------------------------------------

*   The **uAgent** handles structured messages using Fetch.aiâ€™s uAgents framework.
*   The **AI Agent** utilizes Fetch.aiâ€™s SDK for message transmission and parsing.
*   Communication between the two agents follows a request-response pattern using their respective handlers.

This setup can be extended to build more complex agent interactions involving dynamic data exchange, service orchestration, and autonomous decision-making.</content>
</page>

<page>
  <title>AI Agent Creation | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/agent-creation/sdk-creation</url>
  <content>Creation and Registration of AI Agent using SDK
-----------------------------------------------

Why SDK Agents?[â€‹](#why-sdk-agents "Direct link to Why SDK Agents?")
--------------------------------------------------------------------

While the uAgents framework is great for basic microservices, the Fetch.ai SDK provides enhanced capabilities for building AI agents with greater flexibility and adaptability. Key differences include:

*   **Dynamic Messaging**: Processes flexible or unstructured data, unlike uAgents that require strict data models.
    
*   **Flexible Handlers**: Relies on Flask endpoints instead of predefined handlers, enabling more scalable and customizable communication.
    

These features are especially helpful if your AI agent needs advanced reasoning or can benefit from rapid decision-making and adaptability.

Installing fetchai SDK[â€‹](#installing-fetchai-sdk "Direct link to Installing fetchai SDK")
------------------------------------------------------------------------------------------

Fetch.ai's [SDK](https://pypi.org/project/fetchai/) package is a Python library running on Ubuntu/Debian, macOS, and Windows systems.

On your computer, you may need to install:

*   [Python 3.8+](https://www.python.org/downloads/)
*   [PIP](https://pypi.org/project/pip/) - the Python package manager.
*   [fetchai](https://pypi.org/project/fetchai/) library

### Install with Pip[â€‹](#install-with-pip "Direct link to Install with Pip")

*   Install Fetch.ai uagents library:

*   Check if installation was successful:

### Creating and Registering an AI Agent[â€‹](#creating-and-registering-an-ai-agent "Direct link to Creating and Registering an AI Agent")

In this example, you will create an AI agent identity and register it on Agentverse.

1.  **Create a python script** named `register_agent.py`.

2.  **Add the following code** to `register_agent.py`.

register\_ai\_agent.py

    import osfrom uagents_core.crypto import Identityfrom fetchai.registration import register_with_agentverse# Store your Agentverse API Key in the environment variables. AGENTVERSE_KEY = os.getenv("AGENTVERSE_KEY")# Your Agent's unique key for generating an address on Agentverseai_identity = Identity.from_seed(os.getenv("AGENT_SECRET_KEY"), 0)# Give your Agent a name. This allows you to easily identify one# of your Agents from other agents on Agentverse.name = "My AI's Name"# This is how you optimize your Agent's search engine performancereadme = """![domain:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)domain:domain-of-your-agent<description>My AI's description of capabilities and offerings</description><use_cases>    <use_case>An example of one of your AI's use cases.</use_case></use_cases><payload_requirements><description>The requirements your AI has for requests</description><payload>    <requirement>        <parameter>question</parameter>        <description>The question that you would like this AI work with you to solve</description>    </requirement></payload></payload_requirements>"""# The webhook that your AI receives messages on.ai_webhook = "https://api.sampleurl.com/webhook"register_with_agentverse(    ai_identity,    ai_webhook,    AGENTVERSE_KEY,    name,    readme,)

3.  Set up environment variables in a `.env` file (or similar approach):

    AGENTVERSE_KEY='YOUR_AGENTVERSE_API_KEY'AGENT_SECRET_KEY='YOUR_RANDOM_SECRET_SEED'

*   The `AGENTVERSE_KEY` is obtained from your Agentverse account.
*   The `AGENT_SECRET_KEY` should be a unique, random phrase to ensure your agent consistently generates the same address.

4.  Run the Script

    python3 regsiter_agent.py

*   **Sample Output:**

    INFO:fetchai:Registering with Almanac APIINFO:fetchai:Successfully registered as custom agent in AgentverseINFO:fetchai:Completed registering agent with Agentverse

### Checking Your Agent on Agentverse[â€‹](#checking-your-agent-on-agentverse "Direct link to Checking Your Agent on Agentverse")

1.  **Go to the Agents tab** in the Agentverse interface.
    
2.  **Select â€œLocal Agentsâ€** and look for the name used in your script (e.g., â€œMy AIâ€™s Nameâ€).
    

3.  **Click on the agent name** to view its details (address, registration data, etc.).

You have successfully created and registered an AI agent with the Fetch.ai SDK. You can now integrate this agent into your applications, allowing it to interact with other agents or services via Agentverse.

#### Next Steps:[â€‹](#next-steps "Direct link to Next Steps:")

*   Further develop your agentâ€™s code and logic.
*   Integrate custom Flask endpoints to handle advanced communication.
*   Explore additional SDK features (transaction handling, smart contracts, etc.) to expand the agentâ€™s capabilities.</content>
</page>

<page>
  <title>End to End Application with Agentverse Architecture | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.2/agentverse/agentverse-based-application</url>
  <content>This diagram depicts an agent-based architecture, where the Client Application interacts with a Prime Agent to discover and utilize various specialized Agents registered within the AgentVerse. The Prime Agent orchestrates the communication between the Client and the Agents to fulfill the client's requests.

The flow of the system is as follows:

1.  The client application searches the AgentVerse to discover available agents.
2.  The client application sends a request to the Prime Agent.
3.  The Prime Agent communicates with the AgentVerse to find the appropriate Agent.
4.  The Prime Agent sends the request to the Agent.
5.  The Agent processes the request and sends the response back to the Prime Agent.
6.  The Prime Agent returns the final response to the client application.

This architecture allows for a modular and extensible system, where new agents can be easily integrated, and the Prime Agent can orchestrate the interactions between agents to fulfill complex client requests.

Core Components and Implementation[â€‹](#core-components-and-implementation "Direct link to Core Components and Implementation")
------------------------------------------------------------------------------------------------------------------------------

The system enables intelligent financial analysis through a team of specialized agents, coordinated via Agentverse. It combines frontend user interaction with backend processing through multiple agent layers.

### 1\. React Frontend (Client Application)[â€‹](#1-react-frontend-client-application "Direct link to 1. React Frontend (Client Application)")

*   React-based user interface (OptimusPrime component)
*   Two main API endpoints:
    *   `/api/send-request`: Sends analysis queries
    *   `/api/get-response`: Polls for results
*   Handles message display and user interactions

### 2\. Primary Agent (Query Router)[â€‹](#2-primary-agent-query-router "Direct link to 2. Primary Agent (Query Router)")

*   **Port**: 5001
*   **Role**: Routes queries to Financial Analysis Agent
*   **Key Functions**:
    *   Discovers Financial Analysis Agent through Agentverse
    *   Forwards user queries
    *   Manages response polling
*   **Endpoints**:
    *   `/webhook`: Receives responses from Financial Agent

### 3\. Financial Analysis Agent[â€‹](#3-financial-analysis-agent "Direct link to 3. Financial Analysis Agent")

*   **Port**: 5008
*   **Role**: Processes financial analysis requests
*   **Components**:
    *   Supervisor Agent: Coordinates analysis
    *   Search Agent: Handles market research
    *   SEC Analyst: Processes SEC filings
*   **Tools**:
    *   RAG System for document analysis
    *   Tavily Search for market data

### 4\. Agentverse Integration[â€‹](#4-agentverse-integration "Direct link to 4. Agentverse Integration")

*   **Agent Discovery**: Allows finding agents by capability
*   **Agent Registry**: Manages agent registration and lookup
*   **Message Routing**: Handles inter-agent communication

Communication Flow[â€‹](#communication-flow "Direct link to Communication Flow")
------------------------------------------------------------------------------

1.  User submits query through React UI
2.  Primary Agent searches for Financial Analysis Agent
3.  Primary Agent forwards query to Financial Agent
4.  Financial Agent processes query using specialist team
5.  Response returns through Agentverse to Primary Agent
6.  Frontend polls Primary Agent for results
7.  Results displayed to user

Key Implementation Details[â€‹](#key-implementation-details "Direct link to Key Implementation Details")
------------------------------------------------------------------------------------------------------

### Frontend[â€‹](#frontend "Direct link to Frontend")

    // Sends request and starts pollingconst handleSendMessage = async () => {    await fetch('/api/send-request', {...});    startPolling();  // Begin checking for response};

### Primary Agent[â€‹](#primary-agent "Direct link to Primary Agent")

    # Agent discovery and routingdef find_financial_agent():    available_ais = fetch.ai("Financial Analysis Agent")    return available_ais.get('ais', [0])@app.route('/webhook', methods=['POST'])def webhook():    message = parse_message_from_agent(data)    primary_agent.latest_response = message.payload

### Financial Agent registration with Agentverse[â€‹](#financial-agent-registration-with-agentverse "Direct link to Financial Agent registration with Agentverse")

    # Agent registrationregister_with_agentverse(    identity=financial_identity,    url="http://localhost:5008/webhook",    agent_title="Financial Analysis Agent",    readme="..."  # Capabilities description)

Detailed Implementation Details
-------------------------------

1\. Frontend Implementation (React)[â€‹](#1-frontend-implementation-react "Direct link to 1. Frontend Implementation (React)")
----------------------------------------------------------------------------------------------------------------------------

### Message Handling and UI State[â€‹](#message-handling-and-ui-state "Direct link to Message Handling and UI State")

    const OptimusPrime = () => {    const [messages, setMessages] = useState([]);    const [inputText, setInputText] = useState('');    const [isProcessing, setIsProcessing] = useState(false);    // Handles submitting new messages    const handleSendMessage = async () => {        if (!inputText.trim() || isProcessing) return;        // Add user message to UI        const userMessage = {            type: 'user',            content: inputText,            timestamp: new Date().toLocaleTimeString()        };        setMessages(prev => [...prev, userMessage]);        setInputText('');        setIsProcessing(true);        try {            // Send request to primary agent                        await fetch('/api/send-request', {                method: 'POST',                headers: { 'Content-Type': 'application/json' },                body: JSON.stringify({ input: inputText }),            });            // Start polling for response            startPollingForResponse();        } catch (error) {            handleError(error);        }    };

### Response Polling System[â€‹](#response-polling-system "Direct link to Response Polling System")

        // Polls for agent response    const startPollingForResponse = () => {        const pollInterval = setInterval(async () => {            try {                const responseData = await fetch('/api/get-response');                const data = await responseData.json();                if (data.status !== 'waiting' && data.analysis_result) {                    clearInterval(pollInterval);                    setIsProcessing(false);                    // Process agent responses                    data.analysis_result.analysis.forEach(response => {                        setMessages(prev => [...prev, {                            type: 'agent',                            agentName: response.name || 'Agent',                            content: response.content,                            timestamp: new Date().toLocaleTimeString()                        }]);                    });                }            } catch (error) {                clearInterval(pollInterval);                setIsProcessing(false);                handleError(error);            }        }, 1000);    };

2\. Primary Agent Implementation[â€‹](#2-primary-agent-implementation "Direct link to 2. Primary Agent Implementation")
---------------------------------------------------------------------------------------------------------------------

### Agent Setup and Initialization[â€‹](#agent-setup-and-initialization "Direct link to Agent Setup and Initialization")

    class PrimaryAgent:    def __init__(self):        self.identity = None        self.latest_response = None        def initialize(self):        try:            # Initialize agent identity            self.identity = Identity.from_seed(                os.getenv("PRIMARY_AGENT_KEY"),                 0            )                        # Register with Agentverse            register_with_agentverse(                identity=self.identity,                url="http://localhost:5001/webhook",                agentverse_token=os.getenv("AGENTVERSE_API_KEY"),                agent_title="Financial Query Router",                readme="<description>Routes queries to Financial Analysis Agent</description>"            )        except Exception as e:            logger.error(f"Initialization error: {e}")            raise

### Financial Agent Discovery and Communication[â€‹](#financial-agent-discovery-and-communication "Direct link to Financial Agent Discovery and Communication")

        def find_financial_agent(self):        """Find registered financial analysis agent"""        try:            # Search for financial agent in Agentverse            available_ais = fetch.ai("Financial Analysis Agent")            agents = available_ais.get('ais', [])                        if agents:                logger.info(f"Found financial agent at address: {agents[0]['address']}")                return agents[0]            return None                    except Exception as e:            logger.error(f"Error finding financial agent: {e}")            return None    @app.route('/api/send-request', methods=['POST'])    def send_request():        try:            # Extract user query            data = request.json            user_input = data.get('input')                        # Find and validate financial agent            agent = primary_agent.find_financial_agent()            if not agent:                return jsonify({"error": "Financial analysis agent not available"}), 404                        # Forward request to financial agent            send_message_to_agent(                primary_agent.identity,                agent['address'],                {"request": user_input}            )                        return jsonify({"status": "request_sent"})                    except Exception as e:            logger.error(f"Error processing request: {e}")            return jsonify({"error": str(e)}), 500

### Response Management[â€‹](#response-management "Direct link to Response Management")

        @app.route('/webhook', methods=['POST'])    def webhook():        try:            # Parse incoming agent message            data = request.get_data().decode("utf-8")            message = parse_message_from_agent(data)                        # Store response for polling            primary_agent.latest_response = message.payload                        return jsonify({"status": "success"})                    except Exception as e:            logger.error(f"Error in webhook: {e}")            return jsonify({"error": str(e)}), 500    @app.route('/api/get-response', methods=['GET'])    def get_response():        try:            if primary_agent.latest_response:                response = primary_agent.latest_response                primary_agent.latest_response = None                return jsonify(response)            return jsonify({"status": "waiting"})        except Exception as e:            logger.error(f"Error getting response: {e}")            return jsonify({"error": str(e)}), 500

3\. Financial Analysis Agent Registration[â€‹](#3-financial-analysis-agent-registration "Direct link to 3. Financial Analysis Agent Registration")
------------------------------------------------------------------------------------------------------------------------------------------------

### Agent Registration and Setup[â€‹](#agent-registration-and-setup "Direct link to Agent Registration and Setup")

    def init_agent():    """Initialize and register the agent with agentverse"""    global financial_identity, research_chain    try:        # Initialize the research chain        research_chain = init_financial_system()                # Initialize identity and register with Agentverse        financial_identity = Identity.from_seed(            os.getenv("FINANCIAL_AGENT_KEY"),             0        )                # Register with detailed capabilities description        register_with_agentverse(            identity=financial_identity,            url="http://localhost:5008/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="Financial Analysis Agent",            readme = """                <description>A comprehensive financial analysis agent that combines                 SEC filing analysis with real-time market data for Apple Inc.</description>                <use_cases>                    <use_case>Get detailed revenue analysis from SEC filings</use_case>                    <use_case>Analyze risk factors from latest 10-K</use_case>                    <use_case>Track financial metrics and trends</use_case>                </use_cases>                <payload_requirements>                    <payload>                        <requirement>                            <parameter>query</parameter>                            <description>What would you like to know about Apple's financials?</description>                        </requirement>                    </payload>                </payload_requirements>            """        )

### Query Processing and Response[â€‹](#query-processing-and-response "Direct link to Query Processing and Response")

    @app.route('/webhook', methods=['POST'])def webhook():    try:        # Parse incoming message        data = request.get_data().decode('utf-8')        message = parse_message_from_agent(data)        query = message.payload.get("request", "")        agent_address = message.sender        # Validate query        if not query:            return jsonify({"status": "error", "message": "No query provided"}), 400        # Process query using research chain        result = research_chain.invoke({            "messages": [HumanMessage(content=query)],            "team_members": ["Search", "SECAnalyst"]        })        # Format response for client        formatted_result = {            "analysis": [                {                    "role": msg.type if hasattr(msg, 'type') else "message",                    "content": msg.content,                    "name": msg.name if hasattr(msg, 'name') else None                }                for msg in result.get('messages', [])            ]        }        # Send response back through Agentverse        send_message_to_agent(            financial_identity,            agent_address,            {'analysis_result': formatted_result}        )        return jsonify({"status": "analysis_sent"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"status": "error", "message": str(e)}), 500</content>
</page>

<page>
  <title>Search Agent and MicroServices | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.2/agentverse/searching</url>
  <content>Searching microservices and AI Agents on Agentverse
---------------------------------------------------

When you want to discover or connect with microservices or AI agents dynamically on Agentverse, you can use the \_\_Agentverse Search AP\_\_I. Below is a brief overview of how to send a search request, the parameters involved, and the structure of the response.

Making a Search Request[â€‹](#making-a-search-request "Direct link to Making a Search Request")
---------------------------------------------------------------------------------------------

*   Python
*   Curl
*   JavaScript

    body = {    "filters": {        "state": [],        "category": [],        "agent_type": [],        "protocol_digest": []    },    "sort": "relevancy",    "direction": "asc",    "search_text": "<string>",    "offset": 0,    "limit": 1,}await fetch("https://agentverse.ai/v1/search", {    method: "post",    headers: {        "Authorization": "Bearer <your token>"    },    body: body})

Response[â€‹](#response "Direct link to Response")
------------------------------------------------

You will receive a list of JSON objects with details about each agent:

    [  {    "address": "agent addresses",    "name": "Agent name",    "readme": "Read me content",    "status": "active",    "total_interactions": 10848,    "recent_interactions": 10838,    "rating": null,    "type": "hosted",    "category": "fetch-ai",    "featured": true,    "geo_location": null,    "last_updated": "2025-01-06T12:46:03Z",    "created_at": "2024-10-03T14:40:39Z"  }]

### Available Filters[â€‹](#available-filters "Direct link to Available Filters")

*   **state** : `active`, `inactive`.
*   **category** : `fetch-ai`, `community`.
*   **agent\_type** : `hosted`, `local`, `mailbox`, `proxy`, `custom`.
*   **protocol\_digest** : The protocol in which agent is included into.
*   **model\_digest** : Model digest in which agent is included into.

### Importance of Good Readme[â€‹](#importance-of-good-readme "Direct link to Importance of Good Readme")

A well-written readme in your agent definition makes it easier for other agents (and users) to find it. Make sure you:

*   Include descriptive names, tags, or domains. You can mention `[tags : ]` and `[domain : ]` in your agent.
    
*   Describe the main functions or services the agent provides.
    
*   Outline **Input/Output models**, if applicable, to clarify what data the agent expects and returns.
    
    *   For SDK AI Agent
    
        ![domain:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)domain:domain-of-your-agent<description>My AI's description of capabilities and offerings</description><use_cases>    <use_case>An example of one of your AI's use cases.</use_case></use_cases><payload_requirements><description>The requirements your AI has for requests</description><payload>    <requirement>        <parameter>question</parameter>        <description>The question that you would like this AI work with you to solve</description>    </requirement></payload></payload_requirements>
    
    *   For uAgents
    
        ![tag:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)domain:domain-of-your-agent**Description**:  This AI Agent retrieves real-time stock prices for any publicly traded company based on its ticker symbol. It provides  share prices, stock quotes, and stock prices to users. Simply input a stock ticker (e.g., AAPL, TSLA)  to get the latest stock price.**Input Data Model**class StockPriceRequest(Model):    ticker: str**Output Data Model**class StockPriceResponse(Model):    price: float
    

By following these guidelines, you can improve your agentâ€™s visibility in search results and help others understand its capabilities and usage requirements.

note

**Note:** If you are creating your agents in **`Hackathon`**, do remember to include the innovation labs tags.

    ![domain:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)</content>
</page>

<page>
  <title>AI Agent Creation | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.1/agent-creation/sdk-creation</url>
  <content>Creation and Registration of AI Agent using SDK
-----------------------------------------------

Why SDK Agents?[â€‹](#why-sdk-agents "Direct link to Why SDK Agents?")
--------------------------------------------------------------------

While the uAgents framework is great for basic microservices, the Fetch.ai SDK provides enhanced capabilities for building AI agents with greater flexibility and adaptability. Key differences include:

*   **Dynamic Messaging**: Processes flexible or unstructured data, unlike uAgents that require strict data models.
    
*   **Flexible Handlers**: Relies on Flask endpoints instead of predefined handlers, enabling more scalable and customizable communication.
    

These features are especially helpful if your AI agent needs advanced reasoning or can benefit from rapid decision-making and adaptability.

Installing fetchai SDK[â€‹](#installing-fetchai-sdk "Direct link to Installing fetchai SDK")
------------------------------------------------------------------------------------------

Fetch.ai's [SDK](https://pypi.org/project/fetchai/) package is a Python library running on Ubuntu/Debian, macOS, and Windows systems.

On your computer, you may need to install:

*   [Python 3.8+](https://www.python.org/downloads/)
*   [PIP](https://pypi.org/project/pip/) - the Python package manager.
*   [fetchai](https://pypi.org/project/fetchai/) library

### Install with Pip[â€‹](#install-with-pip "Direct link to Install with Pip")

*   Install Fetch.ai uagents library:

*   Check if installation was successful:

### Creating and Registering an AI Agent[â€‹](#creating-and-registering-an-ai-agent "Direct link to Creating and Registering an AI Agent")

In this example, you will create an AI agent identity and register it on Agentverse.

1.  **Create a python script** named `register_agent.py`.

2.  **Add the following code** to `register_agent.py`.

register\_ai\_agent.py

    import osfrom uagents_core.crypto import Identityfrom fetchai.registration import register_with_agentverse# Store your Agentverse API Key in the environment variables. AGENTVERSE_KEY = os.getenv("AGENTVERSE_KEY")# Your Agent's unique key for generating an address on Agentverseai_identity = Identity.from_seed(os.getenv("AGENT_SECRET_KEY"), 0)# Give your Agent a name. This allows you to easily identify one# of your Agents from other agents on Agentverse.name = "My AI's Name"# This is how you optimize your Agent's search engine performancereadme = """![domain:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)domain:domain-of-your-agent<description>My AI's description of capabilities and offerings</description><use_cases>    <use_case>An example of one of your AI's use cases.</use_case></use_cases><payload_requirements><description>The requirements your AI has for requests</description><payload>    <requirement>        <parameter>question</parameter>        <description>The question that you would like this AI work with you to solve</description>    </requirement></payload></payload_requirements>"""# The webhook that your AI receives messages on.ai_webhook = "https://api.sampleurl.com/webhook"register_with_agentverse(    ai_identity,    ai_webhook,    AGENTVERSE_KEY,    name,    readme,)

3.  Set up environment variables in a `.env` file (or similar approach):

    AGENTVERSE_KEY='YOUR_AGENTVERSE_API_KEY'AGENT_SECRET_KEY='YOUR_RANDOM_SECRET_SEED'

*   The `AGENTVERSE_KEY` is obtained from your Agentverse account.
*   The `AGENT_SECRET_KEY` should be a unique, random phrase to ensure your agent consistently generates the same address.

4.  Run the Script

    python3 regsiter_agent.py

*   **Sample Output:**

    INFO:fetchai:Registering with Almanac APIINFO:fetchai:Successfully registered as custom agent in AgentverseINFO:fetchai:Completed registering agent with Agentverse

### Checking Your Agent on Agentverse[â€‹](#checking-your-agent-on-agentverse "Direct link to Checking Your Agent on Agentverse")

1.  **Go to the Agents tab** in the Agentverse interface.
    
2.  **Select â€œLocal Agentsâ€** and look for the name used in your script (e.g., â€œMy AIâ€™s Nameâ€).
    

3.  **Click on the agent name** to view its details (address, registration data, etc.).

You have successfully created and registered an AI agent with the Fetch.ai SDK. You can now integrate this agent into your applications, allowing it to interact with other agents or services via Agentverse.

#### Next Steps:[â€‹](#next-steps "Direct link to Next Steps:")

*   Further develop your agentâ€™s code and logic.
*   Integrate custom Flask endpoints to handle advanced communication.
*   Explore additional SDK features (transaction handling, smart contracts, etc.) to expand the agentâ€™s capabilities.</content>
</page>

<page>
  <title>Mettalex - A Practical Implementation of AI Agents in the Web3 Ecosystem | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.2/examples/on-chain-examples/mettalex-agents</url>
  <content>Mettalex: A Practical Implementation of AI Agents in the Web3 Ecosystem - Powered by Fetch.ai Agent Tech
--------------------------------------------------------------------------------------------------------

Introduction[â€‹](#introduction "Direct link to Introduction")
------------------------------------------------------------

[Mettalex](https://www.mettalex.ai/) stands out as the **first P2P orderbook** and **agent-based DEX** for commodity and digital (tokenized) assets trading. It harnesses **Fetch.aiâ€™s uAgents** to power **autonomous order matching**, **secure on-chain escrow**, and **cross-chain operations**. By eliminating reliance on centralized order books or liquidity pools, Mettalex aims to offer **slippage-free**, trustless trades with **maximum transparency**.

Key uAgents features in this use case include their **wallet- and chain-agnostic** capabilities. Additionally, the agents provide a robust **communication** and **execution** layer, making the trading process seamless and efficient.

Mettalex: Agent-Based Commodity Trading in a Nutshell[â€‹](#mettalex-agent-based-commodity-trading-in-a-nutshell "Direct link to Mettalex: Agent-Based Commodity Trading in a Nutshell")
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

1.  Direct P2P Order Matching
    
    *   **No Liquidity Pools:** Mettalex uses uAgents to match buyers and sellers directly, eliminating AMMs and reducing slippage.
    *   **Zero Slippage Execution:** Final prices match exactly what each party has agreed toâ€”crucial for volatile or low-liquidity commodity markets.
2.  Escrow-Backed Settlement
    
    *   **On-Chain Escrow:** Traders lock funds in a smart contract. The agent only completes a trade if both sides have met the exact terms.
    *   **Fail-Safe Mechanism:** If either side fails to finalize the transaction, agents revert the escrow to protect user funds.
3.  Multi-Wallet & Cross-Chain Support
    
    *   **Wallet-Agnostic:** Users may use MetaMask, Ledger, or any Web3-compatible wallet. The agent logic remains the same, ensuring a uniform trading experience.
    *   **Chain-Agnostic:** Mettalex agents can run on multiple blockchains in parallel (e.g., Ethereum, BNB Chain, Cosmos). They coordinate escrow locks and trades across networks if bridging solutions exist.
4.  On-Chain Registration & Discovery
    
    *   **Almanac Registry:** Agents register on a Cosmos-based on-chain Almanac contract, allowing other agents/dApps to verify their identity.
5.  Transparent Execution & Governance
    
    *   **Public Transaction Logs:** Every escrow creation, signature, and fund release is recorded on-chain. Users can view agent logs and track progress in real time.
    *   **MTLX Governance:** Mettalexâ€™s governance token (MTLX) can let agents automate fee changes or protocol upgrades, broadening the platformâ€™s agent-driven ecosystem.

Putting It All Together[â€‹](#putting-it-all-together "Direct link to Putting It All Together")
---------------------------------------------------------------------------------------------

*   User places a trade -> Funds locked in escrow.
*   Agent checks buyer/seller positions -> Confirms each sideâ€™s escrow.
*   Agents sign trade parameters on-chain -> Escrows release funds upon successful match.
*   Settlement -> Both parties receive respective assets with no slippage and full transparency.

If you wish to learn more about Mettalex, please visit [Mettalex Docs](https://www.mettalex.com/docs).</content>
</page>

<page>
  <title>AI Agent Creation | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.0/agent-creation/sdk-creation</url>
  <content>Creation and Registration of AI Agent using SDK
-----------------------------------------------

Why SDK Agents?[â€‹](#why-sdk-agents "Direct link to Why SDK Agents?")
--------------------------------------------------------------------

While the uAgents framework is great for basic microservices, the Fetch.ai SDK provides enhanced capabilities for building AI agents with greater flexibility and adaptability. Key differences include:

*   **Dynamic Messaging**: Processes flexible or unstructured data, unlike uAgents that require strict data models.
    
*   **Flexible Handlers**: Relies on Flask endpoints instead of predefined handlers, enabling more scalable and customizable communication.
    

These features are especially helpful if your AI agent needs advanced reasoning or can benefit from rapid decision-making and adaptability.

Installing fetchai SDK[â€‹](#installing-fetchai-sdk "Direct link to Installing fetchai SDK")
------------------------------------------------------------------------------------------

Fetch.ai's [SDK](https://pypi.org/project/fetchai/) package is a Python library running on Ubuntu/Debian, macOS, and Windows systems.

On your computer, you may need to install:

*   [Python 3.8+](https://www.python.org/downloads/)
*   [PIP](https://pypi.org/project/pip/) - the Python package manager.
*   [fetchai](https://pypi.org/project/fetchai/) library

### Install with Pip[â€‹](#install-with-pip "Direct link to Install with Pip")

*   Install Fetch.ai uagents library:

*   Check if installation was successful:

### Creating and Registering an AI Agent[â€‹](#creating-and-registering-an-ai-agent "Direct link to Creating and Registering an AI Agent")

In this example, you will create an AI agent identity and register it on Agentverse.

1.  **Create a python script** named `register_agent.py`.

2.  **Add the following code** to `register_agent.py`.

register\_ai\_agent.py

    import osfrom uagents_core.crypto import Identityfrom fetchai.registration import register_with_agentverse# Store your Agentverse API Key in the environment variables. AGENTVERSE_KEY = os.getenv("AGENTVERSE_KEY")# Your Agent's unique key for generating an address on Agentverseai_identity = Identity.from_seed(os.getenv("AGENT_SECRET_KEY"), 0)# Give your Agent a name. This allows you to easily identify one# of your Agents from other agents on Agentverse.name = "My AI's Name"# This is how you optimize your Agent's search engine performancereadme = """![domain:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)domain:domain-of-your-agent<description>My AI's description of capabilities and offerings</description><use_cases>    <use_case>An example of one of your AI's use cases.</use_case></use_cases><payload_requirements><description>The requirements your AI has for requests</description><payload>    <requirement>        <parameter>question</parameter>        <description>The question that you would like this AI work with you to solve</description>    </requirement></payload></payload_requirements>"""# The webhook that your AI receives messages on.ai_webhook = "https://api.sampleurl.com/webhook"register_with_agentverse(    ai_identity,    ai_webhook,    AGENTVERSE_KEY,    name,    readme,)

3.  Set up environment variables in a `.env` file (or similar approach):

    AGENTVERSE_KEY='YOUR_AGENTVERSE_API_KEY'AGENT_SECRET_KEY='YOUR_RANDOM_SECRET_SEED'

*   The `AGENTVERSE_KEY` is obtained from your Agentverse account.
*   The `AGENT_SECRET_KEY` should be a unique, random phrase to ensure your agent consistently generates the same address.

4.  Run the Script

    python3 regsiter_agent.py

*   **Sample Output:**

    INFO:fetchai:Registering with Almanac APIINFO:fetchai:Successfully registered as custom agent in AgentverseINFO:fetchai:Completed registering agent with Agentverse

### Checking Your Agent on Agentverse[â€‹](#checking-your-agent-on-agentverse "Direct link to Checking Your Agent on Agentverse")

1.  **Go to the Agents tab** in the Agentverse interface.
    
2.  **Select â€œLocal Agentsâ€** and look for the name used in your script (e.g., â€œMy AIâ€™s Nameâ€).
    

3.  **Click on the agent name** to view its details (address, registration data, etc.).

You have successfully created and registered an AI agent with the Fetch.ai SDK. You can now integrate this agent into your applications, allowing it to interact with other agents or services via Agentverse.

#### Next Steps:[â€‹](#next-steps "Direct link to Next Steps:")

*   Further develop your agentâ€™s code and logic.
*   Integrate custom Flask endpoints to handle advanced communication.
*   Explore additional SDK features (transaction handling, smart contracts, etc.) to expand the agentâ€™s capabilities.</content>
</page>

<page>
  <title>Solana Agent Integration with Fetch.ai uAgents | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.2/examples/on-chain-examples/solana-agents</url>
  <content>This example shows how to integrate Solana wallets within **Fetch.aiâ€™s uAgents** framework. Weâ€™ll walk through the **EscrowAgent, PlayerAgent, and ChallengerAgent** scripts, detailing how each agent:

1.  Registers with the **Almanac** contract (for discoverability)
2.  Loads Solana **private keys** from environment variables
3.  Executes transfers, checks balances, and handles bet-based business logic via **Solana Devnet**
4.  Communicates with other agents through **uAgents** messaging

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

*   **Solana CLI** (configured to Devnet)
*   **Poetry** (for dependency management)
*   **Python 3.8+**
*   Fetch.aiâ€™s **uagents** library
*   **Solders, requests**, etc. (handled by `poetry install`)
*   `.env` with your Solana private keys (Base64 arrays) for each agent

note

**Note:** Each agent script runs on a different portâ€”EscrowAgent uses `:8000`, PlayerAgent uses `:8001`, and ChallengerAgent uses `:8002` by default.

High-Level Architecture[â€‹](#high-level-architecture "Direct link to High-Level Architecture")
---------------------------------------------------------------------------------------------

1.  **PlayerAgent** & **ChallengerAgent** each place a bet by transferring SOL to the **Escrow** wallet (managed by the EscrowAgent).
2.  **EscrowAgent** collects two bets, checks the BTC price via an external API, and decides a winner. 90% of the total stake is transferred to the winnerâ€™s Solana wallet; the loser forfeits.

Escrow Agent[â€‹](#escrow-agent "Direct link to Escrow Agent")
------------------------------------------------------------

### Overview[â€‹](#overview "Direct link to Overview")

The **EscrowAgent**:

*   Registers on **Almanac** so other agents (Player/Challenger) can discover it
*   Waits for two **escrowRequest** messages
*   Fetches **BTC price from Binance**
*   **Transfers** the correct portion of **SOL** to the winner

### Script Breakdown[â€‹](#script-breakdown "Direct link to Script Breakdown")

**Required Libraries**

    import osimport base58import astfrom uagents import Agent, Context, Modelfrom solders.keypair import Keypairfrom functions import get_latest_btc_price, transfer_solimport time

*   os & ast for environment handling
*   uagents for agent creation
*   solders.keypair for Solana KeyPair
*   functions for helper utilities (price fetch & SOL transfer)

**Key Classes & Models**

    class escrowRequest(Model):    amount: float    price: float    public_key: strclass escrowResponse(Model):    result: str

*   **escrowRequest** holds the userâ€™s desired bet: `amount`, `price`, and the userâ€™s **Solana public key**.
*   **escrowResponse** returns the result to the user: either `"You Won"` or `"You Lost"`.

**Initialization & Identity**

    # Retrieve ESCROW_SECRET_LIST from the .envescrow_secret_key_str = os.getenv('ESCROW_SECRET_LIST')escrow_secret_key_list = ast.literal_eval(escrow_secret_key_str)escrow_secret_key_bytes = bytes(escrow_secret_key_list)escrow_keypair = Keypair.from_bytes(escrow_secret_key_bytes)escrow_pubkey_base58 = base58.b58encode(bytes(escrow_keypair.pubkey())).decode('utf-8')agent = Agent(    name="EscrowAgent",    port=8000,    seed="Escrow Wallet",    endpoint=["http://127.0.0.1:8000/submit"],)

*   We decode the **Solana private key** from `.env`.
*   Create a `Keypair` for the Escrowâ€™s wallet.
*   Instantiate a `uagents.Agent` with the name â€œEscrowAgentâ€ listening on port `8000`.

**On Startup**

    @agent.on_event('startup')async def saf(ctx: Context):    ctx.logger.info("Escrow agent initialized, ready for bids.")    ctx.logger.info(f"Escrow agent address: {agent.address}")    ctx.storage.set("bids_count", 0)

*   Logs that the Escrow is online.
*   Initializes a storage key bids\_count=0 to track how many requests have come in.

**Message Handling**

Receiving Bets

    @agent.on_message(model=escrowRequest, replies={escrowResponse})async def escrow_request_handler(ctx: Context, sender: str, msg: escrowRequest):    current_count = ctx.storage.get("bids_count") or 0    ...    if current_count == 0:        # Store first bet    elif current_count == 1:        # Store second bet        # Compare        # Transfer to winner        # Respond with escrowResponse        # Reset storage

*   **First Bet:** If `bids_count=0`, store the details (amount, price, userâ€™s pubkey).
*   **Second Bet:** If `bids_count=1`, store the second userâ€™s bet, then call `get_latest_btc_price()`.
*   Calculate each userâ€™s distance from the real BTC price.
*   Transfer 90% of the total stake to the winnerâ€™s public key with `transfer_sol()`.
*   Send escrowResponse messages to both the winner `(You Won)` and loser `(You Lost)`.
*   Reset internal storage.

**Running the EscrowAgent**

    if __name__ == "__main__":    agent.run()

*   Save the script as `escrow_agent.py`
*   Simply run `poetry run python escrow_agent.py`.

Player Agent[â€‹](#player-agent "Direct link to Player Agent")
------------------------------------------------------------

### Overview[â€‹](#overview-1 "Direct link to Overview")

The **PlayerAgent** simulates a user placing a bet on BTCâ€™s future price.

### Script Breakdown[â€‹](#script-breakdown-1 "Direct link to Script Breakdown")

**Required Libraries**

    import osimport astimport base58from uagents import Agent, Context, Modelfrom solders.keypair import Keypairfrom functions import check_balance, transfer_solfrom uagents.setup import fund_agent_if_lowcheck_balance and transfer_sol from functions.py to manage SOL balances/transfersfund_agent_if_low from uagents.setup can top up the agentâ€™s fetch-side address if needed

**Key Classes**

    class escrowRequest(Model):    amount: float    price: float    public_key: strclass escrowResponse(Model):    result: str

*   Re-used from the Escrow flow: escrowRequest is how we send the userâ€™s bet to the EscrowAgent.

**Initialization & Identity**

    secret_key_str = os.getenv('PLAYER_SECRET_LIST')secret_key_list = ast.literal_eval(secret_key_str)secret_key_bytes = bytes(secret_key_list)agent_keypair = Keypair.from_bytes(secret_key_bytes)agent_pubkey_base58 = base58.b58encode(bytes(agent_keypair.pubkey())).decode('utf-8')agent = Agent(    name="PlayerAgent",    port=8001,    seed="Player Escrow Wallet 1",    endpoint=["http://127.0.0.1:8001/submit"],)

*   Decodes the **PLAYER\_SECRET\_LIST** from `.env`.
*   Assigns the Playerâ€™s Solana wallet keypair.
*   Sets up a uAgent on port `8001`.

**Startup Sequence**

    @agent.on_event('startup')async def starter_function(ctx: Context):    initial_balance = check_balance(agent_keypair.pubkey())    ctx.logger.info(f"Initial agent balance: {initial_balance} SOL")    amount = float(input('What is the amount of SOL you want to deposit? '))    price = float(input('What is the price of Bitcoin you want to bid at? '))    # Send bet to Escrow    await ctx.send(        'agent1qd6ts50kuy3vqq36s5yg2dkzujq60x0l0sr2acfafnp5zea749yvvvq2qm7',        escrowRequest(amount=amount, price=price, public_key=agent_pubkey_base58)    )    # Transfer SOL to escrowâ€™s base58 key    transfer_result = transfer_sol(agent_keypair, '8WMWFo13At1REkwy5t7ck6sLgCUrJ9dn66mbaccPiJ26', amount)    ctx.logger.info(f"Transfer result: {transfer_result}")    final_balance = check_balance(agent_keypair.pubkey())    ctx.logger.info(f"Final agent balance: {final_balance} SOL")

*   **Check initial SOL** in the userâ€™s wallet.
*   **Ask** for deposit & BTC price guess.
*   **Send** an `escrowRequest` message to the known Escrow agent address.
*   `transfer_sol` to the Escrow agentâ€™s public key.
*   Log final SOL.

**Receiving Escrow Responses**

    @agent.on_message(model=escrowResponse)async def escrow_request_handler(ctx: Context, sender: str, msg: escrowResponse):    balance = check_balance(agent_keypair.pubkey())    ctx.logger.info(f'{msg.result}. Updated account balance: {balance} SOL')

*   When the **EscrowAgent** decides the outcome, it sends an `escrowResponse`.
*   This handler logs either â€œYou Wonâ€ or â€œYou Lostâ€ plus the updated balance.

**Running PlayerAgent**

    if __name__ == "__main__":    agent.run()

*   Save the script as `player_agent.py`
*   Run with poetry `run python player_agent.py`.

Challenger Agent[â€‹](#challenger-agent "Direct link to Challenger Agent")
------------------------------------------------------------------------

### Overview[â€‹](#overview-2 "Direct link to Overview")

Nearly identical to `player_agent.py`, but simulates another user (Challenger) placing a competing bet.

### Script Breakdown[â€‹](#script-breakdown-2 "Direct link to Script Breakdown")

**Required Libraries**

    import osimport astimport base58from uagents import Agent, Context, Modelfrom solders.keypair import Keypairfrom functions import check_balance, transfer_solfrom uagents.setup import fund_agent_if_low

**Key Classes**

    class escrowRequest(Model):    amount: float    price: float    public_key: strclass escrowResponse(Model):    result: str

**Initialization & Startup**

    secret_key_str = os.getenv('CHALLENGER_SECRET_LIST')secret_key_list = ast.literal_eval(secret_key_str)secret_key_bytes = bytes(secret_key_list)agent_keypair = Keypair.from_bytes(secret_key_bytes)agent_pubkey_base58 = base58.b58encode(bytes(agent_keypair.pubkey())).decode('utf-8')agent = Agent(    name="Challenger",    port=8002,    seed="Challenger Escrow Wallet 2",    endpoint=["http://127.0.0.1:8002/submit"],)fund_agent_if_low(agent.wallet.address())@agent.on_event('startup')async def starter_function(ctx: Context):    initial_balance = check_balance(agent_keypair.pubkey())    ctx.logger.info(f"Initial agent balance: {initial_balance} SOL")    amount = float(input('What is the amount of SOL you want to deposit? '))    price = float(input('What is the price of Bitcoin you want to bid at? '))    # Send the bet    await ctx.send(        'agent1qd6ts50kuy3vqq36s5yg2dkzujq60x0l0sr2acfafnp5zea749yvvvq2qm7',        escrowRequest(amount=amount, price=price, public_key=agent_pubkey_base58)    )    # Transfer SOL    transfer_result = transfer_sol(agent_keypair, '8WMWFo13At1REkwy5t7ck6sLgCUrJ9dn66mbaccPiJ26', amount)    ctx.logger.info(f"Transfer result: {transfer_result}")    final_balance = check_balance(agent_keypair.pubkey())    ctx.logger.info(f"Final agent balance: {final_balance} SOL")@agent.on_message(model=escrowResponse)async def escrow_request_handler(ctx: Context, sender: str, msg: escrowResponse):    balance = check_balance(agent_keypair.pubkey())    ctx.logger.info(f'{msg.result}. Updated account balance: {balance} SOL')if __name__ == "__main__":    agent.run()

*   Exactly the same flow: read user input, transfer SOL, wait for a response from the Escrow agent.

Utility Script : functions.py[â€‹](#utility-script--functionspy "Direct link to Utility Script : functions.py")
-------------------------------------------------------------------------------------------------------------

Below is a brief summary of the key utility functions. For full code, see the repository.

    import base58from solders.keypair import Keypairfrom solders.pubkey import Pubkeyfrom solders.transaction import Transactionfrom solders.system_program import TransferParams, transferfrom solana.rpc.api import Clientfrom solana.rpc.types import TxOptsimport requestsdef get_keypair_details(secret_key_list):    """    Given a list of secret key bytes (integers), returns a dictionary with the keypair, public key,    private key in bytes, and private key in Base58 encoding.    """    # Convert the list of integers into a bytes object (private key)    secret_key_bytes = bytes(secret_key_list)    # Restore the Keypair using the secret key    keypair = Keypair.from_bytes(secret_key_bytes)    # Public key (from keypair)    public_key = keypair.pubkey()    # Private key in Base58 encoding (for readability)    private_key_base58 = base58.b58encode(secret_key_bytes).decode()    # Return all necessary details in a dictionary    return {        "keypair": keypair,        "public_key": public_key,  # Solders Pubkey object        "private_key_bytes": secret_key_bytes,  # Private key in bytes        "private_key_base58": private_key_base58  # Private key in Base58    }client = Client("https://api.devnet.solana.com")# Function to check balancedef check_balance(pubkey):    balance_resp = client.get_balance(Pubkey.from_bytes(bytes(pubkey)))    print(f'balance_resp :{balance_resp}')    balance = balance_resp.value  # Extract balance in lamports    return balance / 1_000_000_000  # Convert lamports to SOLdef transfer_sol(from_keypair, to_pubkey_base58, amount_sol):    # Convert SOL to lamports (1 SOL = 1 billion lamports)    lamports = int(amount_sol * 1_000_000_000)    # Convert the recipient's Base58 public key string to a Pubkey object    to_pubkey = Pubkey.from_string(to_pubkey_base58)    # Get latest blockhash    blockhash_resp = client.get_latest_blockhash()    recent_blockhash = blockhash_resp.value.blockhash    # Create a transfer instruction    transfer_instruction = transfer(        TransferParams(from_pubkey=from_keypair.pubkey(), to_pubkey=to_pubkey, lamports=lamports)    )    # Create the transaction with the instruction directly    transaction = Transaction.new_signed_with_payer(        [transfer_instruction],  # Pass the list of instructions directly        from_keypair.pubkey(),  # Fee-payer (challenger)        [from_keypair],  # Signers (challenger)        recent_blockhash  # Use recent blockhash directly    )    # Send the transaction    result = client.send_raw_transaction(bytes(transaction), opts=TxOpts(skip_confirmation=False))    return resultdef get_latest_btc_price():    try:        # Binance API endpoint for fetching the latest BTC price in USDT        url = 'https://api.binance.com/api/v3/ticker/price?symbol=BTCUSDT'        response = requests.get(url)        response.raise_for_status()  # Raise an error for bad responses        data = response.json()        return float(data['price'])  # Return the latest BTC price as a float    except requests.exceptions.RequestException as e:        print(f"Error fetching BTC price: {e}")        return None

.env File Example[â€‹](#env-file-example "Direct link to .env File Example")
--------------------------------------------------------------------------

    # .envAGENTVERSE_API_KEY="<Your_FetchAI_Agentverse_Token>"PLAYER_SECRET_LIST="[79,79,237,8,87,104,75,156,47,204,53,127,171,9,114,244,...]"CHALLENGER_SECRET_LIST="[134,53,148,91,88,30,254,53,171,183,219,91,33,67,24,9,65,...]"ESCROW_SECRET_LIST="[251,164,58,0,121,167,133,83,114,82,162,22,88,214,195,91,82,...]"

Ensure each secret list matches the integer arrays from your `player-wallet.json`, `challenger-wallet.json`, and `escrow-wallet.json`. Also, do not commit your `.env` to source control.

Steps to Run the Agents[â€‹](#steps-to-run-the-agents "Direct link to Steps to Run the Agents")
---------------------------------------------------------------------------------------------

1.  Set Up Virtual Environment & Dependencies

2.  Fund Each Wallet on Devnet

    solana airdrop 5 <PLAYER_PUBKEY> --url devnetsolana airdrop 5 <CHALLENGER_PUBKEY> --url devnetsolana airdrop 5 <ESCROW_PUBKEY> --url devnet

3.  Start EscrowAgent

    poetry run python escrow_agent.py

*   Wait for it to display Escrow agent initialized, ready for bids.

4.  Start PlayerAgent

    poetry run python player_agent.py

*   Input the deposit amount & BTC guess when prompted.

5.  Start ChallengerAgent

    poetry run python challenger_agent.py

*   Similarly input deposit & guess. Once the Escrow receives the second bet, it decides a winner.

### Sample Output[â€‹](#sample-output "Direct link to Sample Output")

**EscrowAgent:**

    INFO: [EscrowAgent]: Escrow agent initialized, ready for bids.INFO: [EscrowAgent]: Received escrowRequest messageINFO: [EscrowAgent]: Storing first request ...INFO: [EscrowAgent]: Received escrowRequest messageINFO: [EscrowAgent]: Storing second request ...INFO: [EscrowAgent]: Processing bids to determine the winner.INFO: [EscrowAgent]: First difference: 1820.0, Second difference: 586820.0INFO: [EscrowAgent]: Transferring 0.9 SOL to winner ...INFO: [EscrowAgent]: Notifying winner and loser.

**PlayerAgent:**

    What is the amount of SOL you want to deposit? 0.5What is the price of Bitcoin you want to bid at? 65000INFO: [PlayerAgent]: Transfer result: ...INFO: [PlayerAgent]: Final agent balance: 4.31998 SOLINFO: [PlayerAgent]: You Won. Updated account balance: 5.21998 SOL

Debugging Common Issues[â€‹](#debugging-common-issues "Direct link to Debugging Common Issues")
---------------------------------------------------------------------------------------------

1.  Key Decoding Errors
    
    *   Ensure your `.env` secret lists are valid JSON arrays of integers.
    *   If you see `ValueError` or `Cannot decode secret key`, confirm you have no trailing commas.
2.  Faucet or Balance Issues
    
    *   Double-check `solana balance <PUBKEY> --url devnet`. If less then 1 SOL, some transactions might fail due to insufficient lamports for fees.
3.  Agent Registration Problems
    
    *   Confirm **AGENTVERSE\_API\_KEY** is correct in your `.env`.
    *   Make sure each agent can reach the default Almanac endpoint (requires internet connection).
4.  BTC Price Fetch Errors
    
    *   If Binance is unreachable or rate-limits your IP, consider adding retry logic or a fallback endpoint.

By following this **Solana + uAgents** guide, youâ€™ve set up three distinct agents that:

*   Load private keys from .env
*   Register on Fetch.aiâ€™s Almanac for discovery
*   Communicate using @agent.on\_message and typed models (escrowRequest, escrowResponse)
*   Interact with Solana Devnet for safe, low-cost experimentation

This architecture can be extended for **NFT auctions, DeFi ops, cross-chain bridging**, or any scenario where you need **agent-driven** logic plus on-chain Solana transactions. Enjoy building!

note

**Note:** GitHub repository for this example is available [here](https://github.com/abhifetch/solana-fetch-uagents-integration).</content>
</page>

<page>
  <title>ASI-1 Mini Examples | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.2/examples/asi-mini-example</url>
  <content>ASI1-Mini Example Repositories
------------------------------

Below is a **brief overview** of various open-source repositories demonstrating how to integrate and use **ASI1-Mini** in different scenarios, from simple scripts to advanced multi-agent systems.

**GitHub**: [ASI-1-Mini-simple-Examples](https://github.com/abhifetch/ASI-1-Mini-simple-Examples)  
A **collection of simple agents** built with the uAgents framework. Each script focuses on a specialized task (language tutor, LeetCode solver, fun fact generator, etc.) and is powered by **ASI1-Mini**.

2\. langchain-asi[â€‹](#2-langchain-asi "Direct link to 2. langchain-asi")
------------------------------------------------------------------------

**GitHub**: [langchain-asi integration](https://github.com/rajashekarcs2023/langchain-asi)  
A **lightweight integration package** connecting ASI1â€™s API with LangChain. Ideal if you want to easily swap out other LLM providers for **ASI1-Mini** while retaining multi-turn conversations, agent support, system messages, and more.

3\. ASI-1\_mini\_Langchain[â€‹](#3-asi-1_mini_langchain "Direct link to 3. ASI-1_mini_Langchain")
-----------------------------------------------------------------------------------------------

**GitHub**: [ASI-1\_mini\_Langchain and tavily](https://github.com/abhifetch/ASI-1_mini_Langchain)  
Shows a **custom LLM integration** with LangChain and **Tavily Search**. Learn how to build a custom LangChain `LLM` class that calls the ASI1-Mini API and fetches external information for more sophisticated query handling.

4\. DeFI-Agent-Starter[â€‹](#4-defi-agent-starter "Direct link to 4. DeFI-Agent-Starter")
---------------------------------------------------------------------------------------

**GitHub**: [DeFI-Agent-Starter](https://github.com/RoyceBraden/DeFI-Agent-Starter)  
A **multi-agent** system focusing on **DeFi (Decentralized Finance)** use cases. It leverages **AgentVerse** and ASI1-Mini to determine whether to hold or sell a crypto asset based on Fear and Greed Index and sentiment analysis.

5\. ASI1-Mini-Chat-System[â€‹](#5-asi1-mini-chat-system "Direct link to 5. ASI1-Mini-Chat-System")
------------------------------------------------------------------------------------------------

**GitHub**: [ASI1-Mini-Chat-System](https://github.com/abhifetch/ASI1-Mini-Chat-System)  
A **modular, agent-based chat system** powered by uAgents. It features a **Client Agent** and a **Server Agent** that communicate in real time, with queries relayed to the ASI1 API for intelligent responses.

Getting Started[â€‹](#getting-started "Direct link to Getting Started")
---------------------------------------------------------------------

1.  **Clone any repository** that fits your interests.
2.  **Install dependencies** and create a `.env` file (or set environment variables) with your `ASI1_API_KEY`. Get your API Key [here](https://asi1.ai/dashboard/api-keys).
3.  **Run the sample scripts** or follow the instructions in each repoâ€™s README.
4.  **Experiment & Customize** â€“ tweak prompts, add tools, or integrate more data sources.

note

**Note:** You can learn more about ASI-1 Mini APIs [**here**](https://docs.asi1.ai/docs/).

With **ASI1-Mini**, you can rapidly build anything from simple chatbots to complex, multi-agent DeFi applications.</content>
</page>

<page>
  <title>Langchain Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.2/examples/other-frameworks/langchain</url>
  <content>Getting a langchain tool on Agentverse Marketplace
--------------------------------------------------

This document is a comprehensive guide to help users integrate **LangChain** tools into **Fetch.ai's** ecosystem using the provided **Alphavantage Agent** and **User Agent scripts**. These agents work together to fetch stock prices and demonstrate how LangChain tools can interact seamlessly with Agentverse agents.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

### Purpose[â€‹](#purpose "Direct link to Purpose")

The purpose of this script is to:

*   **Use LangChain integration** to fetch stock prices from the Alpha Vantage API.
*   **Demonstrate communication** between agents within the Fetch.ai Agentverse.
*   **Provide** a clear example of webhook-based communication.

### Key Features[â€‹](#key-features "Direct link to Key Features")

*   The\_\_ Alphavantage Agent\_\_ handles requests for stock prices using the Alpha Vantage API.
*   The **User Agent** acts as a client to forward requests and retrieve responses.
*   Communication is done via **HTTP webhooks** registered with Fetch.aiâ€™s Agentverse.

### Architecture Diagram[â€‹](#architecture-diagram "Direct link to Architecture Diagram")

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

### Environment Setup[â€‹](#environment-setup "Direct link to Environment Setup")

*   **Python 3.8** or higher.
*   A **virtual environment** (recommended).
*   **Flask** and related libraries installed.

### Required Libraries[â€‹](#required-libraries "Direct link to Required Libraries")

Install the necessary libraries using:

    pip install flask flask-cors fetchai langchain â€œflask[async]â€

note

**Note:** For more advanced dependency management, consider creating a `requirements.txt` and using `pip install -r requirements.txt`.

### Environment Variables[â€‹](#environment-variables "Direct link to Environment Variables")

Create a `.env` file in the project directory with the following content:

    AV_STOCKPRICE_AI_KEY=<your_alpha_vantage_ai_key>AGENTVERSE_API_KEY=<your_agentverse_api_key>USER_STOCKPRICE_AI_KEY=<your_user_stockprice_ai_key>ALPHAVANTAGE_API_KEY=<your_agentverse_api_key>

Replace the placeholders with your respective API keys. Sign up for a free API key at [Alpha Vantage](https://www.alphavantage.co/).

Alphvantage Agent Script[â€‹](#alphvantage-agent-script "Direct link to Alphvantage Agent Script")
------------------------------------------------------------------------------------------------

The **Alphavantage Agent** Script focuses on:

*   **Registering** with Fetch.aiâ€™s Agentverse.
*   **Handling requests** to fetch stock prices.
*   **Responding** to the User Agent with stock price data.

### Script Breakdown (stockprice.py)[â€‹](#script-breakdown-stockpricepy "Direct link to Script Breakdown (stockprice.py)")

**Required Libraries**

The script imports the required modules for:

*   **Identity Management**: Fetch.aiâ€™s Identity and registration modules.
*   **Communication**: Flask for HTTP handling and Fetch.aiâ€™s communication utilities.
*   **Alpha Vantage API**: LangChainâ€™s `AlphaVantageAPIWrapper`.

    import osimport loggingimport asynciofrom uagents_core.crypto import Identityfrom flask import Flask, request, jsonifyfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentfrom langchain_community.utilities.alpha_vantage import AlphaVantageAPIWrapper

**Setting Up Logging**

Logging is configured to monitor the agentâ€™s activities:

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)

**Flask Webhook**

The Flask app is created to handle incoming webhook requests:

    flask_app = Flask(__name__)

**Alpha Vantage Integration**

An instance of the Alpha Vantage wrapper is created:

    alpha_vantage = AlphaVantageAPIWrapper()

**Fetch Stock Price Function**

The get\_stock\_price function fetches stock prices asynchronously from the Alpha Vantage API:

    async def get_stock_price(symbol):    response = alpha_vantage._get_quote_endpoint(symbol)    return response['Global Quote']['05. price']

**Webhook Endpoint**

The /webhook route handles requests sent by other agents:

    @flask_app.route('/webhook', methods=['POST'])async def webhook():    global response    global av_identity    try:        # Parse incoming message        data = request.get_data().decode('utf-8')        message = parse_message_from_agent(data)        symbol = message.payload.get("request", "")        agent_address = message.sender        if not symbol:            return jsonify({"status": "error", "message": "No data path provided"}), 400        price = await get_stock_price(symbol)        print(price)        payload = {'price': price}        send_message_to_agent(av_identity, agent_address, payload)        return jsonify({"status": "wiki_content_sent"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"status": "error", "message": str(e)}), 500

**Registering the Agent**

The init\_agent function registers the Alphavantage Agent with Fetch.aiâ€™s Agentverse:

    def init_agent():   global av_identity   try:       av_identity = Identity.from_seed(os.getenv("AV_STOCKPRICE_AI_KEY"), 0)       register_with_agentverse(           identity=av_identity,           url="http://localhost:5008/webhook",           agentverse_token=os.getenv("AGENTVERSE_API_KEY"),           agent_title="Alphavantage Stock Price Langchain tool",           # Define the client agent's metadata           readme = """               <description>This is langchain based alphavantage tool to get the latest stock price for the given symbol.</description>               <use_cases>                   <use_case>Gives you the stock price of given symbol.</use_case>               </use_cases>               <payload_requirements>               <description>Expects the symbol for which you want stock price for.</description>                   <payload>                       <requirement>                           <parameter>symbol</parameter>                           <description>Symbol for which you want to check the stock price?</description>                       </requirement>                   </payload>               </payload_requirements>           """       )       logger.info("Langchain Alphavantage tool agent registered successfully!")   except Exception as e:       logger.error(f"Error initializing agent: {e}")       raise

**Running the Agent**

The Flask server runs on port 5008 to handle requests:

    if __name__ == "__main__":    init_agent()    flask_app.run(host="0.0.0.0", port=5008, debug=True)

User Agent Script[â€‹](#user-agent-script "Direct link to User Agent Script")
---------------------------------------------------------------------------

The User Agent script acts as the client that forwards requests to the Alphavantage Agent and retrieves responses.

### Script Breakdown (user.py)[â€‹](#script-breakdown-userpy "Direct link to Script Breakdown (user.py)")

**Importing Libraries**

The script imports required libraries for Flask, Fetch.ai, and communication:

    from flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai import fetchfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osfrom dotenv import load_dotenvfrom flask import Flask, jsonify, request

**Setting Up Logging and Flask**

Logging and Flask are configured similarly to the Alphavantage Agent:

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app)

**Initializing the User Agent**

The init\_client function initializes and registers the User Agent with Agentverse:

    def init_client():   """Initialize and register the client agent."""   global client_identity   try:       # Load the client identity from environment variables       client_identity = Identity.from_seed(os.getenv("USER_STOCKPRICE_AI_KEY"), 0)       logger.info(f"Client agent started with address: {client_identity.address}")       # Define the client agent's metadata       readme = """       <description>Frontend client that tests with Stocks price using alphavantage.</description>       <use_cases>           <use_case>Sends Request to the AI Agent to check stock price</use_case>       </use_cases>       <payload_requirements>           <description>Expects request which is to be sent to another agent.</description>           <payload>               <requirement>                   <parameter>request</parameter>                   <description>This is the request which is to be sent to other agent</description>               </requirement>           </payload>       </payload_requirements>       """       # Register the agent with Agentverse       register_with_agentverse(           identity=client_identity,           url="http://localhost:5002/api/webhook",           agentverse_token=os.getenv("AGENTVERSE_API_KEY"),           agent_title="User agent for Stock Price check",           readme=readme       )       logger.info("Client agent registration complete!")   except Exception as e:       logger.error(f"Initialization error: {e}")       raise

**Searching Agents**

The /api/search-agents endpoint allows users to search for available agents:

    # searching the agents which can create dashboard on agentverse@app.route('/api/search-agents', methods=['GET'])def search_agents():   """Search for available dashboard agents based on user input."""   try:       # Extract user input from query parameters       user_query = request.args.get('query', '')       if not user_query:           return jsonify({"error": "Query parameter 'query' is required."}), 400       # Fetch available agents based on user query       available_ais = fetch.ai(user_query)  # Pass the user query to the fetch.ai function       print(f'---------------------{available_ais}----------------------')       # Access the 'ais' list within 'agents' (assuming fetch.ai returns the correct structure)       agents = available_ais.get('ais', [])       print(f'----------------------------------{agents}------------------------------------')       extracted_data = []       for agent in agents:           name = agent.get('name')  # Extract agent name           address = agent.get('address')           # Append formatted data to extracted_data list           extracted_data.append({               'name': name,               'address': address,           })       # Format the response with indentation for readability       response = jsonify(extracted_data)       response.headers.add('Content-Type', 'application/json; charset=utf-8')       response.headers.add('Access-Control-Allow-Origin', '*')       response.headers.add('Access-Control-Allow-Headers', 'Content-Type')       return response, 200   except Exception as e:       logger.error(f"Error finding agents: {e}")       return jsonify({"error": str(e)}), 500

**Sending Data to Alphavantage Agent**

The /api/send-data endpoint forwards stock price requests to the Alphavantage Agent:

    @app.route('/api/send-data', methods=['POST'])def send_data():   """Send payload to the selected agent based on provided address."""   global agent_response   agent_response = None   try:       # Parse the request payload       data = request.json       payload = data.get('payload')  # Extract the payload dictionary       agent_address = data.get('agentAddress')  # Extract the agent address       # Validate the input data       if not payload or not agent_address:           return jsonify({"error": "Missing payload or agent address"}), 400       logger.info(f"Sending payload to agent: {agent_address}")       logger.info(f"Payload: {payload}")       # Send the payload to the specified agent       send_message_to_agent(           client_identity,  # Frontend client identity           agent_address,    # Agent address where we have to send the data           payload           # Payload containing the data       )       return jsonify({"status": "request_sent", "agent_address": agent_address, "payload": payload})   except Exception as e:       logger.error(f"Error sending data to agent: {e}")       return jsonify({"error": str(e)}), 500

**Webhook Endpoint**

The /webhook route handles requests sent by other agents:

    # app route to get recieve the messages on the agent@app.route('/api/webhook', methods=['POST'])def webhook():    """Handle incoming messages from the dashboard agent."""    global agent_response    try:        # Parse the incoming webhook message        data = request.get_data().decode("utf-8")        logger.info("Received response")        message = parse_message_from_agent(data)        agent_response = message.payload        logger.info(f"Processed response: {agent_response}")        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500

**Retrieving the Agent's Response**

The /api/get-response endpoint retrieves the response (stock price) from the stocks price Agent and saves it :

    @app.route('/api/get-response', methods=['GET'])def get_response():    global agent_response    try:        if agent_response:            response = agent_response            print(f'stock_price : {response}')            agent_response = None  # Clear the response after sending                        keys = list(response.keys())  # Convert dict_keys to a list            first_key = keys[0]           # Get the first key            response_final = response.get(first_key, "")             logger.info(f"Got response for the stock price {response_final}")            return jsonify({"stock_price": response_final})        else:            return jsonify({"error": "No storm response available"}), 404    except Exception as e:        logger.error(f"Error creating markdown file: {e}")        return jsonify({"error": str(e)}), 500

**Running the agent**

    # function to start the flask serverdef start_server():    """Start the Flask server."""    try:        # Load environment variables        load_dotenv()        init_client()        app.run(host="0.0.0.0", port=5002)    except Exception as e:        logger.error(f"Server error: {e}")        raiseif __name__ == "__main__":    start_server()

Steps to run Scripts[â€‹](#steps-to-run-scripts "Direct link to Steps to run Scripts")
------------------------------------------------------------------------------------

Follow these steps to get both the Alphavantage Agent and User Agent scripts up and running.

**1\. Start the Alphavantage Agent**

Ensure the `AV_STOCKPRICE_AI_KEY` and `AGENTVERSE_API_KEY` values are correctly set in the `.env` file.

#### Run the Alphavantage Agent script[â€‹](#run-the-alphavantage-agent-script "Direct link to Run the Alphavantage Agent script")

#### Output[â€‹](#output "Direct link to Output")

    (my_env) abhi@Fetchs-MacBook-Pro Langchain Tool Example % python3 av_agent.pyINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Langchain Alphavantage tool agent registered successfully! * Serving Flask app 'av_agent' * Debug mode: onINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5008INFO:werkzeug:Press CTRL+C to quitINFO:werkzeug: * Restarting with statINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Langchain Alphavantage tool agent registered successfully!

The agent listens on **port 5008** for requests.

**2\. Start the User Agent**

Ensure the `USER_STOCKPRICE_AI_KEY` and `AGENTVERSE_API_KEY` values are correctly set in the `.env` file.

#### Run the User Agent script[â€‹](#run-the-user-agent-script "Direct link to Run the User Agent script")

#### Output[â€‹](#output-1 "Direct link to Output")

    (my_env) abhi@Fetchs-MacBook-Pro Langchain Tool Example % python3 user_agent.pyINFO:__main__:Client agent started with address: agent1qtyqvwlqgd96jzdcd9yj9nem09j48xel2envjdcnvxpx2nfq0rygwxfg3kvINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Client agent registration complete! * Serving Flask app 'user_agent' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5002 * Running on http://192.168.0.106:5002

### Interacting with the Agents[â€‹](#interacting-with-the-agents "Direct link to Interacting with the Agents")

Once both agents are running, use the following endpoints from another terminal:

**1\. Search for Agents**

    curl -X GET "http://localhost:5002/api/search-agents?query=I%20want%20to%20check%20the%20stock%20price"

**Sample Output**

    [{"address":"agent1qwkzy0twyd27egqfrcw8m6vdtsz7hrm9leu7jfezzzlgckywj7shytz5kf6","name":"Alphavantage Stock Price Langchain tool"},{"address":"agent1qtzp03dh92dldvsmr7v0xxvwcfwnpctftjqwdrqrmfszex0dal7pzapsy8d","name":"User agent for Stock Price check"},{"address":"agent1qtnc7ruc63g3n84qczekqc6qwp0e5ayvs3v8e5pae563qrqcf852xlqz2pk","name":"Alphavantage Stock Price Langchain tool"},{"address":"agent1qtyqvwlqgd96jzdcd9yj9nem09j48xel2envjdcnvxpx2nfq0rygwxfg3kv","name":"User agent for Stock Price check"},{"address":"agent1q05k0g4f90zka3m34zj52sdyqtvvvzyry0lc0sdzcdm2tjllkqeruqhue4z","name":"IL Testing user"},{"address":"agent1qtmkpt8lhyx4u2ndzcgr7jkjuuade4ay78yld2ngaghs03z6qve3senwrsa","name":"Penny"},{"address":"agent1qtxx79e689zwjlh7m8dsfuv7lyn45d2xapr4p9w2u9axvzm487tzjfwqzdp","name":"Penny"},{"address":"agent1qvc0em6vrlr80txaz3ksy7tspzaxl564jwqtaana7exehg0hz7gl6zg0wl0","name":"Penny"},{"address":"agent1qw5s6e7mxcd8mp6f2vvunk9rklwnv8ynft43unx77syea4nnskxyj4yk5xq","name":"Penny"},{"address":"agent1qwlyun6slk2gy8uxnnmvy35tuw2ftp44wza6yfgw06ca9axlllzaxgj5jut","name":"Penny"}]

**2\. Send a Stock Price Request**

    curl -X POST "http://localhost:5002/api/send-data" \-H "Content-Type: application/json" \-d '{    "payload": {"request": "AAPL"},    "agentAddress": "agent1qwkzy0twyd27egqfrcw8m6vdtsz7hrm9leu7jfezzzlgckywj7shytz5kf6"}'

**Sample Output**

    {"agent_address":"agent1qwkzy0twyd27egqfrcw8m6vdtsz7hrm9leu7jfezzzlgckywj7shytz5kf6","payload":{"request":"AAPL"},"status":"request_sent"}

**3\. Retrieve the Stock Price Response**

Fetch the stock price response from the User Agent:

    curl -X GET "http://localhost:5002/api/get-response"

**Sample Output**

    {"stock_price":"234.4000"}

Debugging Common Issues[â€‹](#debugging-common-issues "Direct link to Debugging Common Issues")
---------------------------------------------------------------------------------------------

Agent Registration Fails:

a. Check the .env file for correct API keys.

b. Ensure the AGENTVERSE\_API\_KEY is valid.

404 Errors:

a. Verify that both agents are running on their respective ports (5008 and 5002 in this case).

b. Double-check the agentAddress in requests.

You now have an **Alphavantage Agent** integrated with the **Fetch.ai Agentverse** for fetching stock prices via **LangChain**. Feel free to adapt the example to include other APIs, advanced AI features, or additional endpoints.</content>
</page>

<page>
  <title>Understanding Agentic Systems | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.1/concepts-ai-agents/understanding-agentic-systems</url>
  <content>### The Spectrum of LLM Applications[â€‹](#the-spectrum-of-llm-applications "Direct link to The Spectrum of LLM Applications")

It's crucial to understand where different types of LLM-based systems fall on the complexity spectrum:

1.  **Simple LLM Applications**
    
    *   Single LLM calls with prompt engineering
    *   Basic RAG (Retrieval-Augmented Generation) implementations
    *   Structured output parsing
    *   These are NOT agents, just applications using LLM capabilities
2.  **Multi-Step LLM Applications**
    
    *   Multiple LLM calls in sequence
    *   API integrations and external tool usage
    *   Basic state management
    *   While more complex, these still aren't necessarily agents
3.  **True AI Agents**
    
    *   Dynamic decision-making about process flow
    *   Autonomous tool selection and usage
    *   Complex state management and memory
    *   Goal-oriented behavior with adaptive strategies

### Common Misconceptions[â€‹](#common-misconceptions "Direct link to Common Misconceptions")

#### Misconception 1: "My application uses multiple LLM calls, so it's an agent"[â€‹](#misconception-1-my-application-uses-multiple-llm-calls-so-its-an-agent "Direct link to Misconception 1: \"My application uses multiple LLM calls, so it's an agent\"")

    # This is NOT an agent - it's a multi-step LLM applicationdef process_document(doc):    summary = llm.generate_summary(doc)    keywords = llm.extract_keywords(summary)    sentiment = llm.analyze_sentiment(summary)    return {        "summary": summary,        "keywords": keywords,        "sentiment": sentiment    }

#### Misconception 2: "I'm using tools and APIs, so it's an agent"[â€‹](#misconception-2-im-using-tools-and-apis-so-its-an-agent "Direct link to Misconception 2: \"I'm using tools and APIs, so it's an agent\"")

    # This is NOT an agent - it's an automated workflowdef analyze_stock(symbol):    price_data = stock_api.get_price(symbol)    news = news_api.get_recent_news(symbol)    analysis = llm.analyze(f"Price: {price_data}, News: {news}")    return analysis

### Understanding Agentic Systems[â€‹](#understanding-agentic-systems "Direct link to Understanding Agentic Systems")

#### 1\. Workflows vs Agents[â€‹](#1-workflows-vs-agents "Direct link to 1. Workflows vs Agents")

**Workflows:**

    # Predefined workflow exampleclass FinancialAnalysisWorkflow:    def execute(self, query):        # Fixed sequence of steps        market_data = self.get_market_data()        financial_statements = self.get_financial_statements()        competitor_analysis = self.get_competitor_data()                return self.generate_analysis(            market_data,            financial_statements,            competitor_analysis        )

**Agents:**

    # True agent exampleclass FinancialAnalysisAgent:    def analyze(self, query):        # Dynamic planning        analysis_plan = self.plan_analysis_strategy(query)                # Flexible execution        for step in analysis_plan:            if self.needs_revision(step, self.current_state):                analysis_plan = self.revise_plan()                        tool = self.select_appropriate_tool(step)            result = self.execute_tool(tool, step)            self.update_state(result)                        if self.goal_achieved():                break                return self.synthesize_findings()

### When to Use Each Approach[â€‹](#when-to-use-each-approach "Direct link to When to Use Each Approach")

1.  **Use Simple LLM Applications When:**
    
    *   Tasks are straightforward and well-defined
    *   Minimal or no tool interaction is needed
    *   Quick response times are critical
    *   Cost efficiency is a primary concern
2.  **Use Workflows When:**
    
    *   Process steps are well-understood and consistent
    *   Predictability is more important than flexibility
    *   You need tight control over execution
    *   Performance and reliability are crucial
3.  **Use Agents When:**
    
    *   Tasks require dynamic decision-making
    *   Problems have multiple valid solution paths
    *   Flexibility and adaptation are crucial
    *   Complex tool interactions are needed
    *   Tasks benefit from maintaining context

### Implementation Best Practices[â€‹](#implementation-best-practices "Direct link to Implementation Best Practices")

1.  **Start Simple**
    
    *   Begin with the simplest solution that could work
    *   Only add complexity when clearly needed
    *   Validate the need for agency before implementing it
2.  **Consider Trade-offs**
    
    *   Agents typically have higher latency
    *   Cost increases with complexity
    *   Maintenance burden grows with sophistication
3.  **Design for Observability**
    
    *   Implement comprehensive logging
    *   Track decision-making processes
    *   Monitor tool usage patterns
4.  **Build in Safeguards**
    
    *   Implement rate limiting
    *   Add timeout mechanisms
    *   Create fallback strategies

### Evaluation Criteria for True Agency[â€‹](#evaluation-criteria-for-true-agency "Direct link to Evaluation Criteria for True Agency")

Before implementing an agent, evaluate if your system really needs true agency by checking these criteria:

1.  **Decision Autonomy**
    
    *   Can the system choose different paths based on context?
    *   Does it make meaningful decisions about tool usage?
    *   Can it adapt its strategy during execution?
2.  **State Management**
    
    *   Does it maintain meaningful state?
    *   Can it use past interactions to inform decisions?
    *   Does it track progress toward goals?
3.  **Tool Integration**
    
    *   Can it choose tools dynamically?
    *   Does it understand tool capabilities?
    *   Can it combine tools in novel ways?
4.  **Goal Orientation**
    
    *   Does it understand and work toward specific objectives?
    *   Can it recognize when goals are achieved?
    *   Can it adjust goals based on new information?</content>
</page>

<page>
  <title>Team Based Artchitectures | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.1/concepts-ai-agents/team-based-architectures</url>
  <content>### Multi-Agent Architectures[â€‹](#multi-agent-architectures "Direct link to Multi-Agent Architectures")

Multi-agent systems distribute complex tasks across specialized agents, each with distinct roles and capabilities. This architecture provides several advantages:

1.  **Specialization**
    
    *   Agents can focus on specific domains
    *   Reduced prompt complexity per agent
    *   Clear separation of concerns
2.  **Scalability**
    
    *   Parallel processing capabilities
    *   Distributed decision making
    *   Independent scaling of components
3.  **Robustness**
    
    *   Graceful degradation
    *   Fault isolation
    *   Easier maintenance

### Team-Based Agent Architecture[â€‹](#team-based-agent-architecture "Direct link to Team-Based Agent Architecture")

A team-based architecture typically consists of three main components:

1.  **Supervisor Agent**
    
    *   Coordinates team activities
    *   Makes routing decisions
    *   Ensures task completion
2.  **Specialist Agents**
    
    *   Handle domain-specific tasks
    *   Maintain focused expertise
    *   Provide detailed analysis
3.  **State Management System**
    
    *   Maintains conversation context
    *   Tracks team progress
    *   Manages shared resources

### Implementing a Team-Based System[â€‹](#implementing-a-team-based-system "Direct link to Implementing a Team-Based System")

#### 1\. Supervisor Agent Implementation[â€‹](#1-supervisor-agent-implementation "Direct link to 1. Supervisor Agent Implementation")

    def create_supervisor_agent(llm: ChatOpenAI):    """    Create a supervisor agent that coordinates team activities.        Key responsibilities:    - Task decomposition    - Agent selection    - Progress monitoring    - Final response synthesis    """    supervisor_prompt = """    Core responsibilities:    1. Analyze incoming queries    2. Determine required information    3. Select appropriate specialist    4. Monitor progress    5. Decide when to conclude    """        return create_team_supervisor(        llm=llm,        system_prompt=supervisor_prompt,        members=["SpecialistA", "SpecialistB"]    )

#### 2\. Specialist Agent Implementation[â€‹](#2-specialist-agent-implementation "Direct link to 2. Specialist Agent Implementation")

    def create_specialist_agent(    llm: ChatOpenAI,    domain: str,    tools: List[Tool]):    """    Create a specialist agent with domain-specific capabilities.        Key features:    - Focused expertise    - Specific tool access    - Clear output format    """    system_prompt = f"""    You are specialized in {domain}.    When responding:    1. Use domain-specific tools    2. Provide structured output    3. Request missing information    """        return create_agent(        llm=llm,        tools=tools,        system_prompt=system_prompt    )

#### 3\. State Management[â€‹](#3-state-management "Direct link to 3. State Management")

    class TeamState(TypedDict):    """    Manage team-wide state and context.        Components:    - Message history    - Team composition    - Current status    - Required information    """    messages: List[BaseMessage]    team_members: List[str]    next: str    information_needed: List[str]    reasoning: str

### Building the Team Graph[â€‹](#building-the-team-graph "Direct link to Building the Team Graph")

The team graph defines how agents interact and how information flows:

    def create_team_graph():    """    Create a coordinated team of agents with defined interactions.        Structure:    1. Initialize components    2. Define connections    3. Set workflow rules    4. Establish entry points    """    # Initialize agents    specialist_a = create_specialist_agent(...)    specialist_b = create_specialist_agent(...)    supervisor = create_supervisor_agent(...)        # Create graph    graph = StateGraph(TeamState)        # Add nodes    graph.add_node("SpecialistA", specialist_a)    graph.add_node("SpecialistB", specialist_b)    graph.add_node("supervisor", supervisor)        # Define workflows    graph.add_edge("SpecialistA", "supervisor")    graph.add_edge("SpecialistB", "supervisor")        # Add conditional routing    graph.add_conditional_edges(        "supervisor",        lambda x: x["next"],        {            "SpecialistA": "SpecialistA",            "SpecialistB": "SpecialistB",            "FINISH": END        }    )        return graph.compile()</content>
</page>

<page>
  <title>Microservices and AI Agents | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.1/concepts-ai-agents/agent-comparison</url>
  <content>Fetch.ai Microservice Agent and AI Agent
----------------------------------------

Before diving into the world of AI agents, letâ€™s first explore the key differences between microservices and AI agents.

*   **Microservice**: If you want to fetch specific information or perform an isolated action, you typically use tools or APIsâ€”this is known as a microservice. Imagine you want to check available flights from London to New York. Youâ€™d use an API or tool to fetch the available options. Thatâ€™s a microservice in actionâ€”focused, specific, and task-oriented.
    
*   **AI Agent**: If you have a larger objective that requires coordination between multiple microservices or tools, thatâ€™s where an AI agent steps in. Now, letâ€™s say you want to book the best possible flight. This involves more than just checking flight options: youâ€™ll compare prices across different days, check your personal schedule, and evaluate additional services like baggage policies or meal preferences. To achieve this goal, you need reasoning and decision-making capabilitiesâ€”this is where an AI agent comes into play.
    

The Fetch.ai ecosystem offers a variety of tools to build both microservices and AI agents, enabling developers to cater to different use cases.

### Microservices with uAgents[â€‹](#microservices-with-uagents "Direct link to Microservices with uAgents")

uAgents is Fetch.ai's lightweight Python framework for creating agents (microservices). Itâ€™s designed to be simple and efficient, making it a great choice for building basic microservices.

### AI Agents with the Fetch.ai SDK[â€‹](#ai-agents-with-the-fetchai-sdk "Direct link to AI Agents with the Fetch.ai SDK")

Fetch.ai's SDK is built on top of the uAgents framework. It simplifies the creation of intelligent, dynamic AI agents that can interact with other agents and microservices seamlessly. The SDK allows for more flexibility, reasoning, and adaptability compared to uAgents.

Youâ€™ll learn in the upcoming sections how to decide between these tools based on the problem youâ€™re solving.

How Fetch.aiâ€™s Ecosystem Works[â€‹](#how-fetchais-ecosystem-works "Direct link to How Fetch.aiâ€™s Ecosystem Works")
----------------------------------------------------------------------------------------------------------------

Fetch.ai provides an integrated ecosystem to host and manage your agents, ensuring connectivity and scalability.

1.  **Agentverse Platform**:
    
    *   Agents can be hosted on the Agentverse platform, allowing them to stay active and accessible.
    *   Due to security constraints, Agentverse only supports a limited set of packages.
2.  **Mailbox Service**:
    
    *   For scenarios requiring custom environments (like virtual machines), you can use the Mailbox Service. This lets you host agents outside Agentverse while maintaining connectivity.
    *   The mailbox ensures that messages sent to an agent are queued, so the agent can process them once it comes online.
3.  **Agent Registration with the Almanac Contract**:
    
    *   When agents are registered on Agentverse, they are also listed in the Almanac Contract on Fetch.aiâ€™s blockchain. This registry allows other agents to discover your agent using its unique address.

Why Use the Fetch.ai SDK?[â€‹](#why-use-the-fetchai-sdk "Direct link to Why Use the Fetch.ai SDK?")
-------------------------------------------------------------------------------------------------

While the uAgents framework is sufficient for basic microservices, the Fetch.ai SDK offers additional capabilities suited for more complex AI agents. Hereâ€™s why the SDK exists:

*   Dynamic Messaging:
    *   uAgents require precise Data Models to identify and process messages. In contrast, the SDK enables agents to handle dynamic messages, making it more adaptable.
*   Handler Differences:
    *   uAgents rely on predefined handlers (e.g., interval-based or message-based handlers).
    *   SDK-based agents leverage Flask webhooks, enabling more flexible and scalable communication.

With these distinctions, the SDK empowers developers to build agents capable of reasoning, adaptability, and complex decision-making.</content>
</page>

<page>
  <title>AI Agent to AI Agent Communication | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.1/agent-communication/sdk-sdk-communication</url>
  <content>In this guide, we will create two AI agents using the fetchai SDK and then see how to send and handle requests received by these agents. You can find all our supporting code files in our dedicated [github repo](https://github.com/fetchai/fetchai).

AI Agent creation[â€‹](#ai-agent-creation "Direct link to AI Agent creation")
---------------------------------------------------------------------------

Let's start by creating an AI Agent using the [fetchai SDK](https://github.com/fetchai/fetchai).

Please remember to have the **fetchai** package installed in the terminal in order to create and run the agents.

### Creating our first agent to receive the message[â€‹](#creating-our-first-agent-to-receive-the-message "Direct link to Creating our first agent to receive the message")

Create a new Python script:

    touch my_first_sdk_agent.py

Open `my_first_sdk_agent.py` in your text editor and add the following code which helps you register your agent and handle messages:

my\_first\_sdk\_agent.py

    from flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai import fetchfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osfrom dotenv import load_dotenv# Configure logginglogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app)# Initialising client identity to get registered on agentverseclient_identity = None # Function to register agentdef init_client():    """Initialize and register the client agent."""    global client_identity    try:        # Load the agent secret key from environment variables        client_identity = Identity.from_seed(os.getenv("AGENT_SECRET_KEY_1"), 0)        logger.info(f"Client agent started with address: {client_identity.address}")        readme = """            ![domain:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)            domain:domain-of-your-agent            <description>This Agent can only receive a message from another agent in string format.</description>            <use_cases>                <use_case>To receive a message from another agent.</use_case>            </use_cases>            <payload_requirements>            <description>This agent only requires a message in the text format.</description>            <payload>                <requirement>                    <parameter>message</parameter>                    <description>The agent can receive any kind of message.</description>                </requirement>            </payload>            </payload_requirements>        """                # Register the agent with Agentverse        register_with_agentverse(            identity=client_identity,            url="http://localhost:5002/api/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="Quickstart Agent 1",            readme=readme        )        logger.info("Quickstart agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise# app route to recieve the messages from other agents@app.route('/api/webhook', methods=['POST'])def webhook():    """Handle incoming messages"""    global agent_response    try:        # Parse the incoming webhook message        data = request.get_data().decode("utf-8")        logger.info("Received response")        message = parse_message_from_agent(data)        agent_response = message.payload        logger.info(f"Processed response: {agent_response}")        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500if __name__ == "__main__":    load_dotenv()       # Load environment variables    init_client()       #Register your agent on Agentverse    app.run(host="0.0.0.0", port=5002)      

Save the `.env` file with your `AGENTVERSE_KEY` and `AGENT_SECRET_KEY_1`, get your `AGENTVERSE_KEY` by referring to this [guide](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key) and the AGENT\_SECRET\_KEY\_1 is a random unique key just for your AI Agent. Â 

    AGENT_SECRET_KEY_1='Your random secret seed phrase for the agent'AGENTVERSE_API_KEY='YOUR AGENTVERSE API KEY FROM AGENTVERSE'

Start the first agent as follows:

#### Expected Output[â€‹](#expected-output "Direct link to Expected Output")

    (my_venv) abhi@Fetchs-MacBook-Pro chorot 3 session % python3 my_sdk_agent_1.pyINFO:__main__:Client agent started with address: agent1q2d7qdxxpc2emmph6wze47p0g7vnfxwuzk3h6yfnaem0px0hc3acu3yzz8kINFO:fetchai:Registering with Almanac APIINFO:fetchai:Successfully registered as custom agent in AgentverseINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Quickstart agent registration complete! * Serving Flask app 'my_sdk_agent_1' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5002 * Running on http://192.168.0.105:5002INFO:werkzeug:Press CTRL+C to quit

You can verify that your agent is registered by checking in your [Agentverse Local Agents](https://agentverse.ai/agents/local) â†—ï¸.

Webhook will help you receive and handle messages, we will learn more about communication here.

### Creating our second agent to send the message[â€‹](#creating-our-second-agent-to-send-the-message "Direct link to Creating our second agent to send the message")

Open `my_second_sdk_agent.py` in your text editor and add the following code which helps you register agent and handle messages:

my\_second\_sdk\_agent.py

    from flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai import fetchfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osfrom dotenv import load_dotenv# Configure logginglogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app)# Initialising client identity to get registered on agentverseclient_identity = None agent_response = None# Function to register agentdef init_client():    """Initialize and register the client agent."""    global client_identity    try:        # Load the agent secret key from environment variables        client_identity = Identity.from_seed(os.getenv("AGENT_SECRET_KEY_2"), 0)        logger.info(f"Client agent started with address: {client_identity.address}")        readme = """            ![domain:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)            domain:domain-of-your-agent            <description>This Agent can only send a message to another agent in string format.</description>            <use_cases>                <use_case>To send a message to another agent.</use_case>            </use_cases>            <payload_requirements>            <description>This agent can only send a message in the text format.</description>            <payload>                <requirement>                    <parameter>message</parameter>                    <description>The agent can send a message to another agent.</description>                </requirement>            </payload>            </payload_requirements>        """        # Register the agent with Agentverse        register_with_agentverse(            identity=client_identity,            url="http://localhost:5005/api/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="Quickstart Agent 2",            readme=readme        )        logger.info("Quickstart agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise@app.route('/api/send-data', methods=['POST'])def send_data():   """Send payload to the selected agent based on provided address."""   global agent_response   agent_response = None   try:       # Parse the request payload       data = request.json       payload = data.get('payload')  # Extract the payload dictionary       agent_address = data.get('agentAddress')  # Extract the agent address       # Validate the input data       if not payload or not agent_address:           return jsonify({"error": "Missing payload or agent address"}), 400       logger.info(f"Sending payload to agent: {agent_address}")       logger.info(f"Payload: {payload}")       # Send the payload to the specified agent       send_message_to_agent(           client_identity,  # Frontend client identity           agent_address,    # Agent address where we have to send the data           payload           # Payload containing the data       )       return jsonify({"status": "request_sent", "agent_address": agent_address, "payload": payload})   except Exception as e:       logger.error(f"Error sending data to agent: {e}")       return jsonify({"error": str(e)}), 500if __name__ == "__main__":    load_dotenv()   # Load environment variables    init_client()   #Register your Agent on Agentverse    app.run(host="0.0.0.0", port=5005)

Start the second agent as follows.

    python my_second_agent.py

#### Expected Output[â€‹](#expected-output-1 "Direct link to Expected Output")

    (my_venv) abhi@Fetchs-MacBook-Pro chorot 3 session % python3 my_sdk_agent_2.pyINFO:__main__:Client agent started with address: agent1qvgxzj40657wh4ftk65scvqqwv7sw5lf7z5uf3mdnc8d6zmrugumu4g78wsINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Quickstart agent registration complete! * Serving Flask app 'my_sdk_agent_2' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5005 * Running on http://192.168.0.105:5005INFO:werkzeug:Press CTRL+C to quit

### Sending message from Agent2 to Agent1[â€‹](#sending-message-from-agent2-to-agent1 "Direct link to Sending message from Agent2 to Agent1")

We will use curl command to send message from agent2 to agent1. We will send a payload with Hello Message.

    curl -X POST http://localhost:5005/api/send-data \-H "Content-Type: application/json" \-d '{  "payload": {"message":"Hello This is message from agent2"},  "agentAddress": <Your agent 1 address>}'

#### Curl Command Output[â€‹](#curl-command-output "Direct link to Curl Command Output")

    {"agent_address":"agent1q2d7qdxxpc2emmph6wze47p0g7vnfxwuzk3h6yfnaem0px0hc3acu3yzz8k","payload":{"message":"Hello This is message from agent2"},"status":"request_sent"}

#### Agent2 Logs[â€‹](#agent2-logs "Direct link to Agent2 Logs")

    INFO:__main__:Sending payload to agent: agent1q2d7qdxxpc2emmph6wze47p0g7vnfxwuzk3h6yfnaem0px0hc3acu3yzz8kINFO:__main__:Payload: {'message': 'Hello This is message from agent2'}{"version":1,"sender":"agent1qvgxzj40657wh4ftk65scvqqwv7sw5lf7z5uf3mdnc8d6zmrugumu4g78ws","target":"agent1q2d7qdxxpc2emmph6wze47p0g7vnfxwuzk3h6yfnaem0px0hc3acu3yzz8k","session":"e025c919-2c6d-4e9c-b562-a879cbb35aec","schema_digest":"model:708d789bb90924328daa69a47f7a8f3483980f16a1142c24b12972a2e4174bc6","protocol_digest":"proto:a03398ea81d7aaaf67e72940937676eae0d019f8e1d8b5efbadfef9fd2e98bb2","payload":"eyJtZXNzYWdlIjoiSGVsbG8gVGhpcyBpcyBtZXNzYWdlIGZyb20gYWdlbnQyIn0=","expires":null,"nonce":null,"signature":"sig10phxfu9s9lsrm3ux4pffufw6yxs37z0ag82hhlw0mkkwap74av7cjwk86dr9nnmgpgxwhyqdmnl90um8wu77emafutfw77jdvlswg9spmc899"}INFO:fetchai:Got response looking up agent endpointhttp://localhost:5002/api/webhookINFO:fetchai:Sent message to agentINFO:werkzeug:127.0.0.1 - - [29/Jan/2025 11:17:59] "POST /api/send-data HTTP/1.1" 200 -

#### Agent1 Logs[â€‹](#agent1-logs "Direct link to Agent1 Logs")

    INFO:__main__:Received responseINFO:__main__:Processed response: {'message': 'Hello This is message from agent2'}INFO:werkzeug:127.0.0.1 - - [29/Jan/2025 11:17:59] "POST /api/webhook HTTP/1.1" 200 -

This is how messages can be sent and handled between agents.

### Parse and Send Message functions.[â€‹](#parse-and-send-message-functions "Direct link to Parse and Send Message functions.")

The `send_message_to_agent` function is used to send a message from one agent to the other, while the `parse_message_from_agent` function is used to handle incoming messages.

#### Sending a Message[â€‹](#sending-a-message "Direct link to Sending a Message")

The `send_message_to_agent` function constructs an Envelope containing the sender's identity, target agent address, message payload, and other metadata.

    from uagents_core.crypto import Identityfrom fetchai.communication import send_message_to_agentsend_message_to_agent(    sender=sender_identity,             #sender_address    target="receiver_agent_address",    #target agent address    payload={"message": "Hello, Agent!"})# payload is in {str : str} format

#### Parsing an Incoming Message[â€‹](#parsing-an-incoming-message "Direct link to Parsing an Incoming Message")

The `parse_message_from_agent` function extracts messages received from other agents by decoding the payload.

    from fetchai.communication import parse_message_from_agenttry:    message = parse_message_from_agent(data)except ValueError as e:    print(f"Error parsing message: {e}")    return# Extract sender and payload detailssender_address = message.sendermessage_payload = message.payload</content>
</page>

<page>
  <title>AI Agent to uAgent Communication | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.1/agent-communication/sdk-uagent-communication</url>
  <content>uAgent â†” AI Agent Communication Using Fetch.ai SDK and uAgents
--------------------------------------------------------------

This guide demonstrates how to enable communication between a Microservice Agent created using the uagents framework and an AI Agent created using the Fetch.ai SDK.

### uAgent Script (uagent.py)[â€‹](#uagent-script-uagentpy "Direct link to uAgent Script (uagent.py)")

Please remember to have the **uagents** and the **fetchai** package installed in the terminal in order to create and run the agents.

This uAgent will receive a message from the AI Agent and send a response back.

uagent.py

    from uagents import Agent, Context, Model# Define the request model the uAgent will handleclass Request(Model):    message: str# Define the response model the uAgent will send backclass Response(Model):    response: str# Initialize the uAgentuagent = Agent(    name="Sample uAgent",    port=8000,    endpoint=["http://localhost:8000/submit"])# Handle incoming messages with the Request model@uagent.on_message(model=Request)async def message_handler(ctx: Context, sender: str, msg: Request):    ctx.logger.info(f"Received message from {sender}: {msg.message}")    # Generate a response message    response = Response(response=f'Hello, AI Agent! I received your message:{msg.message}')        # Send the response back to the AI Agent    await ctx.send(sender, response)if __name__ == "__main__":    uagent.run()

### Explanation[â€‹](#explanation "Direct link to Explanation")

*   **Agent Initialization**: The uAgent listens on port 8000 for incoming messages.
*   **Message Handling**: When a message matching the `Request` model is received, the agent logs it and responds with a predefined message using the `Response` model.

Setting Up the AI Agent[â€‹](#setting-up-the-ai-agent "Direct link to Setting Up the AI Agent")
---------------------------------------------------------------------------------------------

The AI Agent will send messages to the uAgent and handle the response received from the uAgent.

### AI Agent Script (ai\_agent.py)[â€‹](#ai-agent-script-ai_agentpy "Direct link to AI Agent Script (ai_agent.py)")

ai\_agent.py

    import osfrom flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai.communication import send_message_to_agent, parse_message_from_agentimport loggingfrom dotenv import load_dotenv# Configure logginglogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)# Initialize Flask appapp = Flask(__name__)CORS(app)client_identity = Noneagent_response = Noneclass Request(Model):    message: str# Load environment variables from .env fileload_dotenv()def init_client():    """Initialize and register the client agent."""    global client_identity    try:        # Load the client identity from environment variables        client_identity = Identity.from_seed("Sample AI AGENT SEED PHRASE for communication", 0)        logger.info(f"Client agent started with address: {client_identity.address}")        readme = """        ![domain:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)        domain:domain-of-your-agent        <description>This Agent can send a message to a uAgent and receive a message from a uAgent in string format.</description>        <use_cases>        <use_case>Send and receive messages with another uAgent.</use_case>        </use_cases>        <payload_requirements>            <description>This agent can only send and receive messages in text format.</description>            <payload>                <requirement>                    <parameter>message</parameter>                    <description>The agent sends and receives messages in text format.</description>                </requirement>            </payload>        </payload_requirements>        """        # Register the agent with Agentverse        register_with_agentverse(            identity=client_identity,            url="http://localhost:5002/api/webhook",            agentverse_token = os.getenv("AGENTVERSE_API_KEY"),            agent_title="Sample AI Agent communication"            readme=readme        )        logger.info("Client agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise@app.route('/request', methods=['POST'])def send_data():    """Send payload to the selected agent based on provided address."""    global agent_response    agent_response = None    try:        # Parse the request payload        data = request.json        payload = data.get('payload')  # Extract the payload dictionary        uagent_address = "agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6" #run the uagent.py copy the address and paste here                # Build the Data Model digest for the Request model to ensure message format consistency between the uAgent and AI Agent        model_digest = Model.build_schema_digest(Request)        # Send the payload to the specified agent        send_message_to_agent(            client_identity,  # Frontend client identity            uagent_address,  # Agent address where we have to send the data            payload,  # Payload containing the data            model_digest=model_digest        )        return jsonify({"status": "request_sent", "payload": payload})    except Exception as e:        logger.error(f"Error sending data to agent: {e}")        return jsonify({"error": str(e)}), 500# app route to get recieve the messages on the agent@app.route('/api/webhook', methods=['POST'])def webhook():    """Handle incoming messages from the dashboard agent."""    global agent_response    try:        # Parse the incoming webhook message        data = request.get_data().decode("utf-8")        logger.info("Received response")        message = parse_message_from_agent(data)        agent_response = message.payload        logger.info(f"Processed response: {agent_response}")        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500if __name__ == "__main__":    load_dotenv()    init_client()    app.run(host="0.0.0.0", port=5002)

### Explanation[â€‹](#explanation-1 "Direct link to Explanation")

*   **Data Model**: The Request and Response models define the structure of messages exchanged between the AI Agent and the uAgent. A correctly defined data model is essential for sending a message to the uAgent from an SDK-based AI Agent.
*   **Schema Validation**: The model digest ensures that messages conform to the expected schema before transmission, preventing format mismatches.
*   **Sending Data**: The AI Agent sends a message to the uAgent using the /request endpoint.
*   **Handling Response**: The AI Agent listens for responses using the /api/webhook endpoint.

Environment Variables[â€‹](#environment-variables "Direct link to Environment Variables")
---------------------------------------------------------------------------------------

Create a .env file and add the following environment variables:

    AGENTVERSE_API_KEY="YOUR_AGENTVERSE_API_KEY"AGENT_SECRET_KEY="YOUR_SECRET_KEY_FOR_AI_AGENT"

Replace the placeholders with your actual API keys and agent secrets. Refer to this [guide](https://innovationlab.fetch.ai/resources/docs/1.0.1/agentverse/agentverse-api-key) to get your Agentverse API Key.

Testing the Communication[â€‹](#testing-the-communication "Direct link to Testing the Communication")
---------------------------------------------------------------------------------------------------

### Step 1: Running the uAgent[â€‹](#step-1-running-the-uagent "Direct link to Step 1: Running the uAgent")

Start the uAgent by running the following command in your terminal:

#### uAgent Logs[â€‹](#uagent-logs "Direct link to uAgent Logs")

    (venv) abhi@Fetchs-MacBook-Pro ILAgents % python3 uagent.pyINFO:     [Sample uAgent]: Starting agent with address: agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: My name is Sample uAgent and my address is agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8000&address=agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: Starting server on http://0.0.0.0:8000 (Press CTRL+C to quit)INFO:     [uagents.registration]: Registration on Almanac API successfulINFO:     [uagents.registration]: Registering on almanac contract...INFO:     [uagents.registration]: Registering on almanac contract...complete

#### Copy the uAgent Address[â€‹](#copy-the-uagent-address "Direct link to Copy the uAgent Address")

Look for the uAgent address in the logs, which appears in this format:

    agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6

Copy this uAgent address and paste it into the AI Agent script at the following line:

    uagent_address="PASTE YOUR UAGENT ADDRESS HERE"

### Step 2: Running the AI Agent[â€‹](#step-2-running-the-ai-agent "Direct link to Step 2: Running the AI Agent")

Start the AI Agent by running the following command in your terminal:

#### AI Agent Logs[â€‹](#ai-agent-logs "Direct link to AI Agent Logs")

    (venv) abhi@Fetchs-MacBook-Pro ILAgents % python3 ai_agent.pyINFO:__main__:Client agent started with address: agent1qw7u5sw63a88kmcn5j5kxf7q326u5hgmppvy2vpxlh3re6y0yp8253ec7xlINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Client agent registration complete! * Serving Flask app 'ai_agent' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5002 * Running on http://172.20.10.2:5002INFO:werkzeug:Press CTRL+C to quit

### Step 3: Sending a message from Agent2 to Agent1[â€‹](#step-3-sending-a-message-from-agent2-to-agent1 "Direct link to Step 3: Sending a message from Agent2 to Agent1")

We will use the following curl command to send a message from the AI Agent to the uAgent:

    curl -X POST http://localhost:5002/request \-H "Content-Type: application/json" \-d '{  "payload": {"message": "Hello uAgent!"}}'

#### Expected Logs on the uAgent Terminal[â€‹](#expected-logs-on-the-uagent-terminal "Direct link to Expected Logs on the uAgent Terminal")

    (venv) abhi@Fetchs-MacBook-Pro ILAgents % python3 uagent.pyINFO:     [Sample uAgent]: Starting agent with address: agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: My name is Sample uAgent and my address is agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8000&address=agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: Starting server on http://0.0.0.0:8000 (Press CTRL+C to quit)INFO:     [uagents.registration]: Registration on Almanac API successfulINFO:     [uagents.registration]: Registering on almanac contract...INFO:     [uagents.registration]: Registering on almanac contract...completeINFO:     [Sample uAgent]: Received message from agent1qw7u5sw63a88kmcn5j5kxf7q326u5hgmppvy2vpxlh3re6y0yp8253ec7xl: Hello uAgent!

#### Expected Logs on the AI Agent Terminal[â€‹](#expected-logs-on-the-ai-agent-terminal "Direct link to Expected Logs on the AI Agent Terminal")

    (venv) abhi@Fetchs-MacBook-Pro ILAgents % python3 ai_agent.pyINFO:__main__:Client agent started with address: agent1qw7u5sw63a88kmcn5j5kxf7q326u5hgmppvy2vpxlh3re6y0yp8253ec7xlINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Client agent registration complete! * Serving Flask app 'ai_agent' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5002 * Running on http://172.20.10.2:5002INFO:werkzeug:Press CTRL+C to quit{"version":1,"sender":"agent1qw7u5sw63a88kmcn5j5kxf7q326u5hgmppvy2vpxlh3re6y0yp8253ec7xl","target":"agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6","session":"c28d03fd-ad42-4c09-80b8-2bb8df9d7c3e","schema_digest":"model:14d760ab9a6127711e530c0f1bd84d5caa48c6bc6566ca489581d6918e6dff85","protocol_digest":"proto:a03398ea81d7aaaf67e72940937676eae0d019f8e1d8b5efbadfef9fd2e98bb2","payload":"eyJtZXNzYWdlIjoiaGV5In0=","expires":null,"nonce":null,"signature":"sig14ukfdrc98924wvx66xa2c32pk2wqyn69cepkvcpwahlygv3tc2t8hrzfkuzc9earscpnasdt8txpu99cyw24gmyspfdhqkxsn7a3lnq60mpaq"}INFO:fetchai:Got response looking up agent endpointhttp://localhost:8000/submitINFO:fetchai:Sent message to agentINFO:werkzeug:127.0.0.1 - - [30/Jan/2025 14:21:58] "POST /request HTTP/1.1" 200 -INFO:__main__:Received responseINFO:__main__:Processed response: {'response': 'Hello, AI Agent! I received your message: Hello uAgent!'}INFO:werkzeug:127.0.0.1 - - [30/Jan/2025 14:21:58] "POST /api/webhook HTTP/1.1" 200 -

Explanation of Communication Flow[â€‹](#explanation-of-communication-flow "Direct link to Explanation of Communication Flow")
---------------------------------------------------------------------------------------------------------------------------

*   The AI Agent sends a message using the **send\_message\_to\_agent** function.
*   The uAgent receives the message via the **@uagent.on\_message** handler.
*   The uAgent processes the message and responds using **await ctx.send()**.
*   The AI Agent receives the response through the **/api/webhook** endpoint.

Key Takeaways[â€‹](#key-takeaways "Direct link to Key Takeaways")
---------------------------------------------------------------

*   The **uAgent** handles structured messages using Fetch.aiâ€™s uAgents framework.
*   The **AI Agent** utilizes Fetch.aiâ€™s SDK for message transmission and parsing.
*   Communication between the two agents follows a request-response pattern using their respective handlers.

This setup can be extended to build more complex agent interactions involving dynamic data exchange, service orchestration, and autonomous decision-making.</content>
</page>

<page>
  <title>Agentverse API Key | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.1/agentverse/agentverse-api-key</url>
  <content>Getting Agentverse API Key
--------------------------

To get [Agentverse](https://agentverse.ai/) API Key, just login to Agentverse using your gmail address and go to profile section as shown in picture below:

Click on **\+ New API Key** button, give name to your API Key and with write permission to Access to all resources in Agentverse. Your permission can be for 30 days now. Check below image for more clarity on the process :Â 

Click on **Generate API Key**, it will take you through the authentication process again. Once you do the authentication and Allow Access the API key will pop-up on your screen like the image below and save it somewhere as it cannot be regenerated.

Keep your API Key stored somewhere safe. You wonâ€™t be able to access it again.</content>
</page>

<page>
  <title>Search Agent and MicroServices | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.1/agentverse/searching</url>
  <content>Searching microservices and AI Agents on Agentverse
---------------------------------------------------

When you want to discover or connect with microservices or AI agents dynamically on Agentverse, you can use the \_\_Agentverse Search AP\_\_I. Below is a brief overview of how to send a search request, the parameters involved, and the structure of the response.

Making a Search Request[â€‹](#making-a-search-request "Direct link to Making a Search Request")
---------------------------------------------------------------------------------------------

*   Python
*   Curl
*   JavaScript

    body = {    "filters": {        "state": [],        "category": [],        "agent_type": [],        "protocol_digest": []    },    "sort": "relevancy",    "direction": "asc",    "search_text": "<string>",    "offset": 0,    "limit": 1,}await fetch("https://agentverse.ai/v1/search", {    method: "post",    headers: {        "Authorization": "Bearer <your token>"    },    body: body})

Response[â€‹](#response "Direct link to Response")
------------------------------------------------

You will receive a list of JSON objects with details about each agent:

    [  {    "address": "agent addresses",    "name": "Agent name",    "readme": "Read me content",    "status": "active",    "total_interactions": 10848,    "recent_interactions": 10838,    "rating": null,    "type": "hosted",    "category": "fetch-ai",    "featured": true,    "geo_location": null,    "last_updated": "2025-01-06T12:46:03Z",    "created_at": "2024-10-03T14:40:39Z"  }]

### Available Filters[â€‹](#available-filters "Direct link to Available Filters")

*   **state** : `active`, `inactive`.
*   **category** : `fetch-ai`, `community`.
*   **agent\_type** : `hosted`, `local`, `mailbox`, `proxy`, `custom`.
*   **protocol\_digest** : The protocol in which agent is included into.
*   **model\_digest** : Model digest in which agent is included into.

### Importance of Good Readme[â€‹](#importance-of-good-readme "Direct link to Importance of Good Readme")

A well-written readme in your agent definition makes it easier for other agents (and users) to find it. Make sure you:

*   Include descriptive names, tags, or domains. You can mention `[tags : ]` and `[domain : ]` in your agent.
    
*   Describe the main functions or services the agent provides.
    
*   Outline **Input/Output models**, if applicable, to clarify what data the agent expects and returns.
    
    *   For SDK AI Agent
    
        ![domain:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)domain:domain-of-your-agent<description>My AI's description of capabilities and offerings</description><use_cases>    <use_case>An example of one of your AI's use cases.</use_case></use_cases><payload_requirements><description>The requirements your AI has for requests</description><payload>    <requirement>        <parameter>question</parameter>        <description>The question that you would like this AI work with you to solve</description>    </requirement></payload></payload_requirements>
    
    *   For uAgents
    
        ![tag:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)domain:domain-of-your-agent**Description**:  This AI Agent retrieves real-time stock prices for any publicly traded company based on its ticker symbol. It provides  share prices, stock quotes, and stock prices to users. Simply input a stock ticker (e.g., AAPL, TSLA)  to get the latest stock price.**Input Data Model**class StockPriceRequest(Model):    ticker: str**Output Data Model**class StockPriceResponse(Model):    price: float
    

By following these guidelines, you can improve your agentâ€™s visibility in search results and help others understand its capabilities and usage requirements.

note

**Note:** If you are creating your agents in **`Hackathon`**, do remember to include the innovation labs tags.

    ![domain:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)</content>
</page>

<page>
  <title>Understanding Agentic Systems | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.0/concepts-ai-agents/understanding-agentic-systems</url>
  <content>### The Spectrum of LLM Applications[â€‹](#the-spectrum-of-llm-applications "Direct link to The Spectrum of LLM Applications")

It's crucial to understand where different types of LLM-based systems fall on the complexity spectrum:

1.  **Simple LLM Applications**
    
    *   Single LLM calls with prompt engineering
    *   Basic RAG (Retrieval-Augmented Generation) implementations
    *   Structured output parsing
    *   These are NOT agents, just applications using LLM capabilities
2.  **Multi-Step LLM Applications**
    
    *   Multiple LLM calls in sequence
    *   API integrations and external tool usage
    *   Basic state management
    *   While more complex, these still aren't necessarily agents
3.  **True AI Agents**
    
    *   Dynamic decision-making about process flow
    *   Autonomous tool selection and usage
    *   Complex state management and memory
    *   Goal-oriented behavior with adaptive strategies

### Common Misconceptions[â€‹](#common-misconceptions "Direct link to Common Misconceptions")

#### Misconception 1: "My application uses multiple LLM calls, so it's an agent"[â€‹](#misconception-1-my-application-uses-multiple-llm-calls-so-its-an-agent "Direct link to Misconception 1: \"My application uses multiple LLM calls, so it's an agent\"")

    # This is NOT an agent - it's a multi-step LLM applicationdef process_document(doc):    summary = llm.generate_summary(doc)    keywords = llm.extract_keywords(summary)    sentiment = llm.analyze_sentiment(summary)    return {        "summary": summary,        "keywords": keywords,        "sentiment": sentiment    }

#### Misconception 2: "I'm using tools and APIs, so it's an agent"[â€‹](#misconception-2-im-using-tools-and-apis-so-its-an-agent "Direct link to Misconception 2: \"I'm using tools and APIs, so it's an agent\"")

    # This is NOT an agent - it's an automated workflowdef analyze_stock(symbol):    price_data = stock_api.get_price(symbol)    news = news_api.get_recent_news(symbol)    analysis = llm.analyze(f"Price: {price_data}, News: {news}")    return analysis

### Understanding Agentic Systems[â€‹](#understanding-agentic-systems "Direct link to Understanding Agentic Systems")

#### 1\. Workflows vs Agents[â€‹](#1-workflows-vs-agents "Direct link to 1. Workflows vs Agents")

**Workflows:**

    # Predefined workflow exampleclass FinancialAnalysisWorkflow:    def execute(self, query):        # Fixed sequence of steps        market_data = self.get_market_data()        financial_statements = self.get_financial_statements()        competitor_analysis = self.get_competitor_data()                return self.generate_analysis(            market_data,            financial_statements,            competitor_analysis        )

**Agents:**

    # True agent exampleclass FinancialAnalysisAgent:    def analyze(self, query):        # Dynamic planning        analysis_plan = self.plan_analysis_strategy(query)                # Flexible execution        for step in analysis_plan:            if self.needs_revision(step, self.current_state):                analysis_plan = self.revise_plan()                        tool = self.select_appropriate_tool(step)            result = self.execute_tool(tool, step)            self.update_state(result)                        if self.goal_achieved():                break                return self.synthesize_findings()

### When to Use Each Approach[â€‹](#when-to-use-each-approach "Direct link to When to Use Each Approach")

1.  **Use Simple LLM Applications When:**
    
    *   Tasks are straightforward and well-defined
    *   Minimal or no tool interaction is needed
    *   Quick response times are critical
    *   Cost efficiency is a primary concern
2.  **Use Workflows When:**
    
    *   Process steps are well-understood and consistent
    *   Predictability is more important than flexibility
    *   You need tight control over execution
    *   Performance and reliability are crucial
3.  **Use Agents When:**
    
    *   Tasks require dynamic decision-making
    *   Problems have multiple valid solution paths
    *   Flexibility and adaptation are crucial
    *   Complex tool interactions are needed
    *   Tasks benefit from maintaining context

### Implementation Best Practices[â€‹](#implementation-best-practices "Direct link to Implementation Best Practices")

1.  **Start Simple**
    
    *   Begin with the simplest solution that could work
    *   Only add complexity when clearly needed
    *   Validate the need for agency before implementing it
2.  **Consider Trade-offs**
    
    *   Agents typically have higher latency
    *   Cost increases with complexity
    *   Maintenance burden grows with sophistication
3.  **Design for Observability**
    
    *   Implement comprehensive logging
    *   Track decision-making processes
    *   Monitor tool usage patterns
4.  **Build in Safeguards**
    
    *   Implement rate limiting
    *   Add timeout mechanisms
    *   Create fallback strategies

### Evaluation Criteria for True Agency[â€‹](#evaluation-criteria-for-true-agency "Direct link to Evaluation Criteria for True Agency")

Before implementing an agent, evaluate if your system really needs true agency by checking these criteria:

1.  **Decision Autonomy**
    
    *   Can the system choose different paths based on context?
    *   Does it make meaningful decisions about tool usage?
    *   Can it adapt its strategy during execution?
2.  **State Management**
    
    *   Does it maintain meaningful state?
    *   Can it use past interactions to inform decisions?
    *   Does it track progress toward goals?
3.  **Tool Integration**
    
    *   Can it choose tools dynamically?
    *   Does it understand tool capabilities?
    *   Can it combine tools in novel ways?
4.  **Goal Orientation**
    
    *   Does it understand and work toward specific objectives?
    *   Can it recognize when goals are achieved?
    *   Can it adjust goals based on new information?</content>
</page>

<page>
  <title>End to End Application with Agentverse Architecture | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.1/agentverse/agentverse-based-application</url>
  <content>This diagram depicts an agent-based architecture, where the Client Application interacts with a Prime Agent to discover and utilize various specialized Agents registered within the AgentVerse. The Prime Agent orchestrates the communication between the Client and the Agents to fulfill the client's requests.

The flow of the system is as follows:

1.  The client application searches the AgentVerse to discover available agents.
2.  The client application sends a request to the Prime Agent.
3.  The Prime Agent communicates with the AgentVerse to find the appropriate Agent.
4.  The Prime Agent sends the request to the Agent.
5.  The Agent processes the request and sends the response back to the Prime Agent.
6.  The Prime Agent returns the final response to the client application.

This architecture allows for a modular and extensible system, where new agents can be easily integrated, and the Prime Agent can orchestrate the interactions between agents to fulfill complex client requests.

Core Components and Implementation[â€‹](#core-components-and-implementation "Direct link to Core Components and Implementation")
------------------------------------------------------------------------------------------------------------------------------

The system enables intelligent financial analysis through a team of specialized agents, coordinated via Agentverse. It combines frontend user interaction with backend processing through multiple agent layers.

### 1\. React Frontend (Client Application)[â€‹](#1-react-frontend-client-application "Direct link to 1. React Frontend (Client Application)")

*   React-based user interface (OptimusPrime component)
*   Two main API endpoints:
    *   `/api/send-request`: Sends analysis queries
    *   `/api/get-response`: Polls for results
*   Handles message display and user interactions

### 2\. Primary Agent (Query Router)[â€‹](#2-primary-agent-query-router "Direct link to 2. Primary Agent (Query Router)")

*   **Port**: 5001
*   **Role**: Routes queries to Financial Analysis Agent
*   **Key Functions**:
    *   Discovers Financial Analysis Agent through Agentverse
    *   Forwards user queries
    *   Manages response polling
*   **Endpoints**:
    *   `/webhook`: Receives responses from Financial Agent

### 3\. Financial Analysis Agent[â€‹](#3-financial-analysis-agent "Direct link to 3. Financial Analysis Agent")

*   **Port**: 5008
*   **Role**: Processes financial analysis requests
*   **Components**:
    *   Supervisor Agent: Coordinates analysis
    *   Search Agent: Handles market research
    *   SEC Analyst: Processes SEC filings
*   **Tools**:
    *   RAG System for document analysis
    *   Tavily Search for market data

### 4\. Agentverse Integration[â€‹](#4-agentverse-integration "Direct link to 4. Agentverse Integration")

*   **Agent Discovery**: Allows finding agents by capability
*   **Agent Registry**: Manages agent registration and lookup
*   **Message Routing**: Handles inter-agent communication

Communication Flow[â€‹](#communication-flow "Direct link to Communication Flow")
------------------------------------------------------------------------------

1.  User submits query through React UI
2.  Primary Agent searches for Financial Analysis Agent
3.  Primary Agent forwards query to Financial Agent
4.  Financial Agent processes query using specialist team
5.  Response returns through Agentverse to Primary Agent
6.  Frontend polls Primary Agent for results
7.  Results displayed to user

Key Implementation Details[â€‹](#key-implementation-details "Direct link to Key Implementation Details")
------------------------------------------------------------------------------------------------------

### Frontend[â€‹](#frontend "Direct link to Frontend")

    // Sends request and starts pollingconst handleSendMessage = async () => {    await fetch('/api/send-request', {...});    startPolling();  // Begin checking for response};

### Primary Agent[â€‹](#primary-agent "Direct link to Primary Agent")

    # Agent discovery and routingdef find_financial_agent():    available_ais = fetch.ai("Financial Analysis Agent")    return available_ais.get('ais', [0])@app.route('/webhook', methods=['POST'])def webhook():    message = parse_message_from_agent(data)    primary_agent.latest_response = message.payload

### Financial Agent registration with Agentverse[â€‹](#financial-agent-registration-with-agentverse "Direct link to Financial Agent registration with Agentverse")

    # Agent registrationregister_with_agentverse(    identity=financial_identity,    url="http://localhost:5008/webhook",    agent_title="Financial Analysis Agent",    readme="..."  # Capabilities description)

Detailed Implementation Details
-------------------------------

1\. Frontend Implementation (React)[â€‹](#1-frontend-implementation-react "Direct link to 1. Frontend Implementation (React)")
----------------------------------------------------------------------------------------------------------------------------

### Message Handling and UI State[â€‹](#message-handling-and-ui-state "Direct link to Message Handling and UI State")

    const OptimusPrime = () => {    const [messages, setMessages] = useState([]);    const [inputText, setInputText] = useState('');    const [isProcessing, setIsProcessing] = useState(false);    // Handles submitting new messages    const handleSendMessage = async () => {        if (!inputText.trim() || isProcessing) return;        // Add user message to UI        const userMessage = {            type: 'user',            content: inputText,            timestamp: new Date().toLocaleTimeString()        };        setMessages(prev => [...prev, userMessage]);        setInputText('');        setIsProcessing(true);        try {            // Send request to primary agent                        await fetch('/api/send-request', {                method: 'POST',                headers: { 'Content-Type': 'application/json' },                body: JSON.stringify({ input: inputText }),            });            // Start polling for response            startPollingForResponse();        } catch (error) {            handleError(error);        }    };

### Response Polling System[â€‹](#response-polling-system "Direct link to Response Polling System")

        // Polls for agent response    const startPollingForResponse = () => {        const pollInterval = setInterval(async () => {            try {                const responseData = await fetch('/api/get-response');                const data = await responseData.json();                if (data.status !== 'waiting' && data.analysis_result) {                    clearInterval(pollInterval);                    setIsProcessing(false);                    // Process agent responses                    data.analysis_result.analysis.forEach(response => {                        setMessages(prev => [...prev, {                            type: 'agent',                            agentName: response.name || 'Agent',                            content: response.content,                            timestamp: new Date().toLocaleTimeString()                        }]);                    });                }            } catch (error) {                clearInterval(pollInterval);                setIsProcessing(false);                handleError(error);            }        }, 1000);    };

2\. Primary Agent Implementation[â€‹](#2-primary-agent-implementation "Direct link to 2. Primary Agent Implementation")
---------------------------------------------------------------------------------------------------------------------

### Agent Setup and Initialization[â€‹](#agent-setup-and-initialization "Direct link to Agent Setup and Initialization")

    class PrimaryAgent:    def __init__(self):        self.identity = None        self.latest_response = None        def initialize(self):        try:            # Initialize agent identity            self.identity = Identity.from_seed(                os.getenv("PRIMARY_AGENT_KEY"),                 0            )                        # Register with Agentverse            register_with_agentverse(                identity=self.identity,                url="http://localhost:5001/webhook",                agentverse_token=os.getenv("AGENTVERSE_API_KEY"),                agent_title="Financial Query Router",                readme="<description>Routes queries to Financial Analysis Agent</description>"            )        except Exception as e:            logger.error(f"Initialization error: {e}")            raise

### Financial Agent Discovery and Communication[â€‹](#financial-agent-discovery-and-communication "Direct link to Financial Agent Discovery and Communication")

        def find_financial_agent(self):        """Find registered financial analysis agent"""        try:            # Search for financial agent in Agentverse            available_ais = fetch.ai("Financial Analysis Agent")            agents = available_ais.get('ais', [])                        if agents:                logger.info(f"Found financial agent at address: {agents[0]['address']}")                return agents[0]            return None                    except Exception as e:            logger.error(f"Error finding financial agent: {e}")            return None    @app.route('/api/send-request', methods=['POST'])    def send_request():        try:            # Extract user query            data = request.json            user_input = data.get('input')                        # Find and validate financial agent            agent = primary_agent.find_financial_agent()            if not agent:                return jsonify({"error": "Financial analysis agent not available"}), 404                        # Forward request to financial agent            send_message_to_agent(                primary_agent.identity,                agent['address'],                {"request": user_input}            )                        return jsonify({"status": "request_sent"})                    except Exception as e:            logger.error(f"Error processing request: {e}")            return jsonify({"error": str(e)}), 500

### Response Management[â€‹](#response-management "Direct link to Response Management")

        @app.route('/webhook', methods=['POST'])    def webhook():        try:            # Parse incoming agent message            data = request.get_data().decode("utf-8")            message = parse_message_from_agent(data)                        # Store response for polling            primary_agent.latest_response = message.payload                        return jsonify({"status": "success"})                    except Exception as e:            logger.error(f"Error in webhook: {e}")            return jsonify({"error": str(e)}), 500    @app.route('/api/get-response', methods=['GET'])    def get_response():        try:            if primary_agent.latest_response:                response = primary_agent.latest_response                primary_agent.latest_response = None                return jsonify(response)            return jsonify({"status": "waiting"})        except Exception as e:            logger.error(f"Error getting response: {e}")            return jsonify({"error": str(e)}), 500

3\. Financial Analysis Agent Registration[â€‹](#3-financial-analysis-agent-registration "Direct link to 3. Financial Analysis Agent Registration")
------------------------------------------------------------------------------------------------------------------------------------------------

### Agent Registration and Setup[â€‹](#agent-registration-and-setup "Direct link to Agent Registration and Setup")

    def init_agent():    """Initialize and register the agent with agentverse"""    global financial_identity, research_chain    try:        # Initialize the research chain        research_chain = init_financial_system()                # Initialize identity and register with Agentverse        financial_identity = Identity.from_seed(            os.getenv("FINANCIAL_AGENT_KEY"),             0        )                # Register with detailed capabilities description        register_with_agentverse(            identity=financial_identity,            url="http://localhost:5008/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="Financial Analysis Agent",            readme = """                <description>A comprehensive financial analysis agent that combines                 SEC filing analysis with real-time market data for Apple Inc.</description>                <use_cases>                    <use_case>Get detailed revenue analysis from SEC filings</use_case>                    <use_case>Analyze risk factors from latest 10-K</use_case>                    <use_case>Track financial metrics and trends</use_case>                </use_cases>                <payload_requirements>                    <payload>                        <requirement>                            <parameter>query</parameter>                            <description>What would you like to know about Apple's financials?</description>                        </requirement>                    </payload>                </payload_requirements>            """        )

### Query Processing and Response[â€‹](#query-processing-and-response "Direct link to Query Processing and Response")

    @app.route('/webhook', methods=['POST'])def webhook():    try:        # Parse incoming message        data = request.get_data().decode('utf-8')        message = parse_message_from_agent(data)        query = message.payload.get("request", "")        agent_address = message.sender        # Validate query        if not query:            return jsonify({"status": "error", "message": "No query provided"}), 400        # Process query using research chain        result = research_chain.invoke({            "messages": [HumanMessage(content=query)],            "team_members": ["Search", "SECAnalyst"]        })        # Format response for client        formatted_result = {            "analysis": [                {                    "role": msg.type if hasattr(msg, 'type') else "message",                    "content": msg.content,                    "name": msg.name if hasattr(msg, 'name') else None                }                for msg in result.get('messages', [])            ]        }        # Send response back through Agentverse        send_message_to_agent(            financial_identity,            agent_address,            {'analysis_result': formatted_result}        )        return jsonify({"status": "analysis_sent"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"status": "error", "message": str(e)}), 500</content>
</page>

<page>
  <title>Team Based Artchitectures | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.0/concepts-ai-agents/team-based-architectures</url>
  <content>### Multi-Agent Architectures[â€‹](#multi-agent-architectures "Direct link to Multi-Agent Architectures")

Multi-agent systems distribute complex tasks across specialized agents, each with distinct roles and capabilities. This architecture provides several advantages:

1.  **Specialization**
    
    *   Agents can focus on specific domains
    *   Reduced prompt complexity per agent
    *   Clear separation of concerns
2.  **Scalability**
    
    *   Parallel processing capabilities
    *   Distributed decision making
    *   Independent scaling of components
3.  **Robustness**
    
    *   Graceful degradation
    *   Fault isolation
    *   Easier maintenance

### Team-Based Agent Architecture[â€‹](#team-based-agent-architecture "Direct link to Team-Based Agent Architecture")

A team-based architecture typically consists of three main components:

1.  **Supervisor Agent**
    
    *   Coordinates team activities
    *   Makes routing decisions
    *   Ensures task completion
2.  **Specialist Agents**
    
    *   Handle domain-specific tasks
    *   Maintain focused expertise
    *   Provide detailed analysis
3.  **State Management System**
    
    *   Maintains conversation context
    *   Tracks team progress
    *   Manages shared resources

### Implementing a Team-Based System[â€‹](#implementing-a-team-based-system "Direct link to Implementing a Team-Based System")

#### 1\. Supervisor Agent Implementation[â€‹](#1-supervisor-agent-implementation "Direct link to 1. Supervisor Agent Implementation")

    def create_supervisor_agent(llm: ChatOpenAI):    """    Create a supervisor agent that coordinates team activities.        Key responsibilities:    - Task decomposition    - Agent selection    - Progress monitoring    - Final response synthesis    """    supervisor_prompt = """    Core responsibilities:    1. Analyze incoming queries    2. Determine required information    3. Select appropriate specialist    4. Monitor progress    5. Decide when to conclude    """        return create_team_supervisor(        llm=llm,        system_prompt=supervisor_prompt,        members=["SpecialistA", "SpecialistB"]    )

#### 2\. Specialist Agent Implementation[â€‹](#2-specialist-agent-implementation "Direct link to 2. Specialist Agent Implementation")

    def create_specialist_agent(    llm: ChatOpenAI,    domain: str,    tools: List[Tool]):    """    Create a specialist agent with domain-specific capabilities.        Key features:    - Focused expertise    - Specific tool access    - Clear output format    """    system_prompt = f"""    You are specialized in {domain}.    When responding:    1. Use domain-specific tools    2. Provide structured output    3. Request missing information    """        return create_agent(        llm=llm,        tools=tools,        system_prompt=system_prompt    )

#### 3\. State Management[â€‹](#3-state-management "Direct link to 3. State Management")

    class TeamState(TypedDict):    """    Manage team-wide state and context.        Components:    - Message history    - Team composition    - Current status    - Required information    """    messages: List[BaseMessage]    team_members: List[str]    next: str    information_needed: List[str]    reasoning: str

### Building the Team Graph[â€‹](#building-the-team-graph "Direct link to Building the Team Graph")

The team graph defines how agents interact and how information flows:

    def create_team_graph():    """    Create a coordinated team of agents with defined interactions.        Structure:    1. Initialize components    2. Define connections    3. Set workflow rules    4. Establish entry points    """    # Initialize agents    specialist_a = create_specialist_agent(...)    specialist_b = create_specialist_agent(...)    supervisor = create_supervisor_agent(...)        # Create graph    graph = StateGraph(TeamState)        # Add nodes    graph.add_node("SpecialistA", specialist_a)    graph.add_node("SpecialistB", specialist_b)    graph.add_node("supervisor", supervisor)        # Define workflows    graph.add_edge("SpecialistA", "supervisor")    graph.add_edge("SpecialistB", "supervisor")        # Add conditional routing    graph.add_conditional_edges(        "supervisor",        lambda x: x["next"],        {            "SpecialistA": "SpecialistA",            "SpecialistB": "SpecialistB",            "FINISH": END        }    )        return graph.compile()</content>
</page>

<page>
  <title>Microservices and AI Agents | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.0/concepts-ai-agents/agent-comparison</url>
  <content>Fetch.ai Microservice Agent and AI Agent
----------------------------------------

Before diving into the world of AI agents, letâ€™s first explore the key differences between microservices and AI agents.

*   **Microservice**: If you want to fetch specific information or perform an isolated action, you typically use tools or APIsâ€”this is known as a microservice. Imagine you want to check available flights from London to New York. Youâ€™d use an API or tool to fetch the available options. Thatâ€™s a microservice in actionâ€”focused, specific, and task-oriented.
    
*   **AI Agent**: If you have a larger objective that requires coordination between multiple microservices or tools, thatâ€™s where an AI agent steps in. Now, letâ€™s say you want to book the best possible flight. This involves more than just checking flight options: youâ€™ll compare prices across different days, check your personal schedule, and evaluate additional services like baggage policies or meal preferences. To achieve this goal, you need reasoning and decision-making capabilitiesâ€”this is where an AI agent comes into play.
    

The Fetch.ai ecosystem offers a variety of tools to build both microservices and AI agents, enabling developers to cater to different use cases.

### Microservices with uAgents[â€‹](#microservices-with-uagents "Direct link to Microservices with uAgents")

uAgents is Fetch.ai's lightweight Python framework for creating agents (microservices). Itâ€™s designed to be simple and efficient, making it a great choice for building basic microservices.

### AI Agents with the Fetch.ai SDK[â€‹](#ai-agents-with-the-fetchai-sdk "Direct link to AI Agents with the Fetch.ai SDK")

Fetch.ai's SDK is built on top of the uAgents framework. It simplifies the creation of intelligent, dynamic AI agents that can interact with other agents and microservices seamlessly. The SDK allows for more flexibility, reasoning, and adaptability compared to uAgents.

Youâ€™ll learn in the upcoming sections how to decide between these tools based on the problem youâ€™re solving.

How Fetch.aiâ€™s Ecosystem Works[â€‹](#how-fetchais-ecosystem-works "Direct link to How Fetch.aiâ€™s Ecosystem Works")
----------------------------------------------------------------------------------------------------------------

Fetch.ai provides an integrated ecosystem to host and manage your agents, ensuring connectivity and scalability.

1.  **Agentverse Platform**:
    
    *   Agents can be hosted on the Agentverse platform, allowing them to stay active and accessible.
    *   Due to security constraints, Agentverse only supports a limited set of packages.
2.  **Mailbox Service**:
    
    *   For scenarios requiring custom environments (like virtual machines), you can use the Mailbox Service. This lets you host agents outside Agentverse while maintaining connectivity.
    *   The mailbox ensures that messages sent to an agent are queued, so the agent can process them once it comes online.
3.  **Agent Registration with the Almanac Contract**:
    
    *   When agents are registered on Agentverse, they are also listed in the Almanac Contract on Fetch.aiâ€™s blockchain. This registry allows other agents to discover your agent using its unique address.

Why Use the Fetch.ai SDK?[â€‹](#why-use-the-fetchai-sdk "Direct link to Why Use the Fetch.ai SDK?")
-------------------------------------------------------------------------------------------------

While the uAgents framework is sufficient for basic microservices, the Fetch.ai SDK offers additional capabilities suited for more complex AI agents. Hereâ€™s why the SDK exists:

*   Dynamic Messaging:
    *   uAgents require precise Data Models to identify and process messages. In contrast, the SDK enables agents to handle dynamic messages, making it more adaptable.
*   Handler Differences:
    *   uAgents rely on predefined handlers (e.g., interval-based or message-based handlers).
    *   SDK-based agents leverage Flask webhooks, enabling more flexible and scalable communication.

With these distinctions, the SDK empowers developers to build agents capable of reasoning, adaptability, and complex decision-making.</content>
</page>

<page>
  <title>Autogen Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.1/other-frameworks/autogen</url>
  <content>Registering an Autogen Agent on Agentverse Marketplace
------------------------------------------------------

This document is a comprehensive guide to help users register a **Multi-Agent Autogen System** on **Fetch.ai's Agentverse** ecosystem using the provided **Autogen Code Execution Agent** and **User Agent** scripts. AutoGen is an open-source programming framework for building AI agents and facilitating cooperation among multiple agents to solve tasks. These agents work together to execute a Python code and demonstrate how Autogen Agents can interact seamlessly with other agents on Agentverse.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

### Purpose[â€‹](#purpose "Direct link to Purpose")

This project showcases:

*   **Registering** a Multi-Agent System (Autogen) on Agentverse to execute Python code.
*   **Facilitating Communication** between agents within Fetch.aiâ€™s Agentverse via webhooks.
*   **Providing** a clear example of how to return code execution results in a human-friendly format.

### Key Features[â€‹](#key-features "Direct link to Key Features")

*   The **Autogen Code Execution Agent** handles requests for code generation and execution using the Autogen tools.
*   The **User Agent** acts as a client to forward requests and retrieve responses from the Autogen Code Execution Agent.
*   Communication is done via **HTTP webhooks** registered with **Fetch.aiâ€™s Agentverse**.

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

### Environmental Setup[â€‹](#environmental-setup "Direct link to Environmental Setup")

*   Python 3.8 or higher.
*   A virtual environment (recommended).
*   Flask and related libraries installed.

### Architecture Diagram[â€‹](#architecture-diagram "Direct link to Architecture Diagram")

### Required Libraries[â€‹](#required-libraries "Direct link to Required Libraries")

Install the necessary libraries using:

    pip install flask flask-cors fetchai autogen "flask[async]"

### Environment Variables[â€‹](#environment-variables "Direct link to Environment Variables")

Create a `.env` file in your project directory:

    AV_AUTOGEN_CODE_EXECUTION_AI_KEY='<your_autogen_code_execution_ai_key>'USER_AUTOGEN_AI_KEY='<your_user_ai_key>'AGENTVERSE_API_KEY='<your_agentverse_api_key>'OPENAI_API_KEY='<your_openai_api_key>'

Replace placeholders with your actual keys:

*   **AGENTVERSE\_API\_KEY**: Refer this [guide here](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key).
    
*   **OPENAI\_API\_KEY**: Sign up on the [OpenAI](https://platform.openai.com/api-keys) website.
    

Autogen Code Execution Agent Script[â€‹](#autogen-code-execution-agent-script "Direct link to Autogen Code Execution Agent Script")
---------------------------------------------------------------------------------------------------------------------------------

In this example, the **Autogen Code Execution Agent** registered on Agentverse comprises an **AssistantAgent** and a **UserProxyAgent** to write code and execute it. The system focuses on:

*   **Registering** with Fetch.aiâ€™s Agentverse
*   **Handling** requests to return the outcome of a Python code in a human friendly format.
*   **Responding** to the User Agent with the outcome of the code.

### Script Breakdown (autogen-agent.py)[â€‹](#script-breakdown-autogen-agentpy "Direct link to Script Breakdown (autogen-agent.py)")

**Importing Required Libraries**

    import osimport autogenfrom autogen.coding import LocalCommandLineCodeExecutorfrom flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osimport openaifrom dotenv import load_dotenvfrom fetchai import fetchimport asyncio

**Setting Up Logging**

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)

**Flask Webhook**

The Flask app is created to handle incoming webhook requests:

**Code Execution Function**

Below is the code execution function using the **Autogen Multi-Agent** system, where an **AssistantAgent** and a **UserProxyAgent** communicate until the task is done.

    async def execute_code(prompt):    config_list = [{"model": "gpt-4o", "api_key": os.getenv("OPENAI_API_KEY")}]    # create an AssistantAgent named "assistant"    assistant = autogen.AssistantAgent(        name="assistant",        llm_config={            "cache_seed": 41,  # seed for caching and reproducibility            "config_list": config_list,  # a list of OpenAI API configurations            "temperature": 0,  # temperature for sampling        },  # configuration for autogen's enhanced inference API which is compatible with OpenAI API    )    # create a UserProxyAgent instance named "user_proxy"    user_proxy = autogen.UserProxyAgent(        name="user_proxy",        human_input_mode="NEVER",        max_consecutive_auto_reply=10,        is_termination_msg=lambda x: x.get("content", "").rstrip().endswith("TERMINATE"),        code_execution_config={            # the executor to run the generated code            "executor": LocalCommandLineCodeExecutor(work_dir="coding"),        },    )    # the assistant receives a message from the user_proxy, which contains the task description    chat_res = user_proxy.initiate_chat(        assistant,        message=prompt,        summary_method="reflection_with_llm",    )    if hasattr(chat_res, 'chat_history') and chat_res.chat_history:        last_message = chat_res.chat_history[-1]['content']        return last_message    return "No messages found."

**Webhook Endpoint**

    @app.route('/webhook', methods=['POST'])async def webhook():    try:        # Parse the incoming message        logging.info("Parsing message")        data = request.get_data().decode('utf-8')        message = parse_message_from_agent(data)	# Extract the prompt from the message         prompt = message.payload.get("prompt", "")        logger.info(prompt)        agent_address = message.sender	#Call the Autogen Agents to execute the code        response = await execute_code(prompt)        payload = {"response":response}	#Return the message back to the User Agent        send_message_to_agent(            sender=ai_identity,            target=agent_address,            payload=payload        )        return jsonify({"status": "graphs_sent"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"status": "error", "message": str(e)}), 500

**Registering the Agent**

    def init_agent():    """Initialize and register the client agent."""    global ai_identity    try:        ai_identity = Identity.from_seed(os.getenv("AV_AUTOGEN_CODE_EXECUTION_AI_KEY"), 0)        register_with_agentverse(            identity = ai_identity,            url = "http://localhost:5001/webhook", ## The webhook that your AI receives messages on.            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title = "Autogen Code Execution Agent",            readme = """        <description>This agent generates code and executes it to provide final output to the user.</description>        <use_cases>            <use_case>What date is today? Compare the year-to-date gain for META and TESLA.</use_case>        </use_cases>        <payload_requirements>        <description>Please provide a prompt for code execution.</description>        <payload>            <requirement>                <parameter>prompt</parameter>                <description>Please provide a prompt for code execution.The prompt will help this AI generate code and return the output to the user. </description>            </requirement>        </payload>        </payload_requirements>        """        )        logger.info("Autogen Code Execution agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise

**Running the Agent**

    if __name__ == "__main__":    init_agent()    app.run(host="0.0.0.0", port=5001, debug=True)

User Agent Script[â€‹](#user-agent-script "Direct link to User Agent Script")
---------------------------------------------------------------------------

The **User Agent** script acts as the client that forwards requests to the **Autogen Code Execution Agent** and retrieves responses.

### Script Breakdown (autogen-user-agent.py)[â€‹](#script-breakdown-autogen-user-agentpy "Direct link to Script Breakdown (autogen-user-agent.py)")

**Importing Libraries**

    from flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai import fetchfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osimport timefrom dotenv import load_dotenv

**Setting Up Logging and Flask**

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app)

**Initializing the User Agent**

    def init_agent():    """Initialize and register the client agent."""    global client_identity    try:        # Load the client identity from environment variables        client_identity = Identity.from_seed(os.getenv("USER_AUTOGEN_AI_KEY"), 0)        logger.info(f"Client agent started with address: {client_identity.address}")        # Define the client agent's metadata        readme = """        <description>Frontend client that interacts with Autogen Agent to execute code.</description>        <use_cases>            <use_case>Sends Request to the AI Agent to execute a code</use_case>        </use_cases>        <payload_requirements>            <description>Expects request which is to be sent to another agent.</description>            <payload>                <requirement>                    <parameter>request</parameter>                    <description>This is the request which is to be sent to other agent</description>                </requirement>            </payload>        </payload_requirements>        """        # Register the agent with Agentverse        register_with_agentverse(            identity=client_identity,            url="http://localhost:5002/api/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="User agent for Autogen Code Execution Agent",            readme=readme        )        logger.info("Client agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise

**Searching Agents**

    # searching the agents which can create dashboard on agentverse@app.route('/api/search-agents', methods=['GET'])def search_agents():   """Search for available dashboard agents based on user input."""   try:       # Extract user input from query parameters       user_query = request.args.get('query', '')       if not user_query:           return jsonify({"error": "Query parameter 'query' is required."}), 400       # Fetch available agents based on user query       available_ais = fetch.ai(user_query)  # Pass the user query to the fetch.ai function       print(f'---------------------{available_ais}----------------------')       # Access the 'ais' list within 'agents' (assuming fetch.ai returns the correct structure)       agents = available_ais.get('ais', [])       print(f'----------------------------------{agents}------------------------------------')       extracted_data = []       for agent in agents:           name = agent.get('name')  # Extract agent name           address = agent.get('address')           # Append formatted data to extracted_data list           extracted_data.append({               'name': name,               'address': address,           })       # Format the response with indentation for readability       response = jsonify(extracted_data)       response.headers.add('Content-Type', 'application/json; charset=utf-8')       response.headers.add('Access-Control-Allow-Origin', '*')       response.headers.add('Access-Control-Allow-Headers', 'Content-Type')       return response, 200   except Exception as e:       logger.error(f"Error finding agents: {e}")       return jsonify({"error": str(e)}), 500

**Sending Data to Autogen Code Execution Agent**

    @app.route('/api/send-data', methods=['POST'])def send_data():   """Send payload to the selected agent based on provided address."""   global agent_response   agent_response = None   try:       # Parse the request payload       data = request.json       payload = data.get('payload')  # Extract the payload dictionary       agent_address = data.get('agentAddress')  # Extract the agent address       # Validate the input data       if not payload or not agent_address:           return jsonify({"error": "Missing payload or agent address"}), 400       logger.info(f"Sending payload to agent: {agent_address}")       logger.info(f"Payload: {payload}")       # Send the payload to the specified agent       send_message_to_agent(           client_identity,  # Frontend client identity           agent_address,    # Agent address where we have to send the data           payload           # Payload containing the data       )       return jsonify({"status": "request_sent", "agent_address": agent_address, "payload": payload})   except Exception as e:       logger.error(f"Error sending data to agent: {e}")       return jsonify({"error": str(e)}), 500

**Webhook Endpoint**

    # app route to get recieve the messages on the agent@app.route('/webhook', methods=['POST'])def webhook():    """Handle incoming messages from the dashboard agent."""    global agent_response    try:        # Parse the incoming webhook message        data = request.get_data().decode("utf-8")        logger.info("Received response")        message = parse_message_from_agent(data)        agent_response = message.payload        logger.info(f"Processed response: {agent_response}")        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500

**Retrieving the Agent's Response**

    @app.route('/api/get-response', methods=['GET'])def get_response():    global agent_response    try:        if agent_response:            response = agent_response            print(f'response : {response}')            agent_response = None  # Clear the response after sending            keys = list(response.keys())  # Convert dict_keys to a list            first_key = keys[0]  # Get the first key            response_final = response.get(first_key, "")            logger.info(f"Got response for after code execution {response_final}")            return jsonify({"response": response_final})        else:            return jsonify({"error": "No storm response available"}), 404    except Exception as e:        logger.error(f"Error creating markdown file: {e}")        return jsonify({"error": str(e)}), 500

**Running the agent**

    if __name__ == "__main__":    init_agent()    app.run(host="0.0.0.0", port=5002)

Steps to Run the Scripts[â€‹](#steps-to-run-the-scripts "Direct link to Steps to Run the Scripts")
------------------------------------------------------------------------------------------------

Follow these steps to get both the **Autogen Code Execution Agent** and **User Agent** scripts running.

**1\. Start the Alphavantage Agent**

Ensure the `AV_AUTOGEN_CODE_EXECUTION_AI_KEY` and `AGENTVERSE_API_KEY` values are correctly set in the `.env` file. Then Run:

**Expected Output:**

    (.venv) kshipra@MacBook-Pro autogen-code-generation-debugging-example % python3 autogen-agent.pyflaml.automl is not available. Please install flaml[automl] to enable AutoML functionalities.INFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Autogen Code Execution agent registration complete! * Serving Flask app 'autogen-agent' * Debug mode: onINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5001 * Running on http://192.168.6.11:5001INFO:werkzeug:Press CTRL+C to quitINFO:werkzeug: * Restarting with statflaml.automl is not available. Please install flaml[automl] to enable AutoML functionalities.INFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Autogen Code Execution agent registration complete!WARNING:werkzeug: * Debugger is active!INFO:werkzeug: * Debugger PIN: 471-951-559

The agent listens on **port 5001**.

**2\. Start the User Agent**

Ensure the `USER_AUTOGEN_AI_KEY` and `AGENTVERSE_API_KEY` values are set in the `.env` file.

    python3 autogen-user-agent.py

**Expected Output:**

    kshipra@MacBook-Pro autogen-code-generation-debugging-example % python3 autogen-user-agent.py INFO:__main__:Client agent started with address: agent1q0dfdn7073m75c3dc2sdd5gajevswmmwkw6wxgqhjca9ug6c2ruzk7nak0dINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Client agent registration complete! * Serving Flask app 'autogen-user-agent' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5002 * Running on http://192.168.6.11:5002

### Interacting with the Agents[â€‹](#interacting-with-the-agents "Direct link to Interacting with the Agents")

Once both agents are running, use the following endpoints to interact with them from another terminal:

**1\. Search for Agents**

To list agents in the Agentverse:

    curl -X GET "http://localhost:5002/api/search-agents?query=I%20want%20to%20write%20a%20oython%20code"

**Sample Output**

    [{"address":"agent1qwkzy0twyd27egqfrcw8m6vdtsz7hrm9leu7jfezzzlgckywj7shytz5kf6","name":"Alphavantage Stock Price Langchain tool"},{"address":"agent1qtnc7ruc63g3n84qczekqc6qwp0e5ayvs3v8e5pae563qrqcf852xlqz2pk","name":"Alphavantage Stock Price Langchain tool"},{"address":"agent1qdwp929xdwzm0pgxflz29c3x6ltha97t2em88vrh588nehz6su0u55zaxp9","name":"Autogen Code Execution Agent"},{"address":"agent1qgjj8476mr8cy2dvpc3ec5w4ye56kqswvr6hkl55q8775ghm9h7wwevh6lj","name":"Autogen Code Execution Agent"},{"address":"agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae","name":"Job Description Creation Agent"},{"address":"agent1qtmkpt8lhyx4u2ndzcgr7jkjuuade4ay78yld2ngaghs03z6qve3senwrsa","name":"Penny"},{"address":"agent1qtxx79e689zwjlh7m8dsfuv7lyn45d2xapr4p9w2u9axvzm487tzjfwqzdp","name":"Penny"},{"address":"agent1qv2u6v4ueea9kmvsyyclz9reh7qmzc6gr0fn7aeup93xm6ld8ynwc65nsks","name":"Penny"},{"address":"agent1qvc0em6vrlr80txaz3ksy7tspzaxl564jwqtaana7exehg0hz7gl6zg0wl0","name":"Penny"},{"address":"agent1qw5s6e7mxcd8mp6f2vvunk9rklwnv8ynft43unx77syea4nnskxyj4yk5xq","name":"Penny"}]

**2\. Send a Request to the Autogen Code Execution Agent**

Send a prompt from the User Agent to the Autogen Code Execution Agent:

    kshipra@MacBook-Pro autogen-code-generation-debugging-example % curl -X POST "http://localhost:5002/api/send-data" \-H "Content-Type: application/json" \-d '{  "payload": {    "prompt": "What date is today? Compare the year-to-date gain for META and TESLA."  },  "agentAddress": "agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae"}'

**Sample Output**

    {"status": "request_sent", "agent_address": "agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae", "payload": "{"prompt": "What date is today? Compare the year-to-date gain for META and TESLA."  }"}

**3\. Retrieve the Stock Price Response**

Fetch the stock price response from the User Agent:

    curl -X GET "http://localhost:5002/api/get-response"

**Sample Output**

    {"response":"The code executed successfully, and we have the year-to-date gain for both META and TESLA:\n\n- **META (Meta Platforms, Inc.)**: Year-to-Date Gain is **17.63%**\n- **TESLA (Tesla, Inc.)**: Year-to-Date Gain is **-0.29%**\n\nThis means that since the beginning of the year, META's stock price has increased by 17.63%, while TESLA's stock price has decreased by 0.29%.\n\nTERMINATE"}

Debugging Common Issues[â€‹](#debugging-common-issues "Direct link to Debugging Common Issues")
---------------------------------------------------------------------------------------------

Agent Registration Fails:

a. Check the .env file for correct API keys. b. Ensure the AGENTVERSE\_API\_KEY is valid.

404 Errors:

a. Verify that both agents are running on their respective ports (5001 and 5002 in this case). b. Double-check the agentAddress in requests.

You now have an **Autogen Code Execution Agent** integrated with the **Fetch.ai Agentverse**, orchestrating multi-agent code generation and execution. Feel free to extend the example to include additional tasks, multiple code executors, or advanced debugging logic.</content>
</page>

<page>
  <title>Agentverse API Key | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.0/agentverse/agentverse-api-key</url>
  <content>Getting Agentverse API Key
--------------------------

To get [Agentverse](https://agentverse.ai/) API Key, just login to Agentverse using your gmail address and go to profile section as shown in picture below:

Click on **\+ New API Key** button, give name to your API Key and with write permission to Access to all resources in Agentverse. Your permission can be for 30 days now. Check below image for more clarity on the process :Â 

Click on **Generate API Key**, it will take you through the authentication process again. Once you do the authentication and Allow Access the API key will pop-up on your screen like the image below and save it somewhere as it cannot be regenerated.

Keep your API Key stored somewhere safe. You wonâ€™t be able to access it again.</content>
</page>

<page>
  <title>CrewAI Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.1/other-frameworks/crewai</url>
  <content>Creating and Registering CrewAI based Job descriptions creator
--------------------------------------------------------------

This documentation explains how to build **CrewAI** agents that generate tailored job descriptions, register them on **Fetch.aiâ€™s Agentverse**, and enable collaboration between agents for dynamic data sharing. Below, you'll see the **main.py** script containing the workflow logic `run_job_posting_workflow`, as well as sample scripts for a **Job Description Agent** and a **User Agent**.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

### Purpose[â€‹](#purpose "Direct link to Purpose")

This project showcases:

*   **Creating CrewAI agents** for complex tasks (e.g., analyzing company requirements and generating job descriptions).
*   **Integrating with Fetch.aiâ€™s Agentverse**, enabling seamless communication and data exchange.
*   **Using autonomous agents** to streamline job description creation while aligning with company culture and specific role requirements.

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

### Environment Setup[â€‹](#environment-setup "Direct link to Environment Setup")

To begin creating and registering CrewAI agents, ensure you have the following:

*   **Python 3.10+** or higher
*   A **virtual environment** (recommended)
*   **Fetch.ai SDK** for agent creation and registration
*   **CrewAI Framework** for defining tasks and workflows

### Key Components[â€‹](#key-components "Direct link to Key Components")

*   **Job Description Agent**: A specialized CrewAI agent that processes input (company domain, hiring needs, etc.) to generate a job description.
*   **User Agent**: Acts as a client to discover and interact with the Job Description Agent.
*   **Agent Collaboration**: Multiple agents communicate via Agentverse, orchestrating tasks through CrewAI workflows.

### Architecture Diagram[â€‹](#architecture-diagram "Direct link to Architecture Diagram")

### Environment Variables[â€‹](#environment-variables "Direct link to Environment Variables")

Create a `.env` file with your API keys:

    SERPER_API_KEY=<your_serper_api_key>OPENAI_API_KEY=<your_openai_api_key>AGENTVERSE_API_KEY=<your_agentverse_api_key>

Replace the placeholders with your actual keys.

*   [OPENAI\_API\_KEY](https://openai.com/index/openai-api/)
*   [SERPER\_API\_KEY](https://serper.dev/)
*   [AGENTVERSE\_API\_KEY](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key)

Project Directory & Configuration Files[â€‹](#project-directory--configuration-files "Direct link to Project Directory & Configuration Files")
--------------------------------------------------------------------------------------------------------------------------------------------

For a clean, maintainable setup, we recommend the following structure:

    crewai_job_descriptions/â”œâ”€â”€ .envâ”œâ”€â”€ config/â”‚   â”œâ”€â”€ agents.yamlâ”‚   â””â”€â”€ tasks.yamlâ”œâ”€â”€ crew.pyâ”œâ”€â”€ jd_agent.pyâ”œâ”€â”€ main.pyâ”œâ”€â”€ requirements.txtâ”œâ”€â”€ user_agent.pyâ”œâ”€â”€ job_description_example.mdâ””â”€â”€ README.md

**1\. requirements.txt**

**Purpose**: Declares project dependencies so you (or anyone) can install them quickly with `pip install -r requirements.txt`.

Create a file called `requirements.txt` at the root of your project (alongside `jd_agent.py` and `main.py`). Example contents:

    flask==2.2.5flask-cors==3.0.10fetchai==0.16.3crewai==0.100.1crewai_tools==0.33.0python-dotenv==1.0.0

Install everything at once via:

    pip install -r requirements.txt

**2\. Crew.py**

**Purpose:** Defines your `JobPostingCrew` class, specifying the Agents and Tasks.

Place `crew.py` at the root directory. It contains the code:

    from typing import Listfrom crewai import Agent, Crew, Process, Taskfrom crewai.project import CrewBase, agent, crew, task# Check our tools documentations for more information on how to use themfrom crewai_tools import SerperDevTool, ScrapeWebsiteTool, WebsiteSearchTool, FileReadToolfrom pydantic import BaseModel, Fieldweb_search_tool = WebsiteSearchTool()seper_dev_tool = SerperDevTool()file_read_tool = FileReadTool(    file_path='./job_description_example.md',    description='A tool to read the job description example file.')class ResearchRoleRequirements(BaseModel):    """Research role requirements model"""    skills: List[str] = Field(..., description="List of recommended skills ...")    experience: List[str] = Field(..., description="List of recommended experience ...")    qualities: List[str] = Field(..., description="List of recommended qualities ...")@CrewBaseclass JobPostingCrew:    """JobPosting crew"""    agents_config = 'config/agents.yaml'    tasks_config = 'config/tasks.yaml'    @agent    def research_agent(self) -> Agent:        return Agent(            config=self.agents_config['research_agent'],            tools=[web_search_tool, seper_dev_tool],            verbose=True        )        @agent    def writer_agent(self) -> Agent:        return Agent(            config=self.agents_config['writer_agent'],            tools=[web_search_tool, seper_dev_tool, file_read_tool],            verbose=True        )        @agent    def review_agent(self) -> Agent:        return Agent(            config=self.agents_config['review_agent'],            tools=[web_search_tool, seper_dev_tool, file_read_tool],            verbose=True        )        @task    def research_company_culture_task(self) -> Task:        return Task(            config=self.tasks_config['research_company_culture_task'],            agent=self.research_agent()        )    @task    def research_role_requirements_task(self) -> Task:        return Task(            config=self.tasks_config['research_role_requirements_task'],            agent=self.research_agent(),            output_json=ResearchRoleRequirements        )    @task    def draft_job_posting_task(self) -> Task:        return Task(            config=self.tasks_config['draft_job_posting_task'],            agent=self.writer_agent()        )    @task    def review_and_edit_job_posting_task(self) -> Task:        return Task(            config=self.tasks_config['review_and_edit_job_posting_task'],            agent=self.review_agent()        )    @task    def industry_analysis_task(self) -> Task:        return Task(            config=self.tasks_config['industry_analysis_task'],            agent=self.research_agent()        )    @crew    def crew(self) -> Crew:        """Creates the JobPostingCrew"""        return Crew(            agents=self.agents,  # Automatically created by the @agent decorator            tasks=self.tasks,    # Automatically created by the @task decorator            process=Process.sequential,            verbose=True,        )

**3\. config/agents.yaml**

**Purpose**: Specifies each agentâ€™s role, goal, and backstory. Create a folder named `config/` at your project root. Inside it, **create** `agents.yaml`:

    research_agent:  role: >    Research Analyst  goal: >    Analyze the company website and provided description...  backstory: >    Expert in analyzing company cultures and identifying key values...writer_agent:  role: >    Job Description Writer  goal: >    Use insights from the Research Analyst to create...  backstory: >    Skilled in crafting compelling job descriptions...review_agent:  role: >    Review and Editing Specialist  goal: >    Review the job posting for clarity, engagement...  backstory: >    A meticulous editor ensuring every piece of content is polished...

**4\. config/tasks.yaml**

**Purpose**: Defines the text prompts (description) and expected output for each task. In the same `config/` folder, make a second file called `tasks.yaml`:

    research_company_culture_task:  description: >    Analyze {company_domain} and {company_description}...  expected_output: >    A comprehensive report detailing the company's culture...research_role_requirements_task:  description: >    Based on {hiring_needs}, list recommended skills, experience...  expected_output: >    A list of recommended skills, experiences, and qualities...draft_job_posting_task:  description: >    Draft a job posting for {hiring_needs}, using {specific_benefits}...  expected_output: >    A detailed, engaging job posting that includes an introduction...review_and_edit_job_posting_task:  description: >    Review the job posting for {hiring_needs} for clarity...  expected_output: >    A polished, error-free job posting with final approval in markdown.industry_analysis_task:  description: >    Conduct an in-depth analysis of {company_domain}'s industry...  expected_output: >    A detailed analysis highlighting major industry trends, opportunities...

Main.py (Workflow Definition)[â€‹](#mainpy-workflow-definition "Direct link to Main.py (Workflow Definition)")
------------------------------------------------------------------------------------------------------------

Below is an example `main.py` script containing the `run_job_posting_workflow` function that the Job Description Agent calls to generate job descriptions. It uses **Crew** to structure tasks and (optionally) a local SQLite database for storage or logging.

    import sysfrom crew import JobPostingCrewimport sqlite3async def run_job_posting_workflow(company_domain, company_description, hiring_needs, specific_benefits):    """    Executes the job posting workflow with proper connection handling and debugging output.        Args:        company_domain (str): The company's domain.        company_description (str): A description of the company.        hiring_needs (str): Description of the role being hired for.        specific_benefits (str): Specific benefits offered for the role.        Returns:        str: The raw output of the 'industry_analysis_task' if found, else None.    """    try:        # Initialize the JobPostingCrew        job_posting_crew = JobPostingCrew().crew()                # Define the inputs        inputs = {            'company_domain': company_domain,            'company_description': company_description,            'hiring_needs': hiring_needs,            'specific_benefits': specific_benefits,        }        # Execute the job posting process        result = job_posting_crew.kickoff(inputs=inputs)        # Debugging output        print("Result type:", type(result))        print("Attributes:", dir(result))        # Attempt to retrieve and print tasks_output        if hasattr(result, 'tasks_output') and result.tasks_output:            print("Tasks Output:")            for task_output in result.tasks_output:                print(f"Task Name: {task_output.name}")                if task_output.name == "industry_analysis_task":                    return task_output.raw            print("Task 'review_and_edit_job_posting_task' not found in tasks_output.")        else:            print("No 'tasks_output' attribute found or it's empty.")    except Exception as e:        print(f"Error occurred: {e}")        return None

The `run_job_posting_workflow` function is invoked by the Job Description Agentâ€™s webhook to handle incoming requests and generate the job description.

Job Description Agent Script[â€‹](#job-description-agent-script "Direct link to Job Description Agent Script")
------------------------------------------------------------------------------------------------------------

A Job Description Agent is a specialized CrewAI agent that creates detailed job descriptions based on:

*   **Company Domain**
*   **Company Description**
*   **Hiring Needs**
*   **Specific Benefits**

It registers itself with [**Agentverse**](https://agentverse.ai/), processes incoming requests (via a Flask webhook), and sends the generated output back to the requester.

### Script Breakdown (JD\_Agent.py)[â€‹](#script-breakdown-jd_agentpy "Direct link to Script Breakdown (JD_Agent.py)")

**Importing Required Libraries**

The script imports essential libraries for agent registration, communication, and Flask-based HTTP handling:

    import osimport loggingimport asynciofrom uagents_core.crypto import Identityfrom flask import Flask, request, jsonifyfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentfrom main import run_job_posting_workflow

**Setting Up Flask and Logging**

The script initializes Flask for handling webhooks and sets up logging to monitor activities:

    # Configure logginglogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)# Flask app for webhookflask_app = Flask(__name__)# Identity for the agentjd_agent_identity = Nonecontent = ''

**Webhook Endpoint**

The /webhook endpoint handles incoming requests and processes inputs to generate a job description:

    @flask_app.route('/webhook', methods=['POST'])async def webhook():    global content    global jd_agent_identity    try:        # Parse the incoming message        data = request.get_data().decode('utf-8')        message = parse_message_from_agent(data)        company_domain = message.payload.get("company_domain", "")        company_description = message.payload.get("company_description", "")        hiring_needs = message.payload.get("hiring_needs", "")        specific_benefits = message.payload.get("specific_benefits", "")        agent_address = message.sender        # Run the job posting workflow        content = await run_job_posting_workflow(            company_domain=company_domain,            company_description=company_description,            hiring_needs=hiring_needs,            specific_benefits=specific_benefits        )        # Send the generated content back to the requesting agent        payload = {'content': content}        send_message_to_agent(jd_agent_identity, agent_address, payload)        return jsonify({"status": "job_description_sent"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"status": "error", "message": str(e)}), 500

**Registering the Agent**

The init\_agent function registers the Job Description Agent with Fetch.ai's Agentverse, providing metadata such as agent title, description, and use cases:

    def init_agent():    global jd_agent_identity    try:        jd_agent_identity = Identity.from_seed("Job Description Agent Seed", 0)        register_with_agentverse(            identity=jd_agent_identity,            url="http://localhost:5008/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="Job Description Creation Agent",            readme="""                <description>Job Description creator to create JD according to the company's career website and description using CrewAI.</description>                <use_cases>                    <use_case>Generates job descriptions for specified roles.</use_case>                </use_cases>                <payload_requirements>                    <description>Expects company domain, description, hiring needs, and specific benefits.</description>                    <payload>                        <requirement>                            <parameter>company_domain</parameter>                            <description>Career Page for the company</description>                            <parameter>company_description</parameter>                            <description>Description of the company</description>                            <parameter>hiring_needs</parameter>                            <description>Role for which the person is needed</description>                            <parameter>specific_benefits</parameter>                            <description>Benefits offered with the role</description>                        </requirement>                    </payload>                </payload_requirements>            """        )        logger.info("Job Description Creator Agent registered successfully!")    except Exception as e:        logger.error(f"Error initializing agent: {e}")        raise

**Running the Agent**

The script starts the Flask server on port 5008:

    if __name__ == "__main__":    init_agent()    flask_app.run(host="0.0.0.0", port=5008, debug=True)

User Agent Script (user.py)[â€‹](#user-agent-script-userpy "Direct link to User Agent Script (user.py)")
------------------------------------------------------------------------------------------------------

The **User Agent** serves as a client. It registers itself with Agentverse, can search for existing agents, and forwards relevant data (e.g., company info, role) to the Job Description Agent. Afterwards, it retrieves the response and saves it locally.

### Script Breakdown[â€‹](#script-breakdown "Direct link to Script Breakdown")

**Importing Required Libraries**

The script imports libraries for Flask-based HTTP handling, Fetch.ai's identity and communication utilities, and environment variable management:

    from flask_cors import CORSfrom flask import Flask, request, jsonifyfrom uagents_core.crypto import Identityfrom fetchai import fetchfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osfrom dotenv import load_dotenv

**Setting Up Flask and Logging**

Logging and Flask are initialised to handle web requests and log agent activities:

    # Configure logginglogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app)# Global variables for client identity and agent responseclient_identity = Noneagent_response = None

**Initialising the User Agent**

The init\_client function registers the User Agent with Fetch.ai's Agentverse, providing metadata such as the agent's purpose and use cases:

    def init_client():    """Initialize and register the client agent."""    global client_identity    try:        # Load the client identity from environment variables        client_identity = Identity.from_seed("User for Testing JD Agents", 0)        logger.info(f"Client agent started with address: {client_identity.address}")        # Register the agent with Agentverse        register_with_agentverse(            identity=client_identity,            url="http://localhost:5002/api/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="JD User Testing Agent",            readme="""                <description>Frontend client that interacts with JD agents to fetch job descriptions.</description>                <use_cases>                    <use_case>Searches for agents and interacts with them.</use_case>                </use_cases>                <payload_requirements>                    <description>Handles responses related to job descriptions.</description>                    <payload>                        <requirement>                            <parameter>field</parameter>                            <description>Data required for job description creation</description>                        </requirement>                    </payload>                </payload_requirements>            """        )        logger.info("Client agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise

**Searching for Agents**

The /api/search-agents endpoint allows the User Agent to search for agents in Fetch.ai's Agentverse/Almanac Contract based on a query:

    @app.route('/api/search-agents', methods=['GET'])def search_agents():    """Search for available agents based on user input."""    try:        # Extract the query from the request        user_query = request.args.get('query', '')        if not user_query:            return jsonify({"error": "Query parameter 'query' is required."}), 400        # Search agents in Agentverse/almanac contract        available_ais = fetch.ai(user_query)        agents = available_ais.get('ais', [])        # Format the agent data        extracted_data = [            {'name': agent.get('name'), 'address': agent.get('address')}            for agent in agents        ]        return jsonify(extracted_data), 200    except Exception as e:        logger.error(f"Error finding agents: {e}")        return jsonify({"error": str(e)}), 500

**Sending Data to Another Agent**

The /api/send-data endpoint forwards input data (such as company details and hiring needs) to the selected Job Description Agent:

    @app.route('/api/send-data', methods=['POST'])def send_data():    """Send payload to the selected agent based on the provided address."""    global agent_response    agent_response = None    try:        # Parse the request payload        data = request.json        payload = data.get('payload')  # Extract the payload dictionary        agent_address = data.get('agentAddress')  # Extract the agent address        # Validate input data        if not payload or not agent_address:            return jsonify({"error": "Missing payload or agent address"}), 400        # Send the payload to the agent        send_message_to_agent(client_identity, agent_address, payload)        logger.info(f"Payload sent to agent: {agent_address}")        return jsonify({"status": "request_sent", "agent_address": agent_address, "payload": payload})    except Exception as e:        logger.error(f"Error sending data to agent: {e}")        return jsonify({"error": str(e)}), 500

**Retrieving the Agent's Response**

The /api/get-response endpoint retrieves the response (job description) from the Job Description Agent and saves it as a markdown file:

    @app.route('/api/get-response', methods=['GET'])def get_response():    """Fetch the response from the agent and save it to a file."""    global agent_response    try:        if agent_response:            response = agent_response            agent_response = None  # Clear the response after fetching            # Save the response to a markdown file            file_path = os.path.join(os.getcwd(), "jd_response.md")            with open(file_path, "w", encoding="utf-8") as md_file:                md_file.write(response.get("content", ""))            logger.info(f"Markdown file created at {file_path}")            return jsonify({"status": "file_created", "file_path": file_path})        else:            return jsonify({"error": "No response available"}), 404    except Exception as e:        logger.error(f"Error creating markdown file: {e}")        return jsonify({"error": str(e)}), 500

**Webhook Endpoint**

The webhook endpoint processes messages sent by the Job Description Agent and stores the response globally:

    @app.route('/api/webhook', methods=['POST'])def webhook():    """Handle incoming messages from the Job Description Agent."""    global agent_response    try:        # Parse the incoming message        data = request.get_data().decode("utf-8")        logger.info("Received response")        message = parse_message_from_agent(data)        agent_response = message.payload        logger.info(f"Processed response: {agent_response}")        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500

**Running the User Agent**

The Flask server runs on port 5002 to handle requests:

    def start_server():    """Start the Flask server."""    try:        # Load environment variables        load_dotenv()        init_client()        app.run(host="0.0.0.0", port=5002)    except Exception as e:        logger.error(f"Server error: {e}")        raiseif __name__ == "__main__":    start_server()

Step-by-Step Execution[â€‹](#step-by-step-execution "Direct link to Step-by-Step Execution")
------------------------------------------------------------------------------------------

**1\. Environment Variables**

Update '.env' with `SERPER_API_KEY`, `OPENAI_API_KEY`, and `AGENTVERSE_API_KEY`.

**2\. Start the Job Description Agent**

*   Registers the agent on Agentverse.
*   Launches JD Agent with Flask on port 5008.

**3\. Start the User Agent**

*   Registers the agent on Agentverse.
*   Launches Flask on port 5002.

### Interacting with the Agents[â€‹](#interacting-with-the-agents "Direct link to Interacting with the Agents")

**Search for Agents**

To list agents in the Agentverse:

    curl -X GET "http://localhost:5002/api/search-agents?query=I%20want%20to%20write%20job%20description"

**Sample Output**

    [    {        "name": "Job Description Creation Agent",        "address": "agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae"    },    ...]

**Send Data to the Job Description Agent**

Send JD details from the User Agent to the JD Agent:

    curl -X POST "http://localhost:5002/api/send-data" \-H "Content-Type: application/json" \-d '{    "payload": {        "company_domain": "careers.wbd.com",        "company_description": "Warner Bros. Discovery is a premier global media and entertainment company, offering audiences the world\u2019s most differentiated and complete portfolio of content, brands and franchises across television, film, sports, news, streaming and gaming. We\u2019re home to the world\u2019s best storytellers, creating world-class products for consumers.",        "hiring_needs": "Production Assistant, for a TV production set in Los Angeles in June 2025",        "specific_benefits": "Weekly Pay, Employee Meals, healthcare"    },    "agentAddress": "agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae"}'

**Sample Output**

    {    "status": "request_sent",    "agent_address": "agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae",    "payload": {        "company_domain": "careers.wbd.com",        "company_description": "Warner Bros. Discovery is a premier global media and entertainment company...",        "hiring_needs": "Production Assistant, for a TV production set in Los Angeles in June 2025",        "specific_benefits": "Weekly Pay, Employee Meals, healthcare"    }}

**Retrieve the Agentâ€™s Response**

Fetch the JD document response from the User Agent:

    curl -X GET "http://localhost:5002/api/get-response"

**Sample Output**

*   Stores the Job Description result in `jd_response.md`.

    {    "status": "file_created",    "file_path": "/path/to/jd_response.md"}

You can open the [jd\_response.md](https://drive.google.com/file/d/12oHIKeOIAjVs8dxP-xg9SmtWR4JnTbqx/view?usp=sharing) file to review the generated job description.

You now have **CrewAI agents** integrated with the **Fetch.ai Agentverse**, collaborating to create custom job descriptions. Feel free to adapt the main.py workflow, agent scripts, or user scripts to suit your own requirementsâ€”whether youâ€™re refining AI logic, adding advanced search filters, or customizing the final job posting format.</content>
</page>

<page>
  <title>LangGraph Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.1/other-frameworks/financial-analysis-ai-agent</url>
  <content>Creating and Registering LangGraph based Financial Analysis Agent
-----------------------------------------------------------------

This documentation explains how to build **LangGraph** agents that perform comprehensive financial analysis, register them on **Fetch.ai's Agentverse**, and enable collaboration between specialized agents for dynamic data analysis. Below, you'll see the **main.py** script containing the workflow logic `run_financial_analysis_workflow`, as well as specialized agent implementations for financial analysis.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

### Purpose[â€‹](#purpose "Direct link to Purpose")

This project showcases:

*   **Creating LangGraph agents** for complex tasks (e.g., analyzing SEC filings, market research, and financial metrics).
*   **Integrating with Fetch.ai's Agentverse**, enabling seamless communication and data exchange.
*   **Using autonomous agents** to streamline financial analysis while combining multiple data sources and expert analysis.

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

### Environment Setup[â€‹](#environment-setup "Direct link to Environment Setup")

To begin creating and registering LangGraph agents, ensure you have the following:

*   **Python 3.10+** or higher
*   A **virtual environment** (recommended)
*   **Fetch.ai SDK** for agent creation and registration
*   **LangGraph Framework** for defining states and workflows
*   **LangChain** for agent tools and chains

### Key Components[â€‹](#key-components "Direct link to Key Components")

*   **SEC Analysis Agent**: A specialized agent that processes SEC filings and extracts financial metrics
*   **Search Agent**: Gathers real-time market data and analyst opinions
*   **Supervisor Agent**: Coordinates analysis flow and combines insights
*   **Agent Collaboration**: Multiple agents work together via LangGraph state management

### Architecture Diagram[â€‹](#architecture-diagram "Direct link to Architecture Diagram")

### Environment Variables[â€‹](#environment-variables "Direct link to Environment Variables")

Create a `.env` file with your API keys:

    OPENAI_API_KEY=<your_openai_api_key>TAVILY_API_KEY=<your_tavily_api_key>AGENTVERSE_API_KEY=<your_agentverse_api_key>

Replace the placeholders with your actual keys:

*   [OPENAI\_API\_KEY](https://openai.com/api/)
*   [TAVILY\_API\_KEY](https://tavily.com/)
*   [AGENTVERSE\_API\_KEY](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key)

Project Directory & Configuration Files[â€‹](#project-directory--configuration-files "Direct link to Project Directory & Configuration Files")
--------------------------------------------------------------------------------------------------------------------------------------------

For a clean, maintainable setup, we recommend the following structure:

    financial_analysis_agent/â”œâ”€â”€ .envâ”œâ”€â”€ src/â”‚   â”œâ”€â”€ agents/â”‚   â”‚   â”œâ”€â”€ __init__.pyâ”‚   â”‚   â”œâ”€â”€ search_agent.py      # Market research specialistâ”‚   â”‚   â”œâ”€â”€ sec_agent.py         # SEC filings specialistâ”‚   â”‚   â””â”€â”€ supervisor.py        # Team coordinatorâ”‚   â”œâ”€â”€ tools/â”‚   â”‚   â”œâ”€â”€ __init__.pyâ”‚   â”‚   â”œâ”€â”€ search.py            # Tavily search implementationâ”‚   â”‚   â””â”€â”€ analysis.py          # RAG implementationâ”‚   â”œâ”€â”€ rag/â”‚   â”‚   â”œâ”€â”€ __init__.pyâ”‚   â”‚   â”œâ”€â”€ chain.py             # RAG chain setupâ”‚   â”‚   â””â”€â”€ loader.py            # Document processingâ”‚   â”œâ”€â”€ graph/â”‚   â”‚   â”œâ”€â”€ __init__.pyâ”‚   â”‚   â””â”€â”€ state.py             # State managementâ”‚   â””â”€â”€ utils/â”‚       â”œâ”€â”€ __init__.pyâ”‚       â””â”€â”€ helpers.py           # Helper functionsâ”œâ”€â”€ main.py                      # Main workflowâ”œâ”€â”€ register.py                  # Agentverse registrationâ”œâ”€â”€ requirements.txtâ””â”€â”€ README.md

Core Implementation Files[â€‹](#core-implementation-files "Direct link to Core Implementation Files")
---------------------------------------------------------------------------------------------------

**1\. requirements.txt**

**Purpose**: Declares project dependencies for installing all necessary packages quickly with `pip install -r requirements.txt`.

Create a file called `requirements.txt` at the root of your project. Example contents:

    langchain==0.1.0langchain-core==0.1.10langgraph==0.0.10fetchai-sdk==0.16.3flask==2.2.5flask-cors==3.0.10python-dotenv==1.0.0tavily-python==0.2.6qdrant-client==1.7.0tiktoken==0.5.2

Install everything at once via:

    pip install -r requirements.txt

**2\. State Management (src/graph/state.py)**

**Purpose**: Defines the state structure and management for the research team's workflow.

Place `state.py` in the `src/graph` directory. It contains:

    from typing import TypedDict, List, Annotatedfrom langchain_core.messages import BaseMessageimport operatorclass ResearchTeamState(TypedDict):    """State structure for research team coordination."""    messages: Annotated[List[BaseMessage], operator.add]  # Conversation history    team_members: List[str]                              # Available agents    next: str                                           # Next agent to act    information_needed: List[str]                       # Required information    reasoning: str                                      # Decision reasoningdef create_initial_state(query: str) -> ResearchTeamState:    """Create initial state from user query."""    return {        "messages": [HumanMessage(content=query)],        "team_members": ["Search", "SECAnalyst"],        "next": "",        "information_needed": [],        "reasoning": ""    }def update_state(state: ResearchTeamState, agent_response: dict) -> ResearchTeamState:    """Update state with agent response."""    new_state = state.copy()    new_state["messages"].extend(agent_response["messages"])    return new_state

**3\. Tools Implementation (src/tools)**

**Purpose**: Implements specialized tools for market research and SEC filing analysis.

#### A. Search Tool (search.py)[â€‹](#a-search-tool-searchpy "Direct link to A. Search Tool (search.py)")

    from typing import Annotatedfrom langchain_core.tools import toolfrom tavily import TavilyClientimport os@tooldef tavily_search(query: str) -> str:    """Search for real-time market information."""    try:        client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))        result = client.search(query)        return str(result)    except Exception as e:        return f"Error performing search: {str(e)}"

#### B. Analysis Tool (analysis.py)[â€‹](#b-analysis-tool-analysispy "Direct link to B. Analysis Tool (analysis.py)")

    from typing import Annotatedfrom langchain_core.tools import toolfrom ..rag.chain import create_rag_chain@tooldef retrieve_information(query: Annotated[str, "query to analyze financial documents"]) -> str:    """Analyze SEC filings using RAG."""    try:        rag_chain = create_rag_chain()        return rag_chain.invoke(query)    except Exception as e:        return f"Error analyzing documents: {str(e)}"

**4\. RAG Implementation (src/rag)**

**Purpose**: Handles document processing and retrieval for SEC filing analysis.

#### A. Document Loader (loader.py)[â€‹](#a-document-loader-loaderpy "Direct link to A. Document Loader (loader.py)")

    class DocumentLoader:    def __init__(self, file_path: str):        self.file_path = file_path        @staticmethod    def tiktoken_len(text):        tokens = tiktoken.encoding_for_model("gpt-4").encode(text)        return len(tokens)        def load_and_split(self):        """Load and chunk documents for processing."""        docs = PyMuPDFLoader(self.file_path).load()        splitter = RecursiveCharacterTextSplitter(            chunk_size=300,            chunk_overlap=0,            length_function=self.tiktoken_len        )        return splitter.split_documents(docs)

#### B. RAG Chain (chain.py)[â€‹](#b-rag-chain-chainpy "Direct link to B. RAG Chain (chain.py)")

    def create_rag_chain(file_path: str = "data/raw/apple_10k.pdf"):    """Create RAG chain for SEC filing analysis."""    # Initialize document processing    loader = DocumentLoader(file_path)    chunks = loader.load_and_split()        # Set up vector store    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")    vectorstore = Qdrant.from_documents(        chunks,        embeddings,        location=":memory:",        collection_name="sec_filings"    )        # Create retrieval chain    template = """Use the context to answer financial questions.    Context: {context}    Question: {question}    Answer with specific numbers and data when available."""        prompt = ChatPromptTemplate.from_template(template)    chain = (        {"context": vectorstore.as_retriever(), "question": RunnablePassthrough()}        | prompt        | ChatOpenAI(model="gpt-4-turbo-preview")        | StrOutputParser()    )        return chain

**5\. Helper Functions (src/utils/helpers.py)**

**Purpose**: Provides utility functions for agent creation and node management.

    def create_agent(llm: ChatOpenAI, tools: list, system_prompt: str) -> AgentExecutor:    """Create a specialized agent with tools and prompt."""    try:        system_prompt += (            "\nWork autonomously using your tools."            " Do not ask for clarification."            " Your team members will help with their specialties."        )                prompt = ChatPromptTemplate.from_messages([            ("system", system_prompt),            MessagesPlaceholder(variable_name="messages"),            MessagesPlaceholder(variable_name="agent_scratchpad"),        ])                agent = create_openai_functions_agent(llm, tools, prompt)        return AgentExecutor(agent=agent, tools=tools)            except Exception as e:        logger.error(f"Error creating agent: {e}")        raisedef agent_node(state, agent, name):    """Create agent node for the graph."""    try:        if "information_needed" in state:            message_content = f"""Information needed:            {', '.join(state['information_needed'])}            Query: {state['messages'][-1].content}"""            state['messages'][-1] = HumanMessage(content=message_content)        result = agent.invoke(state)        return {            "messages": [                HumanMessage(content=result["output"], name=name)            ]        }    except Exception as e:        logger.error(f"Error in agent node {name}: {e}")        raise

Financial Analysis Agent Implementation[â€‹](#financial-analysis-agent-implementation "Direct link to Financial Analysis Agent Implementation")
---------------------------------------------------------------------------------------------------------------------------------------------

### A. SEC Analysis Agent (src/agents/sec\_agent.py)[â€‹](#a-sec-analysis-agent-srcagentssec_agentpy "Direct link to A. SEC Analysis Agent (src/agents/sec_agent.py)")

A specialized agent that processes SEC filings and financial documents. It uses RAG to analyze documents and extract relevant financial metrics.

    # src/agents/sec_agent.pyfrom langchain.chat_models import ChatOpenAIfrom ..tools.analysis import retrieve_informationdef create_sec_agent(llm: ChatOpenAI):    """Creates an agent specialized in SEC filings analysis."""        system_prompt = """You are a financial analyst specialized in SEC filings analysis.    After analyzing SEC filings:    1. If you need market context, clearly state what specific market data you need    2. If numbers need industry comparison, explicitly request competitor data    3. Always include specific numbers and trends from the filings    4. If you spot significant changes or unusual patterns, highlight them        Format your response as:    1. Data from SEC Filings: [your findings]    2. Additional Context Needed: [if any]    3. Analysis: [your insights]    """        return create_agent(        llm=llm,        tools=[retrieve_information],        system_prompt=system_prompt    )

### B. Search Agent (src/agents/search\_agent.py)[â€‹](#b-search-agent-srcagentssearch_agentpy "Direct link to B. Search Agent (src/agents/search_agent.py)")

Handles real-time market research and data gathering using external search tools.

    # src/agents/search_agent.pyfrom langchain.chat_models import ChatOpenAIfrom ..tools.search import tavily_searchdef create_search_agent(llm: ChatOpenAI):    """Creates a search agent specialized in market research."""        system_prompt = """You are a research assistant who can search for up-to-date     financial information using the tavily search engine.        When responding:    1. Always cite sources    2. Focus on recent market data and analyst reports    3. If SEC data is mentioned, compare it with current market views    4. Highlight any significant discrepancies with official filings        Format your response as:    1. Market Data: [your findings]    2. Analyst Views: [key opinions]    3. Relevance to SEC Data: [if applicable]    """        return create_agent(        llm=llm,        tools=[tavily_search],        system_prompt=system_prompt    )

### C. Supervisor Agent (src/agents/supervisor.py)[â€‹](#c-supervisor-agent-srcagentssupervisorpy "Direct link to C. Supervisor Agent (src/agents/supervisor.py)")

Coordinates between agents and manages the analysis workflow.

    # src/agents/supervisor.pydef create_supervisor_agent(llm: ChatOpenAI):    """Creates the supervisor agent for coordinating analysis."""        function_def = {        "name": "route",        "description": "Select the next role based on query analysis.",        "parameters": {            "title": "routeSchema",            "type": "object",            "properties": {                "next": {                    "title": "Next",                    "anyOf": [{"enum": ["Search", "SECAnalyst", "FINISH"]}],                },                "reasoning": {                    "title": "Reasoning",                    "type": "string",                    "description": "Explanation for why this agent should act next"                },                "information_needed": {                    "title": "Information Needed",                    "type": "array",                    "items": {"type": "string"},                    "description": "List of specific information needed from this agent"                }            },            "required": ["next", "reasoning", "information_needed"],        },    }    prompt = ChatPromptTemplate.from_messages([        ("system", """You are a financial research team supervisor.        Your role is to:        1. Analyze incoming queries        2. Determine what information is needed        3. Choose the appropriate agent for each task        4. Coordinate between agents        5. Ensure comprehensive analysis"""),        MessagesPlaceholder(variable_name="messages"),        ("system", "Who should act next? Consider available information and agent specialties.")    ])    return (        prompt        | llm.bind_functions(functions=[function_def], function_call="route")        | JsonOutputFunctionsParser()    )

Main Workflow Implementation (main.py)[â€‹](#main-workflow-implementation-mainpy "Direct link to Main Workflow Implementation (main.py)")
---------------------------------------------------------------------------------------------------------------------------------------

The main script that initializes the LangGraph workflow and handles financial analysis requests.

    import osfrom dotenv import load_dotenvfrom src.rag.chain import create_rag_chainfrom src.graph.state import create_research_graphdef init_financial_system():    """Initialize the RAG and research system."""    try:        # Create RAG chain for SEC document analysis        rag_chain = create_rag_chain("data/raw/apple_10k.pdf")                # Initialize research graph with RAG chain        chain = create_research_graph(rag_chain)                return chain    except Exception as e:        logger.error(f"Error initializing system: {e}")        raiseasync def run_financial_analysis(query: str):    """    Process financial analysis queries through the research graph.        Args:        query (str): The financial analysis query to process        Returns:        dict: Analysis results from multiple agents    """    try:        # Initialize state with query        state = {            "messages": [HumanMessage(content=query)],            "team_members": ["Search", "SECAnalyst"],            "information_needed": [],            "reasoning": ""        }                # Process through research chain        result = research_chain.invoke(state)                return {            "status": "success",            "analysis": result.get("messages", [])        }            except Exception as e:        logger.error(f"Analysis error: {e}")        return {            "status": "error",            "message": str(e)        }if __name__ == "__main__":    # Load environment variables    load_dotenv()        # Initialize the system    research_chain = init_financial_system()

This workflow:

1.  Initializes the research system with RAG capabilities
2.  Sets up the specialized agents and their tools
3.  Creates the research graph for coordinated analysis
4.  Processes queries through the team of agents
5.  Returns comprehensive financial analysis results

Agentverse Integration (register.py)[â€‹](#agentverse-integration-registerpy "Direct link to Agentverse Integration (register.py)")
---------------------------------------------------------------------------------------------------------------------------------

The Financial Analysis Agent registers itself with Agentverse and handles incoming analysis requests:

    import osimport loggingfrom flask import Flask, request, jsonifyfrom uagents_core.crypto import Identityfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentfrom main import init_financial_system# Configure logginglogging.basicConfig(level=logging.INFO)logger = logging.getLogger(__name__)# Initialize Flask appapp = Flask(__name__)# Global variablesfinancial_identity = Noneresearch_chain = Nonedef init_agent():    """Initialize and register the agent with agentverse"""    global financial_identity, research_chain    try:        # Initialize the research chain        research_chain = init_financial_system()                # Initialize identity and register with Agentverse        financial_identity = Identity.from_seed("Financial Analysis Agent", 0)                # Register with detailed capabilities description        register_with_agentverse(            identity=financial_identity,            url="http://localhost:5008/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="Financial Analysis Agent",            readme = """                <description>A comprehensive financial analysis agent that combines                 SEC filing analysis with real-time market data.</description>                <use_cases>                    <use_case>Analyze company financial metrics from SEC filings</use_case>                    <use_case>Research market trends and analyst opinions</use_case>                    <use_case>Compare financial performance with competitors</use_case>                </use_cases>                <payload_requirements>                    <payload>                        <requirement>                            <parameter>query</parameter>                            <description>What would you like to know about the company's financials?</description>                        </requirement>                    </payload>                </payload_requirements>            """        )        logger.info("Financial Analysis Agent registered successfully!")    except Exception as e:        logger.error(f"Error initializing agent: {e}")        raise@app.route('/webhook', methods=['POST'])async def webhook():    """Handle incoming requests from other agents"""    try:        data = request.get_data().decode('utf-8')        message = parse_message_from_agent(data)        query = message.payload.get("request", "")        agent_address = message.sender        if not query:            return jsonify({"status": "error", "message": "No query provided"}), 400        # Process query using research chain        result = research_chain.invoke({            "messages": [HumanMessage(content=query)],            "team_members": ["Search", "SECAnalyst"]        })        # Format response        formatted_result = {            "analysis": [                {                    "role": msg.type if hasattr(msg, 'type') else "message",                    "content": msg.content,                    "name": msg.name if hasattr(msg, 'name') else None                }                for msg in result.get('messages', [])            ]        }        # Send response back through Agentverse        send_message_to_agent(            financial_identity,            agent_address,            {'analysis_result': formatted_result}        )        return jsonify({"status": "analysis_sent"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"status": "error", "message": str(e)}), 500def run_agent():    """Initialize and start the agent"""    try:        init_agent()        app.run(host="0.0.0.0", port=5008, debug=True)    except Exception as e:        logger.error(f"Error running agent: {e}")        raise

Step-by-Step Execution[â€‹](#step-by-step-execution "Direct link to Step-by-Step Execution")
------------------------------------------------------------------------------------------

1.  **Environment Setup**
    
        # Create and activate virtual environmentpython -m venv venvsource venv/bin/activate  # On Windows: venv\Scripts\activate# Install requirementspip install -r requirements.txt
    
2.  **Configure Environment Variables** Create `.env` file with required API keys:
    
        OPENAI_API_KEY=<your_openai_api_key>TAVILY_API_KEY=<your_tavily_api_key>AGENTVERSE_API_KEY=<your_agentverse_api_key>
    
3.  **Start the Financial Analysis Agent**
    
    This will:
    
    *   Initialize the RAG system
    *   Register the agent with Agentverse
    *   Start the Flask server on port 5008
4.  **Verify Agent Registration**
    
    *   Check logs for successful registration message
    *   Verify agent appears in Agentverse registry

User Agent Script (user\_agent.py)[â€‹](#user-agent-script-user_agentpy "Direct link to User Agent Script (user_agent.py)")
-------------------------------------------------------------------------------------------------------------------------

The **User Agent** serves as a client. It registers itself with Agentverse, can search for existing agents, and forwards financial analysis queries to the Financial Analysis Agent. Afterwards, it retrieves the response and processes it.

### Script Breakdown[â€‹](#script-breakdown "Direct link to Script Breakdown")

**Importing Required Libraries**

The script imports libraries for Flask-based HTTP handling, Fetch.ai's identity and communication utilities, and environment variable management:

    from flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentfrom fetchai import fetchimport loggingimport osfrom dotenv import load_dotenv

**Setting Up Flask and Logging**

Logging and Flask are initialised to handle web requests and log agent activities:

    logging.basicConfig(level=logging.DEBUG)logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app, resources={r"/api/*": {'origins': 'http://localhost:5174'}})# Global variables for client identity and responsesprimary_agent = None

**Initialising the User Agent**

The PrimaryAgent class handles initialization and registration with Fetch.ai's Agentverse:

    class PrimaryAgent:    def __init__(self):        self.identity = None        self.latest_response = None        def initialize(self):        try:            self.identity = Identity.from_seed(os.getenv("PRIMARY_AGENT_KEY"), 0)                        register_with_agentverse(                identity=self.identity,                url="http://localhost:5001/webhook",                agentverse_token=os.getenv("AGENTVERSE_API_KEY"),                agent_title="Financial Query Router",                readme="<description>Routes queries to Financial Analysis Agent</description>"            )            logger.info("Primary agent initialized successfully!")                        except Exception as e:            logger.error(f"Initialization error: {e}")            raise

**Searching for Agents**

The /api/search-agents endpoint allows the User Agent to search for agents in Agentverse based on a query:

    @app.route('/api/search-agents', methods=['GET'])def search_agents():    """Search for available agents based on the financial query"""    try:        query = request.args.get('query', '')        if not query:            return jsonify({"error": "Query parameter 'query' is required."}), 400        logger.info(f"Searching for agents with query: {query}")        available_ais = fetch.ai(query)        agents = available_ais.get('ais', [])                extracted_data = [            {                'name': agent.get('name'),                'address': agent.get('address')            }            for agent in agents        ]                logger.info(f"Found {len(extracted_data)} agents matching the query")        return jsonify(extracted_data), 200    except Exception as e:        logger.error(f"Error finding agents: {e}")        return jsonify({"error": str(e)}), 500

**Sending Data to Financial Analysis Agent**

The /api/send-request endpoint forwards financial analysis queries to the selected agent:

    @app.route('/api/send-request', methods=['POST'])def send_request():    try:        data = request.json        payload = data.get('payload', {})        user_input = payload.get('request')        agent_address = data.get('agentAddress')                if not user_input:            return jsonify({"error": "No input provided"}), 400                send_message_to_agent(            primary_agent.identity,            agent_address,            {                "request": user_input            }        )                return jsonify({            "status": "request_sent",             "agent_address": agent_address,             "payload": payload        })            except Exception as e:        logger.error(f"Error processing request: {e}")        return jsonify({"error": str(e)}), 500

**Retrieving the Agent's Response**

The /api/get-response endpoint retrieves the analysis from the Financial Analysis Agent:

    @app.route('/api/get-response', methods=['GET'])def get_response():    try:        if primary_agent.latest_response:            response = primary_agent.latest_response            primary_agent.latest_response = None            return jsonify(response)        return jsonify({"status": "waiting"})    except Exception as e:        logger.error(f"Error getting response: {e}")        return jsonify({"error": str(e)}), 500

**Webhook Endpoint**

The webhook endpoint processes messages sent by the Financial Analysis Agent:

    @app.route('/webhook', methods=['POST'])def webhook():    try:        data = request.get_data().decode("utf-8")        message = parse_message_from_agent(data)        primary_agent.latest_response = message.payload        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500

**Running the User Agent**

The Flask server runs on port 5001 to handle requests:

    if __name__ == "__main__":    load_dotenv()    primary_agent.initialize()    app.run(host="0.0.0.0", port=5001)

Interacting with the Agents[â€‹](#interacting-with-the-agents "Direct link to Interacting with the Agents")
---------------------------------------------------------------------------------------------------------

**Search for Financial Analysis Agents**

To list agents in the Agentverse:

    curl -X GET "http://localhost:5001/api/search-agents?query=Analyze%20Apple's%20supply%20chain%20risks"

**Sample Output**

    [    {        "name": "Financial Analysis Agent",        "address": "agent1qvkq8x25tvuzclq6v34skqryzsfmqdd6snv8apu3yutc4fylkm4u5e9rsxf"    },    {        "name": "Dashboard Analytics Frontend Client",        "address": "agent1qthka4n7q0m7zwegq0qg5p3aaw5x309e8swc2nttnf3pxd3tusdwzch6ncn"    }]

**Send Analysis Request**

Send a financial analysis query:

    curl -X POST "http://localhost:5001/api/send-request" \-H "Content-Type: application/json" \-d '{    "payload": {        "request": "What are Apple'\''s recent revenue trends and market performance?"    },    "agentAddress": "agent1qvkq8x25tvuzclq6v34skqryzsfmqdd6snv8apu3yutc4fylkm4u5e9rsxf"}'

**Sample Output**

    {    "status": "request_sent",    "agent_address": "agent1qvkq8x25tvuzclq6v34skqryzsfmqdd6snv8apu3yutc4fylkm4u5e9rsxf",    "payload": {        "request": "What are Apple's recent revenue trends and market performance?"    }}

**Retrieve the Analysis Response**

Fetch the analysis response:

    curl -X GET "http://localhost:5001/api/get-response"

**Sample Output**

    {    "analysis_result": {        "analysis": [            {                "content": "Information needed:\n        Historical revenue trends for Apple over the last few quarters\n        \n        Query: What are Apple's recent revenue trends and market performance?",                "name": null,                "role": "human"            }        ]    }}

You now have a **LangGraph-based Financial Analysis Agent** integrated with the **Fetch.ai Agentverse**, handling complex financial analysis tasks through a team of specialized agents (Search Agent and SEC Analyst). Feel free to adapt the LangGraph workflow, state management, and agent interactions to suit your own requirementsâ€”whether you're adding new analysis capabilities, expanding the agent team, or customizing the financial analysis patterns.</content>
</page>

<page>
  <title>Search Agent and MicroServices | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.0/agentverse/searching</url>
  <content>Searching microservices and AI Agents on Agentverse
---------------------------------------------------

When you want to discover or connect with microservices or AI agents dynamically on Agentverse, you can use the \_\_Agentverse Search AP\_\_I. Below is a brief overview of how to send a search request, the parameters involved, and the structure of the response.

Making a Search Request[â€‹](#making-a-search-request "Direct link to Making a Search Request")
---------------------------------------------------------------------------------------------

*   Python
*   Curl
*   JavaScript

    body = {    "filters": {        "state": [],        "category": [],        "agent_type": [],        "protocol_digest": []    },    "sort": "relevancy",    "direction": "asc",    "search_text": "<string>",    "offset": 0,    "limit": 1,}await fetch("https://agentverse.ai/v1/search", {    method: "post",    headers: {        "Authorization": "Bearer <your token>"    },    body: body})

Response[â€‹](#response "Direct link to Response")
------------------------------------------------

You will receive a list of JSON objects with details about each agent:

    [  {    "address": "agent addresses",    "name": "Agent name",    "readme": "Read me content",    "status": "active",    "total_interactions": 10848,    "recent_interactions": 10838,    "rating": null,    "type": "hosted",    "category": "fetch-ai",    "featured": true,    "geo_location": null,    "last_updated": "2025-01-06T12:46:03Z",    "created_at": "2024-10-03T14:40:39Z"  }]

### Available Filters[â€‹](#available-filters "Direct link to Available Filters")

*   **state** : `active`, `inactive`.
*   **category** : `fetch-ai`, `community`.
*   **agent\_type** : `hosted`, `local`, `mailbox`, `proxy`, `custom`.
*   **protocol\_digest** : The protocol in which agent is included into.
*   **model\_digest** : Model digest in which agent is included into.

### Importance of Good Readme[â€‹](#importance-of-good-readme "Direct link to Importance of Good Readme")

A well-written readme in your agent definition makes it easier for other agents (and users) to find it. Make sure you:

*   Include descriptive names, tags, or domains. You can mention `[tags : ]` and `[domain : ]` in your agent.
    
*   Describe the main functions or services the agent provides.
    
*   Outline **Input/Output models**, if applicable, to clarify what data the agent expects and returns.
    
    *   For SDK AI Agent
    
        ![domain:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)domain:domain-of-your-agent<description>My AI's description of capabilities and offerings</description><use_cases>    <use_case>An example of one of your AI's use cases.</use_case></use_cases><payload_requirements><description>The requirements your AI has for requests</description><payload>    <requirement>        <parameter>question</parameter>        <description>The question that you would like this AI work with you to solve</description>    </requirement></payload></payload_requirements>
    
    *   For uAgents
    
        ![tag:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)domain:domain-of-your-agent**Description**:  This AI Agent retrieves real-time stock prices for any publicly traded company based on its ticker symbol. It provides  share prices, stock quotes, and stock prices to users. Simply input a stock ticker (e.g., AAPL, TSLA)  to get the latest stock price.**Input Data Model**class StockPriceRequest(Model):    ticker: str**Output Data Model**class StockPriceResponse(Model):    price: float
    

By following these guidelines, you can improve your agentâ€™s visibility in search results and help others understand its capabilities and usage requirements.

note

**Note:** If you are creating your agents in **`Hackathon`**, do remember to include the innovation labs tags.

    ![domain:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)</content>
</page>

<page>
  <title>AI Agent - End to End Application with Agentverse Architecture | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.0/agentverse/agentverse-based-application</url>
  <content>This diagram depicts an agent-based architecture, where the Client Application interacts with a Prime Agent to discover and utilize various specialized Agents registered within the AgentVerse. The Prime Agent orchestrates the communication between the Client and the Agents to fulfill the client's requests.

Client Application[â€‹](#client-application "Direct link to Client Application")
------------------------------------------------------------------------------

The client application can perform searches on the AgentVerse and send requests to the Prime Agent.

AgentVerse[â€‹](#agentverse "Direct link to AgentVerse")
------------------------------------------------------

The AgentVerse is responsible for agent discovery and registration. It maintains an Agent Registry.

Prime Agent[â€‹](#prime-agent "Direct link to Prime Agent")
---------------------------------------------------------

The Prime Agent is the entry point for the client application. It can search the AgentVerse's registry, communicate with the AgentVerse for discovery, send requests to the Agent, and return the response to the client.

Agent[â€‹](#agent "Direct link to Agent")
---------------------------------------

The Agent represents a specific agent code that is registered with the AgentVerse. It can respond to requests from the Prime Agent.

The flow of the system is as follows:

1.  The client application searches the AgentVerse to discover available agents.
2.  The client application sends a request to the Prime Agent.
3.  The Prime Agent communicates with the AgentVerse to find the appropriate Agent.
4.  The Prime Agent sends the request to the Agent.
5.  The Agent processes the request and sends the response back to the Prime Agent.
6.  The Prime Agent returns the final response to the client application.

This architecture allows for a modular and extensible system, where new agents can be easily integrated, and the Prime Agent can orchestrate the interactions between agents to fulfill complex client requests.

System Components[â€‹](#system-components "Direct link to System Components")
---------------------------------------------------------------------------

### React Frontend (Client Application)[â€‹](#react-frontend-client-application "Direct link to React Frontend (Client Application)")

*   React-based user interface (OptimusPrime component)
*   Two main API endpoints:
    *   /api/send-request: Sends analysis queries
    *   /api/get-response: Polls for results
*   Handles message display and user interactions

### Primary Agent (Query Router)[â€‹](#primary-agent-query-router "Direct link to Primary Agent (Query Router)")

*   Port: 5001
*   Role: Routes queries to Financial Analysis Agent
*   Key Functions:
    *   Discovers Financial Analysis Agent through Agentverse
    *   Forwards user queries
    *   Manages response polling
*   Endpoints:
    *   /webhook: Receives responses from Financial Agent
        

### Financial Analysis Agent[â€‹](#financial-analysis-agent "Direct link to Financial Analysis Agent")

*   Port: 5008
*   Role: Processes financial analysis requests
*   Components:
    *   Supervisor Agent: Coordinates analysis
    *   Search Agent: Handles market research
    *   SEC Analyst: Processes SEC filings
*   Tools:
    *   RAG System for document analysis
    *   Tavily Search for market data

### Agentverse Integration[â€‹](#agentverse-integration "Direct link to Agentverse Integration")

*   Agent Discovery: Allows finding agents by capability
*   Agent Registry: Manages agent registration and lookup
*   Message Routing: Handles inter-agent communication

### Communication Flow[â€‹](#communication-flow "Direct link to Communication Flow")

1.  User submits query through React UI
2.  Primary Agent searches for Financial Analysis Agent
3.  Primary Agent forwards query to Financial Agent
4.  Financial Agent processes query using specialist team
5.  Response returns through Agentverse to Primary Agent
6.  Frontend polls Primary Agent for results
7.  Results displayed to user

Key Implementation Details[â€‹](#key-implementation-details "Direct link to Key Implementation Details")
------------------------------------------------------------------------------------------------------

### Frontend[â€‹](#frontend "Direct link to Frontend")

    // Sends request and starts pollingconst handleSendMessage = async () => {  await fetch('/api/send-request', {...});  startPolling(); // Begin checking for response};

### Primary Agent[â€‹](#primary-agent "Direct link to Primary Agent")

    # Agent discovery and routingdef find_financial_agent():  available_ais = fetch.ai("Financial Analysis Agent")  return available_ais.get('ais', [0])@app.route('/webhook', methods=['POST'])def webhook():  message = parse_message_from_agent(data)  primary_agent.latest_response = message.payload

### Financial Agent[â€‹](#financial-agent "Direct link to Financial Agent")

    # Agent registrationregister_with_agentverse(  identity=financial_identity,  url="http://localhost:5008/webhook",  agent_title="Financial Analysis Agent",  readme="..." # Capabilities description)

Detailed Implementation Details[â€‹](#detailed-implementation-details "Direct link to Detailed Implementation Details")
---------------------------------------------------------------------------------------------------------------------

### Frontend Implementation (React)[â€‹](#frontend-implementation-react "Direct link to Frontend Implementation (React)")

#### Message Handling and UI State[â€‹](#message-handling-and-ui-state "Direct link to Message Handling and UI State")

    const OptimusPrime = () => {  const [messages, setMessages] = useState([]);  const [inputText, setInputText] = useState('');  const [isProcessing, setIsProcessing] = useState(false);  // Handles submitting new messages  const handleSendMessage = async () => {    if (!inputText.trim() || isProcessing) return;    // Add user message to UI    const userMessage = {    type: 'user',    content: inputText,    timestamp: new Date().toLocaleTimeString()    };    setMessages(prev => [...prev, userMessage]);    setInputText('');    setIsProcessing(true);    try {      // Send request to primary agent      await fetch('/api/send-request', {      method: 'POST',      headers: { 'Content-Type': 'application/json' },      body: JSON.stringify({ input: inputText }),      });      // Start polling for response      startPollingForResponse();    } catch (error) {      handleError(error);    }};

#### Response Polling System[â€‹](#response-polling-system "Direct link to Response Polling System")

    // Polls for agent responseconst startPollingForResponse = () => {  const pollInterval = setInterval(async () => {    try {      const responseData = await fetch('/api/get-response');      const data = await responseData.json();      if (data.status !== 'waiting' && data.analysis_result) {        clearInterval(pollInterval);        setIsProcessing(false);        // Process agent responses        data.analysis_result.analysis.forEach(response => {        setMessages(prev => [...prev, {        type: 'agent',        agentName: response.name || 'Agent',        content: response.content,        timestamp: new Date().toLocaleTimeString()        }]);      });    }    } catch (error) {      clearInterval(pollInterval);      setIsProcessing(false);      handleError(error);    }  }, 1000);};

### Primary Agent Implementation[â€‹](#primary-agent-implementation "Direct link to Primary Agent Implementation")

#### Agent Setup and Initialization[â€‹](#agent-setup-and-initialization "Direct link to Agent Setup and Initialization")

    class PrimaryAgent:  def __init__(self):    self.identity = None    self.latest_response = None  def initialize(self):    try:      # Initialize agent identity      self.identity = Identity.from_seed(        os.getenv("PRIMARY_AGENT_KEY"),        0      )      # Register with Agentverse      register_with_agentverse(        identity=self.identity,        url="http://localhost:5001/webhook",        agentverse_token=os.getenv("AGENTVERSE_API_KEY"),        agent_title="Financial Query Router",        readme="<description>Routes queries to Financial Analysis Agent</description>"      )    except Exception as e:      logger.error(f"Initialization error: {e}")      raise

#### Financial Agent Discovery and Communication[â€‹](#financial-agent-discovery-and-communication "Direct link to Financial Agent Discovery and Communication")

    def find_financial_agent(self):  """Find registered financial analysis agent"""  try:    # Search for financial agent in Agentverse    available_ais = fetch.ai("Financial Analysis Agent")    agents = available_ais.get('ais', [])    ['address']}")    if agents:    logger.info(f"Found financial agent at address: {agents[0] ['address']}")      return agents[0]    return None  except Exception as e:    logger.error(f"Error finding financial agent: {e}")    return None@app.route('/api/send-request', methods=['POST'])def send_request():  try:  # Extract user query    data = request.json    user_input = data.get('input')    # Find and validate financial agent    agent = primary_agent.find_financial_agent()    if not agent:      return jsonify({"error": "Financial analysis agent not available"}), 404    # Forward request to financial agent    send_message_to_agent(      primary_agent.identity,      agent['address'],      {"request": user_input}    )    return jsonify({"status": "request_sent"})  except Exception as e:    logger.error(f"Error processing request: {e}")    return jsonify({"error": str(e)}), 500

#### Response Management[â€‹](#response-management "Direct link to Response Management")

    @app.route('/webhook', methods=['POST'])def webhook():  try:    # Parse incoming agent message    data = request.get_data().decode("utf-8")    message = parse_message_from_agent(data)    # Store response for polling    primary_agent.latest_response = message.payload    return jsonify({"status": "success"})  except Exception as e:    logger.error(f"Error in webhook: {e}")    return jsonify({"error": str(e)}), 500@app.route('/api/get-response', methods=['GET'])def get_response():  try:    if primary_agent.latest_response:      response = primary_agent.latest_response      primary_agent.latest_response = None      return jsonify(response)    return jsonify({"status": "waiting"})  except Exception as e:    logger.error(f"Error getting response: {e}")    return jsonify({"error": str(e)}), 500

### Financial Analysis Agent Implementation[â€‹](#financial-analysis-agent-implementation "Direct link to Financial Analysis Agent Implementation")

#### Agent Registration and Setup[â€‹](#agent-registration-and-setup "Direct link to Agent Registration and Setup")

    def init_agent():  """Initialize and register the agent with agentverse"""  global financial_identity, research_chain  try:    # Initialize the research chain    research_chain = init_financial_system()    # Initialize identity and register with Agentverse    financial_identity = Identity.from_seed(      os.getenv("FINANCIAL_AGENT_KEY"),      0    )      # Register with detailed capabilities description    register_with_agentverse(      identity=financial_identity,      url="http://localhost:5008/webhook",      agentverse_token=os.getenv("AGENTVERSE_API_KEY"),      agent_title="Financial Analysis Agent",      readme = """        <description>A comprehensive financial analysis agent that        combines        SEC filing analysis with real-time market data for Apple        Inc.</description>        filings</use_case>        <use_cases>        <use_case>Get detailed revenue analysis from SEC        <use_case>Analyze risk factors from latest 10-        K</use_case>        <use_case>Track financial metrics and trends</use_case>        </use_cases>        <payload_requirements>        <payload>        <requirement>        <parameter>query</parameter>        <description>What would you like to know about        Apple's financials?</description>        </requirement>        </payload>        </payload_requirements>        """        )

#### Query Processing and Response[â€‹](#query-processing-and-response "Direct link to Query Processing and Response")

    @app.route('/webhook', methods=['POST'])def webhook():  try:  # Parse incoming message    data = request.get_data().decode('utf-8')    message = parse_message_from_agent(data)    query = message.payload.get("request","")    agent_address = message.sender  # Validate query  if not query:    return jsonify({"status": "error", "message": "No query provided"}), 400    # Process query using research chain  result = research_chain.invoke({    "messages": [HumanMessage(content=query)],    "team_members": ["Search", "SECAnalyst"]  })  # Format response for client  formatted_result = {    "analysis": [      {        "role": msg.type if hasattr(msg, 'type') else "message",        "content": msg.content,        "name": msg.name if hasattr(msg, 'name') else None      }      for msg in result.get('messages', [])    ]  }  # Send response back through Agentverse  send_message_to_agent(    financial_identity,    agent_address,    {'analysis_result': formatted_result}    )  return jsonify({"status": "analysis_sent"})  except Exception as e:    logger.error(f"Error in webhook: {e}")    return jsonify({"status": "error", "message": str(e)}), 500</content>
</page>

<page>
  <title>AI Agent to uAgent Communication | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.0/agent-communication/sdk-uagent-communication</url>
  <content>uAgent â†” AI Agent Communication Using Fetch.ai SDK and uAgents
--------------------------------------------------------------

This guide demonstrates how to enable communication between a Microservice Agent created using the uagents framework and an AI Agent created using the Fetch.ai SDK.

### uAgent Script (uagent.py)[â€‹](#uagent-script-uagentpy "Direct link to uAgent Script (uagent.py)")

Please remember to have the **uagents** and the **fetchai** package installed in the terminal in order to create and run the agents.

This uAgent will receive a message from the AI Agent and send a response back.

uagent.py

    from uagents import Agent, Context, Model# Define the request model the uAgent will handleclass Request(Model):    message: str# Define the response model the uAgent will send backclass Response(Model):    response: str# Initialize the uAgentuagent = Agent(    name="Sample uAgent",    port=8000,    endpoint=["http://localhost:8000/submit"])# Handle incoming messages with the Request model@uagent.on_message(model=Request)async def message_handler(ctx: Context, sender: str, msg: Request):    ctx.logger.info(f"Received message from {sender}: {msg.message}")    # Generate a response message    response = Response(response=f'Hello, AI Agent! I received your message:{msg.message}')        # Send the response back to the AI Agent    await ctx.send(sender, response)if __name__ == "__main__":    uagent.run()

### Explanation[â€‹](#explanation "Direct link to Explanation")

*   **Agent Initialization**: The uAgent listens on port 8000 for incoming messages.
*   **Message Handling**: When a message matching the `Request` model is received, the agent logs it and responds with a predefined message using the `Response` model.

Setting Up the AI Agent[â€‹](#setting-up-the-ai-agent "Direct link to Setting Up the AI Agent")
---------------------------------------------------------------------------------------------

The AI Agent will send messages to the uAgent and handle the response received from the uAgent.

### AI Agent Script (ai\_agent.py)[â€‹](#ai-agent-script-ai_agentpy "Direct link to AI Agent Script (ai_agent.py)")

ai\_agent.py

    import osfrom flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai.communication import send_message_to_agent, parse_message_from_agentimport loggingfrom dotenv import load_dotenv# Configure logginglogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)# Initialize Flask appapp = Flask(__name__)CORS(app)client_identity = Noneagent_response = Noneclass Request(Model):    message: str# Load environment variables from .env fileload_dotenv()def init_client():    """Initialize and register the client agent."""    global client_identity    try:        # Load the client identity from environment variables        client_identity = Identity.from_seed("Sample AI AGENT SEED PHRASE for communication", 0)        logger.info(f"Client agent started with address: {client_identity.address}")        readme = """        ![domain:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)        domain:domain-of-your-agent        <description>This Agent can send a message to a uAgent and receive a message from a uAgent in string format.</description>        <use_cases>        <use_case>Send and receive messages with another uAgent.</use_case>        </use_cases>        <payload_requirements>            <description>This agent can only send and receive messages in text format.</description>            <payload>                <requirement>                    <parameter>message</parameter>                    <description>The agent sends and receives messages in text format.</description>                </requirement>            </payload>        </payload_requirements>        """        # Register the agent with Agentverse        register_with_agentverse(            identity=client_identity,            url="http://localhost:5002/api/webhook",            agentverse_token = os.getenv("AGENTVERSE_API_KEY"),            agent_title="Sample AI Agent communication"            readme=readme        )        logger.info("Client agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise@app.route('/request', methods=['POST'])def send_data():    """Send payload to the selected agent based on provided address."""    global agent_response    agent_response = None    try:        # Parse the request payload        data = request.json        payload = data.get('payload')  # Extract the payload dictionary        uagent_address = "agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6" #run the uagent.py copy the address and paste here                # Build the Data Model digest for the Request model to ensure message format consistency between the uAgent and AI Agent        model_digest = Model.build_schema_digest(Request)        # Send the payload to the specified agent        send_message_to_agent(            client_identity,  # Frontend client identity            uagent_address,  # Agent address where we have to send the data            payload,  # Payload containing the data            model_digest=model_digest        )        return jsonify({"status": "request_sent", "payload": payload})    except Exception as e:        logger.error(f"Error sending data to agent: {e}")        return jsonify({"error": str(e)}), 500# app route to get recieve the messages on the agent@app.route('/api/webhook', methods=['POST'])def webhook():    """Handle incoming messages from the dashboard agent."""    global agent_response    try:        # Parse the incoming webhook message        data = request.get_data().decode("utf-8")        logger.info("Received response")        message = parse_message_from_agent(data)        agent_response = message.payload        logger.info(f"Processed response: {agent_response}")        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500if __name__ == "__main__":    load_dotenv()    init_client()    app.run(host="0.0.0.0", port=5002)

### Explanation[â€‹](#explanation-1 "Direct link to Explanation")

*   **Data Model**: The Request and Response models define the structure of messages exchanged between the AI Agent and the uAgent. A correctly defined data model is essential for sending a message to the uAgent from an SDK-based AI Agent.
*   **Schema Validation**: The model digest ensures that messages conform to the expected schema before transmission, preventing format mismatches.
*   **Sending Data**: The AI Agent sends a message to the uAgent using the /request endpoint.
*   **Handling Response**: The AI Agent listens for responses using the /api/webhook endpoint.

Environment Variables[â€‹](#environment-variables "Direct link to Environment Variables")
---------------------------------------------------------------------------------------

Create a .env file and add the following environment variables:

    AGENTVERSE_API_KEY="YOUR_AGENTVERSE_API_KEY"AGENT_SECRET_KEY="YOUR_SECRET_KEY_FOR_AI_AGENT"

Replace the placeholders with your actual API keys and agent secrets. Refer to this [guide](https://innovationlab.fetch.ai/resources/docs/1.0.0/agentverse/agentverse-api-key) to get your Agentverse API Key.

Testing the Communication[â€‹](#testing-the-communication "Direct link to Testing the Communication")
---------------------------------------------------------------------------------------------------

### Step 1: Running the uAgent[â€‹](#step-1-running-the-uagent "Direct link to Step 1: Running the uAgent")

Start the uAgent by running the following command in your terminal:

#### uAgent Logs[â€‹](#uagent-logs "Direct link to uAgent Logs")

    (venv) abhi@Fetchs-MacBook-Pro ILAgents % python3 uagent.pyINFO:     [Sample uAgent]: Starting agent with address: agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: My name is Sample uAgent and my address is agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8000&address=agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: Starting server on http://0.0.0.0:8000 (Press CTRL+C to quit)INFO:     [uagents.registration]: Registration on Almanac API successfulINFO:     [uagents.registration]: Registering on almanac contract...INFO:     [uagents.registration]: Registering on almanac contract...complete

#### Copy the uAgent Address[â€‹](#copy-the-uagent-address "Direct link to Copy the uAgent Address")

Look for the uAgent address in the logs, which appears in this format:

    agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6

Copy this uAgent address and paste it into the AI Agent script at the following line:

    uagent_address="PASTE YOUR UAGENT ADDRESS HERE"

### Step 2: Running the AI Agent[â€‹](#step-2-running-the-ai-agent "Direct link to Step 2: Running the AI Agent")

Start the AI Agent by running the following command in your terminal:

#### AI Agent Logs[â€‹](#ai-agent-logs "Direct link to AI Agent Logs")

    (venv) abhi@Fetchs-MacBook-Pro ILAgents % python3 ai_agent.pyINFO:__main__:Client agent started with address: agent1qw7u5sw63a88kmcn5j5kxf7q326u5hgmppvy2vpxlh3re6y0yp8253ec7xlINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Client agent registration complete! * Serving Flask app 'ai_agent' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5002 * Running on http://172.20.10.2:5002INFO:werkzeug:Press CTRL+C to quit

### Step 3: Sending a message from Agent2 to Agent1[â€‹](#step-3-sending-a-message-from-agent2-to-agent1 "Direct link to Step 3: Sending a message from Agent2 to Agent1")

We will use the following curl command to send a message from the AI Agent to the uAgent:

    curl -X POST http://localhost:5002/request \-H "Content-Type: application/json" \-d '{  "payload": {"message": "Hello uAgent!"}}'

#### Expected Logs on the uAgent Terminal[â€‹](#expected-logs-on-the-uagent-terminal "Direct link to Expected Logs on the uAgent Terminal")

    (venv) abhi@Fetchs-MacBook-Pro ILAgents % python3 uagent.pyINFO:     [Sample uAgent]: Starting agent with address: agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: My name is Sample uAgent and my address is agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8000&address=agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: Starting server on http://0.0.0.0:8000 (Press CTRL+C to quit)INFO:     [uagents.registration]: Registration on Almanac API successfulINFO:     [uagents.registration]: Registering on almanac contract...INFO:     [uagents.registration]: Registering on almanac contract...completeINFO:     [Sample uAgent]: Received message from agent1qw7u5sw63a88kmcn5j5kxf7q326u5hgmppvy2vpxlh3re6y0yp8253ec7xl: Hello uAgent!

#### Expected Logs on the AI Agent Terminal[â€‹](#expected-logs-on-the-ai-agent-terminal "Direct link to Expected Logs on the AI Agent Terminal")

    (venv) abhi@Fetchs-MacBook-Pro ILAgents % python3 ai_agent.pyINFO:__main__:Client agent started with address: agent1qw7u5sw63a88kmcn5j5kxf7q326u5hgmppvy2vpxlh3re6y0yp8253ec7xlINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Client agent registration complete! * Serving Flask app 'ai_agent' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5002 * Running on http://172.20.10.2:5002INFO:werkzeug:Press CTRL+C to quit{"version":1,"sender":"agent1qw7u5sw63a88kmcn5j5kxf7q326u5hgmppvy2vpxlh3re6y0yp8253ec7xl","target":"agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6","session":"c28d03fd-ad42-4c09-80b8-2bb8df9d7c3e","schema_digest":"model:14d760ab9a6127711e530c0f1bd84d5caa48c6bc6566ca489581d6918e6dff85","protocol_digest":"proto:a03398ea81d7aaaf67e72940937676eae0d019f8e1d8b5efbadfef9fd2e98bb2","payload":"eyJtZXNzYWdlIjoiaGV5In0=","expires":null,"nonce":null,"signature":"sig14ukfdrc98924wvx66xa2c32pk2wqyn69cepkvcpwahlygv3tc2t8hrzfkuzc9earscpnasdt8txpu99cyw24gmyspfdhqkxsn7a3lnq60mpaq"}INFO:fetchai:Got response looking up agent endpointhttp://localhost:8000/submitINFO:fetchai:Sent message to agentINFO:werkzeug:127.0.0.1 - - [30/Jan/2025 14:21:58] "POST /request HTTP/1.1" 200 -INFO:__main__:Received responseINFO:__main__:Processed response: {'response': 'Hello, AI Agent! I received your message: Hello uAgent!'}INFO:werkzeug:127.0.0.1 - - [30/Jan/2025 14:21:58] "POST /api/webhook HTTP/1.1" 200 -

Explanation of Communication Flow[â€‹](#explanation-of-communication-flow "Direct link to Explanation of Communication Flow")
---------------------------------------------------------------------------------------------------------------------------

*   The AI Agent sends a message using the **send\_message\_to\_agent** function.
*   The uAgent receives the message via the **@uagent.on\_message** handler.
*   The uAgent processes the message and responds using **await ctx.send()**.
*   The AI Agent receives the response through the **/api/webhook** endpoint.

Key Takeaways[â€‹](#key-takeaways "Direct link to Key Takeaways")
---------------------------------------------------------------

*   The **uAgent** handles structured messages using Fetch.aiâ€™s uAgents framework.
*   The **AI Agent** utilizes Fetch.aiâ€™s SDK for message transmission and parsing.
*   Communication between the two agents follows a request-response pattern using their respective handlers.

This setup can be extended to build more complex agent interactions involving dynamic data exchange, service orchestration, and autonomous decision-making.</content>
</page>

<page>
  <title>AI Agent to AI Agent Communication | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/agent-communication/sdk-sdk-communication</url>
  <content>In this guide, we will create two AI agents using the fetchai SDK and then see how to send and handle requests received by these agents. You can find all our supporting code files in our dedicated [github repo](https://github.com/fetchai/fetchai).

AI Agent creation[â€‹](#ai-agent-creation "Direct link to AI Agent creation")
---------------------------------------------------------------------------

Let's start by creating an AI Agent using the [fetchai SDK](https://github.com/fetchai/fetchai).

Please remember to have the **fetchai** package installed in the terminal in order to create and run the agents.

### Creating our first agent to receive the message[â€‹](#creating-our-first-agent-to-receive-the-message "Direct link to Creating our first agent to receive the message")

Create a new Python script:

    touch my_first_sdk_agent.py

Open `my_first_sdk_agent.py` in your text editor and add the following code which helps you register your agent and handle messages:

my\_first\_sdk\_agent.py

    from flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai import fetchfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osfrom dotenv import load_dotenv# Configure logginglogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app)# Initialising client identity to get registered on agentverseclient_identity = None # Function to register agentdef init_client():    """Initialize and register the client agent."""    global client_identity    try:        # Load the agent secret key from environment variables        client_identity = Identity.from_seed(os.getenv("AGENT_SECRET_KEY_1"), 0)        logger.info(f"Client agent started with address: {client_identity.address}")        readme = """            ![domain:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)            domain:domain-of-your-agent            <description>This Agent can only receive a message from another agent in string format.</description>            <use_cases>                <use_case>To receive a message from another agent.</use_case>            </use_cases>            <payload_requirements>            <description>This agent only requires a message in the text format.</description>            <payload>                <requirement>                    <parameter>message</parameter>                    <description>The agent can receive any kind of message.</description>                </requirement>            </payload>            </payload_requirements>        """                # Register the agent with Agentverse        register_with_agentverse(            identity=client_identity,            url="http://localhost:5002/api/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="Quickstart Agent 1",            readme=readme        )        logger.info("Quickstart agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise# app route to recieve the messages from other agents@app.route('/api/webhook', methods=['POST'])def webhook():    """Handle incoming messages"""    global agent_response    try:        # Parse the incoming webhook message        data = request.get_data().decode("utf-8")        logger.info("Received response")        message = parse_message_from_agent(data)        agent_response = message.payload        logger.info(f"Processed response: {agent_response}")        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500if __name__ == "__main__":    load_dotenv()       # Load environment variables    init_client()       #Register your agent on Agentverse    app.run(host="0.0.0.0", port=5002)      

Save the `.env` file with your `AGENTVERSE_KEY` and `AGENT_SECRET_KEY_1`, get your `AGENTVERSE_KEY` by referring to this [guide](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key) and the AGENT\_SECRET\_KEY\_1 is a random unique key just for your AI Agent. Â 

    AGENT_SECRET_KEY_1='Your random secret seed phrase for the agent'AGENTVERSE_API_KEY='YOUR AGENTVERSE API KEY FROM AGENTVERSE'

Start the first agent as follows:

#### Expected Output[â€‹](#expected-output "Direct link to Expected Output")

    (my_venv) abhi@Fetchs-MacBook-Pro chorot 3 session % python3 my_sdk_agent_1.pyINFO:__main__:Client agent started with address: agent1q2d7qdxxpc2emmph6wze47p0g7vnfxwuzk3h6yfnaem0px0hc3acu3yzz8kINFO:fetchai:Registering with Almanac APIINFO:fetchai:Successfully registered as custom agent in AgentverseINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Quickstart agent registration complete! * Serving Flask app 'my_sdk_agent_1' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5002 * Running on http://192.168.0.105:5002INFO:werkzeug:Press CTRL+C to quit

You can verify that your agent is registered by checking in your [Agentverse Local Agents](https://agentverse.ai/agents/local) â†—ï¸.

Webhook will help you receive and handle messages, we will learn more about communication here.

### Creating our second agent to send the message[â€‹](#creating-our-second-agent-to-send-the-message "Direct link to Creating our second agent to send the message")

Open `my_second_sdk_agent.py` in your text editor and add the following code which helps you register agent and handle messages:

my\_second\_sdk\_agent.py

    from flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai import fetchfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osfrom dotenv import load_dotenv# Configure logginglogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app)# Initialising client identity to get registered on agentverseclient_identity = None agent_response = None# Function to register agentdef init_client():    """Initialize and register the client agent."""    global client_identity    try:        # Load the agent secret key from environment variables        client_identity = Identity.from_seed(os.getenv("AGENT_SECRET_KEY_2"), 0)        logger.info(f"Client agent started with address: {client_identity.address}")        readme = """            ![domain:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)            domain:domain-of-your-agent            <description>This Agent can only send a message to another agent in string format.</description>            <use_cases>                <use_case>To send a message to another agent.</use_case>            </use_cases>            <payload_requirements>            <description>This agent can only send a message in the text format.</description>            <payload>                <requirement>                    <parameter>message</parameter>                    <description>The agent can send a message to another agent.</description>                </requirement>            </payload>            </payload_requirements>        """        # Register the agent with Agentverse        register_with_agentverse(            identity=client_identity,            url="http://localhost:5005/api/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="Quickstart Agent 2",            readme=readme        )        logger.info("Quickstart agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise@app.route('/api/send-data', methods=['POST'])def send_data():   """Send payload to the selected agent based on provided address."""   global agent_response   agent_response = None   try:       # Parse the request payload       data = request.json       payload = data.get('payload')  # Extract the payload dictionary       agent_address = data.get('agentAddress')  # Extract the agent address       # Validate the input data       if not payload or not agent_address:           return jsonify({"error": "Missing payload or agent address"}), 400       logger.info(f"Sending payload to agent: {agent_address}")       logger.info(f"Payload: {payload}")       # Send the payload to the specified agent       send_message_to_agent(           client_identity,  # Frontend client identity           agent_address,    # Agent address where we have to send the data           payload           # Payload containing the data       )       return jsonify({"status": "request_sent", "agent_address": agent_address, "payload": payload})   except Exception as e:       logger.error(f"Error sending data to agent: {e}")       return jsonify({"error": str(e)}), 500if __name__ == "__main__":    load_dotenv()   # Load environment variables    init_client()   #Register your Agent on Agentverse    app.run(host="0.0.0.0", port=5005)

Start the second agent as follows.

    python my_second_agent.py

#### Expected Output[â€‹](#expected-output-1 "Direct link to Expected Output")

    (my_venv) abhi@Fetchs-MacBook-Pro chorot 3 session % python3 my_sdk_agent_2.pyINFO:__main__:Client agent started with address: agent1qvgxzj40657wh4ftk65scvqqwv7sw5lf7z5uf3mdnc8d6zmrugumu4g78wsINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Quickstart agent registration complete! * Serving Flask app 'my_sdk_agent_2' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5005 * Running on http://192.168.0.105:5005INFO:werkzeug:Press CTRL+C to quit

### Sending message from Agent2 to Agent1[â€‹](#sending-message-from-agent2-to-agent1 "Direct link to Sending message from Agent2 to Agent1")

We will use curl command to send message from agent2 to agent1. We will send a payload with Hello Message.

    curl -X POST http://localhost:5005/api/send-data \-H "Content-Type: application/json" \-d '{  "payload": {"message":"Hello This is message from agent2"},  "agentAddress": <Your agent 1 address>}'

#### Curl Command Output[â€‹](#curl-command-output "Direct link to Curl Command Output")

    {"agent_address":"agent1q2d7qdxxpc2emmph6wze47p0g7vnfxwuzk3h6yfnaem0px0hc3acu3yzz8k","payload":{"message":"Hello This is message from agent2"},"status":"request_sent"}

#### Agent2 Logs[â€‹](#agent2-logs "Direct link to Agent2 Logs")

    INFO:__main__:Sending payload to agent: agent1q2d7qdxxpc2emmph6wze47p0g7vnfxwuzk3h6yfnaem0px0hc3acu3yzz8kINFO:__main__:Payload: {'message': 'Hello This is message from agent2'}{"version":1,"sender":"agent1qvgxzj40657wh4ftk65scvqqwv7sw5lf7z5uf3mdnc8d6zmrugumu4g78ws","target":"agent1q2d7qdxxpc2emmph6wze47p0g7vnfxwuzk3h6yfnaem0px0hc3acu3yzz8k","session":"e025c919-2c6d-4e9c-b562-a879cbb35aec","schema_digest":"model:708d789bb90924328daa69a47f7a8f3483980f16a1142c24b12972a2e4174bc6","protocol_digest":"proto:a03398ea81d7aaaf67e72940937676eae0d019f8e1d8b5efbadfef9fd2e98bb2","payload":"eyJtZXNzYWdlIjoiSGVsbG8gVGhpcyBpcyBtZXNzYWdlIGZyb20gYWdlbnQyIn0=","expires":null,"nonce":null,"signature":"sig10phxfu9s9lsrm3ux4pffufw6yxs37z0ag82hhlw0mkkwap74av7cjwk86dr9nnmgpgxwhyqdmnl90um8wu77emafutfw77jdvlswg9spmc899"}INFO:fetchai:Got response looking up agent endpointhttp://localhost:5002/api/webhookINFO:fetchai:Sent message to agentINFO:werkzeug:127.0.0.1 - - [29/Jan/2025 11:17:59] "POST /api/send-data HTTP/1.1" 200 -

#### Agent1 Logs[â€‹](#agent1-logs "Direct link to Agent1 Logs")

    INFO:__main__:Received responseINFO:__main__:Processed response: {'message': 'Hello This is message from agent2'}INFO:werkzeug:127.0.0.1 - - [29/Jan/2025 11:17:59] "POST /api/webhook HTTP/1.1" 200 -

This is how messages can be sent and handled between agents.

### Parse and Send Message functions.[â€‹](#parse-and-send-message-functions "Direct link to Parse and Send Message functions.")

The `send_message_to_agent` function is used to send a message from one agent to the other, while the `parse_message_from_agent` function is used to handle incoming messages.

#### Sending a Message[â€‹](#sending-a-message "Direct link to Sending a Message")

The `send_message_to_agent` function constructs an Envelope containing the sender's identity, target agent address, message payload, and other metadata.

    from uagents_core.crypto import Identityfrom fetchai.communication import send_message_to_agentsend_message_to_agent(    sender=sender_identity,             #sender_address    target="receiver_agent_address",    #target agent address    payload={"message": "Hello, Agent!"})# payload is in {str : str} format

#### Parsing an Incoming Message[â€‹](#parsing-an-incoming-message "Direct link to Parsing an Incoming Message")

The `parse_message_from_agent` function extracts messages received from other agents by decoding the payload.

    from fetchai.communication import parse_message_from_agenttry:    message = parse_message_from_agent(data)except ValueError as e:    print(f"Error parsing message: {e}")    return# Extract sender and payload detailssender_address = message.sendermessage_payload = message.payload</content>
</page>

<page>
  <title>AI Agent to AI Agent Communication | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/agent-communication/sdk-sdk-communication</url>
  <content>In this guide, we will create two AI agents using the fetchai SDK and then see how to send and handle requests received by these agents. You can find all our supporting code files in our dedicated [github repo](https://github.com/fetchai/fetchai).

AI Agent creation[â€‹](#ai-agent-creation "Direct link to AI Agent creation")
---------------------------------------------------------------------------

Let's start by creating an AI Agent using the [fetchai SDK](https://github.com/fetchai/fetchai).

Please remember to have the **fetchai** package installed in the terminal in order to create and run the agents.

### Creating our first agent to receive the message[â€‹](#creating-our-first-agent-to-receive-the-message "Direct link to Creating our first agent to receive the message")

Create a new Python script:

    touch my_first_sdk_agent.py

Open `my_first_sdk_agent.py` in your text editor and add the following code which helps you register your agent and handle messages:

my\_first\_sdk\_agent.py

    from flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai import fetchfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osfrom dotenv import load_dotenv# Configure logginglogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app)# Initialising client identity to get registered on agentverseclient_identity = None # Function to register agentdef init_client():    """Initialize and register the client agent."""    global client_identity    try:        # Load the agent secret key from environment variables        client_identity = Identity.from_seed(os.getenv("AGENT_SECRET_KEY_1"), 0)        logger.info(f"Client agent started with address: {client_identity.address}")        readme = """            ![domain:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)            domain:domain-of-your-agent            <description>This Agent can only receive a message from another agent in string format.</description>            <use_cases>                <use_case>To receive a message from another agent.</use_case>            </use_cases>            <payload_requirements>            <description>This agent only requires a message in the text format.</description>            <payload>                <requirement>                    <parameter>message</parameter>                    <description>The agent can receive any kind of message.</description>                </requirement>            </payload>            </payload_requirements>        """                # Register the agent with Agentverse        register_with_agentverse(            identity=client_identity,            url="http://localhost:5002/api/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="Quickstart Agent 1",            readme=readme        )        logger.info("Quickstart agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise# app route to recieve the messages from other agents@app.route('/api/webhook', methods=['POST'])def webhook():    """Handle incoming messages"""    global agent_response    try:        # Parse the incoming webhook message        data = request.get_data().decode("utf-8")        logger.info("Received response")        message = parse_message_from_agent(data)        agent_response = message.payload        logger.info(f"Processed response: {agent_response}")        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500if __name__ == "__main__":    load_dotenv()       # Load environment variables    init_client()       #Register your agent on Agentverse    app.run(host="0.0.0.0", port=5002)      

Save the `.env` file with your `AGENTVERSE_KEY` and `AGENT_SECRET_KEY_1`, get your `AGENTVERSE_KEY` by referring to this [guide](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key) and the AGENT\_SECRET\_KEY\_1 is a random unique key just for your AI Agent. Â 

    AGENT_SECRET_KEY_1='Your random secret seed phrase for the agent'AGENTVERSE_API_KEY='YOUR AGENTVERSE API KEY FROM AGENTVERSE'

Start the first agent as follows:

#### Expected Output[â€‹](#expected-output "Direct link to Expected Output")

    (my_venv) abhi@Fetchs-MacBook-Pro chorot 3 session % python3 my_sdk_agent_1.pyINFO:__main__:Client agent started with address: agent1q2d7qdxxpc2emmph6wze47p0g7vnfxwuzk3h6yfnaem0px0hc3acu3yzz8kINFO:fetchai:Registering with Almanac APIINFO:fetchai:Successfully registered as custom agent in AgentverseINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Quickstart agent registration complete! * Serving Flask app 'my_sdk_agent_1' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5002 * Running on http://192.168.0.105:5002INFO:werkzeug:Press CTRL+C to quit

You can verify that your agent is registered by checking in your [Agentverse Local Agents](https://agentverse.ai/agents/local) â†—ï¸.

Webhook will help you receive and handle messages, we will learn more about communication here.

### Creating our second agent to send the message[â€‹](#creating-our-second-agent-to-send-the-message "Direct link to Creating our second agent to send the message")

Open `my_second_sdk_agent.py` in your text editor and add the following code which helps you register agent and handle messages:

my\_second\_sdk\_agent.py

    from flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai import fetchfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osfrom dotenv import load_dotenv# Configure logginglogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app)# Initialising client identity to get registered on agentverseclient_identity = None agent_response = None# Function to register agentdef init_client():    """Initialize and register the client agent."""    global client_identity    try:        # Load the agent secret key from environment variables        client_identity = Identity.from_seed(os.getenv("AGENT_SECRET_KEY_2"), 0)        logger.info(f"Client agent started with address: {client_identity.address}")        readme = """            ![domain:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)            domain:domain-of-your-agent            <description>This Agent can only send a message to another agent in string format.</description>            <use_cases>                <use_case>To send a message to another agent.</use_case>            </use_cases>            <payload_requirements>            <description>This agent can only send a message in the text format.</description>            <payload>                <requirement>                    <parameter>message</parameter>                    <description>The agent can send a message to another agent.</description>                </requirement>            </payload>            </payload_requirements>        """        # Register the agent with Agentverse        register_with_agentverse(            identity=client_identity,            url="http://localhost:5005/api/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="Quickstart Agent 2",            readme=readme        )        logger.info("Quickstart agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise@app.route('/api/send-data', methods=['POST'])def send_data():   """Send payload to the selected agent based on provided address."""   global agent_response   agent_response = None   try:       # Parse the request payload       data = request.json       payload = data.get('payload')  # Extract the payload dictionary       agent_address = data.get('agentAddress')  # Extract the agent address       # Validate the input data       if not payload or not agent_address:           return jsonify({"error": "Missing payload or agent address"}), 400       logger.info(f"Sending payload to agent: {agent_address}")       logger.info(f"Payload: {payload}")       # Send the payload to the specified agent       send_message_to_agent(           client_identity,  # Frontend client identity           agent_address,    # Agent address where we have to send the data           payload           # Payload containing the data       )       return jsonify({"status": "request_sent", "agent_address": agent_address, "payload": payload})   except Exception as e:       logger.error(f"Error sending data to agent: {e}")       return jsonify({"error": str(e)}), 500if __name__ == "__main__":    load_dotenv()   # Load environment variables    init_client()   #Register your Agent on Agentverse    app.run(host="0.0.0.0", port=5005)

Start the second agent as follows.

    python my_second_agent.py

#### Expected Output[â€‹](#expected-output-1 "Direct link to Expected Output")

    (my_venv) abhi@Fetchs-MacBook-Pro chorot 3 session % python3 my_sdk_agent_2.pyINFO:__main__:Client agent started with address: agent1qvgxzj40657wh4ftk65scvqqwv7sw5lf7z5uf3mdnc8d6zmrugumu4g78wsINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Quickstart agent registration complete! * Serving Flask app 'my_sdk_agent_2' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5005 * Running on http://192.168.0.105:5005INFO:werkzeug:Press CTRL+C to quit

### Sending message from Agent2 to Agent1[â€‹](#sending-message-from-agent2-to-agent1 "Direct link to Sending message from Agent2 to Agent1")

We will use curl command to send message from agent2 to agent1. We will send a payload with Hello Message.

    curl -X POST http://localhost:5005/api/send-data \-H "Content-Type: application/json" \-d '{  "payload": {"message":"Hello This is message from agent2"},  "agentAddress": <Your agent 1 address>}'

#### Curl Command Output[â€‹](#curl-command-output "Direct link to Curl Command Output")

    {"agent_address":"agent1q2d7qdxxpc2emmph6wze47p0g7vnfxwuzk3h6yfnaem0px0hc3acu3yzz8k","payload":{"message":"Hello This is message from agent2"},"status":"request_sent"}

#### Agent2 Logs[â€‹](#agent2-logs "Direct link to Agent2 Logs")

    INFO:__main__:Sending payload to agent: agent1q2d7qdxxpc2emmph6wze47p0g7vnfxwuzk3h6yfnaem0px0hc3acu3yzz8kINFO:__main__:Payload: {'message': 'Hello This is message from agent2'}{"version":1,"sender":"agent1qvgxzj40657wh4ftk65scvqqwv7sw5lf7z5uf3mdnc8d6zmrugumu4g78ws","target":"agent1q2d7qdxxpc2emmph6wze47p0g7vnfxwuzk3h6yfnaem0px0hc3acu3yzz8k","session":"e025c919-2c6d-4e9c-b562-a879cbb35aec","schema_digest":"model:708d789bb90924328daa69a47f7a8f3483980f16a1142c24b12972a2e4174bc6","protocol_digest":"proto:a03398ea81d7aaaf67e72940937676eae0d019f8e1d8b5efbadfef9fd2e98bb2","payload":"eyJtZXNzYWdlIjoiSGVsbG8gVGhpcyBpcyBtZXNzYWdlIGZyb20gYWdlbnQyIn0=","expires":null,"nonce":null,"signature":"sig10phxfu9s9lsrm3ux4pffufw6yxs37z0ag82hhlw0mkkwap74av7cjwk86dr9nnmgpgxwhyqdmnl90um8wu77emafutfw77jdvlswg9spmc899"}INFO:fetchai:Got response looking up agent endpointhttp://localhost:5002/api/webhookINFO:fetchai:Sent message to agentINFO:werkzeug:127.0.0.1 - - [29/Jan/2025 11:17:59] "POST /api/send-data HTTP/1.1" 200 -

#### Agent1 Logs[â€‹](#agent1-logs "Direct link to Agent1 Logs")

    INFO:__main__:Received responseINFO:__main__:Processed response: {'message': 'Hello This is message from agent2'}INFO:werkzeug:127.0.0.1 - - [29/Jan/2025 11:17:59] "POST /api/webhook HTTP/1.1" 200 -

This is how messages can be sent and handled between agents.

### Parse and Send Message functions.[â€‹](#parse-and-send-message-functions "Direct link to Parse and Send Message functions.")

The `send_message_to_agent` function is used to send a message from one agent to the other, while the `parse_message_from_agent` function is used to handle incoming messages.

#### Sending a Message[â€‹](#sending-a-message "Direct link to Sending a Message")

The `send_message_to_agent` function constructs an Envelope containing the sender's identity, target agent address, message payload, and other metadata.

    from uagents_core.crypto import Identityfrom fetchai.communication import send_message_to_agentsend_message_to_agent(    sender=sender_identity,             #sender_address    target="receiver_agent_address",    #target agent address    payload={"message": "Hello, Agent!"})# payload is in {str : str} format

#### Parsing an Incoming Message[â€‹](#parsing-an-incoming-message "Direct link to Parsing an Incoming Message")

The `parse_message_from_agent` function extracts messages received from other agents by decoding the payload.

    from fetchai.communication import parse_message_from_agenttry:    message = parse_message_from_agent(data)except ValueError as e:    print(f"Error parsing message: {e}")    return# Extract sender and payload detailssender_address = message.sendermessage_payload = message.payload</content>
</page>

<page>
  <title>Autogen Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.0/other-frameworks/autogen</url>
  <content>Registering an Autogen Agent on Agentverse Marketplace
------------------------------------------------------

This document is a comprehensive guide to help users register a **Multi-Agent Autogen System** on **Fetch.ai's Agentverse** ecosystem using the provided **Autogen Code Execution Agent** and **User Agent** scripts. AutoGen is an open-source programming framework for building AI agents and facilitating cooperation among multiple agents to solve tasks. These agents work together to execute a Python code and demonstrate how Autogen Agents can interact seamlessly with other agents on Agentverse.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

### Purpose[â€‹](#purpose "Direct link to Purpose")

This project showcases:

*   **Registering** a Multi-Agent System (Autogen) on Agentverse to execute Python code.
*   **Facilitating Communication** between agents within Fetch.aiâ€™s Agentverse via webhooks.
*   **Providing** a clear example of how to return code execution results in a human-friendly format.

### Key Features[â€‹](#key-features "Direct link to Key Features")

*   The **Autogen Code Execution Agent** handles requests for code generation and execution using the Autogen tools.
*   The **User Agent** acts as a client to forward requests and retrieve responses from the Autogen Code Execution Agent.
*   Communication is done via **HTTP webhooks** registered with **Fetch.aiâ€™s Agentverse**.

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

### Environmental Setup[â€‹](#environmental-setup "Direct link to Environmental Setup")

*   Python 3.8 or higher.
*   A virtual environment (recommended).
*   Flask and related libraries installed.

### Required Libraries[â€‹](#required-libraries "Direct link to Required Libraries")

Install the necessary libraries using:

    pip install flask flask-cors fetchai autogen "flask[async]"

### Environment Variables[â€‹](#environment-variables "Direct link to Environment Variables")

Create a `.env` file in your project directory:

    AV_AUTOGEN_CODE_EXECUTION_AI_KEY='<your_autogen_code_execution_ai_key>'USER_AUTOGEN_AI_KEY='<your_user_ai_key>'AGENTVERSE_API_KEY='<your_agentverse_api_key>'OPENAI_API_KEY='<your_openai_api_key>'

Replace placeholders with your actual keys:

*   **AGENTVERSE\_API\_KEY**: Refer this [guide here](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key).
    
*   **OPENAI\_API\_KEY**: Sign up on the [OpenAI](https://platform.openai.com/api-keys) website.
    

Autogen Code Execution Agent Script[â€‹](#autogen-code-execution-agent-script "Direct link to Autogen Code Execution Agent Script")
---------------------------------------------------------------------------------------------------------------------------------

In this example, the **Autogen Code Execution Agent** registered on Agentverse comprises an **AssistantAgent** and a **UserProxyAgent** to write code and execute it. The system focuses on:

*   **Registering** with Fetch.aiâ€™s Agentverse
*   **Handling** requests to return the outcome of a Python code in a human friendly format.
*   **Responding** to the User Agent with the outcome of the code.

### Script Breakdown (autogen-agent.py)[â€‹](#script-breakdown-autogen-agentpy "Direct link to Script Breakdown (autogen-agent.py)")

**Importing Required Libraries**

    import osimport autogenfrom autogen.coding import LocalCommandLineCodeExecutorfrom flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osimport openaifrom dotenv import load_dotenvfrom fetchai import fetchimport asyncio

**Setting Up Logging**

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)

**Flask Webhook**

The Flask app is created to handle incoming webhook requests:

    flask_app = Flask(__name__)

**Code Execution Function**

Below is the code execution function using the **Autogen Multi-Agent** system, where an **AssistantAgent** and a **UserProxyAgent** communicate until the task is done.

    async def execute_code(prompt):    config_list = [{"model": "gpt-4o", "api_key": os.getenv("OPENAI_API_KEY")}]    # create an AssistantAgent named "assistant"    assistant = autogen.AssistantAgent(        name="assistant",        llm_config={            "cache_seed": 41,  # seed for caching and reproducibility            "config_list": config_list,  # a list of OpenAI API configurations            "temperature": 0,  # temperature for sampling        },  # configuration for autogen's enhanced inference API which is compatible with OpenAI API    )    # create a UserProxyAgent instance named "user_proxy"    user_proxy = autogen.UserProxyAgent(        name="user_proxy",        human_input_mode="NEVER",        max_consecutive_auto_reply=10,        is_termination_msg=lambda x: x.get("content", "").rstrip().endswith("TERMINATE"),        code_execution_config={            # the executor to run the generated code            "executor": LocalCommandLineCodeExecutor(work_dir="coding"),        },    )    # the assistant receives a message from the user_proxy, which contains the task description    chat_res = user_proxy.initiate_chat(        assistant,        message=prompt,        summary_method="reflection_with_llm",    )    if hasattr(chat_res, 'chat_history') and chat_res.chat_history:        last_message = chat_res.chat_history[-1]['content']        return last_message    return "No messages found."

**Webhook Endpoint**

    @app.route('/webhook', methods=['POST'])async def webhook():    try:        # Parse the incoming message        logging.info("Parsing message")        data = request.get_data().decode('utf-8')        message = parse_message_from_agent(data)	# Extract the prompt from the message         prompt = message.payload.get("prompt", "")        logger.info(prompt)        agent_address = message.sender	#Call the Autogen Agents to execute the code        response = await execute_code(prompt)        payload = {"response":response}	#Return the message back to the User Agent        send_message_to_agent(            sender=ai_identity,            target=agent_address,            payload=payload        )        return jsonify({"status": "graphs_sent"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"status": "error", "message": str(e)}), 500

**Registering the Agent**

    def init_agent():    """Initialize and register the client agent."""    global ai_identity    try:        ai_identity = Identity.from_seed(os.getenv("AV_AUTOGEN_CODE_EXECUTION_AI_KEY"), 0)        register_with_agentverse(            identity = ai_identity,            url = "http://localhost:5002/webhook", ## The webhook that your AI receives messages on.            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title = "Autogen Code Execution Agent",            readme = """        <description>This agent generates code and executes it to provide final output to the user.</description>        <use_cases>            <use_case>What date is today? Compare the year-to-date gain for META and TESLA.</use_case>        </use_cases>        <payload_requirements>        <description>Please provide a prompt for code execution.</description>        <payload>            <requirement>                <parameter>prompt</parameter>                <description>Please provide a prompt for code execution.The prompt will help this AI generate code and return the output to the user. </description>            </requirement>        </payload>        </payload_requirements>        """        )        logger.info("Autogen Code Execution agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise

**Running the Agent**

    if __name__ == "__main__":    init_agent()    flask_app.run(host="0.0.0.0", port=5001, debug=True)

User Agent Script[â€‹](#user-agent-script "Direct link to User Agent Script")
---------------------------------------------------------------------------

The **User Agent** script acts as the client that forwards requests to the **Autogen Code Execution Agent** and retrieves responses.

### Script Breakdown (autogen-user-agent.py)[â€‹](#script-breakdown-autogen-user-agentpy "Direct link to Script Breakdown (autogen-user-agent.py)")

**Importing Libraries**

    from flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai import fetchfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osimport timefrom dotenv import load_dotenv

**Setting Up Logging and Flask**

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app)

**Initializing the User Agent**

    def init_agent():    """Initialize and register the client agent."""    global client_identity    try:        # Load the client identity from environment variables        client_identity = Identity.from_seed(os.getenv("USER_AUTOGEN_AI_KEY"), 0)        logger.info(f"Client agent started with address: {client_identity.address}")        # Define the client agent's metadata        readme = """        <description>Frontend client that interacts with Autogen Agent to execute code.</description>        <use_cases>            <use_case>Sends Request to the AI Agent to execute a code</use_case>        </use_cases>        <payload_requirements>            <description>Expects request which is to be sent to another agent.</description>            <payload>                <requirement>                    <parameter>request</parameter>                    <description>This is the request which is to be sent to other agent</description>                </requirement>            </payload>        </payload_requirements>        """        # Register the agent with Agentverse        register_with_agentverse(            identity=client_identity,            url="http://localhost:5002/api/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="User agent for Autogen Code Execution Agent",            readme=readme        )        logger.info("Client agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise

**Searching Agents**

    # searching the agents which can create dashboard on agentverse@app.route('/api/search-agents', methods=['GET'])def search_agents():   """Search for available dashboard agents based on user input."""   try:       # Extract user input from query parameters       user_query = request.args.get('query', '')       if not user_query:           return jsonify({"error": "Query parameter 'query' is required."}), 400       # Fetch available agents based on user query       available_ais = fetch.ai(user_query)  # Pass the user query to the fetch.ai function       print(f'---------------------{available_ais}----------------------')       # Access the 'ais' list within 'agents' (assuming fetch.ai returns the correct structure)       agents = available_ais.get('ais', [])       print(f'----------------------------------{agents}------------------------------------')       extracted_data = []       for agent in agents:           name = agent.get('name')  # Extract agent name           address = agent.get('address')           # Append formatted data to extracted_data list           extracted_data.append({               'name': name,               'address': address,           })       # Format the response with indentation for readability       response = jsonify(extracted_data)       response.headers.add('Content-Type', 'application/json; charset=utf-8')       response.headers.add('Access-Control-Allow-Origin', '*')       response.headers.add('Access-Control-Allow-Headers', 'Content-Type')       return response, 200   except Exception as e:       logger.error(f"Error finding agents: {e}")       return jsonify({"error": str(e)}), 500

**Sending Data to Autogen Code Execution Agent**

    @app.route('/api/send-data', methods=['POST'])def send_data():   """Send payload to the selected agent based on provided address."""   global agent_response   agent_response = None   try:       # Parse the request payload       data = request.json       payload = data.get('payload')  # Extract the payload dictionary       agent_address = data.get('agentAddress')  # Extract the agent address       # Validate the input data       if not payload or not agent_address:           return jsonify({"error": "Missing payload or agent address"}), 400       logger.info(f"Sending payload to agent: {agent_address}")       logger.info(f"Payload: {payload}")       # Send the payload to the specified agent       send_message_to_agent(           client_identity,  # Frontend client identity           agent_address,    # Agent address where we have to send the data           payload           # Payload containing the data       )       return jsonify({"status": "request_sent", "agent_address": agent_address, "payload": payload})   except Exception as e:       logger.error(f"Error sending data to agent: {e}")       return jsonify({"error": str(e)}), 500

Steps to Run the Scripts[â€‹](#steps-to-run-the-scripts "Direct link to Steps to Run the Scripts")
------------------------------------------------------------------------------------------------

Follow these steps to get both the **Autogen Code Execution Agent** and **User Agent** scripts running.

**1\. Start the Alphavantage Agent**

Ensure the `AV_STOCKPRICE_AI_KEY` and `AGENTVERSE_API_KEY` values are correctly set in the `.env` file. Then Run:

**Expected Output:**

    (.venv) kshipra@MacBook-Pro autogen-code-generation-debugging-example % python3 autogen-agent.pyflaml.automl is not available. Please install flaml[automl] to enable AutoML functionalities.INFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Autogen Code Execution agent registration complete! * Serving Flask app 'autogen-agent' * Debug mode: onINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5001 * Running on http://192.168.6.11:5001INFO:werkzeug:Press CTRL+C to quitINFO:werkzeug: * Restarting with statflaml.automl is not available. Please install flaml[automl] to enable AutoML functionalities.INFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Autogen Code Execution agent registration complete!WARNING:werkzeug: * Debugger is active!INFO:werkzeug: * Debugger PIN: 471-951-559

The agent listens on **port 5001**.

**2\. Start the User Agent**

Ensure the `USER_AUTOGEN_AI_KEY` and `AGENTVERSE_API_KEY` values are set in the `.env` file.

    python3 autogen-user-agent.py

**Expected Output:**

    kshipra@MacBook-Pro autogen-code-generation-debugging-example % python3 autogen-user-agent.py INFO:__main__:Client agent started with address: agent1q0dfdn7073m75c3dc2sdd5gajevswmmwkw6wxgqhjca9ug6c2ruzk7nak0dINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Client agent registration complete! * Serving Flask app 'autogen-user-agent' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5002 * Running on http://192.168.6.11:5002

### Interacting with the Agents[â€‹](#interacting-with-the-agents "Direct link to Interacting with the Agents")

Once both agents are running, use the following endpoints to interact with them from another terminal:

**1\. Search for Agents**

To list agents in the Agentverse:

    curl -X GET "http://localhost:5002/api/search-agents?query=I%20want%20to%20check%20the%20stock%20price"

**Sample Output**

    [{"address":"agent1qwkzy0twyd27egqfrcw8m6vdtsz7hrm9leu7jfezzzlgckywj7shytz5kf6","name":"Alphavantage Stock Price Langchain tool"},{"address":"agent1qtnc7ruc63g3n84qczekqc6qwp0e5ayvs3v8e5pae563qrqcf852xlqz2pk","name":"Alphavantage Stock Price Langchain tool"},{"address":"agent1qdwp929xdwzm0pgxflz29c3x6ltha97t2em88vrh588nehz6su0u55zaxp9","name":"Autogen Code Execution Agent"},{"address":"agent1qgjj8476mr8cy2dvpc3ec5w4ye56kqswvr6hkl55q8775ghm9h7wwevh6lj","name":"Autogen Code Execution Agent"},{"address":"agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae","name":"Job Description Creation Agent"},{"address":"agent1qtmkpt8lhyx4u2ndzcgr7jkjuuade4ay78yld2ngaghs03z6qve3senwrsa","name":"Penny"},{"address":"agent1qtxx79e689zwjlh7m8dsfuv7lyn45d2xapr4p9w2u9axvzm487tzjfwqzdp","name":"Penny"},{"address":"agent1qv2u6v4ueea9kmvsyyclz9reh7qmzc6gr0fn7aeup93xm6ld8ynwc65nsks","name":"Penny"},{"address":"agent1qvc0em6vrlr80txaz3ksy7tspzaxl564jwqtaana7exehg0hz7gl6zg0wl0","name":"Penny"},{"address":"agent1qw5s6e7mxcd8mp6f2vvunk9rklwnv8ynft43unx77syea4nnskxyj4yk5xq","name":"Penny"}]

**2\. Send a Request to the Autogen Code Execution Agent**

Send a prompt from the User Agent to the Autogen Code Execution Agent:

    kshipra@MacBook-Pro autogen-code-generation-debugging-example % curl -X POST "http://localhost:5002/api/send-data" \-H "Content-Type: application/json" \-d '{  "payload": {    "prompt": "What date is today? Compare the year-to-date gain for META and TESLA."  },  "agentAddress": "agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae"}'

**Sample Output**

    {"status": "request_sent", "agent_address": "agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae", "payload": "{"prompt": "What date is today? Compare the year-to-date gain for META and TESLA."  }"}

**3\. Retrieve the Stock Price Response**

Fetch the stock price response from the User Agent:

    curl -X GET "http://localhost:5002/api/get-response"

**Sample Output**

    {"stock_price":"234.4000"}

Debugging Common Issues[â€‹](#debugging-common-issues "Direct link to Debugging Common Issues")
---------------------------------------------------------------------------------------------

Agent Registration Fails:

a. Check the .env file for correct API keys. b. Ensure the AGENTVERSE\_API\_KEY is valid.

404 Errors:

a. Verify that both agents are running on their respective ports (5008 and 5002 in this case). b. Double-check the agentAddress in requests.

You now have an **Autogen Code Execution Agent** integrated with the **Fetch.ai Agentverse**, orchestrating multi-agent code generation and execution. Feel free to extend the example to include additional tasks, multiple code executors, or advanced debugging logic.</content>
</page>

<page>
  <title>CrewAI Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.0/other-frameworks/crewai</url>
  <content>Creating and Registering CrewAI based Job descriptions creator
--------------------------------------------------------------

This documentation explains how to build **CrewAI** agents that generate tailored job descriptions, register them on **Fetch.aiâ€™s Agentverse**, and enable collaboration between agents for dynamic data sharing. Below, you'll see the **main.py** script containing the workflow logic `run_job_posting_workflow`, as well as sample scripts for a **Job Description Agent** and a **User Agent**.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

### Purpose[â€‹](#purpose "Direct link to Purpose")

This project showcases:

*   **Creating CrewAI agents** for complex tasks (e.g., analyzing company requirements and generating job descriptions).
*   **Integrating with Fetch.aiâ€™s Agentverse**, enabling seamless communication and data exchange.
*   **Using autonomous agents** to streamline job description creation while aligning with company culture and specific role requirements.

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

### Environment Setup[â€‹](#environment-setup "Direct link to Environment Setup")

To begin creating and registering CrewAI agents, ensure you have the following:

*   **Python 3.10+** or higher
*   A **virtual environment** (recommended)
*   **Fetch.ai SDK** for agent creation and registration
*   **CrewAI Framework** for defining tasks and workflows

### Key Components[â€‹](#key-components "Direct link to Key Components")

*   **Job Description Agent**: A specialized CrewAI agent that processes input (company domain, hiring needs, etc.) to generate a job description.
*   **User Agent**: Acts as a client to discover and interact with the Job Description Agent.
*   **Agent Collaboration**: Multiple agents communicate via Agentverse, orchestrating tasks through CrewAI workflows.

### Environment Variables[â€‹](#environment-variables "Direct link to Environment Variables")

Create a `.env` file with your API keys:

    SERPER_API_KEY=<your_serper_api_key>OPENAI_API_KEY=<your_openai_api_key>AGENTVERSE_API_KEY=<your_agentverse_api_key>

Replace the placeholders with your actual keys.

*   [OPENAI\_API\_KEY](https://openai.com/index/openai-api/)
*   [SERPER\_API\_KEY](https://serper.dev/)
*   [AGENTVERSE\_API\_KEY](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key)

Project Directory & Configuration Files[â€‹](#project-directory--configuration-files "Direct link to Project Directory & Configuration Files")
--------------------------------------------------------------------------------------------------------------------------------------------

For a clean, maintainable setup, we recommend the following structure:

    crewai_job_descriptions/â”œâ”€â”€ .envâ”œâ”€â”€ config/â”‚   â”œâ”€â”€ agents.yamlâ”‚   â””â”€â”€ tasks.yamlâ”œâ”€â”€ crew.pyâ”œâ”€â”€ jd_agent.pyâ”œâ”€â”€ main.pyâ”œâ”€â”€ requirements.txtâ”œâ”€â”€ user_agent.pyâ”œâ”€â”€ job_description_example.mdâ””â”€â”€ README.md

**1\. requirements.txt**

**Purpose**: Declares project dependencies so you (or anyone) can install them quickly with `pip install -r requirements.txt`.

Create a file called `requirements.txt` at the root of your project (alongside `jd_agent.py` and `main.py`). Example contents:

    flask==2.2.5flask-cors==3.0.10fetchai==0.16.3crewai==0.100.1crewai_tools==0.33.0python-dotenv==1.0.0

Install everything at once via:

    pip install -r requirements.txt

**2\. Crew.py**

**Purpose:** Defines your `JobPostingCrew` class, specifying the Agents and Tasks.

Place `crew.py` at the root directory. It contains the code:

    from typing import Listfrom crewai import Agent, Crew, Process, Taskfrom crewai.project import CrewBase, agent, crew, task# Check our tools documentations for more information on how to use themfrom crewai_tools import SerperDevTool, ScrapeWebsiteTool, WebsiteSearchTool, FileReadToolfrom pydantic import BaseModel, Fieldweb_search_tool = WebsiteSearchTool()seper_dev_tool = SerperDevTool()file_read_tool = FileReadTool(    file_path='./job_description_example.md',    description='A tool to read the job description example file.')class ResearchRoleRequirements(BaseModel):    """Research role requirements model"""    skills: List[str] = Field(..., description="List of recommended skills ...")    experience: List[str] = Field(..., description="List of recommended experience ...")    qualities: List[str] = Field(..., description="List of recommended qualities ...")@CrewBaseclass JobPostingCrew:    """JobPosting crew"""    agents_config = 'config/agents.yaml'    tasks_config = 'config/tasks.yaml'    @agent    def research_agent(self) -> Agent:        return Agent(            config=self.agents_config['research_agent'],            tools=[web_search_tool, seper_dev_tool],            verbose=True        )        @agent    def writer_agent(self) -> Agent:        return Agent(            config=self.agents_config['writer_agent'],            tools=[web_search_tool, seper_dev_tool, file_read_tool],            verbose=True        )        @agent    def review_agent(self) -> Agent:        return Agent(            config=self.agents_config['review_agent'],            tools=[web_search_tool, seper_dev_tool, file_read_tool],            verbose=True        )        @task    def research_company_culture_task(self) -> Task:        return Task(            config=self.tasks_config['research_company_culture_task'],            agent=self.research_agent()        )    @task    def research_role_requirements_task(self) -> Task:        return Task(            config=self.tasks_config['research_role_requirements_task'],            agent=self.research_agent(),            output_json=ResearchRoleRequirements        )    @task    def draft_job_posting_task(self) -> Task:        return Task(            config=self.tasks_config['draft_job_posting_task'],            agent=self.writer_agent()        )    @task    def review_and_edit_job_posting_task(self) -> Task:        return Task(            config=self.tasks_config['review_and_edit_job_posting_task'],            agent=self.review_agent()        )    @task    def industry_analysis_task(self) -> Task:        return Task(            config=self.tasks_config['industry_analysis_task'],            agent=self.research_agent()        )    @crew    def crew(self) -> Crew:        """Creates the JobPostingCrew"""        return Crew(            agents=self.agents,  # Automatically created by the @agent decorator            tasks=self.tasks,    # Automatically created by the @task decorator            process=Process.sequential,            verbose=True,        )

**3\. config/agents.yaml**

**Purpose**: Specifies each agentâ€™s role, goal, and backstory. Create a folder named `config/` at your project root. Inside it, **create** `agents.yaml`:

    research_agent:  role: >    Research Analyst  goal: >    Analyze the company website and provided description...  backstory: >    Expert in analyzing company cultures and identifying key values...writer_agent:  role: >    Job Description Writer  goal: >    Use insights from the Research Analyst to create...  backstory: >    Skilled in crafting compelling job descriptions...review_agent:  role: >    Review and Editing Specialist  goal: >    Review the job posting for clarity, engagement...  backstory: >    A meticulous editor ensuring every piece of content is polished...

**4\. config/tasks.yaml**

**Purpose**: Defines the text prompts (description) and expected output for each task. In the same `config/` folder, make a second file called `tasks.yaml`:

    research_company_culture_task:  description: >    Analyze {company_domain} and {company_description}...  expected_output: >    A comprehensive report detailing the company's culture...research_role_requirements_task:  description: >    Based on {hiring_needs}, list recommended skills, experience...  expected_output: >    A list of recommended skills, experiences, and qualities...draft_job_posting_task:  description: >    Draft a job posting for {hiring_needs}, using {specific_benefits}...  expected_output: >    A detailed, engaging job posting that includes an introduction...review_and_edit_job_posting_task:  description: >    Review the job posting for {hiring_needs} for clarity...  expected_output: >    A polished, error-free job posting with final approval in markdown.industry_analysis_task:  description: >    Conduct an in-depth analysis of {company_domain}'s industry...  expected_output: >    A detailed analysis highlighting major industry trends, opportunities...

Main.py (Workflow Definition)[â€‹](#mainpy-workflow-definition "Direct link to Main.py (Workflow Definition)")
------------------------------------------------------------------------------------------------------------

Below is an example `main.py` script containing the `run_job_posting_workflow` function that the Job Description Agent calls to generate job descriptions. It uses **Crew** to structure tasks and (optionally) a local SQLite database for storage or logging.

    import sysfrom crew import JobPostingCrewimport sqlite3async def run_job_posting_workflow(company_domain, company_description, hiring_needs, specific_benefits):    """    Executes the job posting workflow with proper connection handling and debugging output.        Args:        company_domain (str): The company's domain.        company_description (str): A description of the company.        hiring_needs (str): Description of the role being hired for.        specific_benefits (str): Specific benefits offered for the role.        Returns:        str: The raw output of the 'industry_analysis_task' if found, else None.    """    try:        # Initialize the JobPostingCrew        job_posting_crew = JobPostingCrew().crew()                # Define the inputs        inputs = {            'company_domain': company_domain,            'company_description': company_description,            'hiring_needs': hiring_needs,            'specific_benefits': specific_benefits,        }        # Execute the job posting process        result = job_posting_crew.kickoff(inputs=inputs)        # Debugging output        print("Result type:", type(result))        print("Attributes:", dir(result))        # Attempt to retrieve and print tasks_output        if hasattr(result, 'tasks_output') and result.tasks_output:            print("Tasks Output:")            for task_output in result.tasks_output:                print(f"Task Name: {task_output.name}")                if task_output.name == "industry_analysis_task":                    return task_output.raw            print("Task 'review_and_edit_job_posting_task' not found in tasks_output.")        else:            print("No 'tasks_output' attribute found or it's empty.")    except Exception as e:        print(f"Error occurred: {e}")        return None

The `run_job_posting_workflow` function is invoked by the Job Description Agentâ€™s webhook to handle incoming requests and generate the job description.

Job Description Agent Script[â€‹](#job-description-agent-script "Direct link to Job Description Agent Script")
------------------------------------------------------------------------------------------------------------

A Job Description Agent is a specialized CrewAI agent that creates detailed job descriptions based on:

*   **Company Domain**
*   **Company Description**
*   **Hiring Needs**
*   **Specific Benefits**

It registers itself with [**Agentverse**](https://agentverse.ai/), processes incoming requests (via a Flask webhook), and sends the generated output back to the requester.

### Script Breakdown (JD\_Agent.py)[â€‹](#script-breakdown-jd_agentpy "Direct link to Script Breakdown (JD_Agent.py)")

**Importing Required Libraries**

The script imports essential libraries for agent registration, communication, and Flask-based HTTP handling:

    import osimport loggingimport asynciofrom uagents_core.crypto import Identityfrom flask import Flask, request, jsonifyfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentfrom main import run_job_posting_workflow

**Setting Up Flask and Logging**

The script initializes Flask for handling webhooks and sets up logging to monitor activities:

    # Configure logginglogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)# Flask app for webhookflask_app = Flask(__name__)# Identity for the agentjd_agent_identity = Nonecontent = ''

**Webhook Endpoint**

The /webhook endpoint handles incoming requests and processes inputs to generate a job description:

    @flask_app.route('/webhook', methods=['POST'])async def webhook():    global content    global jd_agent_identity    try:        # Parse the incoming message        data = request.get_data().decode('utf-8')        message = parse_message_from_agent(data)        company_domain = message.payload.get("company_domain", "")        company_description = message.payload.get("company_description", "")        hiring_needs = message.payload.get("hiring_needs", "")        specific_benefits = message.payload.get("specific_benefits", "")        agent_address = message.sender        # Run the job posting workflow        content = await run_job_posting_workflow(            company_domain=company_domain,            company_description=company_description,            hiring_needs=hiring_needs,            specific_benefits=specific_benefits        )        # Send the generated content back to the requesting agent        payload = {'content': content}        send_message_to_agent(jd_agent_identity, agent_address, payload)        return jsonify({"status": "job_description_sent"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"status": "error", "message": str(e)}), 500

**Registering the Agent**

The init\_agent function registers the Job Description Agent with Fetch.ai's Agentverse, providing metadata such as agent title, description, and use cases:

    def init_agent():    global jd_agent_identity    try:        jd_agent_identity = Identity.from_seed("Job Description Agent Seed", 0)        register_with_agentverse(            identity=jd_agent_identity,            url="http://localhost:5008/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="Job Description Creation Agent",            readme="""                <description>Job Description creator to create JD according to the company's career website and description using CrewAI.</description>                <use_cases>                    <use_case>Generates job descriptions for specified roles.</use_case>                </use_cases>                <payload_requirements>                    <description>Expects company domain, description, hiring needs, and specific benefits.</description>                    <payload>                        <requirement>                            <parameter>company_domain</parameter>                            <description>Career Page for the company</description>                            <parameter>company_description</parameter>                            <description>Description of the company</description>                            <parameter>hiring_needs</parameter>                            <description>Role for which the person is needed</description>                            <parameter>specific_benefits</parameter>                            <description>Benefits offered with the role</description>                        </requirement>                    </payload>                </payload_requirements>            """        )        logger.info("Job Description Creator Agent registered successfully!")    except Exception as e:        logger.error(f"Error initializing agent: {e}")        raise

**Running the Agent**

The script starts the Flask server on port 5008:

    if __name__ == "__main__":    init_agent()    flask_app.run(host="0.0.0.0", port=5008, debug=True)

User Agent Script (user.py)[â€‹](#user-agent-script-userpy "Direct link to User Agent Script (user.py)")
------------------------------------------------------------------------------------------------------

The **User Agent** serves as a client. It registers itself with Agentverse, can search for existing agents, and forwards relevant data (e.g., company info, role) to the Job Description Agent. Afterwards, it retrieves the response and saves it locally.

### Script Breakdown[â€‹](#script-breakdown "Direct link to Script Breakdown")

**Importing Required Libraries**

The script imports libraries for Flask-based HTTP handling, Fetch.ai's identity and communication utilities, and environment variable management:

    from flask_cors import CORSfrom flask import Flask, request, jsonifyfrom uagents_core.crypto import Identityfrom fetchai import fetchfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osfrom dotenv import load_dotenv

**Setting Up Flask and Logging**

Logging and Flask are initialised to handle web requests and log agent activities:

    # Configure logginglogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app)# Global variables for client identity and agent responseclient_identity = Noneagent_response = None

**Initialising the User Agent**

The init\_client function registers the User Agent with Fetch.ai's Agentverse, providing metadata such as the agent's purpose and use cases:

    def init_client():    """Initialize and register the client agent."""    global client_identity    try:        # Load the client identity from environment variables        client_identity = Identity.from_seed("User for Testing JD Agents", 0)        logger.info(f"Client agent started with address: {client_identity.address}")        # Register the agent with Agentverse        register_with_agentverse(            identity=client_identity,            url="http://localhost:5002/api/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="JD User Testing Agent",            readme="""                <description>Frontend client that interacts with JD agents to fetch job descriptions.</description>                <use_cases>                    <use_case>Searches for agents and interacts with them.</use_case>                </use_cases>                <payload_requirements>                    <description>Handles responses related to job descriptions.</description>                    <payload>                        <requirement>                            <parameter>field</parameter>                            <description>Data required for job description creation</description>                        </requirement>                    </payload>                </payload_requirements>            """        )        logger.info("Client agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise

**Searching for Agents**

The /api/search-agents endpoint allows the User Agent to search for agents in Fetch.ai's Agentverse/Almanac Contract based on a query:

    @app.route('/api/search-agents', methods=['GET'])def search_agents():    """Search for available agents based on user input."""    try:        # Extract the query from the request        user_query = request.args.get('query', '')        if not user_query:            return jsonify({"error": "Query parameter 'query' is required."}), 400        # Search agents in Agentverse/almanac contract        available_ais = fetch.ai(user_query)        agents = available_ais.get('ais', [])        # Format the agent data        extracted_data = [            {'name': agent.get('name'), 'address': agent.get('address')}            for agent in agents        ]        return jsonify(extracted_data), 200    except Exception as e:        logger.error(f"Error finding agents: {e}")        return jsonify({"error": str(e)}), 500

**Sending Data to Another Agent**

The /api/send-data endpoint forwards input data (such as company details and hiring needs) to the selected Job Description Agent:

    @app.route('/api/send-data', methods=['POST'])def send_data():    """Send payload to the selected agent based on the provided address."""    global agent_response    agent_response = None    try:        # Parse the request payload        data = request.json        payload = data.get('payload')  # Extract the payload dictionary        agent_address = data.get('agentAddress')  # Extract the agent address        # Validate input data        if not payload or not agent_address:            return jsonify({"error": "Missing payload or agent address"}), 400        # Send the payload to the agent        send_message_to_agent(client_identity, agent_address, payload)        logger.info(f"Payload sent to agent: {agent_address}")        return jsonify({"status": "request_sent", "agent_address": agent_address, "payload": payload})    except Exception as e:        logger.error(f"Error sending data to agent: {e}")        return jsonify({"error": str(e)}), 500

**Retrieving the Agent's Response**

The /api/get-response endpoint retrieves the response (job description) from the Job Description Agent and saves it as a markdown file:

    @app.route('/api/get-response', methods=['GET'])def get_response():    """Fetch the response from the agent and save it to a file."""    global agent_response    try:        if agent_response:            response = agent_response            agent_response = None  # Clear the response after fetching            # Save the response to a markdown file            file_path = os.path.join(os.getcwd(), "jd_response.md")            with open(file_path, "w", encoding="utf-8") as md_file:                md_file.write(response.get("content", ""))            logger.info(f"Markdown file created at {file_path}")            return jsonify({"status": "file_created", "file_path": file_path})        else:            return jsonify({"error": "No response available"}), 404    except Exception as e:        logger.error(f"Error creating markdown file: {e}")        return jsonify({"error": str(e)}), 500

**Webhook Endpoint**

The webhook endpoint processes messages sent by the Job Description Agent and stores the response globally:

    @app.route('/api/webhook', methods=['POST'])def webhook():    """Handle incoming messages from the Job Description Agent."""    global agent_response    try:        # Parse the incoming message        data = request.get_data().decode("utf-8")        logger.info("Received response")        message = parse_message_from_agent(data)        agent_response = message.payload        logger.info(f"Processed response: {agent_response}")        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500

**Running the User Agent**

The Flask server runs on port 5002 to handle requests:

    def start_server():    """Start the Flask server."""    try:        # Load environment variables        load_dotenv()        init_client()        app.run(host="0.0.0.0", port=5002)    except Exception as e:        logger.error(f"Server error: {e}")        raiseif __name__ == "__main__":    start_server()

Step-by-Step Execution[â€‹](#step-by-step-execution "Direct link to Step-by-Step Execution")
------------------------------------------------------------------------------------------

**1\. Environment Variables**

Update '.env' with `SERPER_API_KEY`, `OPENAI_API_KEY`, and `AGENTVERSE_API_KEY`.

**2\. Start the Job Description Agent**

*   Registers the agent on Agentverse.
*   Launches JD Agent with Flask on port 5008.

**3\. Start the User Agent**

*   Registers the agent on Agentverse.
*   Launches Flask on port 5002.

### Interacting with the Agents[â€‹](#interacting-with-the-agents "Direct link to Interacting with the Agents")

**Search for Agents**

To list agents in the Agentverse:

    curl -X GET "http://localhost:5002/api/search-agents?query=I%20want%20to%20write%20job%20description"

**Sample Output**

    [    {        "name": "Job Description Creation Agent",        "address": "agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae"    },    ...]

**Send Data to the Job Description Agent**

Send JD details from the User Agent to the JD Agent:

    curl -X POST "http://localhost:5002/api/send-data" \-H "Content-Type: application/json" \-d '{    "payload": {        "company_domain": "careers.wbd.com",        "company_description": "Warner Bros. Discovery is a premier global media and entertainment company, offering audiences the world\u2019s most differentiated and complete portfolio of content, brands and franchises across television, film, sports, news, streaming and gaming. We\u2019re home to the world\u2019s best storytellers, creating world-class products for consumers.",        "hiring_needs": "Production Assistant, for a TV production set in Los Angeles in June 2025",        "specific_benefits": "Weekly Pay, Employee Meals, healthcare"    },    "agentAddress": "agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae"}'

**Sample Output**

    {    "status": "request_sent",    "agent_address": "agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae",    "payload": {        "company_domain": "careers.wbd.com",        "company_description": "Warner Bros. Discovery is a premier global media and entertainment company...",        "hiring_needs": "Production Assistant, for a TV production set in Los Angeles in June 2025",        "specific_benefits": "Weekly Pay, Employee Meals, healthcare"    }}

**Retrieve the Agentâ€™s Response**

Fetch the JD document response from the User Agent:

    curl -X GET "http://localhost:5002/api/get-response"

**Sample Output**

*   Stores the Job Description result in `jd_response.md`.

    {    "status": "file_created",    "file_path": "/path/to/jd_response.md"}

You can open the [jd\_response.md](https://drive.google.com/file/d/12oHIKeOIAjVs8dxP-xg9SmtWR4JnTbqx/view?usp=sharing) file to review the generated job description.

You now have **CrewAI agents** integrated with the **Fetch.ai Agentverse**, collaborating to create custom job descriptions. Feel free to adapt the main.py workflow, agent scripts, or user scripts to suit your own requirementsâ€”whether youâ€™re refining AI logic, adding advanced search filters, or customizing the final job posting format.</content>
</page>

<page>
  <title>LangGraph | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.0/other-frameworks/financial-analysis-ai-agent</url>
  <content>Financial Analysis AI Agent
---------------------------

Part 1: Core Systems and Architecture[â€‹](#part-1-core-systems-and-architecture "Direct link to Part 1: Core Systems and Architecture")
--------------------------------------------------------------------------------------------------------------------------------------

### System Overview[â€‹](#system-overview "Direct link to System Overview")

The Financial Analysis Agent is an intelligent system designed to analyze financial information using multiple specialized agents that communicate with microservice tools (uAgents) through Agentverse. The system combines SEC filing analysis with real-time market data to provide comprehensive financial insights.

### Architecture Diagram[â€‹](#architecture-diagram "Direct link to Architecture Diagram")

### Implementation Details[â€‹](#implementation-details "Direct link to Implementation Details")

#### Core Components[â€‹](#core-components "Direct link to Core Components")

##### Research Team State[â€‹](#research-team-state "Direct link to Research Team State")

    class ResearchTeamState(TypedDict):    messages: List[BaseMessage] # Conversation history    team_members: List[str] # Active team members    next: str # Next agent to act    information_needed: List[str] # Required information    reasoning: str # Decision reasoning

##### Specialized Agents[â€‹](#specialized-agents "Direct link to Specialized Agents")

**1\. SEC Analysis Agent**

*   Purpose: Analyzes SEC filings and financial documents
*   Key Features:
    *   Communicates with RAG Tool uAgent through Agentverse
    *   Focuses on historical financial data
    *   Processes regulatory filings
*   Implementation:

    class SECAnalyst:    def __init__(self):        # Hardcoded address of RAG Tool uAgent        self.rag_tool_address ="agent1qdaeq9k7ty2xjp5ylpex0ezxzlg30cc8n3lpvrgh4sqjm863hm0vusghkzu"    async def analyze_filings(self, query):        try:            send_message_to_agent(            financial_identity,            self.rag_tool_address,            {"message": query}        )        return {"status": "filing_analysis_requested"}    except Exception as e:        logger.error(f"Error in filing analysis: {e}")        return {"error": str(e)}

**2\. Search Agent**

*   Purpose: Gathers real-time market information
    
*   Key Features:
    
    *   Communicates with Search Tool uAgent through Agentverse
    *   Focuses on current market data
*   Retrieves analyst opinions
    
*   Implementation:
    

    class SearchAgent:    def __init__(self):    # Hardcoded address of Search Tool uAgent        self.search_tool_address = "agent1qgyqf7l0djgvnd37f4fwvrv8xu7he2eatht328u05uy3vc9j33p7qe2dxtr"    async def analyze_market(self, query):        try:            send_message_to_agent(                financial_identity,                self.search_tool_address,                {"message": query}            )            return {"status": "market_analysis_requested"}        except Exception as e:            logger.error(f"Error in market analysis: {e}")            return {"error": str(e)}

**3\. Supervisor Agent**

*   Purpose: Coordinates analysis flow
*   Key Features:
    *   Routes queries to appropriate specialists
    *   Aggregates responses
    *   Manages analysis flow
*   Implementation managed through research graph

##### Message Models[â€‹](#message-models "Direct link to Message Models")

    # Shared between AI Agents and uAgentsclass Request(Model):    message: strclass Response(Model):    response: str

#### Communication Flow[â€‹](#communication-flow "Direct link to Communication Flow")

**1\. Query Flow**

*   User submits query
*   Supervisor analyzes and routes
*   Specialist agents communicate with uAgents
*   Results aggregated and returned

**2\. Message Protocol**

    # Sending messages to uAgentssend_message_to_agent(    sender_identity, # Financial agent identity    tool_address, # uAgent address    payload, # Message payload    )# Receiving messages (webhook)@app.route('/webhook', methods=['POST'])def webhook():    data = request.get_data().decode("utf-8")    message = parse_message_from_agent(data)    return jsonify({"status": "success"})

### Search Tool uAgent[â€‹](#search-tool-uagent "Direct link to Search Tool uAgent")

##### Basic Implementation[â€‹](#basic-implementation "Direct link to Basic Implementation")

    from uagents import Agent, Context, Modelfrom uagents.setup import fund_agent_if_lowfrom tavily import Client# Message Modelsclass Request(Model):    message: strclass Response(Model):    response: str# Agent Setupsearch_agent = Agent(    name="Market Search Tool",    port=8000,    seed="SEARCH_TOOL_SEED",    endpoint=["http://localhost:8000/submit"]    )# Startup Handler@search_agent.on_event("startup")async def startup_handler(ctx: Context):await fund_agent_if_low(ctx.ledger)    ctx.logger.info(f"Search Tool Agent started at address: {ctx.address}")# Message Handler@search_agent.on_message(model=Request)async def handle_search(ctx: Context, sender: str, msg: Request):"""Handle market research requests"""    try:        # Initialize Tavily client        tavily_client = Client(api_key=os.getenv("TAVILY_API_KEY"))        # Perform search        search_result = tavily_client.search(msg.message)        # Send response        await ctx.send(            sender,            Response(response=str(search_result))        )    except Exception as e:        ctx.logger.error(f"Search error: {e}")        await ctx.send(            sender,            Response(response=f"Error performing search: {str(e)}")        )# Run Agentif __name__ == "__main__":    search_agent.run()

### RAG Tool uAgent[â€‹](#rag-tool-uagent "Direct link to RAG Tool uAgent")

##### Basic Implementation[â€‹](#basic-implementation-1 "Direct link to Basic Implementation")

    from uagents import Agent, Context, Modelfrom uagents.setup import fund_agent_if_lowg# Message Modelsclass Request(Model):    message: strclass Response(Model):    response: str# Agent Setuprag_agent = Agent(    name="RAG Analysis Tool",    port=8001,    seed="RAG_TOOL_SEED",    endpoint=["http://localhost:8001/submit"]    )# Startup Handler@rag_agent.on_event("startup")async def startup_handler(ctx: Context):    await fund_agent_if_low(ctx.ledger)    ctx.logger.info(f"RAG Tool Agent started at address: {ctx.address}")# Message Handler@rag_agent.on_message(model=Request)async def handle_analysis(ctx: Context, sender: str, msg: Request):    """Handle document analysis requests"""    try:    # Initialize RAG system        loader = DocumentLoader("data/raw/apple_10k.pdf")        rag_chain = create_rag_chain(loader)    # Perform analysis        analysis_result = rag_chain.invoke(msg.message)    # Send response        await ctx.send(            sender,            Response(response=str(analysis_result))            )    except Exception as e:        ctx.logger.error(f"RAG analysis error: {e}")        await ctx.send(            sender,            Response(response=f"Error performing analysis: {str(e)}")        )    # Run Agentif __name__ == "__main__":    rag_agent.run()

### Tool Agent Setup and Configuration[â€‹](#tool-agent-setup-and-configuration "Direct link to Tool Agent Setup and Configuration")

##### Environment Setup[â€‹](#environment-setup "Direct link to Environment Setup")

    # Required Environment VariablesTAVILY_API_KEY=your_tavily_api_keySEARCH_TOOL_SEED=your_search_tool_seedRAG_TOOL_SEED=your_rag_tool_seed

##### Tool Deployment Script[â€‹](#tool-deployment-script "Direct link to Tool Deployment Script")

    # tools_setup.pyimport osfrom dotenv import load_dotenvdef setup_tools():    """Setup and validate tool environments"""    load_dotenv()    # Validate required environment variables    required_vars = [        "TAVILY_API_KEY",        "SEARCH_TOOL_SEED",        "RAG_TOOL_SEED"        ]    for var in required_vars:        if not os.getenv(var):        raise EnvironmentError(f"Missing required environment variable:{var}")    # Additional setup if neededif __name__ == "__main__":    setup_tools()

### Error Handling and Logging[â€‹](#error-handling-and-logging "Direct link to Error Handling and Logging")

##### Error Response Format[â€‹](#error-response-format "Direct link to Error Response Format")

    async def send_error_response(ctx: Context, sender: str, error: Exception):    """Standardized error response format"""    error_response = {        "status": "error",        "error_type": error.__class__.__name__,        "message": str(error),        "timestamp": ctx.current_timestamp        }    await ctx.send(        sender,        Response(response=str(error_response))        )

#### Logging Configuration[â€‹](#logging-configuration "Direct link to Logging Configuration")

    import loggingdef setup_tool_logging(tool_name: str):    """Configure logging for tool agents"""    logging.basicConfig(        level=logging.INFO,        format=f'%(asctime)s - {tool_name} - %(levelname)s - %(message)s',        handlers=[logging.FileHandler(f"{tool_name.lower()}.log"),logging.StreamHandler()        ]    )

Part 3: Helper Functions Implementation[â€‹](#part-3-helper-functions-implementation "Direct link to Part 3: Helper Functions Implementation")
--------------------------------------------------------------------------------------------------------------------------------------------

### State Management System[â€‹](#state-management-system "Direct link to State Management System")

##### What is State?[â€‹](#what-is-state "Direct link to What is State?")

In our system, state is like the brain's working memory. It keeps track of:

*   The entire conversation history
*   Which agent is currently working
*   What information we still need
*   Why we're making certain decisions

Think of it like a group project manager that:

*   Remembers what everyone has said
*   Knows who should work next
*   Keeps track of what information is still missing
*   Understands why certain team members should handle specific tasks

##### State Structure Explained[â€‹](#state-structure-explained "Direct link to State Structure Explained")

    class ResearchTeamState(TypedDict):    messages: List[BaseMessage]    team_members: List[str]    next: str    information_needed: List[str]    reasoning: str

Each component serves a specific purpose:

**1\. messages:**

*   Works like a conversation transcript
*   Stores every interaction in order
*   Helps agents understand context
*   **Example:** If user asks about "these numbers", agents can look back to see what numbers were discussed

**2\. team\_members:**

*   Like a team roster
*   Lists available specialized agents (Search, SECAnalyst)
*   Helps supervisor know who can be assigned tasks
*   **Example:** \["Search", "SECAnalyst"\]

**3\. next:**

*   Like a "passing the baton" system
*   Tells us which agent should act next
*   Can also signal when we're done ("FINISH")
*   **Example:** If we need market data, next = "Search"

**4\. information\_needed:**

*   Like a shopping list of missing information
*   Helps agents know what to look for
*   Guides the conversation towards completeness
*   **Example:** \["current stock price", "last quarter revenue"\]

**5\. reasoning:**

*   Like notes explaining decisions
*   Helps understand why certain agents were chosen
*   Useful for debugging and improvement
*   **Example:** "Choosing SECAnalyst because we need historical financial data"

### Helper Functions Explained[â€‹](#helper-functions-explained "Direct link to Helper Functions Explained")

##### A. agent\_node Helper[â€‹](#a-agent_node-helper "Direct link to A. agent_node Helper")

This is like a universal translator and task manager for agents. It:

**1\. Prepares Information:**

*   Takes the current state
*   Adds any needed information to the query
*   Makes sure agent has context

**2\. Manages Communication:**

*   Standardizes how agents talk to each other.
*   Maintains consistent message format.
*   Keeps conversation history organized

    def agent_node(state, agent, name):    """Helper function to create agent nodes."""    try:        # Add information needed to the state if available        if "information_needed" in state:        message_content = f"""Information needed:        {', '.join(state['information_needed'])}        Query: {state['messages'][-1].content}"""        state['messages'][-1] = HumanMessage(content=message_content)        # Process through agent        result = agent.invoke(state)        # Format response        return {            "messages": [                HumanMessage(                    content=result["output"],                    name=name    ) ]    }    except Exception as e:        logger.error(f"Error in agent node {name}: {e}")        raise

##### B. create\_agent Helper[â€‹](#b-create_agent-helper "Direct link to B. create_agent Helper")

This is like a specialized agent creator that:

**1\. Sets Up Agent Capabilities:**

*   Gives agents their special tools
*   Defines their area of expertise
*   Sets their behavioral guidelines

**2\. Configures Communication Style:**

*   Sets up how agent should communicate with tools
*   Defines interaction patterns with uAgents
*   Establishes message formats

    def create_agent(    llm: ChatOpenAI,    tools: list,    system_prompt: str,) -> AgentExecutor:    """Create a function-calling agent and add it to the graph."""    try:    # Enhance system prompt with team context        system_prompt += (            """Work autonomously according to your specialty, using the tools available to you."            Do not ask for clarification.            Your other team members will collaborate with you with their own specialties.            You are chosen for a reason! You are one of the following team members: {team_members}.""")        # Create prompt template        prompt = ChatPromptTemplate.from_messages([            ("system", system_prompt),            MessagesPlaceholder(variable_name="messages"),            MessagesPlaceholder(variable_name="agent_scratchpad"),])        # Create and return agent        agent = create_openai_functions_agent(llm, tools, prompt)        executor = AgentExecutor(agent=agent, tools=tools)        return executor    except Exception as e:        logger.error(f"Error creating agent: {e}")        raise

##### C. create\_team\_supervisor Helper[â€‹](#c-create_team_supervisor-helper "Direct link to C. create_team_supervisor Helper")

This helper creates the supervisor agent that coordinates the team:

    def create_team_supervisor(    llm: ChatOpenAI,    system_prompt: str,    members: List[str]) -> Callable:    """Create an LLM-based supervisor with enhanced reasoning."""    # Define possible routing options    options = ["FINISH"] + members    # Define routing function schema    function_def = {        "name": "route",        "description": "Select the next role based on query analysis.",        "parameters": {            "title": "routeSchema",            "type": "object",            "properties": {                        "next": {                            "title": "Next",                            "anyOf": [{"enum": options}],                        },                        "reasoning": {                            "title": "Reasoning",                            "type": "string",                            "description": "Explanation for why this agent should                        },                        "information_needed": {                            "title": "Information Needed",                            "type": "array",                            "items": {"type": "string"},                            "description": "List of specific information needed from        } },                    "required": ["next", "reasoning", "information_needed"],                },        }            # Create prompt template            prompt = ChatPromptTemplate.from_messages([                ("system", system_prompt),                MessagesPlaceholder(variable_name="messages"),                (                    "system",                    """Given the conversation above, who should act next? Consider:                    1. What information do we have?                    2. What's still missing?                    3. Which agent can best provide the missing information?                    Select from: {options}"""                ),            ]).partial(options=str(options), team_members=", ".join(members))            # Create and return supervisor            return (                prompt | llm.bind_functions(functions=[function_def],function_call="route") | JsonOutputFunctionsParser()        )

Why these helpers are important:

*   Standardize agent creation and communication
*   Ensure consistent behavior across the system
*   Make adding new agents and modifying behavior easier Maintain clean separation of concerns
*   Enable structured team coordination

These helper functions form the backbone of our agent system, enabling:

*   Standardized agent creation
*   Consistent communication patterns
*   Proper state management
*   Team coordination
*   Easy system expansion

### Research Graph System[â€‹](#research-graph-system "Direct link to Research Graph System")

The research graph is like a traffic control system that:

**1\. Manages Flow:**

*   Directs queries to right agents
*   Handles responses
*   Coordinates between agents

**2\. Controls Transitions:**

*   Decides when to switch agents
*   Determines when task is complete
*   Manages information flow

Example Flow:

    User Query â†’ Supervisor â†’ Search Agent â†’ Supervisor â†’ SEC Agent â†’ Supervisor â†’ Response

Why it matters:

*   Ensures orderly process
*   Prevents chaos in communication
*   Maintains system organization

### Practical Example[â€‹](#practical-example "Direct link to Practical Example")

Let's say user asks: "How is Apple's revenue trending?"

**1\. State Tracks:**

*   The original question
*   That Search agent checked market data
*   That SEC agent checked financial filings
*   What each agent found
*   Which information is still needed

**2\. Helpers Manage:**

*   Getting market data from Search agent
*   Getting filing data from SEC agent Combining information
*   Formatting response

**3\. Graph Controls:**

*   First getting market trends
*   Then getting SEC filing data
*   Finally combining for complete answer

This system ensures:

*   No information is lost
*   All necessary sources are checked
*   Response is complete and accurate

Part 4: System Integration and Setup[â€‹](#part-4-system-integration-and-setup "Direct link to Part 4: System Integration and Setup")
-----------------------------------------------------------------------------------------------------------------------------------

### System Components Integration[â€‹](#system-components-integration "Direct link to System Components Integration")

**A. Project Structure**

    src/â”œâ”€â”€ agents/                     # AI Agent Componentsâ”‚   â”œâ”€â”€ __init__.pyâ”‚   â”œâ”€â”€ search_agent.py         # Search specialist using Search uAgentâ”‚   â”œâ”€â”€ sec_agent.py            # SEC specialist using RAG uAgentâ”‚   â””â”€â”€ supervisor.py           # Team coordinatorâ”œâ”€â”€ agentverse/â”‚   â”œâ”€â”€ __init__.pyâ”‚   â””â”€â”€ register.py              # Financial Agent registration with Agentverseâ”œâ”€â”€ graph/â”‚   â”œâ”€â”€ __init__.pyâ”‚   â””â”€â”€ state.py                # Research team state managementâ”œâ”€â”€ tool_agents/â”‚   â”œâ”€â”€ __init__.pyâ”‚   â”œâ”€â”€ search_tool/            # uAgent Tools (New)â”‚   â”‚â”‚   â”‚â”‚   â”‚â”‚   â””â”€â”€ rag_tool/â”‚       â”œâ”€â”€ __init__.pyâ”‚       â”œâ”€â”€ agent.py            # RAG Tool uAgentâ”‚       â””â”€â”€ rag_service.pyâ””â”€â”€ utils/    â”œâ”€â”€ __init__.py    â””â”€â”€ helpers.py              # Helper functions for agents

**B. Environment Configuration**

    # .env file# AI Agent ConfigurationFINANCIAL_AGENT_KEY="your_financial_agent_seed"AGENTVERSE_API_KEY="your_agentverse_api_key"# Tool uAgents ConfigurationSEARCH_TOOL_SEED="your_search_tool_seed"RAG_TOOL_SEED="your_rag_tool_seed"# External ServicesTAVILY_API_KEY="your_tavily_api_key"# Server ConfigurationAI_AGENT_PORT=5008SEARCH_TOOL_PORT=8000RAG_TOOL_PORT=8001

### Component Startup Sequence[â€‹](#component-startup-sequence "Direct link to Component Startup Sequence")

**A. Tool uAgents Startup**

    # tools/startup.pyfrom search_tool.agent import search_agentfrom rag_tool.agent import rag_agentimport asyncioasync def start_tools():    """Start all tool uAgents"""    try:        # Start search tool        search_task = asyncio.create_task(search_agent.run())        print(f"Search Tool started at address: {search_agent.address}")        # Start RAG tool        rag_task = asyncio.create_task(rag_agent.run())        print(f"RAG Tool started at address: {rag_agent.address}")        # Wait for both agents        await asyncio.gather(search_task, rag_task)    except Exception as e:        print(f"Error starting tools: {e}")        raiseif __name__ == "__main__":    asyncio.run(start_tools())

**B. Financial Analysis Agent Startup**

    # ai_agent/main.pyfrom dotenv import load_dotenvfrom specialist_agents import SearchAgent, SECAnalystfrom utils.helpers import init_agentdef start_financial_agent():    """Initialize and start the Financial Analysis Agent"""    try:        # Load environment variables        load_dotenv()        # Initialize agent        init_agent()        # Start Flask server        app.run(            host="0.0.0.0",            port=int(os.getenv("AI_AGENT_PORT", 5008))        )    except Exception as e:        print(f"Error starting Financial Agent: {e}")        raiseif __name__ == "__main__":    start_financial_agent()

### Communication Setup[â€‹](#communication-setup "Direct link to Communication Setup")

**A. Tool Address Configuration**

    # ai_agent/config/addresses.pyfrom typing import NamedTupleclass ToolAddresses(NamedTuple):    search_tool: str ="agent1qgyqf7l0djgvnd37f4fwvrv8xu7he2eatht328u05uy3vc9j33p7qe2dxtr"    rag_tool: str ="agent1qdaeq9k7ty2xjp5ylpex0ezxzlg30cc8n3lpvrgh4sqjm863hm0vusghkzu"# Usage in specialist agentsfrom config.addresses import ToolAddressesclass SearchAgent:    def __init__(self):        self.tool_address = ToolAddresses.search_tool

**B. Message Handling Setup**

    # ai_agent/utils/messaging.pyfrom fetchai.communication import send_message_to_agent,parse_message_from_agentasync def send_to_tool(identity, tool_address: str, message: str):    """Send message to tool uAgent"""    try:        send_message_to_agent(            identity,            tool_address,            {"message": message}        )        return {"status": "message_sent"}    except Exception as e:        logger.error(f"Error sending message: {e}")        return {"error": str(e)}# Webhook handler@app.route('/webhook', methods=['POST'])def webhook():    """Handle incoming messages from tools"""    try:        data = request.get_data().decode("utf-8")        message = parse_message_from_agent(data)        # Process response        process_tool_response(message.payload)        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Webhook error: {e}")        return jsonify({"error": str(e)}), 500</content>
</page>

<page>
  <title>LangGraph Adapter Example | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/examples/adapters/langgraph-adapter-example</url>
  <content>LangGraph Adapter for uAgents
-----------------------------

This example demonstrates how to integrate a **LangGraph agent** with the **uAgents ecosystem** using the uAgents Adapter package. LangGraph provides powerful orchestration capabilities for LLM applications through directed graphs.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

The LangGraph adapter allows you to:

*   Wrap LangGraph agents as uAgents for seamless communication
*   Enable LangGraph agents to participate in the Agentverse ecosystem
*   Leverage advanced orchestration for complex reasoning while maintaining uAgent compatibility

Example Implementation[â€‹](#example-implementation "Direct link to Example Implementation")
------------------------------------------------------------------------------------------

*   Create an agent with file name `agent.py`

agent.py

    import osimport timefrom dotenv import load_dotenvfrom langchain_openai import ChatOpenAIfrom langchain_community.tools.tavily_search import TavilySearchResultsfrom langgraph.prebuilt import chat_agent_executorfrom langchain_core.messages import HumanMessagefrom uagents_adapter import LangchainRegisterTool, cleanup_uagent# Load environment variablesload_dotenv()# Set your API keys - for production, use environment variables instead of hardcodingOPENAI_API_KEY = os.environ["OPENAI_API_KEY"]TAVILY_API_KEY = os.environ["TAVILY_API_KEY"]# Get API token for AgentverseAPI_TOKEN = os.environ["AGENTVERSE_API_KEY"]if not API_TOKEN:    raise ValueError("Please set AGENTVERSE_API_KEY environment variable")# Set up tools and LLMtools = [TavilySearchResults(max_results=3)]model = ChatOpenAI(temperature=0)# LangGraph-based executorapp = chat_agent_executor.create_tool_calling_executor(model, tools)# Wrap LangGraph agent into a function for UAgentdef langgraph_agent_func(query):    # Handle input if it's a dict with 'input' key    if isinstance(query, dict) and 'input' in query:        query = query['input']        messages = {"messages": [HumanMessage(content=query)]}    final = None    for output in app.stream(messages):        final = list(output.values())[0]  # Get latest    return final["messages"][-1].content if final else "No response"# Register the LangGraph agent via uAgenttool = LangchainRegisterTool()agent_info = tool.invoke(    {        "agent_obj": langgraph_agent_func,        "name": "langgraph_tavily_agent",        "port": 8080,        "description": "A LangGraph-based Tavily-powered search agent",        "api_token": API_TOKEN,        "mailbox": True    })print(f"âœ… Registered LangGraph agent: {agent_info}")# Keep the agent alivetry:    while True:        time.sleep(1)except KeyboardInterrupt:    print("ğŸ›‘ Shutting down LangGraph agent...")    cleanup_uagent("langgraph_tavily_agent")    print("âœ… Agent stopped.")

Key Components[â€‹](#key-components "Direct link to Key Components")
------------------------------------------------------------------

1.  **LangGraph Setup**:
    
    *   Creates a tool-calling executor using LangGraph's prebuilt components
    *   Configures Tavily search as the tool for retrieving information
    *   Uses OpenAI's ChatGPT for LLM capabilities
2.  **Function Wrapper**:
    
    *   Wraps the LangGraph app in a function that accepts queries
    *   Handles input format conversion
    *   Processes streaming output from LangGraph
3.  **uAgent Registration**:
    
    *   Uses UAgentRegisterTool to register the LangGraph function as a uAgent
    *   Configures a port, description, and mailbox for persistence
    *   Generates a unique address for agent communication

Sample requirements.txt[â€‹](#sample-requirementstxt "Direct link to Sample requirements.txt")
--------------------------------------------------------------------------------------------

Here's a sample `requirements.txt` file you can use for this example:

    uagents==0.22.3uagents-adapter==0.4.1langchain-openai==0.3.14langchain-community==0.3.21langgraph==0.3.31dotenv==0.9.9

Interacting with the Agent[â€‹](#interacting-with-the-agent "Direct link to Interacting with the Agent")
------------------------------------------------------------------------------------------------------

You can interact with this LangGraph agent through any uAgent using the chat protocol **or directly via the "Chat with Agent" button on its Agentverse profile**. Here's a client implementation:

agent\_client.py

    from datetime import datetimefrom uuid import uuid4from uagents import Agent, Protocol, Context#import the necessary components from the chat protocolfrom uagents_core.contrib.protocols.chat import (    ChatAcknowledgement,    ChatMessage,    TextContent,    chat_protocol_spec,)# Initialise agent2agent2 = Agent(name="client_agent",               port = 8082,               mailbox=True,               seed="client agent testing seed"               )# Initialize the chat protocolchat_proto = Protocol(spec=chat_protocol_spec)langgraph_agent_address = "agent1q0zyxrneyaury3f5c7aj67hfa5w65cykzplxkst5f5mnyf4y3em3kplxn4t" # Update with your Langgraph Agent's address# Startup Handler - Print agent details@agent2.on_event("startup")async def startup_handler(ctx: Context):    # Print agent details    ctx.logger.info(f"My name is {ctx.agent.name} and my address is {ctx.agent.address}")    # Send initial message to agent2    initial_message = ChatMessage(        timestamp=datetime.utcnow(),        msg_id=uuid4(),        content=[TextContent(type="text", text="I want to send query to tavily agent that Give me a list of latest agentic AI trends")]    )    await ctx.send(langgraph_agent_address, initial_message)# Message Handler - Process received messages and send acknowledgements@chat_proto.on_message(ChatMessage)async def handle_message(ctx: Context, sender: str, msg: ChatMessage):    for item in msg.content:        if isinstance(item, TextContent):            # Log received message            ctx.logger.info(f"Received message from {sender}: {item.text}")                        # Send acknowledgment            ack = ChatAcknowledgement(                timestamp=datetime.utcnow(),                acknowledged_msg_id=msg.msg_id            )            await ctx.send(sender, ack)            # Acknowledgement Handler - Process received acknowledgements@chat_proto.on_message(ChatAcknowledgement)async def handle_acknowledgement(ctx: Context, sender: str, msg: ChatAcknowledgement):    ctx.logger.info(f"Received acknowledgement from {sender} for message: {msg.acknowledged_msg_id}")# Include the protocol in the agent to enable the chat functionality# This allows the agent to send/receive messages and handle acknowledgements using the chat protocolagent2.include(chat_proto, publish_manifest=True)if __name__ == '__main__':    agent2.run()

### ğŸ—¨ï¸ Chatting via Agentverse UI[â€‹](#chat-with-agent-ui "Direct link to ğŸ—¨ï¸ Chatting via Agentverse UI")

If you prefer a graphical interface, follow these steps to talk to your LangGraph agent directly from the Agentverse dashboard:

1.  Open the Agentverse Studio and navigate to **Agents â†’ Local** (or search for your deployed agent by name).  
    
2.  Locate your agent (e.g., **`langgraph_tavily_agent`**) in the list and click its **profile card** to open the details page.  
    
3.  Press the **"Chat with Agent"** button at the top-right of the profile. A chat panel will appear.
4.  Type your query (for example, _"Give me a list of the latest agentic AI trends"_) and hit **Send**.  
    
5.  The agent will respond in real time; you can continue the conversation as needed.

> **Note**: The placeholder images above (`langgraph_open_local_agents.png`, `langgraph_profile_card.png`, etc.) live under `/static/img/adapters/`. Replace them with updated screenshots once available.

Why Use LangGraph with uAgents?[â€‹](#why-use-langgraph-with-uagents "Direct link to Why Use LangGraph with uAgents?")
--------------------------------------------------------------------------------------------------------------------

LangGraph offers several advantages when combined with uAgents:

*   **Advanced Orchestration**: Create complex reasoning flows using directed graphs
*   **State Management**: Handle complex multi-turn conversations with state persistence
*   **Tool Integration**: Easily connect to external services and APIs
*   **Debugging Capabilities**: Inspect and debug agent reasoning processes

By wrapping LangGraph with the uAgents adapter, you get the best of both worlds: sophisticated LLM orchestration with the decentralized communication capabilities of uAgents.

Getting Started[â€‹](#getting-started "Direct link to Getting Started")
---------------------------------------------------------------------

1.  In a directory, place both `agent.py` and `agent_client.py` shown above.
    
2.  Install required packages:
    
        pip install uagents>=0.22.3 uagents-adapter>=0.4.1 langchain-openai>=0.3.14 langchain-community>=0.3.21 langgraph>=0.3.31  dotenv>=0.9.9
    
3.  Set up your environment variables in a `.env` file:
    
        OPENAI_API_KEY=your_openai_keyTAVILY_API_KEY=your_tavily_key  AGENTVERSE_API_KEY=your_agentverse_key
    
4.  Run the LangGraph agent:
    
5.  In a separate terminal, run the client agent:
    

Expected Outputs[â€‹](#expected-outputs "Direct link to Expected Outputs")
------------------------------------------------------------------------

When running the examples, you should expect to see outputs similar to these:

### LangGraph Agent[â€‹](#langgraph-agent "Direct link to LangGraph Agent")

When running the LangGraph agent:

    (venv) Fetchs-MacBook-Pro test examples % python3 agent.py INFO:     [langgraph_tavily_agent]: Starting agent with address: agent1q0zyxrneyaury3f5c7aj67hfa5w65cykzplxkst5f5mnyf4y3em3kplxn4tINFO:     [langgraph_tavily_agent]: Agent 'langgraph_tavily_agent' started with address: agent1q0zyxrneyaury3f5c7aj67hfa5w65cykzplxkst5f5mnyf4y3em3kplxn4tINFO:     [langgraph_tavily_agent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8080&address=agent1q0zyxrneyaury3f5c7aj67hfa5w65cykzplxkst5f5mnyf4y3em3kplxn4tINFO:     [langgraph_tavily_agent]: Starting server on http://0.0.0.0:8080 (Press CTRL+C to quit)INFO:     [langgraph_tavily_agent]: Starting mailbox client for https://agentverse.aiINFO:     [langgraph_tavily_agent]: Mailbox access token acquiredINFO:     [langgraph_tavily_agent]: Received structured output response from agent1qtlpfshtlcxekgrfcpmv7m9zpajuwu7d5jfyachvpa4u3dkt6k0uwwp2lct: Hello, Tavily Agent. Could you please provide a list of the latest trends in agentic AI? I am interested in understanding how agent-based artificial intelligence is evolving and what innovations or developments stand out in this field. Thank you!INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"âœ… Registered LangGraph agent: Created uAgent 'langgraph_tavily_agent' with address agent1q0zyxrneyaury3f5c7aj67hfa5w65cykzplxkst5f5mnyf4y3em3kplxn4t on port 8080INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"INFO:     [langgraph_tavily_agent]: Got a message from agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49INFO:     [langgraph_tavily_agent]: Got a text message from agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49: I want to send query to tavily agent that Give me a list of latest agentic AI trendsINFO:     [langgraph_tavily_agent]: Sending structured output prompt to {'title': 'QueryMessage', 'type': 'object', 'properties': {'query': {'title': 'Query', 'type': 'string'}}, 'required': ['query']}INFO:     [langgraph_tavily_agent]: Sent structured output prompt to agent1qtlpfshtlcxekgrfcpmv7m9zpajuwu7d5jfyachvpa4u3dkt6k0uwwp2lctINFO:     [langgraph_tavily_agent]: Got an acknowledgement from agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49 for 451f41aa-be41-471f-bddc-276caffb7d94Connecting agent 'langgraph_tavily_agent' to Agentverse...INFO:     [mailbox]: Successfully registered as mailbox agent in AgentverseSuccessfully connected agent 'langgraph_tavily_agent' to AgentverseUpdating agent 'langgraph_tavily_agent' README on Agentverse...Successfully updated agent 'langgraph_tavily_agent' README on AgentverseINFO:     [langgraph_tavily_agent]: Received structured output response from agent1qtlpfshtlcxekgrfcpmv7m9zpajuwu7d5jfyachvpa4u3dkt6k0uwwp2lct: Subject: Request for Information on Latest Agentic AI TrendsHi Tavily Agent,I hope this message finds you well. I am reaching out to inquire about the latest trends in agentic AI technology. As this area is rapidly evolving, I am keen to stay updated on the most recent developments and innovations.Could you please provide me with a comprehensive list of the latest trends in agentic AI? I'm particularly interested in understanding how these trends might impact various industries and potential future applications.Thank you for your assistance. I look forward to your response.---INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"

### Client Agent[â€‹](#client-agent "Direct link to Client Agent")

When running the client agent:

    (venv) Fetchs-MacBook-Pro test examples % python3 agent_client.pyINFO:     [client_agent]: Starting agent with address: agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49INFO:     [client_agent]: My name is client_agent and my address is agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49INFO:     [client_agent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8082&address=agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49INFO:     [client_agent]: Starting server on http://0.0.0.0:8082 (Press CTRL+C to quit)INFO:     [client_agent]: Starting mailbox client for https://agentverse.aiINFO:     [uagents.registration]: Registration on Almanac API successfulINFO:     [uagents.registration]: Almanac contract registration is up to date!INFO:     [client_agent]: Manifest published successfully: AgentChatProtocolINFO:     [client_agent]: Mailbox access token acquiredINFO:     [client_agent]: Received message from agent1q0zyxrneyaury3f5c7aj67hfa5w65cykzplxkst5f5mnyf4y3em3kplxn4t: Here are some of the latest trends in agentic AI:1. **The Next Frontier: The Rise of Agentic AI - Adams Street Partners**   - **Link:** [The Next Frontier: The Rise of Agentic AI - Adams Street Partners](https://www.adamsstreetpartners.com/insights/the-next-frontier-the-rise-of-agentic-ai/)   - **Summary:** Several converging trends have set the stage for agentic AI, including advances in Large Language Models, improved reasoning, planning, and multistep processes.2. **7 Agentic AI Trends To Watch for 2025 - ServiceNow**   - **Link:** [7 Agentic AI Trends To Watch for 2025 - ServiceNow](https://www.servicenow.com/products/ai-agents/agentic-ai-trends.html)   - **Summary:** Explore the latest agentic AI trends shaping the future of work, from hyperautomation to decision intelligence, and how it can transform businesses.3. **Agentic AI: Three themes to watch for 2025 - Constellation Research**   - **Link:** [Agentic AI: Three themes to watch for 2025 - Constellation Research](https://www.constellationr.com/blog-news/insights/agentic-ai-three-themes-watch-2025)   - **Summary:** This article discusses three themes to watch in agentic AI for 2025, including horizontal approaches vs. platform-specific strategies and the proliferation of agentic AI launches by various vendors.These sources provide insights into the evolving landscape of agentic AI and the key trends that are shaping the future of this field.INFO:     [client_agent]: Received acknowledgement from agent1q0zyxrneyaury3f5c7aj67hfa5w65cykzplxkst5f5mnyf4y3em3kplxn4t for message: 1cdda4bd-4597-42a9-b6f1-13c6ca67a0eaINFO:     [client_agent]: Received message from agent1q0zyxrneyaury3f5c7aj67hfa5w65cykzplxkst5f5mnyf4y3em3kplxn4t: ### Latest Trends in Agentic AI Technology:1. **[7 Agentic AI Trends To Watch for 2025 - ServiceNow](https://www.servicenow.com/products/ai-agents/agentic-ai-trends.html)**   - Explore the latest agentic AI trends shaping the future of work, from hyperautomation to decision intelligence, and how it can transform your business.2. **[Agentic AI: Three themes to watch for 2025 - Constellation Research](https://www.constellationr.com/blog-news/insights/agentic-ai-three-themes-watch-2025)**   - Three themes to watch in agentic AI, including horizontal approaches vs. platform-specific trends and the proliferation of agentic AI platforms across various vendors.3. **[The Top Customer Service Trends and Technologies for 2025](https://www.destinationcrm.com/Articles/Editorial/Magazine-Features/The-Top-Customer-Service-Trends-and-Technologies-for-2025-Agentic-AI-Is-Poised-to-Remake-Self-Service-168751.aspx)**   - Agentic AI is poised to remake self-service in customer service, with predictions that by 2030, 50% of service requests will be initiated by machine customers powered by agentic AI systems.4. **[Future Trends in Agentic AI Development: What's Next for Intelligent Automation](https://www.imbrace.co/future-trends-in-agentic-ai-development-whats-next-for-intelligent-automation/)**   - Trends include providing clear insights into decision-making, ensuring alignment with ethical guidelines, expanded applications across industries like logistics and healthcare, and the integration of Explainable AI (XAI).5. **[5 Reasons Why Agentic AI Will Transform Industries by 2030](https://hyperight.com/5-reasons-why-agentic-ai-will-transform-industries-by-2030/)**   - Agentic AI is expected to enhance productivity and efficiency, reshape industries by 2030, and be incorporated into 33% of enterprise software applications by 2028.6. **[Agentic AI Trends - What to expect in the near future - Atera](https://www.atera.com/blog/agentic-ai-trends/)**   - Agentic AI is set to revolutionize customer service, with researchers predicting a significant impact on customer service operations.These trends highlight the advancements and potential impacts of agentic AI technology across various industries and applications.

Try different queries to see how the LangGraph agent processes them and returns search-enhanced responses through the uAgents ecosystem!</content>
</page>

<page>
  <title>Agentverse | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/agentverse</url>
  <content>Agentverse Overview
-------------------

The [**Agentverse**](https://agentverse.ai/) is a cloud-based platform for creating and hosting autonomous Agents without the hassle of managing infrastructure. It provides an integrated development environment (IDE) where you can write, edit, and run Agent code directly in your browser. The [**Agentverse**](https://agentverse.ai/) offers:

*   **Continuous Uptime**: Once you deploy your Agent, it remains online and responsiveâ€”no manual restart or extra hosting steps needed.
*   **Easy Deployment**: Predefined templates and an in-browser editor help you get started quickly, regardless of your coding background.
*   **Blockchain Integration**: Agents have their own wallets for sending/receiving tokens, querying balances, and interacting with on-chain contracts.
*   **Marketplace & Discovery**: Hosted Agents are registered in the **Almanac**, making them discoverable by other Agents or users searching for specific functions.
*   **Mailroom Service**: Allows Agents to receive messages even when offline, retrieving them once they come back online.

Key Benefits[â€‹](#key-benefits "Direct link to Key Benefits")
------------------------------------------------------------

1.  **All-In-One Environment**
    
    *   Integrated code editor, logging console, and file management.
    *   No external servers or manual container setups.
2.  **Framework Agnostic Development**
    
    *   Build your agent with any framework of choice and make it discoverable by any other agent.
    *   Seamlessly register and integrate with Agentverse.
3.  **Scalability & Reliability**
    
    *   Dynamically scales with your Agent's message volume.
    *   High uptime ensures Agents are always available to process requests.
4.  **Secure & Trustless**
    
    *   Blockchain-based identity and registration provide transparency.
    *   Agents operate in isolated Python environments, protecting your code.
5.  **Varied Use Cases**
    
    *   From simple "Hello World" scripts to advanced AI-driven Agents.
    *   Supports Python libraries (uAgents, requests, openai, etc.) for robust functionality.
6.  **User-Friendly Hosting**
    
    *   Deploy Agents with a few clicks; no advanced DevOps skills needed.
    *   Easily manage your Agents' code revisions and logs in one place.

* * *

### Learn More[â€‹](#learn-more "Direct link to Learn More")

*   **Agentverse: Mailroom** â€“ Set up mailboxes for offline Agents.
*   **Agentverse: Marketplace** â€“ Discover public Agents and explore their capabilities.
*   **Agentverse API** â€“ Integrate and automate advanced features programmatically.

With the **Agentverse**, you can focus on Agent logic and features, knowing that security, uptime, and scalability are handled for you. Get started today by creating a new Agent and unleashing the power of automated, decentralized intelligence!</content>
</page>

<page>
  <title>Frontend Web App Integration | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/examples/integrations/frontend-integration</url>
  <content>Build a Frontend Web Application with uAgents and Open Food Facts API
---------------------------------------------------------------------

This guide demonstrates how to create a complete web application that integrates uAgents with external APIs using a Flask frontend. We'll build a food product discovery system using the Open Food Facts API.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

The Frontend Integration example provides:

*   **Two Specialized uAgents** - Search Agent and Info Agent
*   **REST API Endpoints** using uAgents framework
*   **External API Integration** with Open Food Facts
*   **Modern Web Interface** built with Flask and HTML/CSS/JavaScript
*   **Real-time Health Monitoring** of all services

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

Before you begin, ensure you have:

*   **Python 3.11+** installed
*   **Basic knowledge** of Flask and web development
*   **Understanding** of REST APIs and HTTP requests

Installation[â€‹](#installation "Direct link to Installation")
------------------------------------------------------------

### 1\. Clone the Complete Example[â€‹](#1-clone-the-complete-example "Direct link to 1. Clone the Complete Example")

    git clone https://github.com/fetchai/innovation-lab-examples.gitcd innovation-lab-examples/frontend-integration

### 2\. Set Up Virtual Environment[â€‹](#2-set-up-virtual-environment "Direct link to 2. Set Up Virtual Environment")

    # Create virtual environmentpython3 -m venv venv# Activate virtual environment# On macOS/Linux:source venv/bin/activate# On Windows:# venv\Scripts\activate

### 3\. Install Dependencies[â€‹](#3-install-dependencies "Direct link to 3. Install Dependencies")

    pip install -r requirements.txt

Architecture Overview[â€‹](#architecture-overview "Direct link to Architecture Overview")
---------------------------------------------------------------------------------------

### Two Specialized uAgents[â€‹](#two-specialized-uagents "Direct link to Two Specialized uAgents")

Our system consists of two specialized microservices:

1.  **Search Agent** (Port 8001): Handles product search queries
2.  **Info Agent** (Port 8002): Retrieves detailed product information

### Frontend Application[â€‹](#frontend-application "Direct link to Frontend Application")

*   **Flask Web App** (Port 5000): Modern web interface to interact with agents

Quick Start[â€‹](#quick-start "Direct link to Quick Start")
---------------------------------------------------------

### 1\. Start Search Agent[â€‹](#1-start-search-agent "Direct link to 1. Start Search Agent")

**Terminal 1:**

    source venv/bin/activate  # Activate venv if not already activepython3 product_search_agent.py

### 2\. Start Info Agent[â€‹](#2-start-info-agent "Direct link to 2. Start Info Agent")

**Terminal 2:**

    source venv/bin/activate  # Activate venv if not already activepython3 product_info_agent.py

### 3\. Start Frontend[â€‹](#3-start-frontend "Direct link to 3. Start Frontend")

**Terminal 3:**

    source venv/bin/activate  # Activate venv if not already activepython3 frontend_app.py

### 4\. Access the Web Interface[â€‹](#4-access-the-web-interface "Direct link to 4. Access the Web Interface")

Open your browser to: [http://127.0.0.1:5000](http://127.0.0.1:5000/)

Agent Implementation Details[â€‹](#agent-implementation-details "Direct link to Agent Implementation Details")
------------------------------------------------------------------------------------------------------------

### Search Agent (Port 8001)[â€‹](#search-agent-port-8001 "Direct link to Search Agent (Port 8001)")

The search agent handles product discovery using natural language queries:

    from uagents import Agent, Context, Modelimport openfoodfactsclass SearchRequest(Model):    query: strclass ProductInfo(Model):    code: str    product_name: str    brands: str    categories: str    image_url: strclass SearchResponse(Model):    query: str    count: int    products: List[ProductInfo]    error: Optional[str] = Nonesearch_agent = Agent(    name="product_search_agent",    port=8001,    endpoint=["http://127.0.0.1:8001/submit"],)@search_agent.on_rest_post("/search", SearchRequest, SearchResponse)async def search_products(ctx: Context, req: SearchRequest) -> SearchResponse:    try:        query = req.query        ctx.logger.info(f"Searching for products with query: {query}")                # Search products using Open Food Facts API        results = api.product.text_search(query, page_size=10)                # Extract relevant information        products = []        for product in results.get("products", [])[:10]:            product_info = ProductInfo(                code=product.get("code", "N/A"),                product_name=product.get("product_name", "N/A"),                brands=product.get("brands", "N/A"),                categories=product.get("categories", "N/A"),                image_url=product.get("image_url", "")            )            products.append(product_info)                return SearchResponse(            query=query,            count=results.get("count", 0),            products=products        )    except Exception as e:        return SearchResponse(            query=req.query,            count=0,            products=[],            error=f"Failed to search products: {str(e)}"        )

### Info Agent (Port 8002)[â€‹](#info-agent-port-8002 "Direct link to Info Agent (Port 8002)")

The info agent provides detailed product information using exact barcodes:

    from uagents import Agent, Context, Modelimport requestsclass ProductRequest(Model):    barcode: strclass ProductInfoResponse(Model):    barcode: str    product_name: str    brands: str    categories: str    ingredients_text: str    allergens: str    nutrition_grades: str    ecoscore_grade: str    image_url: str    countries: str    stores: str    packaging: str    quantity: str    energy_100g: str    fat_100g: str    sugars_100g: str    salt_100g: str    error: Optional[str] = Noneinfo_agent = Agent(    name="product_info_agent",    port=8002,    endpoint=["http://127.0.0.1:8002/submit"],)@info_agent.on_rest_post("/product", ProductRequest, ProductInfoResponse)async def get_product_info(ctx: Context, req: ProductRequest) -> ProductInfoResponse:    try:        barcode = req.barcode        ctx.logger.info(f"Getting product info for barcode: {barcode}")                # Use direct API call to Open Food Facts        url = f"https://world.openfoodfacts.org/api/v0/product/{barcode}.json"        headers = {            'User-Agent': 'uAgents-FoodInfo/1.0 (https://github.com/fetchai/uAgents)'        }                response = requests.get(url, headers=headers, timeout=10)        data = response.json()                if data.get('status') != 1 or "product" not in data:            return ProductInfoResponse(                barcode=barcode,                # ... other fields with "N/A"                error="Product not found"            )                product = data["product"]                # Extract comprehensive product information        return ProductInfoResponse(            barcode=barcode,            product_name=product.get("product_name", "N/A"),            brands=product.get("brands", "N/A"),            categories=product.get("categories", "N/A"),            ingredients_text=product.get("ingredients_text", "N/A"),            allergens=product.get("allergens", "N/A"),            nutrition_grades=product.get("nutrition_grades", "N/A"),            ecoscore_grade=product.get("ecoscore_grade", "N/A"),            image_url=product.get("image_url", ""),            countries=product.get("countries", "N/A"),            stores=product.get("stores", "N/A"),            packaging=product.get("packaging", "N/A"),            quantity=product.get("quantity", "N/A"),            energy_100g=str(product.get("nutriments", {}).get("energy_100g", "N/A")),            fat_100g=str(product.get("nutriments", {}).get("fat_100g", "N/A")),            sugars_100g=str(product.get("nutriments", {}).get("sugars_100g", "N/A")),            salt_100g=str(product.get("nutriments", {}).get("salt_100g", "N/A"))        )    except Exception as e:        return ProductInfoResponse(            barcode=req.barcode,            # ... other fields with "N/A"            error=f"Failed to get product info: {str(e)}"        )

Frontend Implementation[â€‹](#frontend-implementation "Direct link to Frontend Implementation")
---------------------------------------------------------------------------------------------

### Flask Backend[â€‹](#flask-backend "Direct link to Flask Backend")

The Flask application serves as a bridge between the web interface and uAgents:

    from flask import Flask, render_template, request, jsonifyimport requestsapp = Flask(__name__)# Agent endpointsAGENTS = {    "search": "http://127.0.0.1:8001",    "info": "http://127.0.0.1:8002"}@app.route('/search_products', methods=['POST'])def search_products():    """Search products via search agent"""    try:        query = request.form.get('query', '').strip()        if not query:            return jsonify({"error": "Please provide a search query"})                # Call search agent with POST request        payload = {"query": query}        response = requests.post(f"{AGENTS['search']}/search", json=payload)        response.raise_for_status()                result = response.json()                # Format the results for display        formatted_results = []        if result.get('products'):            for product in result['products'][:10]:                formatted_product = {                    'name': product.get('product_name', 'N/A'),                    'brands': product.get('brands', 'N/A'),                    'barcode': product.get('code', 'N/A'),                    'categories': product.get('categories', 'N/A'),                    'image_url': product.get('image_url', '')                }                formatted_results.append(formatted_product)                return jsonify({            "success": True,             "count": result.get('count', 0),            "query": query,            "products": formatted_results        })            except requests.RequestException as e:        return jsonify({"error": f"Failed to connect to search agent: {str(e)}"})    except Exception as e:        return jsonify({"error": f"Search failed: {str(e)}"})@app.route('/health')def health_check():    """Check health of all agents"""    health_status = {}        for agent_name, agent_url in AGENTS.items():        try:            response = requests.get(f"{agent_url}/health", timeout=5)            if response.status_code == 200:                health_data = response.json()                health_status[agent_name] = {                    "status": "healthy",                     "url": agent_url,                     "agent_info": health_data                }            else:                health_status[agent_name] = {"status": "unhealthy", "url": agent_url}        except:            health_status[agent_name] = {"status": "offline", "url": agent_url}        return jsonify(health_status)

Testing the Application[â€‹](#testing-the-application "Direct link to Testing the Application")
---------------------------------------------------------------------------------------------

### 1\. Product Search Testing[â€‹](#1-product-search-testing "Direct link to 1. Product Search Testing")

**Search Query: "chocolate"**

1.  Navigate to [http://127.0.0.1:5000](http://127.0.0.1:5000/)
2.  In the "Search Products" section, enter `chocolate` in the search field
3.  Click "Search Products" to see results

The search will return multiple chocolate products with product names, brands, categories, barcodes, and images.

### 2\. Product Details Testing[â€‹](#2-product-details-testing "Direct link to 2. Product Details Testing")

**Barcode Query: "3017624010701"**

1.  In the "Get Product Information" section, enter the barcode `3017624010701`
2.  Click "Get Product Info" to retrieve detailed information

This will display comprehensive information for Nutella including basic information, ingredients, nutrition facts, and quality scores.

### 3\. Health Status Monitoring[â€‹](#3-health-status-monitoring "Direct link to 3. Health Status Monitoring")

Click the "Check Agent Status" button to verify all services are running:

This displays the real-time status of both Search Agent (Port 8001) and Info Agent (Port 8002).

Key Features Demonstrated[â€‹](#key-features-demonstrated "Direct link to Key Features Demonstrated")
---------------------------------------------------------------------------------------------------

### 1\. Microservice Architecture[â€‹](#1-microservice-architecture "Direct link to 1. Microservice Architecture")

*   **Separation of Concerns**: Each agent has a specific responsibility
*   **Independent Scaling**: Agents can be scaled independently
*   **Fault Isolation**: Failure in one service doesn't affect others

### 2\. REST API Design[â€‹](#2-rest-api-design "Direct link to 2. REST API Design")

*   **Proper HTTP Methods**: POST for data operations, GET for health checks
*   **Pydantic Models**: Type-safe request/response validation
*   **Error Handling**: Comprehensive error responses

### 3\. Frontend Integration[â€‹](#3-frontend-integration "Direct link to 3. Frontend Integration")

*   **Ajax Requests**: Asynchronous communication with agents
*   **Real-time Updates**: Dynamic UI updates without page refresh
*   **Health Monitoring**: Live status checking of backend services

### 4\. External API Integration[â€‹](#4-external-api-integration "Direct link to 4. External API Integration")

*   **HTTP Client Usage**: Direct API calls to Open Food Facts
*   **Data Transformation**: Converting external API responses to internal models
*   **Error Handling**: Graceful handling of external API failures

GitHub Repository[â€‹](#github-repository "Direct link to GitHub Repository")
---------------------------------------------------------------------------

For the complete code and additional examples, visit the [Frontend Integration Example](https://github.com/fetchai/innovation-lab-examples/tree/main/frontend-integration) repository.

This repository includes:

*   âœ… Complete agent implementations
*   âœ… Flask web application
*   âœ… Modern responsive web interface
*   âœ… Docker configuration
*   âœ… Comprehensive documentation
*   âœ… Testing examples</content>
</page>

<page>
  <title>ASI1 Chat System | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/examples/asione/asi1-chat-system</url>
  <content>ASI1 Chat System using uAgents Framework To Communicate with HuggingFace
------------------------------------------------------------------------

This guide explains how the ASI1 Chat System facilitates real-time communication between autonomous agents and external APIs to process user queries efficiently.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

This project demonstrates how to create a multi-agent system using the uAgents framework and ASI1 Mini. The system, consisting of a server agent and a client agent, is designed to:

*   Enable interactive query handling using the ASI1 API.
*   Allow users to send queries about Hugging Face models.
*   Process responses through a server-client architecture.
*   Demonstrate agent-to-agent communication patterns.

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

Before running this project, ensure you have:

*   Python 3.11 installed.
*   uAgents library installed. If not already installed, use:
*   Requests library installed:
*   A valid API key for ASI1. Obtain your API Key [here](https://innovationlab.fetch.ai/resources/docs/asione/asi1-mini-getting-started#how-to-get-an-api-key).
*   Environment variable set for your ASI1 API key:
    
        export ASI1_API_KEY="your_api_key_here"
    

Server Agent Script[â€‹](#server-agent-script "Direct link to Server Agent Script")
---------------------------------------------------------------------------------

The Server Agent is responsible for receiving queries from the Client Agent, processing them using the ASI1 API, and returning the responses.

### Script Breakdown (server.py)[â€‹](#script-breakdown-serverpy "Direct link to Script Breakdown (server.py)")

#### Importing Required Libraries[â€‹](#importing-required-libraries "Direct link to Importing Required Libraries")

    import requestsimport osfrom uagents import Agent, Context, Model

#### Defining Data Models[â€‹](#defining-data-models "Direct link to Defining Data Models")

    # Request modelclass ASI1Query(Model):    query: str    sender_address: str# Response modelclass ASI1Response(Model):    response: str  # Response from ASI1 API

#### Initializing the Server Agent[â€‹](#initializing-the-server-agent "Direct link to Initializing the Server Agent")

    # Define the main agentmainAgent = Agent(    name='asi1_chat_agent',    port=5068,    endpoint='http://localhost:5068/submit',    seed='asi1_chat_seed')

#### ASI1 API Integration Function[â€‹](#asi1-api-integration-function "Direct link to ASI1 API Integration Function")

    def get_asi1_response(query: str) -> str:    """    Sends a query to ASI1 API and returns the response.    """    # Get API key from environment variable    api_key = os.environ.get("ASI1_API_KEY")    if not api_key:        return "Error: ASI1_API_KEY environment variable not set"            headers = {        "Authorization": f"Bearer {api_key}",        "Content-Type": "application/json"    }    data = {        "model": "asi1-mini",  # Select appropriate ASI1 model        "messages": [            {"role": "system", "content": "You are a helpful AI assistant."},            {"role": "user", "content": query}        ]    }    try:        response = requests.post("https://api.asi1.ai/v1/chat/completions", json=data, headers=headers)        if response.status_code == 200:            result = response.json()            if "choices" in result and len(result["choices"]) > 0:                return result["choices"][0]["message"]["content"].strip()            else:                return "ASI1 API returned an empty response."        else:            return f"ASI1 API Error: {response.status_code}, {response.text}"    except Exception as e:        return f"ASI1 API Error: {str(e)}"

#### Event Handlers[â€‹](#event-handlers "Direct link to Event Handlers")

    @mainAgent.on_event('startup')async def startup_handler(ctx: Context):    ctx.logger.info(f'Agent {ctx.agent.name} started at {ctx.agent.address}')# Handler for receiving query@mainAgent.on_message(model=ASI1Query)async def handle_query(ctx: Context, sender: str, msg: ASI1Query):    ctx.logger.info(f"Received query from {sender}: {msg.query}")    # Call ASI1 API for the response    answer = get_asi1_response(msg.query)    # Respond back with the answer from ASI1    await ctx.send(sender, ASI1Response(response=answer))

#### Running the Server Agent[â€‹](#running-the-server-agent "Direct link to Running the Server Agent")

    if __name__ == "__main__":    mainAgent.run()

Client Agent Script[â€‹](#client-agent-script "Direct link to Client Agent Script")
---------------------------------------------------------------------------------

The Client Agent acts as the user interface, collecting input from the user and sending it to the Server Agent for processing.

### Script Breakdown (client.py)[â€‹](#script-breakdown-clientpy "Direct link to Script Breakdown (client.py)")

#### Importing Required Libraries[â€‹](#importing-required-libraries-1 "Direct link to Importing Required Libraries")

    from uagents import Agent, Context, Model

#### Defining Data Models[â€‹](#defining-data-models-1 "Direct link to Defining Data Models")

    # Query model to send to the server agentclass ASI1Query(Model):    query: str    sender_address: str# Response model to receive from the server agentclass ASI1Response(Model):    response: str

#### Initializing the Client Agent[â€‹](#initializing-the-client-agent "Direct link to Initializing the Client Agent")

    # Client agent setupclientAgent = Agent(    name='asi1_client_agent',    port=5070,    endpoint='http://localhost:5070/submit',    seed='asi1_client_seed')# Server agent address (update with actual address if needed)SERVER_AGENT_ADDRESS = "agent1q0usc8uc5hxes4ckr8624ghdxpn0lvxkgex44jtfv32x2r7ymx8sgg8yt2g"  # Replace with the actual address of your server agent

#### Event Handlers[â€‹](#event-handlers-1 "Direct link to Event Handlers")

    @clientAgent.on_event('startup')async def startup_handler(ctx: Context):    ctx.logger.info(f'Client Agent {ctx.agent.name} started at {ctx.agent.address}')    # Get user input    user_query = input("Ask something: ")    # Send the query to the server agent    await ctx.send(SERVER_AGENT_ADDRESS, ASI1Query(query=user_query, sender_address=ctx.agent.address))    ctx.logger.info(f"Query sent to server agent: {user_query}")@clientAgent.on_message(model=ASI1Response)async def handle_response(ctx: Context, sender: str, msg: ASI1Response):    ctx.logger.info(f"Response received from {sender}: {msg.response}")    print(f"Response from ASI1 API: {msg.response}")

#### Running the Client Agent[â€‹](#running-the-client-agent "Direct link to Running the Client Agent")

    if __name__ == "__main__":    clientAgent.run()

Running the System[â€‹](#running-the-system "Direct link to Running the System")
------------------------------------------------------------------------------

**1\. Set Environment Variable for API Key**

Before running the agents, set the ASI1 API key as an environment variable:

    export ASI1_API_KEY="your_api_key_here"

**2\. Start the Server Agent**

Open a terminal window and run:

You should see output similar to:

    INFO:     [asi1_chat_agent]: Starting agent with address: agent1q0usc8uc5hxes4ckr8624ghdxpn0lvxkgex44jtfv32x2r7ymx8sgg8yt2gINFO:     [asi1_chat_agent]: Agent asi1_chat_agent started at agent1q0usc8uc5hxes4ckr8624ghdxpn0lvxkgex44jtfv32x2r7ymx8sgg8yt2gINFO:     [asi1_chat_agent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A5068&address=agent1q0usc8uc5hxes4ckr8624ghdxpn0lvxkgex44jtfv32x2r7ymx8sgg8yt2gINFO:     [asi1_chat_agent]: Starting server on http://0.0.0.0:5068 (Press CTRL+C to quit)INFO:     [asi1_chat_agent]: Registration on Almanac API successfulINFO:     [asi1_chat_agent]: Almanac contract registration is up to date!

**3\. Start the Client Agent**

Open a new terminal window and run:

You should see output similar to:

    INFO:     [asi1_client_agent]: Starting agent with address: agent1qdszugnhqtadhg38tgxnzlyards0qpf0mx5x4l4ms3qq9g8fadxfgxv5dfrINFO:     [asi1_client_agent]: Client Agent asi1_client_agent started at agent1qdszugnhqtadhg38tgxnzlyards0qpf0mx5x4l4ms3qq9g8fadxfgxv5dfrAsk something: 

**4\. Interact with the System**

When prompted, enter a query about Hugging Face models, for example:

    Ask something: Image Classification

The client will send this query to the server agent:

    INFO:     [asi1_client_agent]: Query sent to server agent: Image ClassificationINFO:     [asi1_client_agent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A5070&address=agent1qdszugnhqtadhg38tgxnzlyards0qpf0mx5x4l4ms3qq9g8fadxfgxv5dfrINFO:     [asi1_client_agent]: Starting server on http://0.0.0.0:5070 (Press CTRL+C to quit)INFO:     [asi1_client_agent]: Registration on Almanac API successfulINFO:     [asi1_client_agent]: Almanac contract registration is up to date!

**5\. View the Response**

The server agent will process the query and send back a response:

    INFO:     [asi1_chat_agent]: Received query from agent1qdszugnhqtadhg38tgxnzlyards0qpf0mx5x4l4ms3qq9g8fadxfgxv5dfr: Image Classification

The client agent will display the response:

    INFO:     [asi1_client_agent]: Response received from agent1q0usc8uc5hxes4ckr8624ghdxpn0lvxkgex44jtfv32x2r7ymx8sgg8yt2g: It's difficult to give a definitive "top 5" list for most downloaded and highest-rated models on Hugging Face for Image Classification as these metrics change frequently. As of my last knowledge update, these are some of the highly regarded and popular models for image classification you could find on Hugging Face:1. **ViT (Vision Transformer):** Developed by Google Research, ViT models are known for their strong performance on image classification tasks, leveraging the transformer architecture originally designed for natural language processing. They are often pre-trained on large datasets and then fine-tuned for specific image classifications.2. **ResNet (Residual Network):** Developed by Microsoft Research, various ResNet architectures (e.g., ResNet50, ResNet101, ResNet152) are widely used. ResNet introduced the concept of skip connections, allowing for the training of very deep networks and achieving excellent performance on image classification.3. **EfficientNet:** Developed by Google Research, EfficientNet models focus on optimizing accuracy and efficiency. They are known for achieving high performance with relatively fewer parameters compared to other models.4. **Inception:** Also developed by Google, Inception models (e.g., InceptionV3) utilize inception modules with multiple convolutional filters operating at different scales to capture features at various levels of detail within an image.5. **MobileNet:** Developed by Google, MobileNet models are designed specifically for mobile and embedded vision applications. They are known for their efficiency and smaller model sizes, allowing for deployment on resource-constrained devices while still maintaining reasonable accuracy for image classification.Keep in mind that popularity and download counts on Hugging Face are dynamic. I recommend checking the Hugging Face model hub directly for the most up-to-date information on downloads and ratings. You can sort models by different metrics there.

Troubleshooting Common Issues[â€‹](#troubleshooting-common-issues "Direct link to Troubleshooting Common Issues")
---------------------------------------------------------------------------------------------------------------

**API Key Not Found**

If you see the error "ASI1\_API\_KEY environment variable not set", make sure you've set the environment variable correctly:

    export ASI1_API_KEY="your_api_key_here"

**Connection Issues**

If the client agent cannot connect to the server agent:

1.  Ensure the server agent is running and note its address.
2.  Update the `SERVER_AGENT_ADDRESS` in the client.py script with the correct address.
3.  Check that both agents are running on their specified ports (5068 and 5070).

**API Response Errors**

If you receive API errors:

1.  Verify your ASI1 API key is valid and has not expired.
2.  Check your internet connection.
3.  Ensure the ASI1 API service is operational.

GitHub Repository[â€‹](#github-repository "Direct link to GitHub Repository")
---------------------------------------------------------------------------

For the complete code, visit the [ASI1 Chat System Repository](https://github.com/Atharva-Pore/AP_uAgents/tree/main/asi1_chat_system).

note

**Note:** You can learn more about ASI1 Mini APIs [**here**](https://docs.asi1.ai/docs/) and about uAgents [**here**](https://innovationlab.fetch.ai/resources/docs/agent-creation/uagent-creation).</content>
</page>

<page>
  <title>DeFi AI Agent Starter Guide | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/examples/asione/asi-defi-ai-agent</url>
  <content>This guide demonstrates how to create a multi-agent system for DeFi analysis using the uAgents framework and ASI1 Mini. The system helps decide whether to hold or sell a long-term crypto asset based on market data and sentiment analysis.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

This project showcases how to build a DeFi analysis system using multiple agents:

*   **Fear and Greed Index (FGI) Agent**: Fetches and analyzes market sentiment data
*   **Coin Info Agent**: Retrieves cryptocurrency market data
*   **Main Agent**: Coordinates between agents and makes trading decisions using ASI1 Mini

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

Before running this project, ensure you have:

*   Python 3.11 installed
*   uAgents library installed:
*   Required Python packages:
    
        pip install requests pydantic python-dotenv
    
*   API Keys:
    *   CoinMarketCap [API key](https://coinmarketcap.com/academy/article/register-for-coinmarketcap-api) for FGI data
    *   ASI1 [API key](https://innovationlab.fetch.ai/resources/docs/asione/asi1-mini-getting-started#how-to-get-an-api-key) for AI analysis

Project Structure[â€‹](#project-structure "Direct link to Project Structure")
---------------------------------------------------------------------------

Your project structure should look like this:

    defi-ai-agent/â”œâ”€â”€ .env                    # Environment variablesâ”œâ”€â”€ main.py                # Main agent scriptâ””â”€â”€ asi/    â”œâ”€â”€ __init__.py       # Empty file to make asi a package    â””â”€â”€ llm.py            # ASI1 LLM implementation

note

**Note:** `FGI Agent` and `Coin Info Agent` are Hosted on [Agentverse](https://agentverse.ai/).

FGI Agent Script[â€‹](#fgi-agent-script "Direct link to FGI Agent Script")
------------------------------------------------------------------------

The Fear and Greed Index Agent fetches market sentiment data from CoinMarketCap.

### Script Breakdown (fgi-agent/agent.py)[â€‹](#script-breakdown-fgi-agentagentpy "Direct link to Script Breakdown (fgi-agent/agent.py)")

#### Importing Required Libraries[â€‹](#importing-required-libraries "Direct link to Importing Required Libraries")

    import osfrom uagents import Agent, Context, Modelimport requestsfrom datetime import datetimefrom typing import Optional

#### Defining Data Models[â€‹](#defining-data-models "Direct link to Defining Data Models")

    class FGIRequest(Model):    limit: Optional[int] = 1class FearGreedData(Model):    value: float    value_classification: str    timestamp: strclass FGIResponse(Model):    data: list[FearGreedData]    status: str    timestamp: str

#### Initializing the FGI Agent[â€‹](#initializing-the-fgi-agent "Direct link to Initializing the FGI Agent")

#### API Integration Function[â€‹](#api-integration-function "Direct link to API Integration Function")

    def get_fear_and_greed_index(limit: int = 1) -> FGIResponse:    """Fetch Fear and Greed index data from CoinMarketCap API"""    url = "https://pro-api.coinmarketcap.com/v3/fear-and-greed/historical"    api_key = CMC_API_KEY        headers = {        "X-CMC_PRO_API_KEY": api_key    }        params = {        "limit": limit    }    response = requests.get(url, headers=headers, params=params)        if response.status_code == 200:        raw_data = response.json()        fear_greed_data = []                for entry in raw_data["data"]:            data = FearGreedData(                value=entry["value"],                value_classification=entry["value_classification"],                timestamp=entry["timestamp"]            )            fear_greed_data.append(data)                return FGIResponse(            data=fear_greed_data,            status="success",            timestamp=datetime.utcnow().isoformat()        )    else:        raise Exception(f"Error fetching data: {response.json()['status']['error_message']}")async def process_response(ctx: Context, msg: FGIRequest) -> FGIResponse:    """Process the request and return formatted response"""    fear_greed_data = get_fear_and_greed_index(msg.limit)        for entry in fear_greed_data.data:        ctx.logger.info(f"Fear and Greed Index: {entry.value}")        ctx.logger.info(f"Classification: {entry.value_classification}")        ctx.logger.info(f"Timestamp: {entry.timestamp}")        return fear_greed_data

#### Event Handlers[â€‹](#event-handlers "Direct link to Event Handlers")

    @agent.on_event("startup")async def startup(ctx: Context):    """Initialize agent with a test request"""    ctx.logger.info(f"Hello, I'm a Fear and Greed Index agent and my address is {ctx.agent.address}.")    dummy_request = FGIRequest(limit=1)    await process_response(ctx, dummy_request)@agent.on_message(model=FGIRequest)async def handle_message(ctx: Context, sender: str, msg: FGIRequest):    """Handle incoming messages requesting Fear and Greed index data"""    ctx.logger.info(f"Received message from {sender}: FGIRequest for {msg.limit} entries")        response = await process_response(ctx, msg)    await ctx.send(sender, response)        return response

note

**Note:** This is an hosted agent on [Agentverse](https://agentverse.ai/). This is the reason we have not provided `name, seed, endpoint` and `port` to the agent.

**Note:** Store `CMC_API_KEY` in the agent secret of your hosted Agent.

Coin Info Agent Script[â€‹](#coin-info-agent-script "Direct link to Coin Info Agent Script")
------------------------------------------------------------------------------------------

The Coin Info Agent fetches cryptocurrency market data from CoinGecko.

### Script Breakdown (coin-info-agent/agent.py)[â€‹](#script-breakdown-coin-info-agentagentpy "Direct link to Script Breakdown (coin-info-agent/agent.py)")

#### Importing Required Libraries[â€‹](#importing-required-libraries-1 "Direct link to Importing Required Libraries")

    import osfrom uagents import Agent, Context, Modelimport requests

#### Defining Data Models[â€‹](#defining-data-models-1 "Direct link to Defining Data Models")

    class CoinRequest(Model):    coin_id: strclass CoinResponse(Model):    name: str    symbol: str    current_price: float    market_cap: float    total_volume: float    price_change_24h: float

#### Initializing the Coin Info Agent[â€‹](#initializing-the-coin-info-agent "Direct link to Initializing the Coin Info Agent")

#### API Integration Function[â€‹](#api-integration-function-1 "Direct link to API Integration Function")

    def get_crypto_info(coin_id: str) -> CoinResponse:    """Fetch cryptocurrency information from CoinGecko API"""    url = f"https://api.coingecko.com/api/v3/coins/{coin_id}"    response = requests.get(url)        if response.status_code == 200:        data = response.json()                return CoinResponse(            name=data['name'],            symbol=data['symbol'].upper(),            current_price=data['market_data']['current_price']['usd'],            market_cap=data['market_data']['market_cap']['usd'],            total_volume=data['market_data']['total_volume']['usd'],            price_change_24h=data['market_data']['price_change_percentage_24h']        )    else:        raise Exception(f"Failed to get crypto info: {response.text}")async def process_response(ctx: Context, msg: CoinRequest) -> CoinResponse:    """Process the crypto request and return formatted response"""    crypto_data = get_crypto_info(msg.coin_id)    ctx.logger.info(f"Crypto data: {crypto_data}")    return crypto_data

#### Event Handlers[â€‹](#event-handlers-1 "Direct link to Event Handlers")

    @agent.on_event("startup")async def startup(ctx: Context):    """Initialize agent with a test request for Bitcoin data"""    ctx.logger.info(f"Hello, I'm a crypto agent and my address is {ctx.agent.address}.")@agent.on_message(model=CoinRequest)async def handle_message(ctx: Context, sender: str, msg: CoinRequest):    """Handle incoming messages requesting crypto information"""    ctx.logger.info(f"Received message from {sender}: {msg.coin_id}")        response = await process_response(ctx, msg)    await ctx.send(sender, response)        return response

note

**Note:** This is an hosted agent on [Agentverse](https://agentverse.ai/). This is the reason we have not provided `name, seed, endpoint` and `port` to the agent.

ASI1 LLM Implementation[â€‹](#asi1-llm-implementation "Direct link to ASI1 LLM Implementation")
---------------------------------------------------------------------------------------------

Create a new file called `asi/llm.py` with the following code:

    import requestsimport osfrom dotenv import load_dotenv# Load environment variables from a .env fileload_dotenv()# Retrieve the API key from environment variablesapi_key = os.getenv("ASI1_LLM_API_KEY")# ASI1-Mini LLM API endpointurl = "https://api.asi1.ai/v1/chat/completions"# Define headers for API requests, including authenticationheaders = {    "Content-Type": "application/json",    "Authorization": f"Bearer {api_key}"}def query_llm(query):    """    Queries the ASI1-Mini LLM with a given prompt and returns the model's response.    Parameters:        query (str): The input question or statement for the language model.    Returns:        str: The response from the LLM.        If an error occurs during the request, the function returns the exception object.    """    data = {        "messages": [{"role": "user", "content": query}],  # User input for the chat model        "conversationId": None,  # No conversation history tracking        "model": "asi1-mini"  # Specifies the model version to use    }    try:        # Send a POST request to the LLM API with the input query        with requests.post(url, headers=headers, json=data) as response:            output = response.json()  # Parse the JSON response            # Extract and return the generated message content            return output["choices"][0]["message"]["content"]        except requests.exceptions.RequestException as e:        # Handle and return any request-related exceptions (e.g., network errors)        return str(e)

Make sure to create the `asi` directory and include an empty `__init__.py` file to make it a Python package.

Main Agent Script[â€‹](#main-agent-script "Direct link to Main Agent Script")
---------------------------------------------------------------------------

The Main Agent coordinates between the FGI and Coin Info agents, using ASI1 Mini for decision-making.

### Script Breakdown (main.py)[â€‹](#script-breakdown-mainpy "Direct link to Script Breakdown (main.py)")

#### Importing Required Libraries[â€‹](#importing-required-libraries-2 "Direct link to Importing Required Libraries")

    from uagents import Agent, Context, Modelfrom typing import Optionalfrom asi.llm import query_llm

#### Agent Configuration and Data Models[â€‹](#agent-configuration-and-data-models "Direct link to Agent Configuration and Data Models")

    # Initialize the agent with a name and mailbox enabled for communicationagent = Agent(name="Sentiment-Based Crypto Sell Alerts Agent", mailbox=True,port = 8001)# Coin to monitorCOIN_ID = "bitcoin"# Agentverse agent addressesCOIN_AGENT = "agent1q0005tewegtmkruc9s3v2zz8p45dd68hu38zurgpwjrrj94h8sl5cz0qev5" #Update this with your Coin agent address on AgentverseFGI_AGENT = "agent1q2eln9t0c70ha0z8uz6q88mrdzsdkxfmk3zmjejecv7skf4844cpvzplp02" #Update this with your FGI agent address on Agentverse### AGENTVERSE INTERACTION CLASSES #### Request model for retrieving coin dataclass CoinRequest(Model):    coin_id: str# Response model for coin dataclass CoinResponse(Model):    name: str    symbol: str    current_price: float    market_cap: float    total_volume: float    price_change_24h: float# Request model for Fear Greed Index (FGI) dataclass FGIRequest(Model):    limit: Optional[int] = 1# Model for individual FGI data pointsclass FearGreedData(Model):    value: float    value_classification: str    timestamp: str# Response model for FGI dataclass FGIResponse(Model):    data: list[FearGreedData]    status: str    timestamp: str

#### Event Handlers[â€‹](#event-handlers-2 "Direct link to Event Handlers")

    @agent.on_event("startup")async def introduce_agent(ctx: Context):    """Introduces the agent when it starts running."""    print(f"Hello! I'm {agent.name} and my address is {agent.address}.")@agent.on_interval(period=24 * 60 * 60.0)  # Runs every 24 hoursasync def check_coin(ctx: Context):    """Requests market data for the monitored coin once a day."""    await ctx.send(COIN_AGENT, CoinRequest(coin_id=COIN_ID))@agent.on_message(model=CoinResponse)async def handle_coin_response(ctx: Context, sender: str, msg: CoinResponse):    """Handles incoming coin market data and requests FGI data if the price drop exceeds 10%."""    global market_data    market_data = msg        # Check if price has dropped by 10% or more before requesting FGI analysis    if msg.price_change_24h <= -10.0:        await ctx.send(FGI_AGENT, FGIRequest())@agent.on_message(model=FGIResponse)async def handle_fgi_response(ctx: Context, sender: str, msg: FGIResponse):    """Analyzes Fear Greed Index data and determines whether to issue a SELL alert."""    global fgi_analysis    fgi_analysis = msg        # Construct the AI prompt based on current market and sentiment analysis    prompt = f'''    Given the following information, respond with either SELL or HOLD for the coin {COIN_ID}.        Below is analysis on the Fear Greed Index:    {fgi_analysis}        Below is analysis on the coin:    {market_data}    '''        response = query_llm(prompt)  # Query ASI1 Mini for a decision        # Interpret the AI response and print decision    if "SELL" in response:        print("SELL")    else:        print("HOLD")

Running the System[â€‹](#running-the-system "Direct link to Running the System")
------------------------------------------------------------------------------

**1\. Start the FGI Agent**

Open a blank agent on AV and write your [script](#coin-info-agent-script) in it. Click on `Start Agent` button.

You should see output similar to:

    INFO: Hello, I'm a Fear and Greed Index agent and my address is agent1q2eln9t0c70ha0z8uz6q88mrdzsdkxfmk3zmjejecv7skf4844cpvzplp02

**2\. Start the Coin Info Agent**

Open another blank agent on AV and write your [script](#fgi-agent-script) in it. Click on `Start Agent` button.

You should see output similar to:

    INFO: Hello, I'm a crypto agent and my address is agent1q0005tewegtmkruc9s3v2zz8p45dd68hu38zurgpwjrrj94h8sl5cz0qev5

**3\. Start the Main Agent**

Open a terminal window and run:

You should see output similar to:

    Hello! I'm Sentiment-Based Crypto Sell Alerts Agent and my address is agent1qxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

**4\. Monitor the System**

The main agent will:

1.  Check cryptocurrency prices every 24 hours
2.  Request FGI analysis if price drops by 10% or more
3.  Use ASI1 Mini to analyze data and make SELL/HOLD recommendations

Troubleshooting Common Issues[â€‹](#troubleshooting-common-issues "Direct link to Troubleshooting Common Issues")
---------------------------------------------------------------------------------------------------------------

### API Key Issues[â€‹](#api-key-issues "Direct link to API Key Issues")

If you see API-related errors:

1.  Verify your API keys are correctly set in the `.env` files
2.  Check API rate limits and quotas
3.  Ensure the API services are operational

### Agent Communication Issues[â€‹](#agent-communication-issues "Direct link to Agent Communication Issues")

If agents aren't communicating:

1.  Verify all agents are running
2.  Check agent addresses in `main.py` match the actual addresses
3.  Ensure network connectivity

### ASI1 Mini Integration Issues[â€‹](#asi1-mini-integration-issues "Direct link to ASI1 Mini Integration Issues")

If AI analysis isn't working:

1.  Check ASI1\_API\_KEY is set correctly
2.  Verify ASI1 Mini service status
3.  Review prompt formatting in `main.py`

Benefits of DeFi AI Agent System[â€‹](#benefits-of-defi-ai-agent-system "Direct link to Benefits of DeFi AI Agent System")
------------------------------------------------------------------------------------------------------------------------

*   **Automated Monitoring**: 24/7 tracking of cryptocurrency prices
*   **Data-Driven Decisions**: Combines market data with sentiment analysis
*   **AI-Powered Analysis**: Leverages ASI1 Mini for objective decision-making
*   **Scalable Architecture**: Easy to add more data sources and analysis types
*   **Real-time Alerts**: Immediate notification of significant market events

GitHub Repository[â€‹](#github-repository "Direct link to GitHub Repository")
---------------------------------------------------------------------------

For the complete code, visit the [DeFi AI Agent Repository](https://github.com/RoyceBraden/DeFI-Agent-Starter/tree/main).

note

**Note:** You can learn more about ASI1 Mini APIs [**here**](https://docs.asi1.ai/docs/) and about uAgents [**here**](https://innovationlab.fetch.ai/resources/docs/agent-creation/uagent-creation).</content>
</page>

<page>
  <title>ASI1-mini LangChain & Tavily Search Integration Guide | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/examples/asione/asi-langchain-tavily</url>
  <content>This guide demonstrates how to integrate the ASI1-mini API with LangChain and leverage the Tavily Search tool to process search queries in a streamlined manner. The project is encapsulated in a single file that implements a custom LangChain `LLM` and integrates it with an agent chain to combine API responses with dynamic search results.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

This project showcases an integration system built on the following key components:

*   **Custom LLM Integration:**  
    Implements a custom LangChain `LLM` that sends user prompts to the ASI1-mini API using a defined JSON payload.
    
*   **Tavily Search Tool:**  
    Uses the Tavily Search API to fetch search results, which are then incorporated into the agent chain to enhance the response.
    
*   **Agent Chain Execution:**  
    Sets up an agent chain that processes search queries, calls the ASI1-mini API, and returns a combined result.
    
*   **Environment-Based Configuration:**  
    Manages API keys and sensitive configurations through environment variables loaded from a `.env` file.
    

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

Before running this project, ensure you have the following:

*   **Python:** Version 3.8 or higher.
    
*   **Required Python Packages:**
    
        pip install requests pydantic python-dotenv langchain
    
*   **Environment Variables:**
    
    *   A valid API key for ASI1. Obtain your API Key [here](https://asi1.ai/dashboard/api-keys).
    *   A valid API key for Tavily. Obtain your API Key [here](https://app.tavily.com/home#).
    
    Create a `.env` file in the project directory with the following keys: `ASI_LLM_KEY=<asi1-api_key>` `TAVILY_API_KEY=<tavily_api_key>`
    

Project Structure[â€‹](#project-structure "Direct link to Project Structure")
---------------------------------------------------------------------------

The entire integration is contained within a single file:

`ASI_Langchain.py` # Contains the custom LLM class and the search handler integration

Script Breakdown[â€‹](#script-breakdown "Direct link to Script Breakdown")
------------------------------------------------------------------------

**1\. Importing Required Libraries**

The script begins by importing the necessary modules:

*   **os**: To get environment variables.
*   **requests:** To perform HTTP requests to the ASI-1 Mini API.
*   **typing:** For getting Python types.
*   **pydantic:** To define pydantic data models required by Langchain.
*   **langchain:** Imports required by Langchain.
*   **langchain\_community:** Imports required to use the TavilySearch tool.

    import osimport requestsfrom typing import Optional, Listfrom pydantic import Fieldfrom langchain.llms.base import LLMfrom langchain_community.utilities.tavily_search import TavilySearchAPIWrapperfrom langchain.agents import initialize_agent, AgentTypefrom langchain_community.tools.tavily_search.tool import TavilySearchResultsfrom dotenv import load_dotenv

**2\. Defining the ASI-1 Mini LLM Class**

Defines a custom LangChain LLM that sends prompts to the ASI1-mini API. It supports parameters such as temperature, fun mode, and web search, and handles API responses by extracting the relevant message content.

    class ASI1MINI(LLM):    api_key: str = Field(...)    api_url: str = Field(...)    model: str = Field(default="asi1-mini")    temperature: float = Field(default=0.7)    fun_mode: bool = Field(default=False)    web_search: bool = Field(default=False)    enable_stream: bool = Field(default=False)    max_tokens: int = Field(default=1024)    @property    def _llm_type(self) -> str:        return "custom_llm"    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:        headers = {            "Authorization": f"Bearer {self.api_key}",            "Content-Type": "application/json",        }        payload = {            "model": self.model,            "messages": [{"role": "user", "content": prompt}],            "temperature": self.temperature,            "fun_mode": self.fun_mode,            "web_search": self.web_search,            "stream": self.enable_stream,            "max_tokens": self.max_tokens,        }        if stop:            payload["stop"] = stop        response = requests.post(self.api_url, headers=headers, json=payload)        response.raise_for_status()        response_data = response.json()        return (            response_data.get("choices", [{}])[0].get("message", {}).get("content", "")        )

**3\. Initializing the Agent**

The agent is defined in the `custom_search_handler` function.

    def custom_search_handler(data):    """    Uses LangChain to process a search query with the custom LLM.    Expects a JSON payload with the key "search_query" and returns the result.    """    search_query = data.get("search_query")    if not search_query:        return {"error": "Missing search query"}    custom_api_key = os.getenv("ASI_LLM_KEY")    custom_api_url = "https://api.asi1.ai/v1/chat/completions"    tavily_api_key = os.getenv("TAVILY_API_KEY")    print("1: ", custom_api_key)    print("2: ", custom_api_url)    print("3: ", tavily_api_key)    if not custom_api_key or not custom_api_url or not tavily_api_key:        return {"error": "Missing API keys"}    try:        # Initialize your custom LLM        llm = ASI1MINI(api_key=custom_api_key, api_url=custom_api_url, temperature=0.7)        # Initialize the Tavily search tool        search = TavilySearchAPIWrapper()        tavily_tool = TavilySearchResults(            api_wrapper=search, tavily_api_key=tavily_api_key        )        # Initialize the agent with your custom LLM and Tavily search tool        agent_chain = initialize_agent(            [tavily_tool],            llm,            agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,            verbose=True,        )        # Run the agent chain with the search query        result = agent_chain.invoke({"input": search_query})        return {"result": result}    except Exception as e:        return {"error": str(e)}

Running the System
------------------

**1\. Activate Your Virtual Environment:**

    source venv/bin/activate   # On Windows: venv\Scripts\activate

**2\. Run the Script:**

Sample Outputs
--------------

1.  **Factual question known by the LLM(Does not use the Tavily tool)**
    
    **Input:** `How tall is the Eiffel tower?`
    
    **Final Output:**
    
        {'result': 'The Eiffel Tower is approximately 330 meters (1,083 feet) tall, including its antennas.'}
    
2.  **Current news not known by the LLM (Uses the Tavily tool)**
    
    **Input:** `Nvidia company news?`
    
    \*\* Final Output:\*\*
    
        {'result': "Here are some recent updates on NVIDIA:\n1. **GTC 2025 Announcement**: NVIDIAâ€™s premier AI conference will take place from March 17-21, 2025, in San Jose, California, featuring advancements in agentic AI and RTX AI tools.\n2. **New Product Launch**: The NVIDIA GeForce RTX 5070 Ti, built on the Blackwell architecture, is now available, boosting generative AI content creation and creative workflows.\n3. **AI Platform Advancements**: NVIDIA has unveiled the Rubin AI platform, set for 2026, and introduced the largest publicly available AI model for genomic data using DGX Cloud.\n4. **Stock Performance**: After a 27% decline over three weeks, Nvidia stock is attempting a rebound, supported by positive analyst reports.\nFor more details, you can visit NVIDIA's official newsroom or recent financial updates."}
    

Troubleshooting
---------------

1.  **Environment Variables**

Ensure that both `ASI_LLM_KEY` and `TAVILY_API_KEY` are correctly defined in your `.env` file.

Missing or incorrect API keys will lead to errors. API Connectivity

Verify that the ASI1-mini API endpoint ([https://api.asi1.ai/v1/chat/completions](https://api.asi1.ai/v1/chat/completions)) is accessible.

Confirm that the Tavily Search API is operational and that your API key is valid.

Debugging
---------

The code includes debug print statements (e.g., printing API responses) to help trace issues with API calls or response handling. Review the console output to diagnose any problems during execution.

Benefits of This Integration
----------------------------

1.  **Seamless API Communication:**
    
    Directly integrates with the ASI1-mini API via a custom LangChain LLM.
    
2.  **Enhanced Search Capabilities:**
    
    Enriches responses by combining LLM outputs with real-time search results using Tavily Search.
    
3.  **Configurable Parameters:**
    
    Offers flexibility through parameters like temperature, fun mode, and maximum tokens.
    
4.  **Simplified Deployment:**
    
    The single-file integration simplifies setup and deployment, making it easy to incorporate into larger projects.
    

Additional Resources
--------------------

*   **ASI1-mini API Documentation**([https://docs.asi1.ai](https://docs.asi1.ai/))
*   **LangChain GitHub Repository**([https://python.langchain.com/docs/introduction/](https://python.langchain.com/docs/introduction/))
*   **Tavily Search Tool Documentation**([https://docs.tavily.com/welcome](https://docs.tavily.com/welcome))

GitHub Repository[â€‹](#github-repository "Direct link to GitHub Repository")
---------------------------------------------------------------------------

For the complete code, visit the [ASI1 Chat System Repository](https://github.com/abhifetch/ASI-1_mini_Langchain).

note

**Note:** You can learn more about ASI1 Mini APIs [**here**](https://docs.asi1.ai/docs/).</content>
</page>

<page>
  <title>Image Analysis Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/examples/chat-protocol/image-analysis-agent</url>
  <content>This guide demonstrates how to create an Image Analysis Agent that can analyze images and provide descriptions using the chat protocol. The agent is compatible with the [Agentverse Chat Interface](http://chat.agentverse.ai/) and can process images to provide detailed analysis.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

In this example, you'll learn how to build a uAgent that can:

*   Accept images through the chat protocol
*   Analyze images using GPT-4 Vision
*   Provide detailed descriptions and analysis
*   Handle various image formats and sizes

For a basic understanding of how to set up an ASI:One compatible agent, please refer to the [ASI:One Compatible Agents](https://innovationlab.fetch.ai/resources/docs/examples/chat-protocol/asi-compatible-uagents) guide first.

Message Flow[â€‹](#message-flow "Direct link to Message Flow")
------------------------------------------------------------

The communication between the User, Chat Interface, and Image Analyser Agent proceeds as follows:

1.  **User Query**
    
    *   The user submits a query along with an image through the [Chat Interface](http://chat.agentverse.ai/).
2.  **Image Upload & Query Forwarding**
    
    *   **2.1**: The Chat Interface uploads the image to the Agent Storage.
    *   **2.2**: The Chat Interface forwards the user's query with a reference to the uploaded image to the Image Analyser Agent as a `ChatMessage`.
3.  **Image Retrieval**
    
    *   The Image Analyser Agent retrieves the image from Agent Storage using the provided reference.
4.  **Image Analysis**
    
    *   **4.1**: The agent passes the query and image to the Image Analysis Function.
    *   **4.2**: The Image Analysis Function processes the image and returns a response.
5.  **Response & Acknowledgement**
    
    *   **5.1**: The agent sends the analysis result back to the Chat Interface as a `ChatMessage`.
    *   **5.2**: The agent also sends a `ChatAcknowledgement` to confirm receipt and processing of the message.
6.  **User Receives Response**
    
    *   The Chat Interface delivers the analysis result to the user.

Implementation[â€‹](#implementation "Direct link to Implementation")
------------------------------------------------------------------

In this example we will create an agent and its associated files on [Agentverse](https://agentverse.ai/) that communicate using the chat protocol with the [Chat Interface](https://agentverse.ai/) Refer to the [Hosted Agents](https://innovationlab.fetch.ai/resources/docs/agent-creation/uagent-creation#hosted-agents) section to understand the detailed steps for agent creation on Agentverse.

Create a new agent named "Image Analysis Agent" on Agentverse and create the following files:

    agent.py            # Main agent file image_analysis.py   # Image analysis functionchat_proto.py       # Chat protocol implementation for enabling text based communication 

To create a new file on Agentverse:

1.  Click on the New File icon
    
2.  Assign a name to the File
    
3.  Directory Structure
    

### 1\. Image Analysis Implementation[â€‹](#1-image-analysis-implementation "Direct link to 1. Image Analysis Implementation")

The `image_analysis.py` file implements the logic for passing both text and image inputs to Claude's vision model. It handles encoding images, constructing the appropriate request, and returning the AI-generated analysis of the image and query.

image\_analysis.py

    import jsonimport osfrom typing import Anyimport requestsCLAUDE_URL = "https://api.anthropic.com/v1/messages"MAX_TOKENS = int(os.getenv("MAX_TOKENS", "1024"))ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY", "YOUR_ANTHROPIC_API_KEY")if ANTHROPIC_API_KEY is None or ANTHROPIC_API_KEY == "YOUR_ANTHROPIC_API_KEY":    raise ValueError(        "You need to provide an API key: https://platform.openai.com/api-keys"    )MODEL_ENGINE = os.getenv("MODEL_ENGINE", "claude-3-5-haiku-latest")HEADERS = {    "x-api-key": ANTHROPIC_API_KEY,    "anthropic-version": "2023-06-01",    "content-type": "application/json",}def get_image_analysis(    content: list[dict[str, Any]], tool: dict[str, Any] | None = None) -> str | None:    processed_content = []    for item in content:        if item.get("type") == "text":            processed_content.append({"type": "text", "text": item["text"]})        elif item.get("type") == "resource":            mime_type = item["mime_type"]            if mime_type.startswith("image/"):                processed_content.append({                    "type": "image",                    "source": {                        "type": "base64",                        "media_type": mime_type,                        "data": item["contents"],                    }                })            else:                return f"Unsupported mime type: {mime_type}"    data = {        "model": MODEL_ENGINE,        "max_tokens": MAX_TOKENS,        "messages": [            {                "role": "user",                "content": processed_content,            }        ],    }    if tool:        data["tools"] = [tool]        data["tool_choice"] = {"type": "tool", "name": tool["name"]}    try:        response = requests.post(            CLAUDE_URL, headers=HEADERS, data=json.dumps(data), timeout=120        )        response.raise_for_status()    except requests.exceptions.Timeout:        return "The request timed out. Please try again."    except requests.exceptions.RequestException as e:        return f"An error occurred: {e}"    # Check if the response was successful    response_data = response.json()    # Handle error responses    if "error" in response_data:        return f"API Error: {response_data['error'].get('message', 'Unknown error')}"    if tool:        for item in response_data["content"]:            if item["type"] == "tool_use":                return item["input"]                messages = response_data["content"]    if messages:        return messages[0]["text"]    else:        return None

### 2\. Chat Protocol Integration[â€‹](#2-chat-protocol-integration "Direct link to 2. Chat Protocol Integration")

The `chat_proto.py` file is essential for enabling natural language communication between your agent and ASI1 LLM.

#### Message Flow[â€‹](#message-flow-1 "Direct link to Message Flow")

1.  **Session Initiation**
    
    *   When a user starts a chat session, the agent receives a `ChatMessage` containing a `StartSessionContent`.
    *   The agent responds with a `MetadataContent` message: `{"attachments": "true"}`. This signals to the chat UI that file attachments (such as images) are supported.
2.  **User Query**
    
    *   The user sends a query as a ChatMessage, which includes:
        *   `TextContent` (the user's question)
        *   `ResourceContent` (an image or other file attachment)
3.  **Message Processing**
    
    *   For each content item:
        *   If `TextContent`, the agent adds the text to the prompt for Claude.
        *   If `ResourceContent`, the agent downloads the image from Agent Storage and adds it to the prompt as an image input for Claude.
4.  **Image Analysis and AI Processing**
    
    *   The agent then analyses the image with the help of `image_analysis.py` and sends back a response with the analysis to the user.

To enable natural language communication with your agent add the following chat protocol in the `chat_proto.py` file created on Agentverse:

chat\_proto.py

    import osfrom datetime import datetimefrom uuid import uuid4from uagents import Context, Protocolfrom uagents_core.contrib.protocols.chat import (    ChatAcknowledgement,    ChatMessage,    MetadataContent,    ResourceContent,    StartSessionContent,    TextContent,    chat_protocol_spec,)from uagents_core.storage import ExternalStoragefrom image_analysis import get_image_analysisSTORAGE_URL = os.getenv("AGENTVERSE_URL", "https://agentverse.ai") + "/v1/storage"def create_text_chat(text: str) -> ChatMessage:    return ChatMessage(        timestamp=datetime.utcnow(),        msg_id=uuid4(),        content=[TextContent(type="text", text=text)],    )def create_metadata(metadata: dict[str, str]) -> ChatMessage:    return ChatMessage(        timestamp=datetime.utcnow(),        msg_id=uuid4(),        content=[MetadataContent(            type="metadata",            metadata=metadata,        )],    )chat_proto = Protocol(spec=chat_protocol_spec)@chat_proto.on_message(ChatMessage)async def handle_message(ctx: Context, sender: str, msg: ChatMessage):    ctx.logger.info(f"Got a message from {sender}")    await ctx.send(        sender,        ChatAcknowledgement(            timestamp=datetime.utcnow(), acknowledged_msg_id=msg.msg_id        ),    )    prompt_content = []    for item in msg.content:        if isinstance(item, StartSessionContent):            await ctx.send(sender, create_metadata({"attachments": "true"}))        elif isinstance(item, TextContent):            prompt_content.append({"text": item.text, "type": "text"})        elif isinstance(item, ResourceContent):            try:                external_storage = ExternalStorage(                    identity=ctx.agent.identity,                    storage_url=STORAGE_URL,                )                data = external_storage.download(str(item.resource_id))                prompt_content.append({                    "type": "resource",                    "mime_type": data["mime_type"],                    "contents": data["contents"],                })            except Exception as ex:                ctx.logger.error(f"Failed to download resource: {ex}")                await ctx.send(sender, create_text_chat("Failed to download resource."))        else:            ctx.logger.warning(f"Got unexpected content from {sender}")    if prompt_content:        response = get_image_analysis(prompt_content)        await ctx.send(sender, create_text_chat(response))@chat_proto.on_message(ChatAcknowledgement)async def handle_ack(ctx: Context, sender: str, msg: ChatAcknowledgement):    ctx.logger.info(        f"Got an acknowledgement from {sender} for {msg.acknowledged_msg_id}"    )

### 3\. Image Analysis Agent Setup[â€‹](#3-image-analysis-agent-setup "Direct link to 3. Image Analysis Agent Setup")

The `agent.py` file is the core of your application. Think of it as the main control center that:

*   Initialises your agent
*   Handles incoming requests

In this example, we focus on the essential setup for an image analysis agent.

**Note:** If you want to add advanced features such as rate limiting or agent health checks, you can refer to the[Football Team Agent section](https://innovationlab.fetch.ai/resources/docs/examples/chat-protocol/asi-compatible-uagents#3-football-team-agent-setup) in the ASI1 Compatible uAgent guide.

agent.py

    from uagents import Agentfrom chat_proto import chat_protoagent = Agent()#Include the chat protocol defined in the previous step to handle text and image contentsagent.include(chat_proto, publish_manifest=True)if __name__ == "__main__":    agent.run()

Adding a README to your Agent[â€‹](#adding-a-readme-to-your-agent "Direct link to Adding a README to your Agent")
---------------------------------------------------------------------------------------------------------------

1.  Go to the Overview section in the Editor.
    
2.  Click on Edit and add a good description for your Agent so that it can be easily searchable by the ASI1 LLM. Please refer the [Importance of Good Readme](https://innovationlab.fetch.ai/resources/docs/agentverse/searching#importance-of-good-readme) section for more details.
    
3.  Make sure the Agent has the right `AgentChatProtocol`.
    

Query your Agent[â€‹](#query-your-agent "Direct link to Query your Agent")
------------------------------------------------------------------------

1.  Start your Agent
    
2.  Navigate to the Overview tab and click on **Chat with Agent** to interact with the agent from the [Agentverse Chat Interface](https://chat.agentverse.ai/).
    

3.  Click on the **Attach** button to upload the image and type in your query for instance 'How many people are present in the image?'

> **Note**: Currently, the image upload feature for agents is supported via the Agentverse Chat Interface. Support for image uploads through ASI:One will be available soon.</content>
</page>

<page>
  <title>Image Generation Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/examples/chat-protocol/image-generation-agent</url>
  <content>This guide demonstrates how to create an Image Generation Agent that can generate images based on text descriptions using the chat protocol. The agent is compatible with the [Agentverse Chat Interface](http://chat.agentverse.ai/) and can process natural language requests to generate images.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

In this example, you'll learn how to build a uAgent that can:

*   Accept text descriptions through the chat protocol
*   Generate images using DALL-E 3
*   Store and manage generated images using Agent storage
*   Send generated images back to the user.

For a basic understanding of how to set up an ASI:One compatible agent, please refer to the [ASI:One Compatible Agents](https://innovationlab.fetch.ai/resources/docs/examples/chat-protocol/asi-compatible-uagents) guide first.

Message Flow[â€‹](#message-flow "Direct link to Message Flow")
------------------------------------------------------------

The communication between the User, Chat Interface, and Image Generator Agent proceeds as follows:

1.  **User Query**
    
    *   **1**: The user submits a text description of the desired image through the [Chat Interface](http://chat.agentverse.ai/).
2.  **Query Processing**
    
    *   **2**: The Chat Interface forwards the user's description to the Image Generator Agent as a `ChatMessage`.
3.  **Image Generation**
    
    *   **3.1** and **3.2**: The agent processes the text description using DALL-E 3.
    *   **4.1** and **4.2**: The generated image is uploaded to External Storage.
4.  **Response & Resource Sharing**
    
    *   **5.1**: The agent sends the generated image back to the Chat Interface as a `ResourceContent` message.
    *   **5.2**: The agent also sends a `ChatAcknowledgement` to confirm receipt and processing of the message.
5.  **User Receives Image**
    
    *   **6**: The Chat Interface displays the generated image to the user.

Implementation[â€‹](#implementation "Direct link to Implementation")
------------------------------------------------------------------

In this example, we will create an agent and its associated files on our local machine that communicate using the chat protocol. The agent will be connected to [Agentverse](https://agentverse.ai/) via **Mailbox**, refer to the [Mailbox Agents](https://innovationlab.fetch.ai/resources/docs/agent-creation/uagent-creation#mailbox-agents) section to understand the detailed steps for connecting a local agent to Agentverse.

Create a new directory named "image-generation" and create the following files:

    mkdir image-generation   #Create a directorycd image-generation      #Navigate to the directorytouch agent.py            # Main agent file touch models.py           # Image generation models and functionstouch chat_proto.py       # Chat protocol implementation for enabling text-based communication 

### 1\. Image Generation Implementation[â€‹](#1-image-generation-implementation "Direct link to 1. Image Generation Implementation")

The `models.py` file implements the logic for generating images using DALL-E 3. It handles the API connection, image generation, and response processing.

models.py

    import osfrom uagents import Modelfrom openai import OpenAI, OpenAIErrorOPENAI_API_KEY = os.getenv("OPENAI_API_KEY")         #Make sure to set your OpenAI API Key in environment variablesif OPENAI_API_KEY is None:    raise ValueError("You need to provide an OpenAI API Key.")client = OpenAI(api_key=OPENAI_API_KEY)class ImageRequest(Model):    image_description: strclass ImageResponse(Model):    image_url: strdef generate_image(prompt: str) -> str:    try:        response = client.images.generate(            model="dall-e-3",            prompt=prompt,        )    except OpenAIError as e:        return f"An error occurred: {e}"    return response.data[0].url

### 2\. Chat Protocol Integration[â€‹](#2-chat-protocol-integration "Direct link to 2. Chat Protocol Integration")

The `chat_protocol.py` file is responsible for orchestrating the entire communication and image generation process when the agent receives a user's request. Here's how it works step by step:

#### i) Receiving the Message[â€‹](#i-receiving-the-message "Direct link to i) Receiving the Message")

    @chat_proto.on_message(ChatMessage)async def handle_message(ctx: Context, sender: str, msg: ChatMessage):

The agent listens for incoming `ChatMessage` instances.

#### ii) Acknowledging Receipt[â€‹](#ii-acknowledging-receipt "Direct link to ii) Acknowledging Receipt")

    await ctx.send(    sender,    ChatAcknowledgement(timestamp=datetime.utcnow(), acknowledged_msg_id=msg.msg_id),)

Once a message is received, the agent immediately sends a `ChatAcknowledgement` back to the sender.

#### iii) Parsing Content[â€‹](#iii-parsing-content "Direct link to iii) Parsing Content")

    for item in msg.content:    if isinstance(item, StartSessionContent):        ...    elif isinstance(item, TextContent):

The message content is iterated over.

*   If it contains `StartSessionContent`, the agent logs it and waits for further input.
*   If it contains `TextContent`, it's treated as the image prompt and passed for processing.

#### iv) Image Generation via DALLÂ·E 3[â€‹](#iv-image-generation-via-dalle-3 "Direct link to iv) Image Generation via DALLÂ·E 3")

    prompt = item.textimage_url = generate_image(prompt)

*   The prompt is extracted and passed to the `generate_image()` function from `models.py` to generate an image URL using DALLÂ·E 3.

#### v) Downloading the Image[â€‹](#v-downloading-the-image "Direct link to v) Downloading the Image")

    response = requests.get(image_url)if response.status_code == 200:    image_data = response.content    content_type = response.headers.get("Content-Type", "")

*   The generated image is downloaded using a direct HTTP request.
*   If successful, the image binary and MIME type are extracted for storage.

#### vi) Uploading to Agent Storage[â€‹](#vi-uploading-to-agent-storage "Direct link to vi) Uploading to Agent Storage")

    asset_id = external_storage.create_asset(    name=str(ctx.session),    content=image_data,    mime_type=content_type)

*   The image is uploaded to the Agent's `ExternalStorage` system.
*   A unique `asset_id` is returned to identify the uploaded image.

#### vii) Permission Management[â€‹](#vii-permission-management "Direct link to vii) Permission Management")

    external_storage.set_permissions(asset_id=asset_id, agent_address=sender)

*   The agent sets viewing permissions so that only the user who requested the image can access it.

#### viii) Responding with the Image[â€‹](#viii-responding-with-the-image "Direct link to viii) Responding with the Image")

    asset_uri = f"agent-storage://{external_storage.storage_url}/{asset_id}"await ctx.send(sender, create_resource_chat(asset_id, asset_uri))

*   The agent constructs a `ResourceContent` message containing the image asset.
*   This message is sent back to the user for viewing in the chat interface.

#### Whole script[â€‹](#whole-script "Direct link to Whole script")

This agent leverages external storage to securely upload, store, and share generated images. An Agentverse API key is required for authentication and to enable interaction with the external storage. You can obtain your API key from [Agentverse](https://agentverse.ai/); for detailed instructions, please refer to the [Agentverse API Key guide](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key).

chat\_proto.py

    import base64import osimport requestsfrom uuid import uuid4from datetime import datetimefrom pydantic.v1 import UUID4from uagents import Context, Protocolfrom uagents_core.contrib.protocols.chat import (    ChatAcknowledgement,    ChatMessage,    EndSessionContent,    Resource,    ResourceContent,    StartSessionContent,    TextContent,    chat_protocol_spec,)from uagents_core.storage import ExternalStoragefrom models import generate_imageAGENTVERSE_API_KEY = os.getenv("AGENTVERSE_API_KEY")STORAGE_URL = os.getenv("AGENTVERSE_URL", "https://agentverse.ai") + "/v1/storage"if AGENTVERSE_API_KEY is None:    raise ValueError("You need to provide an API_TOKEN.")external_storage = ExternalStorage(api_token=AGENTVERSE_API_KEY, storage_url=STORAGE_URL)def create_text_chat(text: str) -> ChatMessage:    return ChatMessage(        timestamp=datetime.utcnow(),        msg_id=uuid4(),        content=[TextContent(type="text", text=text)],    )def create_end_session_chat() -> ChatMessage:    return ChatMessage(        timestamp=datetime.utcnow(),        msg_id=uuid4(),        content=[EndSessionContent(type="end-session")],    )def create_resource_chat(asset_id: str, uri: str) -> ChatMessage:    return ChatMessage(        timestamp=datetime.utcnow(),        msg_id=uuid4(),        content=[            ResourceContent(                type="resource",                resource_id=UUID4(asset_id),                resource=Resource(                    uri=uri,                    metadata={                        "mime_type": "image/png",                        "role": "generated-image"                    }                )            )        ]    )chat_proto = Protocol(spec=chat_protocol_spec)@chat_proto.on_message(ChatMessage)async def handle_message(ctx: Context, sender: str, msg: ChatMessage):    await ctx.send(        sender,        ChatAcknowledgement(timestamp=datetime.utcnow(), acknowledged_msg_id=msg.msg_id),    )    for item in msg.content:        if isinstance(item, StartSessionContent):            ctx.logger.info(f"Got a start session message from {sender}")            continue        elif isinstance(item, TextContent):            ctx.logger.info(f"Got a message from {sender}: {item.text}")            prompt = msg.content[0].text            try:                image_url = generate_image(prompt)                response = requests.get(image_url)                if response.status_code == 200:                    content_type = response.headers.get("Content-Type", "")                    image_data = response.content                                         try:                        asset_id = external_storage.create_asset(                            name=str(ctx.session),                            content=image_data,                            mime_type=content_type                        )                        ctx.logger.info(f"Asset created with ID: {asset_id}")                    except RuntimeError as err:                        ctx.logger.error(f"Asset creation failed: {err}")                    external_storage.set_permissions(asset_id=asset_id, agent_address=sender)                    ctx.logger.info(f"Asset permissions set to: {sender}")                    asset_uri = f"agent-storage://{external_storage.storage_url}/{asset_id}"                    await ctx.send(sender, create_resource_chat(asset_id, asset_uri))                else:                    ctx.logger.error("Failed to download image")                    await ctx.send(                        sender,                        create_text_chat(                            "Sorry, I couldn't process your request. Please try again later."                        ),                    )                    return            except Exception as err:                ctx.logger.error(err)                await ctx.send(                    sender,                    create_text_chat(                        "Sorry, I couldn't process your request. Please try again later."                    ),                )                return            await ctx.send(sender, create_end_session_chat())        else:            ctx.logger.info(f"Got unexpected content from {sender}")@chat_proto.on_message(ChatAcknowledgement)async def handle_ack(ctx: Context, sender: str, msg: ChatAcknowledgement):    ctx.logger.info(f"Got an acknowledgement from {sender} for {msg.acknowledged_msg_id}")

### 3\. Image Generator Agent Setup[â€‹](#3-image-generator-agent-setup "Direct link to 3. Image Generator Agent Setup")

The `agent.py` file initializes your agent and includes necessary protocols for handling user requests.

> **Note:** If you want to add advanced features such as rate limiting or agent health checks, you can refer to the[Football Team Agent section](https://innovationlab.fetch.ai/resources/docs/examples/chat-protocol/asi-compatible-uagents#3-football-team-agent-setup) in the ASI1 Compatible uAgent guide.

agent.py

    import osfrom enum import Enumfrom uagents import Agent, Context, Modelfrom uagents.experimental.quota import QuotaProtocol, RateLimitfrom uagents_core.models import ErrorMessagefrom chat_proto import chat_protofrom models import ImageRequest, ImageResponse, generate_imageAGENT_SEED = os.getenv("AGENT_SEED", "image-generator-agent-seed-phrase")AGENT_NAME = os.getenv("AGENT_NAME", "Image Generator Agent")PORT = 8000agent = Agent(    name=AGENT_NAME,    seed=AGENT_SEED,    port=PORT,    mailbox=True,)# Include protocolagent.include(chat_proto, publish_manifest=True)if __name__ == "__main__":    agent.run()

### Setting up Environment Variables[â€‹](#setting-up-environment-variables "Direct link to Setting up Environment Variables")

Make sure to set the following environment variables:

*   `OPENAI_API_KEY`: Your [OpenAI API key](https://platform.openai.com/api-keys) for DALL-E 3 access
*   `AGENTVERSE_API_KEY`: Your [Agentverse API key](https://agentverse.ai/) for storage access
*   `AGENT_SEED`: (Optional) Custom seed for your agent
*   `AGENT_NAME`: (Optional) Custom name for your agent

Adding a README to your Agent[â€‹](#adding-a-readme-to-your-agent "Direct link to Adding a README to your Agent")
---------------------------------------------------------------------------------------------------------------

1.  Start your agent and connect to Agentverse using the Agent Inspector Link in the logs. Please refer to the [Mailbox Agents](https://innovationlab.fetch.ai/resources/docs/agent-creation/uagent-creation#mailbox-agents) section to understand the detailed steps for connecting a local agent to Agentverse.

**Agent Logs**

> Click on the link, it will open a new window in your browser, click on **Connect** and then select **Mailbox**, this will connect your agent to Agentverse.

2.  Once you connect your Agent via Mailbox, click on Agent Profile and navigate to the Overview section of the Agent. Your Agent will appear under local agents on Agentverse.

2.  Click on Edit and add a good description and name for your Agent so that it can be easily searchable by the ASI1 LLM. Please refer to the [Importance of Good Readme](https://innovationlab.fetch.ai/resources/docs/agentverse/searching#importance-of-good-readme) section for more details.
    
3.  Make sure the Agent has the right `AgentChatProtocol`.
    

Query your Agent[â€‹](#query-your-agent "Direct link to Query your Agent")
------------------------------------------------------------------------

1.  Look for your agent under local agents on Agentverse.
    
2.  Navigate to the Overview tab of the agent and click on **Chat with Agent** to interact with the agent from the [Agentverse Chat Interface](https://chat.agentverse.ai/).
    

3.  Type in your image description, for example: "A serene landscape with mountains and a lake at sunset"
    
4.  The agent will generate an image based on your description and send it back through the chat interface.
    

> **Note**: Currently, the image sharing feature for agents is supported via the Agentverse Chat Interface. Support for image sharing through ASI:One will be available soon.</content>
</page>

<page>
  <title>LangGraph Adapter Example | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/examples/adapters/langgraph-adapter-example</url>
  <content>LangGraph Adapter for uAgents
-----------------------------

This example demonstrates how to integrate a **LangGraph agent** with the **uAgents ecosystem** using the uAgents Adapter package. LangGraph provides powerful orchestration capabilities for LLM applications through directed graphs.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

The LangGraph adapter allows you to:

*   Wrap LangGraph agents as uAgents for seamless communication
*   Enable LangGraph agents to participate in the Agentverse ecosystem
*   Leverage advanced orchestration for complex reasoning while maintaining uAgent compatibility

Example Implementation[â€‹](#example-implementation "Direct link to Example Implementation")
------------------------------------------------------------------------------------------

*   Create an agent with file name `agent.py`

agent.py

    import osimport timefrom dotenv import load_dotenvfrom langchain_openai import ChatOpenAIfrom langchain_community.tools.tavily_search import TavilySearchResultsfrom langgraph.prebuilt import chat_agent_executorfrom langchain_core.messages import HumanMessagefrom uagents_adapter import LangchainRegisterTool, cleanup_uagent# Load environment variablesload_dotenv()# Set your API keys - for production, use environment variables instead of hardcodingOPENAI_API_KEY = os.environ["OPENAI_API_KEY"]TAVILY_API_KEY = os.environ["TAVILY_API_KEY"]# Get API token for AgentverseAPI_TOKEN = os.environ["AGENTVERSE_API_KEY"]if not API_TOKEN:    raise ValueError("Please set AGENTVERSE_API_KEY environment variable")# Set up tools and LLMtools = [TavilySearchResults(max_results=3)]model = ChatOpenAI(temperature=0)# LangGraph-based executorapp = chat_agent_executor.create_tool_calling_executor(model, tools)# Wrap LangGraph agent into a function for UAgentdef langgraph_agent_func(query):    # Handle input if it's a dict with 'input' key    if isinstance(query, dict) and 'input' in query:        query = query['input']        messages = {"messages": [HumanMessage(content=query)]}    final = None    for output in app.stream(messages):        final = list(output.values())[0]  # Get latest    return final["messages"][-1].content if final else "No response"# Register the LangGraph agent via uAgenttool = LangchainRegisterTool()agent_info = tool.invoke(    {        "agent_obj": langgraph_agent_func,        "name": "langgraph_tavily_agent",        "port": 8080,        "description": "A LangGraph-based Tavily-powered search agent",        "api_token": API_TOKEN,        "mailbox": True    })print(f"âœ… Registered LangGraph agent: {agent_info}")# Keep the agent alivetry:    while True:        time.sleep(1)except KeyboardInterrupt:    print("ğŸ›‘ Shutting down LangGraph agent...")    cleanup_uagent("langgraph_tavily_agent")    print("âœ… Agent stopped.")

Key Components[â€‹](#key-components "Direct link to Key Components")
------------------------------------------------------------------

1.  **LangGraph Setup**:
    
    *   Creates a tool-calling executor using LangGraph's prebuilt components
    *   Configures Tavily search as the tool for retrieving information
    *   Uses OpenAI's ChatGPT for LLM capabilities
2.  **Function Wrapper**:
    
    *   Wraps the LangGraph app in a function that accepts queries
    *   Handles input format conversion
    *   Processes streaming output from LangGraph
3.  **uAgent Registration**:
    
    *   Uses UAgentRegisterTool to register the LangGraph function as a uAgent
    *   Configures a port, description, and mailbox for persistence
    *   Generates a unique address for agent communication

Sample requirements.txt[â€‹](#sample-requirementstxt "Direct link to Sample requirements.txt")
--------------------------------------------------------------------------------------------

Here's a sample `requirements.txt` file you can use for this example:

    uagents==0.22.3uagents-adapter==0.4.1langchain-openai==0.3.14langchain-community==0.3.21langgraph==0.3.31dotenv==0.9.9

Interacting with the Agent[â€‹](#interacting-with-the-agent "Direct link to Interacting with the Agent")
------------------------------------------------------------------------------------------------------

You can interact with this LangGraph agent through any uAgent using the chat protocol **or directly via the "Chat with Agent" button on its Agentverse profile**. Here's a client implementation:

agent\_client.py

    from datetime import datetimefrom uuid import uuid4from uagents import Agent, Protocol, Context#import the necessary components from the chat protocolfrom uagents_core.contrib.protocols.chat import (    ChatAcknowledgement,    ChatMessage,    TextContent,    chat_protocol_spec,)# Initialise agent2agent2 = Agent(name="client_agent",               port = 8082,               mailbox=True,               seed="client agent testing seed"               )# Initialize the chat protocolchat_proto = Protocol(spec=chat_protocol_spec)langgraph_agent_address = "agent1q0zyxrneyaury3f5c7aj67hfa5w65cykzplxkst5f5mnyf4y3em3kplxn4t" # Update with your Langgraph Agent's address# Startup Handler - Print agent details@agent2.on_event("startup")async def startup_handler(ctx: Context):    # Print agent details    ctx.logger.info(f"My name is {ctx.agent.name} and my address is {ctx.agent.address}")    # Send initial message to agent2    initial_message = ChatMessage(        timestamp=datetime.utcnow(),        msg_id=uuid4(),        content=[TextContent(type="text", text="I want to send query to tavily agent that Give me a list of latest agentic AI trends")]    )    await ctx.send(langgraph_agent_address, initial_message)# Message Handler - Process received messages and send acknowledgements@chat_proto.on_message(ChatMessage)async def handle_message(ctx: Context, sender: str, msg: ChatMessage):    for item in msg.content:        if isinstance(item, TextContent):            # Log received message            ctx.logger.info(f"Received message from {sender}: {item.text}")                        # Send acknowledgment            ack = ChatAcknowledgement(                timestamp=datetime.utcnow(),                acknowledged_msg_id=msg.msg_id            )            await ctx.send(sender, ack)            # Acknowledgement Handler - Process received acknowledgements@chat_proto.on_message(ChatAcknowledgement)async def handle_acknowledgement(ctx: Context, sender: str, msg: ChatAcknowledgement):    ctx.logger.info(f"Received acknowledgement from {sender} for message: {msg.acknowledged_msg_id}")# Include the protocol in the agent to enable the chat functionality# This allows the agent to send/receive messages and handle acknowledgements using the chat protocolagent2.include(chat_proto, publish_manifest=True)if __name__ == '__main__':    agent2.run()

### ğŸ—¨ï¸ Chatting via Agentverse UI[â€‹](#chat-with-agent-ui "Direct link to ğŸ—¨ï¸ Chatting via Agentverse UI")

If you prefer a graphical interface, follow these steps to talk to your LangGraph agent directly from the Agentverse dashboard:

1.  Open the Agentverse Studio and navigate to **Agents â†’ Local** (or search for your deployed agent by name).  
    
2.  Locate your agent (e.g., **`langgraph_tavily_agent`**) in the list and click its **profile card** to open the details page.  
    
3.  Press the **"Chat with Agent"** button at the top-right of the profile. A chat panel will appear.
4.  Type your query (for example, _"Give me a list of the latest agentic AI trends"_) and hit **Send**.  
    
5.  The agent will respond in real time; you can continue the conversation as needed.

> **Note**: The placeholder images above (`langgraph_open_local_agents.png`, `langgraph_profile_card.png`, etc.) live under `/static/img/adapters/`. Replace them with updated screenshots once available.

Why Use LangGraph with uAgents?[â€‹](#why-use-langgraph-with-uagents "Direct link to Why Use LangGraph with uAgents?")
--------------------------------------------------------------------------------------------------------------------

LangGraph offers several advantages when combined with uAgents:

*   **Advanced Orchestration**: Create complex reasoning flows using directed graphs
*   **State Management**: Handle complex multi-turn conversations with state persistence
*   **Tool Integration**: Easily connect to external services and APIs
*   **Debugging Capabilities**: Inspect and debug agent reasoning processes

By wrapping LangGraph with the uAgents adapter, you get the best of both worlds: sophisticated LLM orchestration with the decentralized communication capabilities of uAgents.

Getting Started[â€‹](#getting-started "Direct link to Getting Started")
---------------------------------------------------------------------

1.  In a directory, place both `agent.py` and `agent_client.py` shown above.
    
2.  Install required packages:
    
        pip install uagents>=0.22.3 uagents-adapter>=0.4.1 langchain-openai>=0.3.14 langchain-community>=0.3.21 langgraph>=0.3.31  dotenv>=0.9.9
    
3.  Set up your environment variables in a `.env` file:
    
        OPENAI_API_KEY=your_openai_keyTAVILY_API_KEY=your_tavily_key  AGENTVERSE_API_KEY=your_agentverse_key
    
4.  Run the LangGraph agent:
    
5.  In a separate terminal, run the client agent:
    

Expected Outputs[â€‹](#expected-outputs "Direct link to Expected Outputs")
------------------------------------------------------------------------

When running the examples, you should expect to see outputs similar to these:

### LangGraph Agent[â€‹](#langgraph-agent "Direct link to LangGraph Agent")

When running the LangGraph agent:

    (venv) Fetchs-MacBook-Pro test examples % python3 agent.py INFO:     [langgraph_tavily_agent]: Starting agent with address: agent1q0zyxrneyaury3f5c7aj67hfa5w65cykzplxkst5f5mnyf4y3em3kplxn4tINFO:     [langgraph_tavily_agent]: Agent 'langgraph_tavily_agent' started with address: agent1q0zyxrneyaury3f5c7aj67hfa5w65cykzplxkst5f5mnyf4y3em3kplxn4tINFO:     [langgraph_tavily_agent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8080&address=agent1q0zyxrneyaury3f5c7aj67hfa5w65cykzplxkst5f5mnyf4y3em3kplxn4tINFO:     [langgraph_tavily_agent]: Starting server on http://0.0.0.0:8080 (Press CTRL+C to quit)INFO:     [langgraph_tavily_agent]: Starting mailbox client for https://agentverse.aiINFO:     [langgraph_tavily_agent]: Mailbox access token acquiredINFO:     [langgraph_tavily_agent]: Received structured output response from agent1qtlpfshtlcxekgrfcpmv7m9zpajuwu7d5jfyachvpa4u3dkt6k0uwwp2lct: Hello, Tavily Agent. Could you please provide a list of the latest trends in agentic AI? I am interested in understanding how agent-based artificial intelligence is evolving and what innovations or developments stand out in this field. Thank you!INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"âœ… Registered LangGraph agent: Created uAgent 'langgraph_tavily_agent' with address agent1q0zyxrneyaury3f5c7aj67hfa5w65cykzplxkst5f5mnyf4y3em3kplxn4t on port 8080INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"INFO:     [langgraph_tavily_agent]: Got a message from agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49INFO:     [langgraph_tavily_agent]: Got a text message from agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49: I want to send query to tavily agent that Give me a list of latest agentic AI trendsINFO:     [langgraph_tavily_agent]: Sending structured output prompt to {'title': 'QueryMessage', 'type': 'object', 'properties': {'query': {'title': 'Query', 'type': 'string'}}, 'required': ['query']}INFO:     [langgraph_tavily_agent]: Sent structured output prompt to agent1qtlpfshtlcxekgrfcpmv7m9zpajuwu7d5jfyachvpa4u3dkt6k0uwwp2lctINFO:     [langgraph_tavily_agent]: Got an acknowledgement from agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49 for 451f41aa-be41-471f-bddc-276caffb7d94Connecting agent 'langgraph_tavily_agent' to Agentverse...INFO:     [mailbox]: Successfully registered as mailbox agent in AgentverseSuccessfully connected agent 'langgraph_tavily_agent' to AgentverseUpdating agent 'langgraph_tavily_agent' README on Agentverse...Successfully updated agent 'langgraph_tavily_agent' README on AgentverseINFO:     [langgraph_tavily_agent]: Received structured output response from agent1qtlpfshtlcxekgrfcpmv7m9zpajuwu7d5jfyachvpa4u3dkt6k0uwwp2lct: Subject: Request for Information on Latest Agentic AI TrendsHi Tavily Agent,I hope this message finds you well. I am reaching out to inquire about the latest trends in agentic AI technology. As this area is rapidly evolving, I am keen to stay updated on the most recent developments and innovations.Could you please provide me with a comprehensive list of the latest trends in agentic AI? I'm particularly interested in understanding how these trends might impact various industries and potential future applications.Thank you for your assistance. I look forward to your response.---INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"

### Client Agent[â€‹](#client-agent "Direct link to Client Agent")

When running the client agent:

    (venv) Fetchs-MacBook-Pro test examples % python3 agent_client.pyINFO:     [client_agent]: Starting agent with address: agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49INFO:     [client_agent]: My name is client_agent and my address is agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49INFO:     [client_agent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8082&address=agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49INFO:     [client_agent]: Starting server on http://0.0.0.0:8082 (Press CTRL+C to quit)INFO:     [client_agent]: Starting mailbox client for https://agentverse.aiINFO:     [uagents.registration]: Registration on Almanac API successfulINFO:     [uagents.registration]: Almanac contract registration is up to date!INFO:     [client_agent]: Manifest published successfully: AgentChatProtocolINFO:     [client_agent]: Mailbox access token acquiredINFO:     [client_agent]: Received message from agent1q0zyxrneyaury3f5c7aj67hfa5w65cykzplxkst5f5mnyf4y3em3kplxn4t: Here are some of the latest trends in agentic AI:1. **The Next Frontier: The Rise of Agentic AI - Adams Street Partners**   - **Link:** [The Next Frontier: The Rise of Agentic AI - Adams Street Partners](https://www.adamsstreetpartners.com/insights/the-next-frontier-the-rise-of-agentic-ai/)   - **Summary:** Several converging trends have set the stage for agentic AI, including advances in Large Language Models, improved reasoning, planning, and multistep processes.2. **7 Agentic AI Trends To Watch for 2025 - ServiceNow**   - **Link:** [7 Agentic AI Trends To Watch for 2025 - ServiceNow](https://www.servicenow.com/products/ai-agents/agentic-ai-trends.html)   - **Summary:** Explore the latest agentic AI trends shaping the future of work, from hyperautomation to decision intelligence, and how it can transform businesses.3. **Agentic AI: Three themes to watch for 2025 - Constellation Research**   - **Link:** [Agentic AI: Three themes to watch for 2025 - Constellation Research](https://www.constellationr.com/blog-news/insights/agentic-ai-three-themes-watch-2025)   - **Summary:** This article discusses three themes to watch in agentic AI for 2025, including horizontal approaches vs. platform-specific strategies and the proliferation of agentic AI launches by various vendors.These sources provide insights into the evolving landscape of agentic AI and the key trends that are shaping the future of this field.INFO:     [client_agent]: Received acknowledgement from agent1q0zyxrneyaury3f5c7aj67hfa5w65cykzplxkst5f5mnyf4y3em3kplxn4t for message: 1cdda4bd-4597-42a9-b6f1-13c6ca67a0eaINFO:     [client_agent]: Received message from agent1q0zyxrneyaury3f5c7aj67hfa5w65cykzplxkst5f5mnyf4y3em3kplxn4t: ### Latest Trends in Agentic AI Technology:1. **[7 Agentic AI Trends To Watch for 2025 - ServiceNow](https://www.servicenow.com/products/ai-agents/agentic-ai-trends.html)**   - Explore the latest agentic AI trends shaping the future of work, from hyperautomation to decision intelligence, and how it can transform your business.2. **[Agentic AI: Three themes to watch for 2025 - Constellation Research](https://www.constellationr.com/blog-news/insights/agentic-ai-three-themes-watch-2025)**   - Three themes to watch in agentic AI, including horizontal approaches vs. platform-specific trends and the proliferation of agentic AI platforms across various vendors.3. **[The Top Customer Service Trends and Technologies for 2025](https://www.destinationcrm.com/Articles/Editorial/Magazine-Features/The-Top-Customer-Service-Trends-and-Technologies-for-2025-Agentic-AI-Is-Poised-to-Remake-Self-Service-168751.aspx)**   - Agentic AI is poised to remake self-service in customer service, with predictions that by 2030, 50% of service requests will be initiated by machine customers powered by agentic AI systems.4. **[Future Trends in Agentic AI Development: What's Next for Intelligent Automation](https://www.imbrace.co/future-trends-in-agentic-ai-development-whats-next-for-intelligent-automation/)**   - Trends include providing clear insights into decision-making, ensuring alignment with ethical guidelines, expanded applications across industries like logistics and healthcare, and the integration of Explainable AI (XAI).5. **[5 Reasons Why Agentic AI Will Transform Industries by 2030](https://hyperight.com/5-reasons-why-agentic-ai-will-transform-industries-by-2030/)**   - Agentic AI is expected to enhance productivity and efficiency, reshape industries by 2030, and be incorporated into 33% of enterprise software applications by 2028.6. **[Agentic AI Trends - What to expect in the near future - Atera](https://www.atera.com/blog/agentic-ai-trends/)**   - Agentic AI is set to revolutionize customer service, with researchers predicting a significant impact on customer service operations.These trends highlight the advancements and potential impacts of agentic AI technology across various industries and applications.

Try different queries to see how the LangGraph agent processes them and returns search-enhanced responses through the uAgents ecosystem!</content>
</page>

<page>
  <title>Solana Wallet Balance Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/examples/chat-protocol/solana-wallet-agent</url>
  <content>This guide demonstrates how to create a Solana Wallet Balance Agent that can check wallet balances using the Solana RPC API. The agent is compatible with ASI1 LLM and can process natural language queries about Solana wallet balances.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

The Solana Wallet Balance Agent allows users to query wallet balances using natural language. It uses the Solana RPC API to fetch real-time balance information and provides formatted responses with both SOL and lamports values.

Message Flow[â€‹](#message-flow "Direct link to Message Flow")
------------------------------------------------------------

The communication flow between ASI1 LLM, the Solana Wallet Agent, and OpenAI Agent follows this sequence:

1.  **Query Initiation (1.1)**
    
    *   ASI1 LLM sends a natural language query (e.g., "What's the balance of wallet address AtTjQKXo1CYTa2MuxPARtr382ZyhPU5YX4wMMpvaa1oy") as a `ChatMessage` to the Solana Wallet Agent.
2.  **Parameter Extraction (2, 3)**
    
    *   The Solana Wallet Agent forwards the query to OpenAI Agent for parameter extraction
    *   OpenAI Agent processes the natural language and extracts the wallet address
    *   The address is returned in a Pydantic Model format as `StructuredOutputResponse`
3.  **Balance Query (4, 5)**
    
    *   The Solana Wallet Agent calls the `get_balance_from_address` function with the extracted address
    *   The function queries the Solana RPC API and returns the balance information
4.  **Agent Response (6.1)**
    
    *   The Solana Wallet Agent sends the formatted response back as a `ChatMessage` to ASI1 LLM
5.  **Message Acknowledgements (1.2, 6.2)**
    
    *   Each message is acknowledged using `ChatAcknowledgement`

Here's a visual representation of the flow:

Implementation[â€‹](#implementation "Direct link to Implementation")
------------------------------------------------------------------

In this example we will create an agent and its associated files on [Agentverse](https://agentverse.ai/) that communicate using the chat protocol with the ASI1 LLM. Refer to the [Hosted Agents](https://innovationlab.fetch.ai/resources/docs/agent-creation/uagent-creation#hosted-agents) section to understand the detailed steps for agent creation on Agentverse.

Create a new agent named "SolanaWalletAgent" on Agentverse and create the following files:

    agent.py            # Main agent filesolana_service.py   # Solana RPC API integrationchat_proto.py       # Chat protocol implementation

To create a new file on Agentverse:

1.  Click on the New File icon

2.  Assign a name to the File

3.  Directory Structure

### 1\. Solana Service Implementation[â€‹](#1-solana-service-implementation "Direct link to 1. Solana Service Implementation")

The `solana_service.py` file handles the interaction with the Solana RPC API:

solana\_service.py

    import osimport loggingimport requestsimport jsonfrom uagents import Model, Field# Configure logginglogging.basicConfig(level=logging.INFO)logger = logging.getLogger(__name__)# Solana RPC endpointSOLANA_RPC_URL = "https://api.mainnet-beta.solana.com"class SolanaRequest(Model):    address: str = Field(        description="Solana wallet address to check",    )class SolanaResponse(Model):    balance: str = Field(        description="Formatted Solana wallet balance",    )async def get_balance_from_address(address: str) -> str:    """    Get the balance for a Solana address using the Solana RPC API        Args:        address: Solana wallet address            Returns:        Formatted balance string    """    try:        logger.info(f"Getting balance for address: {address}")                # Prepare the request payload        payload = {            "jsonrpc": "2.0",            "id": 1,            "method": "getBalance",            "params": [address]        }                # Set headers        headers = {            "Content-Type": "application/json"        }                # Make the API request        response = requests.post(SOLANA_RPC_URL, headers=headers, json=payload)        response.raise_for_status()                # Parse the response        result = response.json()                if "error" in result:            error_msg = f"Error: {result['error']['message']}"            logger.error(error_msg)            return error_msg                    if "result" in result and "value" in result["result"]:            # Convert lamports to SOL (1 SOL = 1,000,000,000 lamports)            lamports = result["result"]["value"]            sol_balance = lamports / 1_000_000_000                        # Format the result            result_str = f"{sol_balance:.9f} SOL ({lamports} lamports)"            logger.info(f"Balance for {address}: {result_str}")            return result_str        else:            error_msg = "No balance information found"            logger.error(error_msg)            return error_msg                except requests.exceptions.RequestException as e:        error_msg = f"Request error: {str(e)}"        logger.error(error_msg)        return error_msg    except json.JSONDecodeError as e:        error_msg = f"JSON decode error: {str(e)}"        logger.error(error_msg)        return error_msg    except Exception as e:        error_msg = f"Unexpected error: {str(e)}"        logger.error(error_msg)        return error_msg

### 2\. Chat Protocol Integration[â€‹](#2-chat-protocol-integration "Direct link to 2. Chat Protocol Integration")

The `chat_proto.py` file is essential for enabling natural language communication between your agent and ASI1 LLM.

#### LLM Integration[â€‹](#llm-integration "Direct link to LLM Integration")

To extract the important parameters from a user query passed by the LLM, you can use any of the following LLMs that support structured output:

*   OpenAI Agent: `agent1q0h70caed8ax769shpemapzkyk65uscw4xwk6dc4t3emvp5jdcvqs9xs32y`
*   Claude.ai Agent: `agent1qvk7q2av3e2y5gf5s90nfzkc8a48q3wdqeevwrtgqfdl0k78rspd6f2l4dx`

> **Note**: To ensure fair usage, each agent is limited to 6 requests per hour. Please implement appropriate rate limiting in your application.

#### Message Flow[â€‹](#message-flow-1 "Direct link to Message Flow")

1.  When a user sends a query (e.g., "What's the balance of wallet address AtTjQKXo1CYTa2MuxPARtr382ZyhPU5YX4wMMpvaa1oy"):
    
    *   The `ChatMessage` handler receives the message
    *   Acknowledges receipt to the sender
    *   Forwards the query to the chosen LLM for processing
2.  The LLM processes the query and returns a `StructuredOutputResponse`:
    
    *   Extracts relevant parameters (e.g., address="AtTjQKXo1CYTa2MuxPARtr382ZyhPU5YX4wMMpvaa1oy")
    *   Returns data in the format specified by your agent's schema
3.  Your agent processes the structured data:
    
    *   Calls appropriate functions (e.g., `get_balance_from_address`)
    *   Formats the response and sends it back.

#### Customizing the Handlers[â€‹](#customizing-the-handlers "Direct link to Customizing the Handlers")

When implementing a new agent for a different use case, you'll need to:

*   Update the schema in `StructuredOutputPrompt` to match your agent's data model
*   Modify the response handling in `handle_structured_output_response` function for your use case.

To enable natural language communication with your agent add the following chat protocol in the `chat_proto.py` file created on Agentverse:

chat\_proto.py

    from datetime import datetimefrom uuid import uuid4from typing import Anyfrom uagents import Context, Model, Protocol# Import the necessary components of the chat protocolfrom uagents_core.contrib.protocols.chat import (    ChatAcknowledgement,    ChatMessage,    EndSessionContent,    StartSessionContent,    TextContent,    chat_protocol_spec,)from solana_service import get_balance_from_address, SolanaRequest# AI Agent Address for structured output processingAI_AGENT_ADDRESS = 'agent1q0h70caed8ax769shpemapzkyk65uscw4xwk6dc4t3emvp5jdcvqs9xs32y'if not AI_AGENT_ADDRESS:    raise ValueError("AI_AGENT_ADDRESS not set")def create_text_chat(text: str, end_session: bool = True) -> ChatMessage:    content = [TextContent(type="text", text=text)]    if end_session:        content.append(EndSessionContent(type="end-session"))    return ChatMessage(        timestamp=datetime.utcnow(),        msg_id=uuid4(),        content=content,    )chat_proto = Protocol(spec=chat_protocol_spec)struct_output_client_proto = Protocol(    name="StructuredOutputClientProtocol", version="0.1.0")class StructuredOutputPrompt(Model):    prompt: str    output_schema: dict[str, Any]class StructuredOutputResponse(Model):    output: dict[str, Any]@chat_proto.on_message(ChatMessage)async def handle_message(ctx: Context, sender: str, msg: ChatMessage):    ctx.logger.info(f"Got a message from {sender}: {msg}")    ctx.storage.set(str(ctx.session), sender)    await ctx.send(        sender,        ChatAcknowledgement(timestamp=datetime.utcnow(), acknowledged_msg_id=msg.msg_id),    )    for item in msg.content:        if isinstance(item, StartSessionContent):            ctx.logger.info(f"Got a start session message from {sender}")            continue        elif isinstance(item, TextContent):            ctx.logger.info(f"Got a message from {sender}: {item.text}")            ctx.storage.set(str(ctx.session), sender)            await ctx.send(                AI_AGENT_ADDRESS,                StructuredOutputPrompt(                    prompt=item.text, output_schema=SolanaRequest.schema()                ),            )        else:            ctx.logger.info(f"Got unexpected content from {sender}")@chat_proto.on_message(ChatAcknowledgement)async def handle_ack(ctx: Context, sender: str, msg: ChatAcknowledgement):    ctx.logger.info(        f"Got an acknowledgement from {sender} for {msg.acknowledged_msg_id}"    )@struct_output_client_proto.on_message(StructuredOutputResponse)async def handle_structured_output_response(    ctx: Context, sender: str, msg: StructuredOutputResponse):    session_sender = ctx.storage.get(str(ctx.session))    if session_sender is None:        ctx.logger.error(            "Discarding message because no session sender found in storage"        )        return    if "<UNKNOWN>" in str(msg.output):        await ctx.send(            session_sender,            create_text_chat(                "Sorry, I couldn't process your request. Please include a valid Solana wallet address."            ),        )        return    try:        # Parse the structured output to get the address        wallet_request = SolanaRequest.parse_obj(msg.output)        address = wallet_request.address                if not address:            await ctx.send(                session_sender,                create_text_chat(                    "Sorry, I couldn't find a valid Solana wallet address in your query."                ),            )            return                # Get the balance for this address        balance = await get_balance_from_address(address)                # Create a nicely formatted response        response_text = f"Wallet Balance for `{address}`:\n{balance}\n\n[View on Solana Explorer](https://explorer.solana.com/address/{address})"                # Send the response back to the user        await ctx.send(session_sender, create_text_chat(response_text))            except Exception as err:        ctx.logger.error(err)        await ctx.send(            session_sender,            create_text_chat(                "Sorry, I couldn't check the wallet balance. Please try again later."            ),        )        return

### 3\. Solana Wallet Agent Setup[â€‹](#3-solana-wallet-agent-setup "Direct link to 3. Solana Wallet Agent Setup")

The `agent.py` file is the core of your application. Think of it as the main control center that:

*   Sets up your agent
*   Handles incoming requests
*   Manages rate limiting
*   Monitors the agent's health

Let's break down each component:

#### Basic Setup[â€‹](#basic-setup "Direct link to Basic Setup")

    from uagents import Agent, Context, Modelfrom uagents.experimental.quota import QuotaProtocol, RateLimitagent = Agent()  # Create a new agent instance

Import the necessary packages and initialise your agent.

#### Rate Limiting Setup[â€‹](#rate-limiting-setup "Direct link to Rate Limiting Setup")

    proto = QuotaProtocol(    storage_reference=agent.storage,    name="Solana-Wallet-Protocol",    version="0.1.0",    default_rate_limit=RateLimit(window_size_minutes=60, max_requests=30),)

To ensure fair usage and prevent API abuse, we recommend implementing the QuotaProtocol. While optional, this protocol helps manage service usage by limiting requests. In this example, we've set a limit of 30 requests per hour per user, but you can adjust this threshold based on your agent's functionality. This helps maintain service reliability and protects your agent from excessive requests.

#### Request Handler[â€‹](#request-handler "Direct link to Request Handler")

    @proto.on_message(    SolanaRequest, replies={SolanaResponse, ErrorMessage})async def handle_request(ctx: Context, sender: str, msg: SolanaRequest):    ctx.logger.info(f"Received wallet balance request for address: {msg.address}")    try:        balance = await get_balance_from_address(msg.address)        ctx.logger.info(f"Successfully fetched wallet balance for {msg.address}")        await ctx.send(sender, SolanaResponse(balance=balance))    except Exception as err:        ctx.logger.error(err)        await ctx.send(sender, ErrorMessage(error=str(err)))

This message handler allows your agent to receive direct requests from other agents in a structured format. While we're using it with ASI1 LLM in this example, it's versatile enough to work with agents that don't have the chat protocol enabled. This makes it particularly useful in multi-agent systems, where other agents can directly request information directly.

#### Health Monitoring[â€‹](#health-monitoring "Direct link to Health Monitoring")

    ### Health check related codedef agent_is_healthy() -> bool:    """    Implement the actual health check logic here.    For example, check if the agent can connect to the Solana RPC API.    """    try:        import asyncio        asyncio.run(get_balance_from_address("AtTjQKXo1CYTa2MuxPARtr382ZyhPU5YX4wMMpvaa1oy"))        return True    except Exception:        return Falseclass HealthCheck(Model):    passclass HealthStatus(str, Enum):    HEALTHY = "healthy"    UNHEALTHY = "unhealthy"class AgentHealth(Model):    agent_name: str    status: HealthStatushealth_protocol = QuotaProtocol(    storage_reference=agent.storage, name="HealthProtocol", version="0.1.0")@health_protocol.on_message(HealthCheck, replies={AgentHealth})async def handle_health_check(ctx: Context, sender: str, msg: HealthCheck):    status = HealthStatus.UNHEALTHY    try:        if agent_is_healthy():            status = HealthStatus.HEALTHY    except Exception as err:        ctx.logger.error(err)    finally:        await ctx.send(sender, AgentHealth(agent_name="solana_wallet_agent", status=status))

The HealthProtocol, while optional, provides a periodic health check system to monitor your agent's performance and reliability. It tests the agent's ability to fetch information about a wallet balance and verifies that all components are working correctly. This monitoring helps quickly identify and address any issues that might affect your agent's service, ensuring reliable operation for your users.

#### Protocol Registration[â€‹](#protocol-registration "Direct link to Protocol Registration")

    agent.include(proto, publish_manifest=True)agent.include(health_protocol, publish_manifest=True)agent.include(chat_proto, publish_manifest=True)agent.include(struct_output_client_proto, publish_manifest=True)

This registers all the necessary protocols with your agent:

*   Main protocol for handling wallet balance requests
*   Health monitoring protocol
*   Chat protocol for natural language communication
*   Structured output protocol for formatted responses

Here's the complete implementation:

agent.py

    import osfrom enum import Enumfrom uagents import Agent, Context, Modelfrom uagents.experimental.quota import QuotaProtocol, RateLimitfrom uagents_core.models import ErrorMessagefrom chat_proto import chat_proto, struct_output_client_protofrom solana_service import get_balance_from_address, SolanaRequest, SolanaResponseagent = Agent()proto = QuotaProtocol(    storage_reference=agent.storage,    name="Solana-Wallet-Protocol",    version="0.1.0",    default_rate_limit=RateLimit(window_size_minutes=60, max_requests=30),)@proto.on_message(    SolanaRequest, replies={SolanaResponse, ErrorMessage})async def handle_request(ctx: Context, sender: str, msg: SolanaRequest):    ctx.logger.info(f"Received wallet balance request for address: {msg.address}")    try:        balance = await get_balance_from_address(msg.address)        ctx.logger.info(f"Successfully fetched wallet balance for {msg.address}")        await ctx.send(sender, SolanaResponse(balance=balance))    except Exception as err:        ctx.logger.error(err)        await ctx.send(sender, ErrorMessage(error=str(err)))agent.include(proto, publish_manifest=True)### Health check related codedef agent_is_healthy() -> bool:    """    Implement the actual health check logic here.    For example, check if the agent can connect to the Solana RPC API.    """    try:        import asyncio        asyncio.run(get_balance_from_address("AtTjQKXo1CYTa2MuxPARtr382ZyhPU5YX4wMMpvaa1oy"))        return True    except Exception:        return Falseclass HealthCheck(Model):    passclass HealthStatus(str, Enum):    HEALTHY = "healthy"    UNHEALTHY = "unhealthy"class AgentHealth(Model):    agent_name: str    status: HealthStatushealth_protocol = QuotaProtocol(    storage_reference=agent.storage, name="HealthProtocol", version="0.1.0")@health_protocol.on_message(HealthCheck, replies={AgentHealth})async def handle_health_check(ctx: Context, sender: str, msg: HealthCheck):    status = HealthStatus.UNHEALTHY    try:        if agent_is_healthy():            status = HealthStatus.HEALTHY    except Exception as err:        ctx.logger.error(err)    finally:        await ctx.send(sender, AgentHealth(agent_name="solana_wallet_agent", status=status))agent.include(health_protocol, publish_manifest=True)agent.include(chat_proto, publish_manifest=True)agent.include(struct_output_client_proto, publish_manifest=True)if __name__ == "__main__":    agent.run()

Adding a README to your Agent[â€‹](#adding-a-readme-to-your-agent "Direct link to Adding a README to your Agent")
---------------------------------------------------------------------------------------------------------------

1.  Go to the Overview section in the Editor.
    
2.  Click on Edit and add a good description for your Agent so that it can be easily searchable by the ASI1 LLM. Please refer the [Importance of Good Readme](https://innovationlab.fetch.ai/resources/docs/agentverse/searching#importance-of-good-readme) section for more details.
    
3.  Make sure the Agent has the right `AgentChatProtocol`.
    

Query your Agent from ASI1 LLM[â€‹](#query-your-agent-from-asi1-llm "Direct link to Query your Agent from ASI1 LLM")
------------------------------------------------------------------------------------------------------------------

1.  Login to the [ASI1 LLM](https://asi1.ai/), either using your Google Account or the ASI1 Wallet and Start a New Chat.
    
2.  Toggle the "Agents" switch to enable ASI1 to connect with Agents on Agentverse.
    

3.  Type in a query to ask your Agent for instance 'What's the balance of wallet address 6wFKPxNToSnggrZr4P4s1r4zRxuJX2nSA7iTdQDPpgHc'.

> **Note**: The ASI1 LLM may not always select your agent for answering the query as it is designed to pick the best agent for a task based on a number of parameters.</content>
</page>

<page>
  <title>Frontend Web App Integration | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/examples/integrations/frontend-integration</url>
  <content>Build a Frontend Web Application with uAgents and Open Food Facts API
---------------------------------------------------------------------

This guide demonstrates how to create a complete web application that integrates uAgents with external APIs using a Flask frontend. We'll build a food product discovery system using the Open Food Facts API.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

The Frontend Integration example provides:

*   **Two Specialized uAgents** - Search Agent and Info Agent
*   **REST API Endpoints** using uAgents framework
*   **External API Integration** with Open Food Facts
*   **Modern Web Interface** built with Flask and HTML/CSS/JavaScript
*   **Real-time Health Monitoring** of all services

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

Before you begin, ensure you have:

*   **Python 3.11+** installed
*   **Basic knowledge** of Flask and web development
*   **Understanding** of REST APIs and HTTP requests

Installation[â€‹](#installation "Direct link to Installation")
------------------------------------------------------------

### 1\. Clone the Complete Example[â€‹](#1-clone-the-complete-example "Direct link to 1. Clone the Complete Example")

    git clone https://github.com/fetchai/innovation-lab-examples.gitcd innovation-lab-examples/frontend-integration

### 2\. Set Up Virtual Environment[â€‹](#2-set-up-virtual-environment "Direct link to 2. Set Up Virtual Environment")

    # Create virtual environmentpython3 -m venv venv# Activate virtual environment# On macOS/Linux:source venv/bin/activate# On Windows:# venv\Scripts\activate

### 3\. Install Dependencies[â€‹](#3-install-dependencies "Direct link to 3. Install Dependencies")

    pip install -r requirements.txt

Architecture Overview[â€‹](#architecture-overview "Direct link to Architecture Overview")
---------------------------------------------------------------------------------------

### Two Specialized uAgents[â€‹](#two-specialized-uagents "Direct link to Two Specialized uAgents")

Our system consists of two specialized microservices:

1.  **Search Agent** (Port 8001): Handles product search queries
2.  **Info Agent** (Port 8002): Retrieves detailed product information

### Frontend Application[â€‹](#frontend-application "Direct link to Frontend Application")

*   **Flask Web App** (Port 5000): Modern web interface to interact with agents

Quick Start[â€‹](#quick-start "Direct link to Quick Start")
---------------------------------------------------------

### 1\. Start Search Agent[â€‹](#1-start-search-agent "Direct link to 1. Start Search Agent")

**Terminal 1:**

    source venv/bin/activate  # Activate venv if not already activepython3 product_search_agent.py

### 2\. Start Info Agent[â€‹](#2-start-info-agent "Direct link to 2. Start Info Agent")

**Terminal 2:**

    source venv/bin/activate  # Activate venv if not already activepython3 product_info_agent.py

### 3\. Start Frontend[â€‹](#3-start-frontend "Direct link to 3. Start Frontend")

**Terminal 3:**

    source venv/bin/activate  # Activate venv if not already activepython3 frontend_app.py

### 4\. Access the Web Interface[â€‹](#4-access-the-web-interface "Direct link to 4. Access the Web Interface")

Open your browser to: [http://127.0.0.1:5000](http://127.0.0.1:5000/)

Agent Implementation Details[â€‹](#agent-implementation-details "Direct link to Agent Implementation Details")
------------------------------------------------------------------------------------------------------------

### Search Agent (Port 8001)[â€‹](#search-agent-port-8001 "Direct link to Search Agent (Port 8001)")

The search agent handles product discovery using natural language queries:

    from uagents import Agent, Context, Modelimport openfoodfactsclass SearchRequest(Model):    query: strclass ProductInfo(Model):    code: str    product_name: str    brands: str    categories: str    image_url: strclass SearchResponse(Model):    query: str    count: int    products: List[ProductInfo]    error: Optional[str] = Nonesearch_agent = Agent(    name="product_search_agent",    port=8001,    endpoint=["http://127.0.0.1:8001/submit"],)@search_agent.on_rest_post("/search", SearchRequest, SearchResponse)async def search_products(ctx: Context, req: SearchRequest) -> SearchResponse:    try:        query = req.query        ctx.logger.info(f"Searching for products with query: {query}")                # Search products using Open Food Facts API        results = api.product.text_search(query, page_size=10)                # Extract relevant information        products = []        for product in results.get("products", [])[:10]:            product_info = ProductInfo(                code=product.get("code", "N/A"),                product_name=product.get("product_name", "N/A"),                brands=product.get("brands", "N/A"),                categories=product.get("categories", "N/A"),                image_url=product.get("image_url", "")            )            products.append(product_info)                return SearchResponse(            query=query,            count=results.get("count", 0),            products=products        )    except Exception as e:        return SearchResponse(            query=req.query,            count=0,            products=[],            error=f"Failed to search products: {str(e)}"        )

### Info Agent (Port 8002)[â€‹](#info-agent-port-8002 "Direct link to Info Agent (Port 8002)")

The info agent provides detailed product information using exact barcodes:

    from uagents import Agent, Context, Modelimport requestsclass ProductRequest(Model):    barcode: strclass ProductInfoResponse(Model):    barcode: str    product_name: str    brands: str    categories: str    ingredients_text: str    allergens: str    nutrition_grades: str    ecoscore_grade: str    image_url: str    countries: str    stores: str    packaging: str    quantity: str    energy_100g: str    fat_100g: str    sugars_100g: str    salt_100g: str    error: Optional[str] = Noneinfo_agent = Agent(    name="product_info_agent",    port=8002,    endpoint=["http://127.0.0.1:8002/submit"],)@info_agent.on_rest_post("/product", ProductRequest, ProductInfoResponse)async def get_product_info(ctx: Context, req: ProductRequest) -> ProductInfoResponse:    try:        barcode = req.barcode        ctx.logger.info(f"Getting product info for barcode: {barcode}")                # Use direct API call to Open Food Facts        url = f"https://world.openfoodfacts.org/api/v0/product/{barcode}.json"        headers = {            'User-Agent': 'uAgents-FoodInfo/1.0 (https://github.com/fetchai/uAgents)'        }                response = requests.get(url, headers=headers, timeout=10)        data = response.json()                if data.get('status') != 1 or "product" not in data:            return ProductInfoResponse(                barcode=barcode,                # ... other fields with "N/A"                error="Product not found"            )                product = data["product"]                # Extract comprehensive product information        return ProductInfoResponse(            barcode=barcode,            product_name=product.get("product_name", "N/A"),            brands=product.get("brands", "N/A"),            categories=product.get("categories", "N/A"),            ingredients_text=product.get("ingredients_text", "N/A"),            allergens=product.get("allergens", "N/A"),            nutrition_grades=product.get("nutrition_grades", "N/A"),            ecoscore_grade=product.get("ecoscore_grade", "N/A"),            image_url=product.get("image_url", ""),            countries=product.get("countries", "N/A"),            stores=product.get("stores", "N/A"),            packaging=product.get("packaging", "N/A"),            quantity=product.get("quantity", "N/A"),            energy_100g=str(product.get("nutriments", {}).get("energy_100g", "N/A")),            fat_100g=str(product.get("nutriments", {}).get("fat_100g", "N/A")),            sugars_100g=str(product.get("nutriments", {}).get("sugars_100g", "N/A")),            salt_100g=str(product.get("nutriments", {}).get("salt_100g", "N/A"))        )    except Exception as e:        return ProductInfoResponse(            barcode=req.barcode,            # ... other fields with "N/A"            error=f"Failed to get product info: {str(e)}"        )

Frontend Implementation[â€‹](#frontend-implementation "Direct link to Frontend Implementation")
---------------------------------------------------------------------------------------------

### Flask Backend[â€‹](#flask-backend "Direct link to Flask Backend")

The Flask application serves as a bridge between the web interface and uAgents:

    from flask import Flask, render_template, request, jsonifyimport requestsapp = Flask(__name__)# Agent endpointsAGENTS = {    "search": "http://127.0.0.1:8001",    "info": "http://127.0.0.1:8002"}@app.route('/search_products', methods=['POST'])def search_products():    """Search products via search agent"""    try:        query = request.form.get('query', '').strip()        if not query:            return jsonify({"error": "Please provide a search query"})                # Call search agent with POST request        payload = {"query": query}        response = requests.post(f"{AGENTS['search']}/search", json=payload)        response.raise_for_status()                result = response.json()                # Format the results for display        formatted_results = []        if result.get('products'):            for product in result['products'][:10]:                formatted_product = {                    'name': product.get('product_name', 'N/A'),                    'brands': product.get('brands', 'N/A'),                    'barcode': product.get('code', 'N/A'),                    'categories': product.get('categories', 'N/A'),                    'image_url': product.get('image_url', '')                }                formatted_results.append(formatted_product)                return jsonify({            "success": True,             "count": result.get('count', 0),            "query": query,            "products": formatted_results        })            except requests.RequestException as e:        return jsonify({"error": f"Failed to connect to search agent: {str(e)}"})    except Exception as e:        return jsonify({"error": f"Search failed: {str(e)}"})@app.route('/health')def health_check():    """Check health of all agents"""    health_status = {}        for agent_name, agent_url in AGENTS.items():        try:            response = requests.get(f"{agent_url}/health", timeout=5)            if response.status_code == 200:                health_data = response.json()                health_status[agent_name] = {                    "status": "healthy",                     "url": agent_url,                     "agent_info": health_data                }            else:                health_status[agent_name] = {"status": "unhealthy", "url": agent_url}        except:            health_status[agent_name] = {"status": "offline", "url": agent_url}        return jsonify(health_status)

Testing the Application[â€‹](#testing-the-application "Direct link to Testing the Application")
---------------------------------------------------------------------------------------------

### 1\. Product Search Testing[â€‹](#1-product-search-testing "Direct link to 1. Product Search Testing")

**Search Query: "chocolate"**

1.  Navigate to [http://127.0.0.1:5000](http://127.0.0.1:5000/)
2.  In the "Search Products" section, enter `chocolate` in the search field
3.  Click "Search Products" to see results

The search will return multiple chocolate products with product names, brands, categories, barcodes, and images.

### 2\. Product Details Testing[â€‹](#2-product-details-testing "Direct link to 2. Product Details Testing")

**Barcode Query: "3017624010701"**

1.  In the "Get Product Information" section, enter the barcode `3017624010701`
2.  Click "Get Product Info" to retrieve detailed information

This will display comprehensive information for Nutella including basic information, ingredients, nutrition facts, and quality scores.

### 3\. Health Status Monitoring[â€‹](#3-health-status-monitoring "Direct link to 3. Health Status Monitoring")

Click the "Check Agent Status" button to verify all services are running:

This displays the real-time status of both Search Agent (Port 8001) and Info Agent (Port 8002).

Key Features Demonstrated[â€‹](#key-features-demonstrated "Direct link to Key Features Demonstrated")
---------------------------------------------------------------------------------------------------

### 1\. Microservice Architecture[â€‹](#1-microservice-architecture "Direct link to 1. Microservice Architecture")

*   **Separation of Concerns**: Each agent has a specific responsibility
*   **Independent Scaling**: Agents can be scaled independently
*   **Fault Isolation**: Failure in one service doesn't affect others

### 2\. REST API Design[â€‹](#2-rest-api-design "Direct link to 2. REST API Design")

*   **Proper HTTP Methods**: POST for data operations, GET for health checks
*   **Pydantic Models**: Type-safe request/response validation
*   **Error Handling**: Comprehensive error responses

### 3\. Frontend Integration[â€‹](#3-frontend-integration "Direct link to 3. Frontend Integration")

*   **Ajax Requests**: Asynchronous communication with agents
*   **Real-time Updates**: Dynamic UI updates without page refresh
*   **Health Monitoring**: Live status checking of backend services

### 4\. External API Integration[â€‹](#4-external-api-integration "Direct link to 4. External API Integration")

*   **HTTP Client Usage**: Direct API calls to Open Food Facts
*   **Data Transformation**: Converting external API responses to internal models
*   **Error Handling**: Graceful handling of external API failures

GitHub Repository[â€‹](#github-repository "Direct link to GitHub Repository")
---------------------------------------------------------------------------

For the complete code and additional examples, visit the [Frontend Integration Example](https://github.com/fetchai/innovation-lab-examples/tree/main/frontend-integration) repository.

This repository includes:

*   âœ… Complete agent implementations
*   âœ… Flask web application
*   âœ… Modern responsive web interface
*   âœ… Docker configuration
*   âœ… Comprehensive documentation
*   âœ… Testing examples</content>
</page>

<page>
  <title>ASI1 Chat System | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/examples/asione/asi1-chat-system</url>
  <content>ASI1 Chat System using uAgents Framework To Communicate with HuggingFace
------------------------------------------------------------------------

This guide explains how the ASI1 Chat System facilitates real-time communication between autonomous agents and external APIs to process user queries efficiently.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

This project demonstrates how to create a multi-agent system using the uAgents framework and ASI1 Mini. The system, consisting of a server agent and a client agent, is designed to:

*   Enable interactive query handling using the ASI1 API.
*   Allow users to send queries about Hugging Face models.
*   Process responses through a server-client architecture.
*   Demonstrate agent-to-agent communication patterns.

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

Before running this project, ensure you have:

*   Python 3.11 installed.
*   uAgents library installed. If not already installed, use:
*   Requests library installed:
*   A valid API key for ASI1. Obtain your API Key [here](https://innovationlab.fetch.ai/resources/docs/next/asione/asi1-mini-getting-started#how-to-get-an-api-key).
*   Environment variable set for your ASI1 API key:
    
        export ASI1_API_KEY="your_api_key_here"
    

Server Agent Script[â€‹](#server-agent-script "Direct link to Server Agent Script")
---------------------------------------------------------------------------------

The Server Agent is responsible for receiving queries from the Client Agent, processing them using the ASI1 API, and returning the responses.

### Script Breakdown (server.py)[â€‹](#script-breakdown-serverpy "Direct link to Script Breakdown (server.py)")

#### Importing Required Libraries[â€‹](#importing-required-libraries "Direct link to Importing Required Libraries")

    import requestsimport osfrom uagents import Agent, Context, Model

#### Defining Data Models[â€‹](#defining-data-models "Direct link to Defining Data Models")

    # Request modelclass ASI1Query(Model):    query: str    sender_address: str# Response modelclass ASI1Response(Model):    response: str  # Response from ASI1 API

#### Initializing the Server Agent[â€‹](#initializing-the-server-agent "Direct link to Initializing the Server Agent")

    # Define the main agentmainAgent = Agent(    name='asi1_chat_agent',    port=5068,    endpoint='http://localhost:5068/submit',    seed='asi1_chat_seed')

#### ASI1 API Integration Function[â€‹](#asi1-api-integration-function "Direct link to ASI1 API Integration Function")

    def get_asi1_response(query: str) -> str:    """    Sends a query to ASI1 API and returns the response.    """    # Get API key from environment variable    api_key = os.environ.get("ASI1_API_KEY")    if not api_key:        return "Error: ASI1_API_KEY environment variable not set"            headers = {        "Authorization": f"Bearer {api_key}",        "Content-Type": "application/json"    }    data = {        "model": "asi1-mini",  # Select appropriate ASI1 model        "messages": [            {"role": "system", "content": "You are a helpful AI assistant."},            {"role": "user", "content": query}        ]    }    try:        response = requests.post("https://api.asi1.ai/v1/chat/completions", json=data, headers=headers)        if response.status_code == 200:            result = response.json()            if "choices" in result and len(result["choices"]) > 0:                return result["choices"][0]["message"]["content"].strip()            else:                return "ASI1 API returned an empty response."        else:            return f"ASI1 API Error: {response.status_code}, {response.text}"    except Exception as e:        return f"ASI1 API Error: {str(e)}"

#### Event Handlers[â€‹](#event-handlers "Direct link to Event Handlers")

    @mainAgent.on_event('startup')async def startup_handler(ctx: Context):    ctx.logger.info(f'Agent {ctx.agent.name} started at {ctx.agent.address}')# Handler for receiving query@mainAgent.on_message(model=ASI1Query)async def handle_query(ctx: Context, sender: str, msg: ASI1Query):    ctx.logger.info(f"Received query from {sender}: {msg.query}")    # Call ASI1 API for the response    answer = get_asi1_response(msg.query)    # Respond back with the answer from ASI1    await ctx.send(sender, ASI1Response(response=answer))

#### Running the Server Agent[â€‹](#running-the-server-agent "Direct link to Running the Server Agent")

    if __name__ == "__main__":    mainAgent.run()

Client Agent Script[â€‹](#client-agent-script "Direct link to Client Agent Script")
---------------------------------------------------------------------------------

The Client Agent acts as the user interface, collecting input from the user and sending it to the Server Agent for processing.

### Script Breakdown (client.py)[â€‹](#script-breakdown-clientpy "Direct link to Script Breakdown (client.py)")

#### Importing Required Libraries[â€‹](#importing-required-libraries-1 "Direct link to Importing Required Libraries")

    from uagents import Agent, Context, Model

#### Defining Data Models[â€‹](#defining-data-models-1 "Direct link to Defining Data Models")

    # Query model to send to the server agentclass ASI1Query(Model):    query: str    sender_address: str# Response model to receive from the server agentclass ASI1Response(Model):    response: str

#### Initializing the Client Agent[â€‹](#initializing-the-client-agent "Direct link to Initializing the Client Agent")

    # Client agent setupclientAgent = Agent(    name='asi1_client_agent',    port=5070,    endpoint='http://localhost:5070/submit',    seed='asi1_client_seed')# Server agent address (update with actual address if needed)SERVER_AGENT_ADDRESS = "agent1q0usc8uc5hxes4ckr8624ghdxpn0lvxkgex44jtfv32x2r7ymx8sgg8yt2g"  # Replace with the actual address of your server agent

#### Event Handlers[â€‹](#event-handlers-1 "Direct link to Event Handlers")

    @clientAgent.on_event('startup')async def startup_handler(ctx: Context):    ctx.logger.info(f'Client Agent {ctx.agent.name} started at {ctx.agent.address}')    # Get user input    user_query = input("Ask something: ")    # Send the query to the server agent    await ctx.send(SERVER_AGENT_ADDRESS, ASI1Query(query=user_query, sender_address=ctx.agent.address))    ctx.logger.info(f"Query sent to server agent: {user_query}")@clientAgent.on_message(model=ASI1Response)async def handle_response(ctx: Context, sender: str, msg: ASI1Response):    ctx.logger.info(f"Response received from {sender}: {msg.response}")    print(f"Response from ASI1 API: {msg.response}")

#### Running the Client Agent[â€‹](#running-the-client-agent "Direct link to Running the Client Agent")

    if __name__ == "__main__":    clientAgent.run()

Running the System[â€‹](#running-the-system "Direct link to Running the System")
------------------------------------------------------------------------------

**1\. Set Environment Variable for API Key**

Before running the agents, set the ASI1 API key as an environment variable:

    export ASI1_API_KEY="your_api_key_here"

**2\. Start the Server Agent**

Open a terminal window and run:

You should see output similar to:

    INFO:     [asi1_chat_agent]: Starting agent with address: agent1q0usc8uc5hxes4ckr8624ghdxpn0lvxkgex44jtfv32x2r7ymx8sgg8yt2gINFO:     [asi1_chat_agent]: Agent asi1_chat_agent started at agent1q0usc8uc5hxes4ckr8624ghdxpn0lvxkgex44jtfv32x2r7ymx8sgg8yt2gINFO:     [asi1_chat_agent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A5068&address=agent1q0usc8uc5hxes4ckr8624ghdxpn0lvxkgex44jtfv32x2r7ymx8sgg8yt2gINFO:     [asi1_chat_agent]: Starting server on http://0.0.0.0:5068 (Press CTRL+C to quit)INFO:     [asi1_chat_agent]: Registration on Almanac API successfulINFO:     [asi1_chat_agent]: Almanac contract registration is up to date!

**3\. Start the Client Agent**

Open a new terminal window and run:

You should see output similar to:

    INFO:     [asi1_client_agent]: Starting agent with address: agent1qdszugnhqtadhg38tgxnzlyards0qpf0mx5x4l4ms3qq9g8fadxfgxv5dfrINFO:     [asi1_client_agent]: Client Agent asi1_client_agent started at agent1qdszugnhqtadhg38tgxnzlyards0qpf0mx5x4l4ms3qq9g8fadxfgxv5dfrAsk something: 

**4\. Interact with the System**

When prompted, enter a query about Hugging Face models, for example:

    Ask something: Image Classification

The client will send this query to the server agent:

    INFO:     [asi1_client_agent]: Query sent to server agent: Image ClassificationINFO:     [asi1_client_agent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A5070&address=agent1qdszugnhqtadhg38tgxnzlyards0qpf0mx5x4l4ms3qq9g8fadxfgxv5dfrINFO:     [asi1_client_agent]: Starting server on http://0.0.0.0:5070 (Press CTRL+C to quit)INFO:     [asi1_client_agent]: Registration on Almanac API successfulINFO:     [asi1_client_agent]: Almanac contract registration is up to date!

**5\. View the Response**

The server agent will process the query and send back a response:

    INFO:     [asi1_chat_agent]: Received query from agent1qdszugnhqtadhg38tgxnzlyards0qpf0mx5x4l4ms3qq9g8fadxfgxv5dfr: Image Classification

The client agent will display the response:

    INFO:     [asi1_client_agent]: Response received from agent1q0usc8uc5hxes4ckr8624ghdxpn0lvxkgex44jtfv32x2r7ymx8sgg8yt2g: It's difficult to give a definitive "top 5" list for most downloaded and highest-rated models on Hugging Face for Image Classification as these metrics change frequently. As of my last knowledge update, these are some of the highly regarded and popular models for image classification you could find on Hugging Face:1. **ViT (Vision Transformer):** Developed by Google Research, ViT models are known for their strong performance on image classification tasks, leveraging the transformer architecture originally designed for natural language processing. They are often pre-trained on large datasets and then fine-tuned for specific image classifications.2. **ResNet (Residual Network):** Developed by Microsoft Research, various ResNet architectures (e.g., ResNet50, ResNet101, ResNet152) are widely used. ResNet introduced the concept of skip connections, allowing for the training of very deep networks and achieving excellent performance on image classification.3. **EfficientNet:** Developed by Google Research, EfficientNet models focus on optimizing accuracy and efficiency. They are known for achieving high performance with relatively fewer parameters compared to other models.4. **Inception:** Also developed by Google, Inception models (e.g., InceptionV3) utilize inception modules with multiple convolutional filters operating at different scales to capture features at various levels of detail within an image.5. **MobileNet:** Developed by Google, MobileNet models are designed specifically for mobile and embedded vision applications. They are known for their efficiency and smaller model sizes, allowing for deployment on resource-constrained devices while still maintaining reasonable accuracy for image classification.Keep in mind that popularity and download counts on Hugging Face are dynamic. I recommend checking the Hugging Face model hub directly for the most up-to-date information on downloads and ratings. You can sort models by different metrics there.

Troubleshooting Common Issues[â€‹](#troubleshooting-common-issues "Direct link to Troubleshooting Common Issues")
---------------------------------------------------------------------------------------------------------------

**API Key Not Found**

If you see the error "ASI1\_API\_KEY environment variable not set", make sure you've set the environment variable correctly:

    export ASI1_API_KEY="your_api_key_here"

**Connection Issues**

If the client agent cannot connect to the server agent:

1.  Ensure the server agent is running and note its address.
2.  Update the `SERVER_AGENT_ADDRESS` in the client.py script with the correct address.
3.  Check that both agents are running on their specified ports (5068 and 5070).

**API Response Errors**

If you receive API errors:

1.  Verify your ASI1 API key is valid and has not expired.
2.  Check your internet connection.
3.  Ensure the ASI1 API service is operational.

GitHub Repository[â€‹](#github-repository "Direct link to GitHub Repository")
---------------------------------------------------------------------------

For the complete code, visit the [ASI1 Chat System Repository](https://github.com/Atharva-Pore/AP_uAgents/tree/main/asi1_chat_system).

note

**Note:** You can learn more about ASI1 Mini APIs [**here**](https://docs.asi1.ai/docs/) and about uAgents [**here**](https://innovationlab.fetch.ai/resources/docs/next/agent-creation/uagent-creation).</content>
</page>

<page>
  <title>DeFi AI Agent Starter Guide | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/examples/asione/asi-defi-ai-agent</url>
  <content>This guide demonstrates how to create a multi-agent system for DeFi analysis using the uAgents framework and ASI1 Mini. The system helps decide whether to hold or sell a long-term crypto asset based on market data and sentiment analysis.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

This project showcases how to build a DeFi analysis system using multiple agents:

*   **Fear and Greed Index (FGI) Agent**: Fetches and analyzes market sentiment data
*   **Coin Info Agent**: Retrieves cryptocurrency market data
*   **Main Agent**: Coordinates between agents and makes trading decisions using ASI1 Mini

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

Before running this project, ensure you have:

*   Python 3.11 installed
*   uAgents library installed:
*   Required Python packages:
    
        pip install requests pydantic python-dotenv
    
*   API Keys:
    *   CoinMarketCap [API key](https://coinmarketcap.com/academy/article/register-for-coinmarketcap-api) for FGI data
    *   ASI1 [API key](https://innovationlab.fetch.ai/resources/docs/next/asione/asi1-mini-getting-started#how-to-get-an-api-key) for AI analysis

Project Structure[â€‹](#project-structure "Direct link to Project Structure")
---------------------------------------------------------------------------

Your project structure should look like this:

    defi-ai-agent/â”œâ”€â”€ .env                    # Environment variablesâ”œâ”€â”€ main.py                # Main agent scriptâ””â”€â”€ asi/    â”œâ”€â”€ __init__.py       # Empty file to make asi a package    â””â”€â”€ llm.py            # ASI1 LLM implementation

note

**Note:** `FGI Agent` and `Coin Info Agent` are Hosted on [Agentverse](https://agentverse.ai/).

FGI Agent Script[â€‹](#fgi-agent-script "Direct link to FGI Agent Script")
------------------------------------------------------------------------

The Fear and Greed Index Agent fetches market sentiment data from CoinMarketCap.

### Script Breakdown (fgi-agent/agent.py)[â€‹](#script-breakdown-fgi-agentagentpy "Direct link to Script Breakdown (fgi-agent/agent.py)")

#### Importing Required Libraries[â€‹](#importing-required-libraries "Direct link to Importing Required Libraries")

    import osfrom uagents import Agent, Context, Modelimport requestsfrom datetime import datetimefrom typing import Optional

#### Defining Data Models[â€‹](#defining-data-models "Direct link to Defining Data Models")

    class FGIRequest(Model):    limit: Optional[int] = 1class FearGreedData(Model):    value: float    value_classification: str    timestamp: strclass FGIResponse(Model):    data: list[FearGreedData]    status: str    timestamp: str

#### Initializing the FGI Agent[â€‹](#initializing-the-fgi-agent "Direct link to Initializing the FGI Agent")

#### API Integration Function[â€‹](#api-integration-function "Direct link to API Integration Function")

    def get_fear_and_greed_index(limit: int = 1) -> FGIResponse:    """Fetch Fear and Greed index data from CoinMarketCap API"""    url = "https://pro-api.coinmarketcap.com/v3/fear-and-greed/historical"    api_key = CMC_API_KEY        headers = {        "X-CMC_PRO_API_KEY": api_key    }        params = {        "limit": limit    }    response = requests.get(url, headers=headers, params=params)        if response.status_code == 200:        raw_data = response.json()        fear_greed_data = []                for entry in raw_data["data"]:            data = FearGreedData(                value=entry["value"],                value_classification=entry["value_classification"],                timestamp=entry["timestamp"]            )            fear_greed_data.append(data)                return FGIResponse(            data=fear_greed_data,            status="success",            timestamp=datetime.utcnow().isoformat()        )    else:        raise Exception(f"Error fetching data: {response.json()['status']['error_message']}")async def process_response(ctx: Context, msg: FGIRequest) -> FGIResponse:    """Process the request and return formatted response"""    fear_greed_data = get_fear_and_greed_index(msg.limit)        for entry in fear_greed_data.data:        ctx.logger.info(f"Fear and Greed Index: {entry.value}")        ctx.logger.info(f"Classification: {entry.value_classification}")        ctx.logger.info(f"Timestamp: {entry.timestamp}")        return fear_greed_data

#### Event Handlers[â€‹](#event-handlers "Direct link to Event Handlers")

    @agent.on_event("startup")async def startup(ctx: Context):    """Initialize agent with a test request"""    ctx.logger.info(f"Hello, I'm a Fear and Greed Index agent and my address is {ctx.agent.address}.")    dummy_request = FGIRequest(limit=1)    await process_response(ctx, dummy_request)@agent.on_message(model=FGIRequest)async def handle_message(ctx: Context, sender: str, msg: FGIRequest):    """Handle incoming messages requesting Fear and Greed index data"""    ctx.logger.info(f"Received message from {sender}: FGIRequest for {msg.limit} entries")        response = await process_response(ctx, msg)    await ctx.send(sender, response)        return response

note

**Note:** This is an hosted agent on [Agentverse](https://agentverse.ai/). This is the reason we have not provided `name, seed, endpoint` and `port` to the agent.

**Note:** Store `CMC_API_KEY` in the agent secret of your hosted Agent.

Coin Info Agent Script[â€‹](#coin-info-agent-script "Direct link to Coin Info Agent Script")
------------------------------------------------------------------------------------------

The Coin Info Agent fetches cryptocurrency market data from CoinGecko.

### Script Breakdown (coin-info-agent/agent.py)[â€‹](#script-breakdown-coin-info-agentagentpy "Direct link to Script Breakdown (coin-info-agent/agent.py)")

#### Importing Required Libraries[â€‹](#importing-required-libraries-1 "Direct link to Importing Required Libraries")

    import osfrom uagents import Agent, Context, Modelimport requests

#### Defining Data Models[â€‹](#defining-data-models-1 "Direct link to Defining Data Models")

    class CoinRequest(Model):    coin_id: strclass CoinResponse(Model):    name: str    symbol: str    current_price: float    market_cap: float    total_volume: float    price_change_24h: float

#### Initializing the Coin Info Agent[â€‹](#initializing-the-coin-info-agent "Direct link to Initializing the Coin Info Agent")

#### API Integration Function[â€‹](#api-integration-function-1 "Direct link to API Integration Function")

    def get_crypto_info(coin_id: str) -> CoinResponse:    """Fetch cryptocurrency information from CoinGecko API"""    url = f"https://api.coingecko.com/api/v3/coins/{coin_id}"    response = requests.get(url)        if response.status_code == 200:        data = response.json()                return CoinResponse(            name=data['name'],            symbol=data['symbol'].upper(),            current_price=data['market_data']['current_price']['usd'],            market_cap=data['market_data']['market_cap']['usd'],            total_volume=data['market_data']['total_volume']['usd'],            price_change_24h=data['market_data']['price_change_percentage_24h']        )    else:        raise Exception(f"Failed to get crypto info: {response.text}")async def process_response(ctx: Context, msg: CoinRequest) -> CoinResponse:    """Process the crypto request and return formatted response"""    crypto_data = get_crypto_info(msg.coin_id)    ctx.logger.info(f"Crypto data: {crypto_data}")    return crypto_data

#### Event Handlers[â€‹](#event-handlers-1 "Direct link to Event Handlers")

    @agent.on_event("startup")async def startup(ctx: Context):    """Initialize agent with a test request for Bitcoin data"""    ctx.logger.info(f"Hello, I'm a crypto agent and my address is {ctx.agent.address}.")@agent.on_message(model=CoinRequest)async def handle_message(ctx: Context, sender: str, msg: CoinRequest):    """Handle incoming messages requesting crypto information"""    ctx.logger.info(f"Received message from {sender}: {msg.coin_id}")        response = await process_response(ctx, msg)    await ctx.send(sender, response)        return response

note

**Note:** This is an hosted agent on [Agentverse](https://agentverse.ai/). This is the reason we have not provided `name, seed, endpoint` and `port` to the agent.

ASI1 LLM Implementation[â€‹](#asi1-llm-implementation "Direct link to ASI1 LLM Implementation")
---------------------------------------------------------------------------------------------

Create a new file called `asi/llm.py` with the following code:

    import requestsimport osfrom dotenv import load_dotenv# Load environment variables from a .env fileload_dotenv()# Retrieve the API key from environment variablesapi_key = os.getenv("ASI1_LLM_API_KEY")# ASI1-Mini LLM API endpointurl = "https://api.asi1.ai/v1/chat/completions"# Define headers for API requests, including authenticationheaders = {    "Content-Type": "application/json",    "Authorization": f"Bearer {api_key}"}def query_llm(query):    """    Queries the ASI1-Mini LLM with a given prompt and returns the model's response.    Parameters:        query (str): The input question or statement for the language model.    Returns:        str: The response from the LLM.        If an error occurs during the request, the function returns the exception object.    """    data = {        "messages": [{"role": "user", "content": query}],  # User input for the chat model        "conversationId": None,  # No conversation history tracking        "model": "asi1-mini"  # Specifies the model version to use    }    try:        # Send a POST request to the LLM API with the input query        with requests.post(url, headers=headers, json=data) as response:            output = response.json()  # Parse the JSON response            # Extract and return the generated message content            return output["choices"][0]["message"]["content"]        except requests.exceptions.RequestException as e:        # Handle and return any request-related exceptions (e.g., network errors)        return str(e)

Make sure to create the `asi` directory and include an empty `__init__.py` file to make it a Python package.

Main Agent Script[â€‹](#main-agent-script "Direct link to Main Agent Script")
---------------------------------------------------------------------------

The Main Agent coordinates between the FGI and Coin Info agents, using ASI1 Mini for decision-making.

### Script Breakdown (main.py)[â€‹](#script-breakdown-mainpy "Direct link to Script Breakdown (main.py)")

#### Importing Required Libraries[â€‹](#importing-required-libraries-2 "Direct link to Importing Required Libraries")

    from uagents import Agent, Context, Modelfrom typing import Optionalfrom asi.llm import query_llm

#### Agent Configuration and Data Models[â€‹](#agent-configuration-and-data-models "Direct link to Agent Configuration and Data Models")

    # Initialize the agent with a name and mailbox enabled for communicationagent = Agent(name="Sentiment-Based Crypto Sell Alerts Agent", mailbox=True,port = 8001)# Coin to monitorCOIN_ID = "bitcoin"# Agentverse agent addressesCOIN_AGENT = "agent1q0005tewegtmkruc9s3v2zz8p45dd68hu38zurgpwjrrj94h8sl5cz0qev5" #Update this with your Coin agent address on AgentverseFGI_AGENT = "agent1q2eln9t0c70ha0z8uz6q88mrdzsdkxfmk3zmjejecv7skf4844cpvzplp02" #Update this with your FGI agent address on Agentverse### AGENTVERSE INTERACTION CLASSES #### Request model for retrieving coin dataclass CoinRequest(Model):    coin_id: str# Response model for coin dataclass CoinResponse(Model):    name: str    symbol: str    current_price: float    market_cap: float    total_volume: float    price_change_24h: float# Request model for Fear Greed Index (FGI) dataclass FGIRequest(Model):    limit: Optional[int] = 1# Model for individual FGI data pointsclass FearGreedData(Model):    value: float    value_classification: str    timestamp: str# Response model for FGI dataclass FGIResponse(Model):    data: list[FearGreedData]    status: str    timestamp: str

#### Event Handlers[â€‹](#event-handlers-2 "Direct link to Event Handlers")

    @agent.on_event("startup")async def introduce_agent(ctx: Context):    """Introduces the agent when it starts running."""    print(f"Hello! I'm {agent.name} and my address is {agent.address}.")@agent.on_interval(period=24 * 60 * 60.0)  # Runs every 24 hoursasync def check_coin(ctx: Context):    """Requests market data for the monitored coin once a day."""    await ctx.send(COIN_AGENT, CoinRequest(coin_id=COIN_ID))@agent.on_message(model=CoinResponse)async def handle_coin_response(ctx: Context, sender: str, msg: CoinResponse):    """Handles incoming coin market data and requests FGI data if the price drop exceeds 10%."""    global market_data    market_data = msg        # Check if price has dropped by 10% or more before requesting FGI analysis    if msg.price_change_24h <= -10.0:        await ctx.send(FGI_AGENT, FGIRequest())@agent.on_message(model=FGIResponse)async def handle_fgi_response(ctx: Context, sender: str, msg: FGIResponse):    """Analyzes Fear Greed Index data and determines whether to issue a SELL alert."""    global fgi_analysis    fgi_analysis = msg        # Construct the AI prompt based on current market and sentiment analysis    prompt = f'''    Given the following information, respond with either SELL or HOLD for the coin {COIN_ID}.        Below is analysis on the Fear Greed Index:    {fgi_analysis}        Below is analysis on the coin:    {market_data}    '''        response = query_llm(prompt)  # Query ASI1 Mini for a decision        # Interpret the AI response and print decision    if "SELL" in response:        print("SELL")    else:        print("HOLD")

Running the System[â€‹](#running-the-system "Direct link to Running the System")
------------------------------------------------------------------------------

**1\. Start the FGI Agent**

Open a blank agent on AV and write your [script](#coin-info-agent-script) in it. Click on `Start Agent` button.

You should see output similar to:

    INFO: Hello, I'm a Fear and Greed Index agent and my address is agent1q2eln9t0c70ha0z8uz6q88mrdzsdkxfmk3zmjejecv7skf4844cpvzplp02

**2\. Start the Coin Info Agent**

Open another blank agent on AV and write your [script](#fgi-agent-script) in it. Click on `Start Agent` button.

You should see output similar to:

    INFO: Hello, I'm a crypto agent and my address is agent1q0005tewegtmkruc9s3v2zz8p45dd68hu38zurgpwjrrj94h8sl5cz0qev5

**3\. Start the Main Agent**

Open a terminal window and run:

You should see output similar to:

    Hello! I'm Sentiment-Based Crypto Sell Alerts Agent and my address is agent1qxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

**4\. Monitor the System**

The main agent will:

1.  Check cryptocurrency prices every 24 hours
2.  Request FGI analysis if price drops by 10% or more
3.  Use ASI1 Mini to analyze data and make SELL/HOLD recommendations

Troubleshooting Common Issues[â€‹](#troubleshooting-common-issues "Direct link to Troubleshooting Common Issues")
---------------------------------------------------------------------------------------------------------------

### API Key Issues[â€‹](#api-key-issues "Direct link to API Key Issues")

If you see API-related errors:

1.  Verify your API keys are correctly set in the `.env` files
2.  Check API rate limits and quotas
3.  Ensure the API services are operational

### Agent Communication Issues[â€‹](#agent-communication-issues "Direct link to Agent Communication Issues")

If agents aren't communicating:

1.  Verify all agents are running
2.  Check agent addresses in `main.py` match the actual addresses
3.  Ensure network connectivity

### ASI1 Mini Integration Issues[â€‹](#asi1-mini-integration-issues "Direct link to ASI1 Mini Integration Issues")

If AI analysis isn't working:

1.  Check ASI1\_API\_KEY is set correctly
2.  Verify ASI1 Mini service status
3.  Review prompt formatting in `main.py`

Benefits of DeFi AI Agent System[â€‹](#benefits-of-defi-ai-agent-system "Direct link to Benefits of DeFi AI Agent System")
------------------------------------------------------------------------------------------------------------------------

*   **Automated Monitoring**: 24/7 tracking of cryptocurrency prices
*   **Data-Driven Decisions**: Combines market data with sentiment analysis
*   **AI-Powered Analysis**: Leverages ASI1 Mini for objective decision-making
*   **Scalable Architecture**: Easy to add more data sources and analysis types
*   **Real-time Alerts**: Immediate notification of significant market events

GitHub Repository[â€‹](#github-repository "Direct link to GitHub Repository")
---------------------------------------------------------------------------

For the complete code, visit the [DeFi AI Agent Repository](https://github.com/RoyceBraden/DeFI-Agent-Starter/tree/main).

note

**Note:** You can learn more about ASI1 Mini APIs [**here**](https://docs.asi1.ai/docs/) and about uAgents [**here**](https://innovationlab.fetch.ai/resources/docs/next/agent-creation/uagent-creation).</content>
</page>

<page>
  <title>ASI1-mini LangChain & Tavily Search Integration Guide | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/examples/asione/asi-langchain-tavily</url>
  <content>This guide demonstrates how to integrate the ASI1-mini API with LangChain and leverage the Tavily Search tool to process search queries in a streamlined manner. The project is encapsulated in a single file that implements a custom LangChain `LLM` and integrates it with an agent chain to combine API responses with dynamic search results.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

This project showcases an integration system built on the following key components:

*   **Custom LLM Integration:**  
    Implements a custom LangChain `LLM` that sends user prompts to the ASI1-mini API using a defined JSON payload.
    
*   **Tavily Search Tool:**  
    Uses the Tavily Search API to fetch search results, which are then incorporated into the agent chain to enhance the response.
    
*   **Agent Chain Execution:**  
    Sets up an agent chain that processes search queries, calls the ASI1-mini API, and returns a combined result.
    
*   **Environment-Based Configuration:**  
    Manages API keys and sensitive configurations through environment variables loaded from a `.env` file.
    

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

Before running this project, ensure you have the following:

*   **Python:** Version 3.8 or higher.
    
*   **Required Python Packages:**
    
        pip install requests pydantic python-dotenv langchain
    
*   **Environment Variables:**
    
    *   A valid API key for ASI1. Obtain your API Key [here](https://asi1.ai/dashboard/api-keys).
    *   A valid API key for Tavily. Obtain your API Key [here](https://app.tavily.com/home#).
    
    Create a `.env` file in the project directory with the following keys: `ASI_LLM_KEY=<asi1-api_key>` `TAVILY_API_KEY=<tavily_api_key>`
    

Project Structure[â€‹](#project-structure "Direct link to Project Structure")
---------------------------------------------------------------------------

The entire integration is contained within a single file:

`ASI_Langchain.py` # Contains the custom LLM class and the search handler integration

Script Breakdown[â€‹](#script-breakdown "Direct link to Script Breakdown")
------------------------------------------------------------------------

**1\. Importing Required Libraries**

The script begins by importing the necessary modules:

*   **os**: To get environment variables.
*   **requests:** To perform HTTP requests to the ASI-1 Mini API.
*   **typing:** For getting Python types.
*   **pydantic:** To define pydantic data models required by Langchain.
*   **langchain:** Imports required by Langchain.
*   **langchain\_community:** Imports required to use the TavilySearch tool.

    import osimport requestsfrom typing import Optional, Listfrom pydantic import Fieldfrom langchain.llms.base import LLMfrom langchain_community.utilities.tavily_search import TavilySearchAPIWrapperfrom langchain.agents import initialize_agent, AgentTypefrom langchain_community.tools.tavily_search.tool import TavilySearchResultsfrom dotenv import load_dotenv

**2\. Defining the ASI-1 Mini LLM Class**

Defines a custom LangChain LLM that sends prompts to the ASI1-mini API. It supports parameters such as temperature, fun mode, and web search, and handles API responses by extracting the relevant message content.

    class ASI1MINI(LLM):    api_key: str = Field(...)    api_url: str = Field(...)    model: str = Field(default="asi1-mini")    temperature: float = Field(default=0.7)    fun_mode: bool = Field(default=False)    web_search: bool = Field(default=False)    enable_stream: bool = Field(default=False)    max_tokens: int = Field(default=1024)    @property    def _llm_type(self) -> str:        return "custom_llm"    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:        headers = {            "Authorization": f"Bearer {self.api_key}",            "Content-Type": "application/json",        }        payload = {            "model": self.model,            "messages": [{"role": "user", "content": prompt}],            "temperature": self.temperature,            "fun_mode": self.fun_mode,            "web_search": self.web_search,            "stream": self.enable_stream,            "max_tokens": self.max_tokens,        }        if stop:            payload["stop"] = stop        response = requests.post(self.api_url, headers=headers, json=payload)        response.raise_for_status()        response_data = response.json()        return (            response_data.get("choices", [{}])[0].get("message", {}).get("content", "")        )

**3\. Initializing the Agent**

The agent is defined in the `custom_search_handler` function.

    def custom_search_handler(data):    """    Uses LangChain to process a search query with the custom LLM.    Expects a JSON payload with the key "search_query" and returns the result.    """    search_query = data.get("search_query")    if not search_query:        return {"error": "Missing search query"}    custom_api_key = os.getenv("ASI_LLM_KEY")    custom_api_url = "https://api.asi1.ai/v1/chat/completions"    tavily_api_key = os.getenv("TAVILY_API_KEY")    print("1: ", custom_api_key)    print("2: ", custom_api_url)    print("3: ", tavily_api_key)    if not custom_api_key or not custom_api_url or not tavily_api_key:        return {"error": "Missing API keys"}    try:        # Initialize your custom LLM        llm = ASI1MINI(api_key=custom_api_key, api_url=custom_api_url, temperature=0.7)        # Initialize the Tavily search tool        search = TavilySearchAPIWrapper()        tavily_tool = TavilySearchResults(            api_wrapper=search, tavily_api_key=tavily_api_key        )        # Initialize the agent with your custom LLM and Tavily search tool        agent_chain = initialize_agent(            [tavily_tool],            llm,            agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,            verbose=True,        )        # Run the agent chain with the search query        result = agent_chain.invoke({"input": search_query})        return {"result": result}    except Exception as e:        return {"error": str(e)}

Running the System
------------------

**1\. Activate Your Virtual Environment:**

    source venv/bin/activate   # On Windows: venv\Scripts\activate

**2\. Run the Script:**

Sample Outputs
--------------

1.  **Factual question known by the LLM(Does not use the Tavily tool)**
    
    **Input:** `How tall is the Eiffel tower?`
    
    **Final Output:**
    
        {'result': 'The Eiffel Tower is approximately 330 meters (1,083 feet) tall, including its antennas.'}
    
2.  **Current news not known by the LLM (Uses the Tavily tool)**
    
    **Input:** `Nvidia company news?`
    
    \*\* Final Output:\*\*
    
        {'result': "Here are some recent updates on NVIDIA:\n1. **GTC 2025 Announcement**: NVIDIAâ€™s premier AI conference will take place from March 17-21, 2025, in San Jose, California, featuring advancements in agentic AI and RTX AI tools.\n2. **New Product Launch**: The NVIDIA GeForce RTX 5070 Ti, built on the Blackwell architecture, is now available, boosting generative AI content creation and creative workflows.\n3. **AI Platform Advancements**: NVIDIA has unveiled the Rubin AI platform, set for 2026, and introduced the largest publicly available AI model for genomic data using DGX Cloud.\n4. **Stock Performance**: After a 27% decline over three weeks, Nvidia stock is attempting a rebound, supported by positive analyst reports.\nFor more details, you can visit NVIDIA's official newsroom or recent financial updates."}
    

Troubleshooting
---------------

1.  **Environment Variables**

Ensure that both `ASI_LLM_KEY` and `TAVILY_API_KEY` are correctly defined in your `.env` file.

Missing or incorrect API keys will lead to errors. API Connectivity

Verify that the ASI1-mini API endpoint ([https://api.asi1.ai/v1/chat/completions](https://api.asi1.ai/v1/chat/completions)) is accessible.

Confirm that the Tavily Search API is operational and that your API key is valid.

Debugging
---------

The code includes debug print statements (e.g., printing API responses) to help trace issues with API calls or response handling. Review the console output to diagnose any problems during execution.

Benefits of This Integration
----------------------------

1.  **Seamless API Communication:**
    
    Directly integrates with the ASI1-mini API via a custom LangChain LLM.
    
2.  **Enhanced Search Capabilities:**
    
    Enriches responses by combining LLM outputs with real-time search results using Tavily Search.
    
3.  **Configurable Parameters:**
    
    Offers flexibility through parameters like temperature, fun mode, and maximum tokens.
    
4.  **Simplified Deployment:**
    
    The single-file integration simplifies setup and deployment, making it easy to incorporate into larger projects.
    

Additional Resources
--------------------

*   **ASI1-mini API Documentation**([https://docs.asi1.ai](https://docs.asi1.ai/))
*   **LangChain GitHub Repository**([https://python.langchain.com/docs/introduction/](https://python.langchain.com/docs/introduction/))
*   **Tavily Search Tool Documentation**([https://docs.tavily.com/welcome](https://docs.tavily.com/welcome))

GitHub Repository[â€‹](#github-repository "Direct link to GitHub Repository")
---------------------------------------------------------------------------

For the complete code, visit the [ASI1 Chat System Repository](https://github.com/abhifetch/ASI-1_mini_Langchain).

note

**Note:** You can learn more about ASI1 Mini APIs [**here**](https://docs.asi1.ai/docs/).</content>
</page>

<page>
  <title>Image Analysis Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/examples/chat-protocol/image-analysis-agent</url>
  <content>This guide demonstrates how to create an Image Analysis Agent that can analyze images and provide descriptions using the chat protocol. The agent is compatible with the [Agentverse Chat Interface](http://chat.agentverse.ai/) and can process images to provide detailed analysis.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

In this example, you'll learn how to build a uAgent that can:

*   Accept images through the chat protocol
*   Analyze images using GPT-4 Vision
*   Provide detailed descriptions and analysis
*   Handle various image formats and sizes

For a basic understanding of how to set up an ASI:One compatible agent, please refer to the [ASI:One Compatible Agents](https://innovationlab.fetch.ai/resources/docs/next/examples/chat-protocol/asi-compatible-uagents) guide first.

Message Flow[â€‹](#message-flow "Direct link to Message Flow")
------------------------------------------------------------

The communication between the User, Chat Interface, and Image Analyser Agent proceeds as follows:

1.  **User Query**
    
    *   The user submits a query along with an image through the [Chat Interface](http://chat.agentverse.ai/).
2.  **Image Upload & Query Forwarding**
    
    *   **2.1**: The Chat Interface uploads the image to the Agent Storage.
    *   **2.2**: The Chat Interface forwards the user's query with a reference to the uploaded image to the Image Analyser Agent as a `ChatMessage`.
3.  **Image Retrieval**
    
    *   The Image Analyser Agent retrieves the image from Agent Storage using the provided reference.
4.  **Image Analysis**
    
    *   **4.1**: The agent passes the query and image to the Image Analysis Function.
    *   **4.2**: The Image Analysis Function processes the image and returns a response.
5.  **Response & Acknowledgement**
    
    *   **5.1**: The agent sends the analysis result back to the Chat Interface as a `ChatMessage`.
    *   **5.2**: The agent also sends a `ChatAcknowledgement` to confirm receipt and processing of the message.
6.  **User Receives Response**
    
    *   The Chat Interface delivers the analysis result to the user.

Implementation[â€‹](#implementation "Direct link to Implementation")
------------------------------------------------------------------

In this example we will create an agent and its associated files on [Agentverse](https://agentverse.ai/) that communicate using the chat protocol with the [Chat Interface](https://agentverse.ai/) Refer to the [Hosted Agents](https://innovationlab.fetch.ai/resources/docs/agent-creation/uagent-creation#hosted-agents) section to understand the detailed steps for agent creation on Agentverse.

Create a new agent named "Image Analysis Agent" on Agentverse and create the following files:

    agent.py            # Main agent file image_analysis.py   # Image analysis functionchat_proto.py       # Chat protocol implementation for enabling text based communication 

To create a new file on Agentverse:

1.  Click on the New File icon
    
2.  Assign a name to the File
    
3.  Directory Structure
    

### 1\. Image Analysis Implementation[â€‹](#1-image-analysis-implementation "Direct link to 1. Image Analysis Implementation")

The `image_analysis.py` file implements the logic for passing both text and image inputs to Claude's vision model. It handles encoding images, constructing the appropriate request, and returning the AI-generated analysis of the image and query.

image\_analysis.py

    import jsonimport osfrom typing import Anyimport requestsCLAUDE_URL = "https://api.anthropic.com/v1/messages"MAX_TOKENS = int(os.getenv("MAX_TOKENS", "1024"))ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY", "YOUR_ANTHROPIC_API_KEY")if ANTHROPIC_API_KEY is None or ANTHROPIC_API_KEY == "YOUR_ANTHROPIC_API_KEY":    raise ValueError(        "You need to provide an API key: https://platform.openai.com/api-keys"    )MODEL_ENGINE = os.getenv("MODEL_ENGINE", "claude-3-5-haiku-latest")HEADERS = {    "x-api-key": ANTHROPIC_API_KEY,    "anthropic-version": "2023-06-01",    "content-type": "application/json",}def get_image_analysis(    content: list[dict[str, Any]], tool: dict[str, Any] | None = None) -> str | None:    processed_content = []    for item in content:        if item.get("type") == "text":            processed_content.append({"type": "text", "text": item["text"]})        elif item.get("type") == "resource":            mime_type = item["mime_type"]            if mime_type.startswith("image/"):                processed_content.append({                    "type": "image",                    "source": {                        "type": "base64",                        "media_type": mime_type,                        "data": item["contents"],                    }                })            else:                return f"Unsupported mime type: {mime_type}"    data = {        "model": MODEL_ENGINE,        "max_tokens": MAX_TOKENS,        "messages": [            {                "role": "user",                "content": processed_content,            }        ],    }    if tool:        data["tools"] = [tool]        data["tool_choice"] = {"type": "tool", "name": tool["name"]}    try:        response = requests.post(            CLAUDE_URL, headers=HEADERS, data=json.dumps(data), timeout=120        )        response.raise_for_status()    except requests.exceptions.Timeout:        return "The request timed out. Please try again."    except requests.exceptions.RequestException as e:        return f"An error occurred: {e}"    # Check if the response was successful    response_data = response.json()    # Handle error responses    if "error" in response_data:        return f"API Error: {response_data['error'].get('message', 'Unknown error')}"    if tool:        for item in response_data["content"]:            if item["type"] == "tool_use":                return item["input"]                messages = response_data["content"]    if messages:        return messages[0]["text"]    else:        return None

### 2\. Chat Protocol Integration[â€‹](#2-chat-protocol-integration "Direct link to 2. Chat Protocol Integration")

The `chat_proto.py` file is essential for enabling natural language communication between your agent and ASI1 LLM.

#### Message Flow[â€‹](#message-flow-1 "Direct link to Message Flow")

1.  **Session Initiation**
    
    *   When a user starts a chat session, the agent receives a `ChatMessage` containing a `StartSessionContent`.
    *   The agent responds with a `MetadataContent` message: `{"attachments": "true"}`. This signals to the chat UI that file attachments (such as images) are supported.
2.  **User Query**
    
    *   The user sends a query as a ChatMessage, which includes:
        *   `TextContent` (the user's question)
        *   `ResourceContent` (an image or other file attachment)
3.  **Message Processing**
    
    *   For each content item:
        *   If `TextContent`, the agent adds the text to the prompt for Claude.
        *   If `ResourceContent`, the agent downloads the image from Agent Storage and adds it to the prompt as an image input for Claude.
4.  **Image Analysis and AI Processing**
    
    *   The agent then analyses the image with the help of `image_analysis.py` and sends back a response with the analysis to the user.

To enable natural language communication with your agent add the following chat protocol in the `chat_proto.py` file created on Agentverse:

chat\_proto.py

    import osfrom datetime import datetimefrom uuid import uuid4from uagents import Context, Protocolfrom uagents_core.contrib.protocols.chat import (    ChatAcknowledgement,    ChatMessage,    MetadataContent,    ResourceContent,    StartSessionContent,    TextContent,    chat_protocol_spec,)from uagents_core.storage import ExternalStoragefrom image_analysis import get_image_analysisSTORAGE_URL = os.getenv("AGENTVERSE_URL", "https://agentverse.ai") + "/v1/storage"def create_text_chat(text: str) -> ChatMessage:    return ChatMessage(        timestamp=datetime.utcnow(),        msg_id=uuid4(),        content=[TextContent(type="text", text=text)],    )def create_metadata(metadata: dict[str, str]) -> ChatMessage:    return ChatMessage(        timestamp=datetime.utcnow(),        msg_id=uuid4(),        content=[MetadataContent(            type="metadata",            metadata=metadata,        )],    )chat_proto = Protocol(spec=chat_protocol_spec)@chat_proto.on_message(ChatMessage)async def handle_message(ctx: Context, sender: str, msg: ChatMessage):    ctx.logger.info(f"Got a message from {sender}")    await ctx.send(        sender,        ChatAcknowledgement(            timestamp=datetime.utcnow(), acknowledged_msg_id=msg.msg_id        ),    )    prompt_content = []    for item in msg.content:        if isinstance(item, StartSessionContent):            await ctx.send(sender, create_metadata({"attachments": "true"}))        elif isinstance(item, TextContent):            prompt_content.append({"text": item.text, "type": "text"})        elif isinstance(item, ResourceContent):            try:                external_storage = ExternalStorage(                    identity=ctx.agent.identity,                    storage_url=STORAGE_URL,                )                data = external_storage.download(str(item.resource_id))                prompt_content.append({                    "type": "resource",                    "mime_type": data["mime_type"],                    "contents": data["contents"],                })            except Exception as ex:                ctx.logger.error(f"Failed to download resource: {ex}")                await ctx.send(sender, create_text_chat("Failed to download resource."))        else:            ctx.logger.warning(f"Got unexpected content from {sender}")    if prompt_content:        response = get_image_analysis(prompt_content)        await ctx.send(sender, create_text_chat(response))@chat_proto.on_message(ChatAcknowledgement)async def handle_ack(ctx: Context, sender: str, msg: ChatAcknowledgement):    ctx.logger.info(        f"Got an acknowledgement from {sender} for {msg.acknowledged_msg_id}"    )

### 3\. Image Analysis Agent Setup[â€‹](#3-image-analysis-agent-setup "Direct link to 3. Image Analysis Agent Setup")

The `agent.py` file is the core of your application. Think of it as the main control center that:

*   Initialises your agent
*   Handles incoming requests

In this example, we focus on the essential setup for an image analysis agent.

**Note:** If you want to add advanced features such as rate limiting or agent health checks, you can refer to the[Football Team Agent section](https://innovationlab.fetch.ai/resources/docs/next/examples/chat-protocol/asi-compatible-uagents#3-football-team-agent-setup) in the ASI1 Compatible uAgent guide.

agent.py

    from uagents import Agentfrom chat_proto import chat_protoagent = Agent()#Include the chat protocol defined in the previous step to handle text and image contentsagent.include(chat_proto, publish_manifest=True)if __name__ == "__main__":    agent.run()

Adding a README to your Agent[â€‹](#adding-a-readme-to-your-agent "Direct link to Adding a README to your Agent")
---------------------------------------------------------------------------------------------------------------

1.  Go to the Overview section in the Editor.
    
2.  Click on Edit and add a good description for your Agent so that it can be easily searchable by the ASI1 LLM. Please refer the [Importance of Good Readme](https://innovationlab.fetch.ai/resources/docs/agentverse/searching#importance-of-good-readme) section for more details.
    
3.  Make sure the Agent has the right `AgentChatProtocol`.
    

Query your Agent[â€‹](#query-your-agent "Direct link to Query your Agent")
------------------------------------------------------------------------

1.  Start your Agent
    
2.  Navigate to the Overview tab and click on **Chat with Agent** to interact with the agent from the [Agentverse Chat Interface](https://chat.agentverse.ai/).
    

3.  Click on the **Attach** button to upload the image and type in your query for instance 'How many people are present in the image?'

> **Note**: Currently, the image upload feature for agents is supported via the Agentverse Chat Interface. Support for image uploads through ASI:One will be available soon.</content>
</page>

<page>
  <title>Image Generation Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/examples/chat-protocol/image-generation-agent</url>
  <content>This guide demonstrates how to create an Image Generation Agent that can generate images based on text descriptions using the chat protocol. The agent is compatible with the [Agentverse Chat Interface](http://chat.agentverse.ai/) and can process natural language requests to generate images.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

In this example, you'll learn how to build a uAgent that can:

*   Accept text descriptions through the chat protocol
*   Generate images using DALL-E 3
*   Store and manage generated images using Agent storage
*   Send generated images back to the user.

For a basic understanding of how to set up an ASI:One compatible agent, please refer to the [ASI:One Compatible Agents](https://innovationlab.fetch.ai/resources/docs/next/examples/chat-protocol/asi-compatible-uagents) guide first.

Message Flow[â€‹](#message-flow "Direct link to Message Flow")
------------------------------------------------------------

The communication between the User, Chat Interface, and Image Generator Agent proceeds as follows:

1.  **User Query**
    
    *   **1**: The user submits a text description of the desired image through the [Chat Interface](http://chat.agentverse.ai/).
2.  **Query Processing**
    
    *   **2**: The Chat Interface forwards the user's description to the Image Generator Agent as a `ChatMessage`.
3.  **Image Generation**
    
    *   **3.1** and **3.2**: The agent processes the text description using DALL-E 3.
    *   **4.1** and **4.2**: The generated image is uploaded to External Storage.
4.  **Response & Resource Sharing**
    
    *   **5.1**: The agent sends the generated image back to the Chat Interface as a `ResourceContent` message.
    *   **5.2**: The agent also sends a `ChatAcknowledgement` to confirm receipt and processing of the message.
5.  **User Receives Image**
    
    *   **6**: The Chat Interface displays the generated image to the user.

Implementation[â€‹](#implementation "Direct link to Implementation")
------------------------------------------------------------------

In this example, we will create an agent and its associated files on our local machine that communicate using the chat protocol. The agent will be connected to [Agentverse](https://agentverse.ai/) via **Mailbox**, refer to the [Mailbox Agents](https://innovationlab.fetch.ai/resources/docs/agent-creation/uagent-creation#mailbox-agents) section to understand the detailed steps for connecting a local agent to Agentverse.

Create a new directory named "image-generation" and create the following files:

    mkdir image-generation   #Create a directorycd image-generation      #Navigate to the directorytouch agent.py            # Main agent file touch models.py           # Image generation models and functionstouch chat_proto.py       # Chat protocol implementation for enabling text-based communication 

### 1\. Image Generation Implementation[â€‹](#1-image-generation-implementation "Direct link to 1. Image Generation Implementation")

The `models.py` file implements the logic for generating images using DALL-E 3. It handles the API connection, image generation, and response processing.

models.py

    import osfrom uagents import Modelfrom openai import OpenAI, OpenAIErrorOPENAI_API_KEY = os.getenv("OPENAI_API_KEY")         #Make sure to set your OpenAI API Key in environment variablesif OPENAI_API_KEY is None:    raise ValueError("You need to provide an OpenAI API Key.")client = OpenAI(api_key=OPENAI_API_KEY)class ImageRequest(Model):    image_description: strclass ImageResponse(Model):    image_url: strdef generate_image(prompt: str) -> str:    try:        response = client.images.generate(            model="dall-e-3",            prompt=prompt,        )    except OpenAIError as e:        return f"An error occurred: {e}"    return response.data[0].url

### 2\. Chat Protocol Integration[â€‹](#2-chat-protocol-integration "Direct link to 2. Chat Protocol Integration")

The `chat_protocol.py` file is responsible for orchestrating the entire communication and image generation process when the agent receives a user's request. Here's how it works step by step:

#### i) Receiving the Message[â€‹](#i-receiving-the-message "Direct link to i) Receiving the Message")

    @chat_proto.on_message(ChatMessage)async def handle_message(ctx: Context, sender: str, msg: ChatMessage):

The agent listens for incoming `ChatMessage` instances.

#### ii) Acknowledging Receipt[â€‹](#ii-acknowledging-receipt "Direct link to ii) Acknowledging Receipt")

    await ctx.send(    sender,    ChatAcknowledgement(timestamp=datetime.utcnow(), acknowledged_msg_id=msg.msg_id),)

Once a message is received, the agent immediately sends a `ChatAcknowledgement` back to the sender.

#### iii) Parsing Content[â€‹](#iii-parsing-content "Direct link to iii) Parsing Content")

    for item in msg.content:    if isinstance(item, StartSessionContent):        ...    elif isinstance(item, TextContent):

The message content is iterated over.

*   If it contains `StartSessionContent`, the agent logs it and waits for further input.
*   If it contains `TextContent`, it's treated as the image prompt and passed for processing.

#### iv) Image Generation via DALLÂ·E 3[â€‹](#iv-image-generation-via-dalle-3 "Direct link to iv) Image Generation via DALLÂ·E 3")

    prompt = item.textimage_url = generate_image(prompt)

*   The prompt is extracted and passed to the `generate_image()` function from `models.py` to generate an image URL using DALLÂ·E 3.

#### v) Downloading the Image[â€‹](#v-downloading-the-image "Direct link to v) Downloading the Image")

    response = requests.get(image_url)if response.status_code == 200:    image_data = response.content    content_type = response.headers.get("Content-Type", "")

*   The generated image is downloaded using a direct HTTP request.
*   If successful, the image binary and MIME type are extracted for storage.

#### vi) Uploading to Agent Storage[â€‹](#vi-uploading-to-agent-storage "Direct link to vi) Uploading to Agent Storage")

    asset_id = external_storage.create_asset(    name=str(ctx.session),    content=image_data,    mime_type=content_type)

*   The image is uploaded to the Agent's `ExternalStorage` system.
*   A unique `asset_id` is returned to identify the uploaded image.

#### vii) Permission Management[â€‹](#vii-permission-management "Direct link to vii) Permission Management")

    external_storage.set_permissions(asset_id=asset_id, agent_address=sender)

*   The agent sets viewing permissions so that only the user who requested the image can access it.

#### viii) Responding with the Image[â€‹](#viii-responding-with-the-image "Direct link to viii) Responding with the Image")

    asset_uri = f"agent-storage://{external_storage.storage_url}/{asset_id}"await ctx.send(sender, create_resource_chat(asset_id, asset_uri))

*   The agent constructs a `ResourceContent` message containing the image asset.
*   This message is sent back to the user for viewing in the chat interface.

#### Whole script[â€‹](#whole-script "Direct link to Whole script")

This agent leverages external storage to securely upload, store, and share generated images. An Agentverse API key is required for authentication and to enable interaction with the external storage. You can obtain your API key from [Agentverse](https://agentverse.ai/); for detailed instructions, please refer to the [Agentverse API Key guide](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key).

chat\_proto.py

    import base64import osimport requestsfrom uuid import uuid4from datetime import datetimefrom pydantic.v1 import UUID4from uagents import Context, Protocolfrom uagents_core.contrib.protocols.chat import (    ChatAcknowledgement,    ChatMessage,    EndSessionContent,    Resource,    ResourceContent,    StartSessionContent,    TextContent,    chat_protocol_spec,)from uagents_core.storage import ExternalStoragefrom models import generate_imageAGENTVERSE_API_KEY = os.getenv("AGENTVERSE_API_KEY")STORAGE_URL = os.getenv("AGENTVERSE_URL", "https://agentverse.ai") + "/v1/storage"if AGENTVERSE_API_KEY is None:    raise ValueError("You need to provide an API_TOKEN.")external_storage = ExternalStorage(api_token=AGENTVERSE_API_KEY, storage_url=STORAGE_URL)def create_text_chat(text: str) -> ChatMessage:    return ChatMessage(        timestamp=datetime.utcnow(),        msg_id=uuid4(),        content=[TextContent(type="text", text=text)],    )def create_end_session_chat() -> ChatMessage:    return ChatMessage(        timestamp=datetime.utcnow(),        msg_id=uuid4(),        content=[EndSessionContent(type="end-session")],    )def create_resource_chat(asset_id: str, uri: str) -> ChatMessage:    return ChatMessage(        timestamp=datetime.utcnow(),        msg_id=uuid4(),        content=[            ResourceContent(                type="resource",                resource_id=UUID4(asset_id),                resource=Resource(                    uri=uri,                    metadata={                        "mime_type": "image/png",                        "role": "generated-image"                    }                )            )        ]    )chat_proto = Protocol(spec=chat_protocol_spec)@chat_proto.on_message(ChatMessage)async def handle_message(ctx: Context, sender: str, msg: ChatMessage):    await ctx.send(        sender,        ChatAcknowledgement(timestamp=datetime.utcnow(), acknowledged_msg_id=msg.msg_id),    )    for item in msg.content:        if isinstance(item, StartSessionContent):            ctx.logger.info(f"Got a start session message from {sender}")            continue        elif isinstance(item, TextContent):            ctx.logger.info(f"Got a message from {sender}: {item.text}")            prompt = msg.content[0].text            try:                image_url = generate_image(prompt)                response = requests.get(image_url)                if response.status_code == 200:                    content_type = response.headers.get("Content-Type", "")                    image_data = response.content                                         try:                        asset_id = external_storage.create_asset(                            name=str(ctx.session),                            content=image_data,                            mime_type=content_type                        )                        ctx.logger.info(f"Asset created with ID: {asset_id}")                    except RuntimeError as err:                        ctx.logger.error(f"Asset creation failed: {err}")                    external_storage.set_permissions(asset_id=asset_id, agent_address=sender)                    ctx.logger.info(f"Asset permissions set to: {sender}")                    asset_uri = f"agent-storage://{external_storage.storage_url}/{asset_id}"                    await ctx.send(sender, create_resource_chat(asset_id, asset_uri))                else:                    ctx.logger.error("Failed to download image")                    await ctx.send(                        sender,                        create_text_chat(                            "Sorry, I couldn't process your request. Please try again later."                        ),                    )                    return            except Exception as err:                ctx.logger.error(err)                await ctx.send(                    sender,                    create_text_chat(                        "Sorry, I couldn't process your request. Please try again later."                    ),                )                return            await ctx.send(sender, create_end_session_chat())        else:            ctx.logger.info(f"Got unexpected content from {sender}")@chat_proto.on_message(ChatAcknowledgement)async def handle_ack(ctx: Context, sender: str, msg: ChatAcknowledgement):    ctx.logger.info(f"Got an acknowledgement from {sender} for {msg.acknowledged_msg_id}")

### 3\. Image Generator Agent Setup[â€‹](#3-image-generator-agent-setup "Direct link to 3. Image Generator Agent Setup")

The `agent.py` file initializes your agent and includes necessary protocols for handling user requests.

> **Note:** If you want to add advanced features such as rate limiting or agent health checks, you can refer to the[Football Team Agent section](https://innovationlab.fetch.ai/resources/docs/next/examples/chat-protocol/asi-compatible-uagents#3-football-team-agent-setup) in the ASI1 Compatible uAgent guide.

agent.py

    import osfrom enum import Enumfrom uagents import Agent, Context, Modelfrom uagents.experimental.quota import QuotaProtocol, RateLimitfrom uagents_core.models import ErrorMessagefrom chat_proto import chat_protofrom models import ImageRequest, ImageResponse, generate_imageAGENT_SEED = os.getenv("AGENT_SEED", "image-generator-agent-seed-phrase")AGENT_NAME = os.getenv("AGENT_NAME", "Image Generator Agent")PORT = 8000agent = Agent(    name=AGENT_NAME,    seed=AGENT_SEED,    port=PORT,    mailbox=True,)# Include protocolagent.include(chat_proto, publish_manifest=True)if __name__ == "__main__":    agent.run()

### Setting up Environment Variables[â€‹](#setting-up-environment-variables "Direct link to Setting up Environment Variables")

Make sure to set the following environment variables:

*   `OPENAI_API_KEY`: Your [OpenAI API key](https://platform.openai.com/api-keys) for DALL-E 3 access
*   `AGENTVERSE_API_KEY`: Your [Agentverse API key](https://agentverse.ai/) for storage access
*   `AGENT_SEED`: (Optional) Custom seed for your agent
*   `AGENT_NAME`: (Optional) Custom name for your agent

Adding a README to your Agent[â€‹](#adding-a-readme-to-your-agent "Direct link to Adding a README to your Agent")
---------------------------------------------------------------------------------------------------------------

1.  Start your agent and connect to Agentverse using the Agent Inspector Link in the logs. Please refer to the [Mailbox Agents](https://innovationlab.fetch.ai/resources/docs/agent-creation/uagent-creation#mailbox-agents) section to understand the detailed steps for connecting a local agent to Agentverse.

**Agent Logs**

> Click on the link, it will open a new window in your browser, click on **Connect** and then select **Mailbox**, this will connect your agent to Agentverse.

2.  Once you connect your Agent via Mailbox, click on Agent Profile and navigate to the Overview section of the Agent. Your Agent will appear under local agents on Agentverse.

2.  Click on Edit and add a good description and name for your Agent so that it can be easily searchable by the ASI1 LLM. Please refer to the [Importance of Good Readme](https://innovationlab.fetch.ai/resources/docs/agentverse/searching#importance-of-good-readme) section for more details.
    
3.  Make sure the Agent has the right `AgentChatProtocol`.
    

Query your Agent[â€‹](#query-your-agent "Direct link to Query your Agent")
------------------------------------------------------------------------

1.  Look for your agent under local agents on Agentverse.
    
2.  Navigate to the Overview tab of the agent and click on **Chat with Agent** to interact with the agent from the [Agentverse Chat Interface](https://chat.agentverse.ai/).
    

3.  Type in your image description, for example: "A serene landscape with mountains and a lake at sunset"
    
4.  The agent will generate an image based on your description and send it back through the chat interface.
    

> **Note**: Currently, the image sharing feature for agents is supported via the Agentverse Chat Interface. Support for image sharing through ASI:One will be available soon.</content>
</page>

<page>
  <title>Solana Wallet Balance Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/examples/chat-protocol/solana-wallet-agent</url>
  <content>This guide demonstrates how to create a Solana Wallet Balance Agent that can check wallet balances using the Solana RPC API. The agent is compatible with ASI1 LLM and can process natural language queries about Solana wallet balances.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

The Solana Wallet Balance Agent allows users to query wallet balances using natural language. It uses the Solana RPC API to fetch real-time balance information and provides formatted responses with both SOL and lamports values.

Message Flow[â€‹](#message-flow "Direct link to Message Flow")
------------------------------------------------------------

The communication flow between ASI1 LLM, the Solana Wallet Agent, and OpenAI Agent follows this sequence:

1.  **Query Initiation (1.1)**
    
    *   ASI1 LLM sends a natural language query (e.g., "What's the balance of wallet address AtTjQKXo1CYTa2MuxPARtr382ZyhPU5YX4wMMpvaa1oy") as a `ChatMessage` to the Solana Wallet Agent.
2.  **Parameter Extraction (2, 3)**
    
    *   The Solana Wallet Agent forwards the query to OpenAI Agent for parameter extraction
    *   OpenAI Agent processes the natural language and extracts the wallet address
    *   The address is returned in a Pydantic Model format as `StructuredOutputResponse`
3.  **Balance Query (4, 5)**
    
    *   The Solana Wallet Agent calls the `get_balance_from_address` function with the extracted address
    *   The function queries the Solana RPC API and returns the balance information
4.  **Agent Response (6.1)**
    
    *   The Solana Wallet Agent sends the formatted response back as a `ChatMessage` to ASI1 LLM
5.  **Message Acknowledgements (1.2, 6.2)**
    
    *   Each message is acknowledged using `ChatAcknowledgement`

Here's a visual representation of the flow:

Implementation[â€‹](#implementation "Direct link to Implementation")
------------------------------------------------------------------

In this example we will create an agent and its associated files on [Agentverse](https://agentverse.ai/) that communicate using the chat protocol with the ASI1 LLM. Refer to the [Hosted Agents](https://innovationlab.fetch.ai/resources/docs/agent-creation/uagent-creation#hosted-agents) section to understand the detailed steps for agent creation on Agentverse.

Create a new agent named "SolanaWalletAgent" on Agentverse and create the following files:

    agent.py            # Main agent filesolana_service.py   # Solana RPC API integrationchat_proto.py       # Chat protocol implementation

To create a new file on Agentverse:

1.  Click on the New File icon

2.  Assign a name to the File

3.  Directory Structure

### 1\. Solana Service Implementation[â€‹](#1-solana-service-implementation "Direct link to 1. Solana Service Implementation")

The `solana_service.py` file handles the interaction with the Solana RPC API:

solana\_service.py

    import osimport loggingimport requestsimport jsonfrom uagents import Model, Field# Configure logginglogging.basicConfig(level=logging.INFO)logger = logging.getLogger(__name__)# Solana RPC endpointSOLANA_RPC_URL = "https://api.mainnet-beta.solana.com"class SolanaRequest(Model):    address: str = Field(        description="Solana wallet address to check",    )class SolanaResponse(Model):    balance: str = Field(        description="Formatted Solana wallet balance",    )async def get_balance_from_address(address: str) -> str:    """    Get the balance for a Solana address using the Solana RPC API        Args:        address: Solana wallet address            Returns:        Formatted balance string    """    try:        logger.info(f"Getting balance for address: {address}")                # Prepare the request payload        payload = {            "jsonrpc": "2.0",            "id": 1,            "method": "getBalance",            "params": [address]        }                # Set headers        headers = {            "Content-Type": "application/json"        }                # Make the API request        response = requests.post(SOLANA_RPC_URL, headers=headers, json=payload)        response.raise_for_status()                # Parse the response        result = response.json()                if "error" in result:            error_msg = f"Error: {result['error']['message']}"            logger.error(error_msg)            return error_msg                    if "result" in result and "value" in result["result"]:            # Convert lamports to SOL (1 SOL = 1,000,000,000 lamports)            lamports = result["result"]["value"]            sol_balance = lamports / 1_000_000_000                        # Format the result            result_str = f"{sol_balance:.9f} SOL ({lamports} lamports)"            logger.info(f"Balance for {address}: {result_str}")            return result_str        else:            error_msg = "No balance information found"            logger.error(error_msg)            return error_msg                except requests.exceptions.RequestException as e:        error_msg = f"Request error: {str(e)}"        logger.error(error_msg)        return error_msg    except json.JSONDecodeError as e:        error_msg = f"JSON decode error: {str(e)}"        logger.error(error_msg)        return error_msg    except Exception as e:        error_msg = f"Unexpected error: {str(e)}"        logger.error(error_msg)        return error_msg

### 2\. Chat Protocol Integration[â€‹](#2-chat-protocol-integration "Direct link to 2. Chat Protocol Integration")

The `chat_proto.py` file is essential for enabling natural language communication between your agent and ASI1 LLM.

#### LLM Integration[â€‹](#llm-integration "Direct link to LLM Integration")

To extract the important parameters from a user query passed by the LLM, you can use any of the following LLMs that support structured output:

*   OpenAI Agent: `agent1q0h70caed8ax769shpemapzkyk65uscw4xwk6dc4t3emvp5jdcvqs9xs32y`
*   Claude.ai Agent: `agent1qvk7q2av3e2y5gf5s90nfzkc8a48q3wdqeevwrtgqfdl0k78rspd6f2l4dx`

> **Note**: To ensure fair usage, each agent is limited to 6 requests per hour. Please implement appropriate rate limiting in your application.

#### Message Flow[â€‹](#message-flow-1 "Direct link to Message Flow")

1.  When a user sends a query (e.g., "What's the balance of wallet address AtTjQKXo1CYTa2MuxPARtr382ZyhPU5YX4wMMpvaa1oy"):
    
    *   The `ChatMessage` handler receives the message
    *   Acknowledges receipt to the sender
    *   Forwards the query to the chosen LLM for processing
2.  The LLM processes the query and returns a `StructuredOutputResponse`:
    
    *   Extracts relevant parameters (e.g., address="AtTjQKXo1CYTa2MuxPARtr382ZyhPU5YX4wMMpvaa1oy")
    *   Returns data in the format specified by your agent's schema
3.  Your agent processes the structured data:
    
    *   Calls appropriate functions (e.g., `get_balance_from_address`)
    *   Formats the response and sends it back.

#### Customizing the Handlers[â€‹](#customizing-the-handlers "Direct link to Customizing the Handlers")

When implementing a new agent for a different use case, you'll need to:

*   Update the schema in `StructuredOutputPrompt` to match your agent's data model
*   Modify the response handling in `handle_structured_output_response` function for your use case.

To enable natural language communication with your agent add the following chat protocol in the `chat_proto.py` file created on Agentverse:

chat\_proto.py

    from datetime import datetimefrom uuid import uuid4from typing import Anyfrom uagents import Context, Model, Protocol# Import the necessary components of the chat protocolfrom uagents_core.contrib.protocols.chat import (    ChatAcknowledgement,    ChatMessage,    EndSessionContent,    StartSessionContent,    TextContent,    chat_protocol_spec,)from solana_service import get_balance_from_address, SolanaRequest# AI Agent Address for structured output processingAI_AGENT_ADDRESS = 'agent1q0h70caed8ax769shpemapzkyk65uscw4xwk6dc4t3emvp5jdcvqs9xs32y'if not AI_AGENT_ADDRESS:    raise ValueError("AI_AGENT_ADDRESS not set")def create_text_chat(text: str, end_session: bool = True) -> ChatMessage:    content = [TextContent(type="text", text=text)]    if end_session:        content.append(EndSessionContent(type="end-session"))    return ChatMessage(        timestamp=datetime.utcnow(),        msg_id=uuid4(),        content=content,    )chat_proto = Protocol(spec=chat_protocol_spec)struct_output_client_proto = Protocol(    name="StructuredOutputClientProtocol", version="0.1.0")class StructuredOutputPrompt(Model):    prompt: str    output_schema: dict[str, Any]class StructuredOutputResponse(Model):    output: dict[str, Any]@chat_proto.on_message(ChatMessage)async def handle_message(ctx: Context, sender: str, msg: ChatMessage):    ctx.logger.info(f"Got a message from {sender}: {msg}")    ctx.storage.set(str(ctx.session), sender)    await ctx.send(        sender,        ChatAcknowledgement(timestamp=datetime.utcnow(), acknowledged_msg_id=msg.msg_id),    )    for item in msg.content:        if isinstance(item, StartSessionContent):            ctx.logger.info(f"Got a start session message from {sender}")            continue        elif isinstance(item, TextContent):            ctx.logger.info(f"Got a message from {sender}: {item.text}")            ctx.storage.set(str(ctx.session), sender)            await ctx.send(                AI_AGENT_ADDRESS,                StructuredOutputPrompt(                    prompt=item.text, output_schema=SolanaRequest.schema()                ),            )        else:            ctx.logger.info(f"Got unexpected content from {sender}")@chat_proto.on_message(ChatAcknowledgement)async def handle_ack(ctx: Context, sender: str, msg: ChatAcknowledgement):    ctx.logger.info(        f"Got an acknowledgement from {sender} for {msg.acknowledged_msg_id}"    )@struct_output_client_proto.on_message(StructuredOutputResponse)async def handle_structured_output_response(    ctx: Context, sender: str, msg: StructuredOutputResponse):    session_sender = ctx.storage.get(str(ctx.session))    if session_sender is None:        ctx.logger.error(            "Discarding message because no session sender found in storage"        )        return    if "<UNKNOWN>" in str(msg.output):        await ctx.send(            session_sender,            create_text_chat(                "Sorry, I couldn't process your request. Please include a valid Solana wallet address."            ),        )        return    try:        # Parse the structured output to get the address        wallet_request = SolanaRequest.parse_obj(msg.output)        address = wallet_request.address                if not address:            await ctx.send(                session_sender,                create_text_chat(                    "Sorry, I couldn't find a valid Solana wallet address in your query."                ),            )            return                # Get the balance for this address        balance = await get_balance_from_address(address)                # Create a nicely formatted response        response_text = f"Wallet Balance for `{address}`:\n{balance}\n\n[View on Solana Explorer](https://explorer.solana.com/address/{address})"                # Send the response back to the user        await ctx.send(session_sender, create_text_chat(response_text))            except Exception as err:        ctx.logger.error(err)        await ctx.send(            session_sender,            create_text_chat(                "Sorry, I couldn't check the wallet balance. Please try again later."            ),        )        return

### 3\. Solana Wallet Agent Setup[â€‹](#3-solana-wallet-agent-setup "Direct link to 3. Solana Wallet Agent Setup")

The `agent.py` file is the core of your application. Think of it as the main control center that:

*   Sets up your agent
*   Handles incoming requests
*   Manages rate limiting
*   Monitors the agent's health

Let's break down each component:

#### Basic Setup[â€‹](#basic-setup "Direct link to Basic Setup")

    from uagents import Agent, Context, Modelfrom uagents.experimental.quota import QuotaProtocol, RateLimitagent = Agent()  # Create a new agent instance

Import the necessary packages and initialise your agent.

#### Rate Limiting Setup[â€‹](#rate-limiting-setup "Direct link to Rate Limiting Setup")

    proto = QuotaProtocol(    storage_reference=agent.storage,    name="Solana-Wallet-Protocol",    version="0.1.0",    default_rate_limit=RateLimit(window_size_minutes=60, max_requests=30),)

To ensure fair usage and prevent API abuse, we recommend implementing the QuotaProtocol. While optional, this protocol helps manage service usage by limiting requests. In this example, we've set a limit of 30 requests per hour per user, but you can adjust this threshold based on your agent's functionality. This helps maintain service reliability and protects your agent from excessive requests.

#### Request Handler[â€‹](#request-handler "Direct link to Request Handler")

    @proto.on_message(    SolanaRequest, replies={SolanaResponse, ErrorMessage})async def handle_request(ctx: Context, sender: str, msg: SolanaRequest):    ctx.logger.info(f"Received wallet balance request for address: {msg.address}")    try:        balance = await get_balance_from_address(msg.address)        ctx.logger.info(f"Successfully fetched wallet balance for {msg.address}")        await ctx.send(sender, SolanaResponse(balance=balance))    except Exception as err:        ctx.logger.error(err)        await ctx.send(sender, ErrorMessage(error=str(err)))

This message handler allows your agent to receive direct requests from other agents in a structured format. While we're using it with ASI1 LLM in this example, it's versatile enough to work with agents that don't have the chat protocol enabled. This makes it particularly useful in multi-agent systems, where other agents can directly request information directly.

#### Health Monitoring[â€‹](#health-monitoring "Direct link to Health Monitoring")

    ### Health check related codedef agent_is_healthy() -> bool:    """    Implement the actual health check logic here.    For example, check if the agent can connect to the Solana RPC API.    """    try:        import asyncio        asyncio.run(get_balance_from_address("AtTjQKXo1CYTa2MuxPARtr382ZyhPU5YX4wMMpvaa1oy"))        return True    except Exception:        return Falseclass HealthCheck(Model):    passclass HealthStatus(str, Enum):    HEALTHY = "healthy"    UNHEALTHY = "unhealthy"class AgentHealth(Model):    agent_name: str    status: HealthStatushealth_protocol = QuotaProtocol(    storage_reference=agent.storage, name="HealthProtocol", version="0.1.0")@health_protocol.on_message(HealthCheck, replies={AgentHealth})async def handle_health_check(ctx: Context, sender: str, msg: HealthCheck):    status = HealthStatus.UNHEALTHY    try:        if agent_is_healthy():            status = HealthStatus.HEALTHY    except Exception as err:        ctx.logger.error(err)    finally:        await ctx.send(sender, AgentHealth(agent_name="solana_wallet_agent", status=status))

The HealthProtocol, while optional, provides a periodic health check system to monitor your agent's performance and reliability. It tests the agent's ability to fetch information about a wallet balance and verifies that all components are working correctly. This monitoring helps quickly identify and address any issues that might affect your agent's service, ensuring reliable operation for your users.

#### Protocol Registration[â€‹](#protocol-registration "Direct link to Protocol Registration")

    agent.include(proto, publish_manifest=True)agent.include(health_protocol, publish_manifest=True)agent.include(chat_proto, publish_manifest=True)agent.include(struct_output_client_proto, publish_manifest=True)

This registers all the necessary protocols with your agent:

*   Main protocol for handling wallet balance requests
*   Health monitoring protocol
*   Chat protocol for natural language communication
*   Structured output protocol for formatted responses

Here's the complete implementation:

agent.py

    import osfrom enum import Enumfrom uagents import Agent, Context, Modelfrom uagents.experimental.quota import QuotaProtocol, RateLimitfrom uagents_core.models import ErrorMessagefrom chat_proto import chat_proto, struct_output_client_protofrom solana_service import get_balance_from_address, SolanaRequest, SolanaResponseagent = Agent()proto = QuotaProtocol(    storage_reference=agent.storage,    name="Solana-Wallet-Protocol",    version="0.1.0",    default_rate_limit=RateLimit(window_size_minutes=60, max_requests=30),)@proto.on_message(    SolanaRequest, replies={SolanaResponse, ErrorMessage})async def handle_request(ctx: Context, sender: str, msg: SolanaRequest):    ctx.logger.info(f"Received wallet balance request for address: {msg.address}")    try:        balance = await get_balance_from_address(msg.address)        ctx.logger.info(f"Successfully fetched wallet balance for {msg.address}")        await ctx.send(sender, SolanaResponse(balance=balance))    except Exception as err:        ctx.logger.error(err)        await ctx.send(sender, ErrorMessage(error=str(err)))agent.include(proto, publish_manifest=True)### Health check related codedef agent_is_healthy() -> bool:    """    Implement the actual health check logic here.    For example, check if the agent can connect to the Solana RPC API.    """    try:        import asyncio        asyncio.run(get_balance_from_address("AtTjQKXo1CYTa2MuxPARtr382ZyhPU5YX4wMMpvaa1oy"))        return True    except Exception:        return Falseclass HealthCheck(Model):    passclass HealthStatus(str, Enum):    HEALTHY = "healthy"    UNHEALTHY = "unhealthy"class AgentHealth(Model):    agent_name: str    status: HealthStatushealth_protocol = QuotaProtocol(    storage_reference=agent.storage, name="HealthProtocol", version="0.1.0")@health_protocol.on_message(HealthCheck, replies={AgentHealth})async def handle_health_check(ctx: Context, sender: str, msg: HealthCheck):    status = HealthStatus.UNHEALTHY    try:        if agent_is_healthy():            status = HealthStatus.HEALTHY    except Exception as err:        ctx.logger.error(err)    finally:        await ctx.send(sender, AgentHealth(agent_name="solana_wallet_agent", status=status))agent.include(health_protocol, publish_manifest=True)agent.include(chat_proto, publish_manifest=True)agent.include(struct_output_client_proto, publish_manifest=True)if __name__ == "__main__":    agent.run()

Adding a README to your Agent[â€‹](#adding-a-readme-to-your-agent "Direct link to Adding a README to your Agent")
---------------------------------------------------------------------------------------------------------------

1.  Go to the Overview section in the Editor.
    
2.  Click on Edit and add a good description for your Agent so that it can be easily searchable by the ASI1 LLM. Please refer the [Importance of Good Readme](https://innovationlab.fetch.ai/resources/docs/agentverse/searching#importance-of-good-readme) section for more details.
    
3.  Make sure the Agent has the right `AgentChatProtocol`.
    

Query your Agent from ASI1 LLM[â€‹](#query-your-agent-from-asi1-llm "Direct link to Query your Agent from ASI1 LLM")
------------------------------------------------------------------------------------------------------------------

1.  Login to the [ASI1 LLM](https://asi1.ai/), either using your Google Account or the ASI1 Wallet and Start a New Chat.
    
2.  Toggle the "Agents" switch to enable ASI1 to connect with Agents on Agentverse.
    

3.  Type in a query to ask your Agent for instance 'What's the balance of wallet address 6wFKPxNToSnggrZr4P4s1r4zRxuJX2nSA7iTdQDPpgHc'.

> **Note**: The ASI1 LLM may not always select your agent for answering the query as it is designed to pick the best agent for a task based on a number of parameters.</content>
</page>

<page>
  <title>Understanding Agentic Systems | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/concepts-ai-agents/understanding-agentic-systems</url>
  <content>### Common Misconceptions[â€‹](#common-misconceptions "Direct link to Common Misconceptions")

The term "agent" in AI is overused and often applied inconsistently, diminishing its meaning and creating confusion about the actual capabilities of AI systems. Some of the common misconceptions are explained below:

#### Misconception 1: "My application uses multiple LLM calls, so it's an agent"[â€‹](#misconception-1-my-application-uses-multiple-llm-calls-so-its-an-agent "Direct link to Misconception 1: \"My application uses multiple LLM calls, so it's an agent\"")

    # This is NOT an agent - it's a multi-step LLM applicationdef process_document(doc):    summary = llm.generate_summary(doc)    keywords = llm.extract_keywords(summary)    sentiment = llm.analyze_sentiment(summary)    return {        "summary": summary,        "keywords": keywords,        "sentiment": sentiment    }

#### Misconception 2: "I'm using tools and APIs, so it's an agent"[â€‹](#misconception-2-im-using-tools-and-apis-so-its-an-agent "Direct link to Misconception 2: \"I'm using tools and APIs, so it's an agent\"")

    # This is NOT an agent - it's an automated workflowdef analyze_stock(symbol):    price_data = stock_api.get_price(symbol)    news = news_api.get_recent_news(symbol)    analysis = llm.analyze(f"Price: {price_data}, News: {news}")    return analysis

#### Misconception 3: "Having memory makes it an agent"[â€‹](#misconception-3-having-memory-makes-it-an-agent "Direct link to Misconception 3: \"Having memory makes it an agent\"")

    # This is NOT an agent - just stateful LLM interactionclass ChatSystem:    def __init__(self):        self.conversation_history = []        def respond(self, user_input):        self.conversation_history.append(user_input)        response = llm.generate(context=self.conversation_history)        self.conversation_history.append(response)        return response

#### Misconception 4: "Using planning means its an agent"[â€‹](#misconception-4-using-planning-means-its-an-agent "Direct link to Misconception 4: \"Using planning means its an agent\"")

    # This is NOT an agent - it's structured task decompositiondef handle_task(task):    # Fixed planning template    steps = llm.break_down_task(task)    results = []    for step in steps:        result = execute_step(step)        results.append(result)    return combine_results(results)

#### Misconception 4: "Complex prompt engineering makes it an agent"[â€‹](#misconception-4-complex-prompt-engineering-makes-it-an-agent "Direct link to Misconception 4: \"Complex prompt engineering makes it an agent\"")

    # This is NOT an agent - just sophisticated promptingdef analyze_with_cot(query):    prompt = f"""    Step 1: Understand the query    {query}    Step 2: Break down the components    Step 3: Analyze each component    Step 4: Synthesize findings    """    return llm.generate(prompt)

#### Misconception 5: "Having a feedback loop makes it an agent"[â€‹](#misconception-5-having-a-feedback-loop-makes-it-an-agent "Direct link to Misconception 5: \"Having a feedback loop makes it an agent\"")

    # This is NOT an agent - just iterative refinementdef iterative_response(query, max_iterations=3):    response = initial_response(query)    for _ in range(max_iterations):        quality = evaluate_response(response)        if quality > threshold:            break        response = improve_response(response)    return response

#### Misconception 6: "The LLM performs the actions in an agent"[â€‹](#misconception-6-the-llm-performs-the-actions-in-an-agent "Direct link to Misconception 6: \"The LLM performs the actions in an agent\"")

    # Common MISCONCEPTION: People think this actually performs actionsdef incorrect_understanding():    llm_response = llm.generate("Please save this file to disk")    # The LLM can't actually save files!    # REALITY: Tools perform actions, LLM orchestratesclass PropertyAgent:    def __init__(self):        self.tools = {            'database': DatabaseTool(),            'email': EmailTool(),            'calendar': CalendarTool()        }        def handle_request(self, query):        # LLM determines what needs to be done        action_plan = llm.plan_actions(query)                # TOOLS actually perform the actions        for action in action_plan:            if action.type == "schedule_viewing":                # Calendar tool performs the actual scheduling                self.tools['calendar'].create_appointment(action.details)            elif action.type == "send_confirmation":                # Email tool performs the actual sending                self.tools['email'].send_message(action.details)

### Key Points About LLM's Role[â€‹](#key-points-about-llms-role "Direct link to Key Points About LLM's Role")

1.  **LLM's Actual Functions:**
    *   Planning and strategizing actions
    *   Reasoning about which tools to use
    *   Interpreting results from tools
    *   Generating natural language responses
2.  **Tools' Actual Functions:**
    *   File operations
    *   Database queries
    *   API calls
    *   Network requests
    *   System modifications
    *   Real-world interactions

    # Clear separation of responsibilitiesclass AgentSystem:    def process_task(self, task):        # LLM PLANS the action        plan = self.llm.create_execution_plan(task)                # TOOLS EXECUTE the action        for step in plan:            if step.requires_web_access:                result = self.web_tool.fetch_data(step.url)            elif step.requires_database:                result = self.db_tool.query(step.sql)            elif step.requires_file_operation:                result = self.file_tool.process(step.path)                            # LLM INTERPRETS results and plans next steps            next_actions = self.llm.analyze_results(result)

This misconception is particularly important because it helps explain:

*   Why tool integration is crucial for practical agent systems
*   Why agents need careful permission and capability management
*   The importance of proper tool abstraction and safety measures
*   Why LLM responses alone can't perform real-world actions

This diagram illustrate several key points:

1.  **Separation of Responsibilities**
    
    *   LLM handles planning, reasoning, and decision-making
    *   Tools perform actual real-world actions
    *   Clear boundaries between thinking and doing
2.  **Flow of Control**
    
    *   User requests flow through the LLM first
    *   LLM determines which tools to use
    *   Tools execute actions and return results
    *   LLM interprets results and plans next steps
3.  **Real World Impact**
    
    *   Only tools can affect the external world
    *   LLM provides intelligence but not execution
    *   Actions are constrained by available tools

This helps explain why:

*   Tool integration is crucial for practical agent systems
*   Security and permissions must be implemented at the tool level
*   LLM capabilities alone don't enable real-world actions
*   System design must carefully consider tool access and limitations

#### Misconception 7:"My AI Assistant/AI chatbot is an AI Agent"[â€‹](#misconception-7my-ai-assistantai-chatbot--is-an-ai-agent "Direct link to Misconception 7:\"My AI Assistant/AI chatbot  is an AI Agent\"")

    # This is NOT an agent - it's an AI Assistantclass BasicAIAssistant:    def chat(self, user_input):        response = llm.generate_response(user_input)        return response# This is CLOSER to an agentclass AIAgent:    def __init__(self):        self.tools = load_available_tools()        self.memory = AgentMemory()        self.planner = ActionPlanner()            def handle_task(self, task):        # Autonomous decision making        goal = self.planner.define_goal(task)        plan = self.planner.create_plan(goal)                # Dynamic tool selection and execution        while not goal.is_achieved():            next_action = self.planner.next_action(plan)            tool = self.select_tool(next_action)            result = tool.execute(next_action.parameters)                        # Adaptive behavior            if not result.is_successful():                plan = self.planner.revise_plan(result)                        self.memory.update(result)

However, sometimes as we discussed in chapter 1, AI assistants could have certain level of Agentic behavior depending on how they are implemented.

### Key Differences:[â€‹](#key-differences "Direct link to Key Differences:")

1.  **Autonomy Level**
    
    *   Assistant: Responds to direct commands and questions
    *   Agent: Makes autonomous decisions about how to achieve goals
2.  **Tool Usage**
    
    *   Assistant: May have access to tools but uses them as instructed
    *   Agent: Autonomously decides which tools to use and when
3.  **Goal Orientation**
    
    *   Assistant: Focuses on responding to immediate requests
    *   Agent: Maintains and works toward longer-term goals
4.  **Memory Usage**
    
    *   Assistant: May maintain conversation history
    *   Agent: Uses memory strategically for goal achievement
5.  **Decision Making**
    
    *   Assistant: Makes limited decisions within conversation scope
    *   Agent: Makes complex decisions about actions, strategy, and resource use

### Example Task Comparison:[â€‹](#example-task-comparison "Direct link to Example Task Comparison:")

    # Research Task Example# AI Assistant Approach:async def assistant_research(query):    """Responds to direct questions with available information"""    response = await llm.generate(        f"Please research about {query}"    )    return response# AI Agent Approach:async def agent_research(query):    """Autonomously conducts comprehensive research"""    plan = await self.create_research_plan(query)    sources = []        for step in plan:        if step.type == "web_search":            results = await self.tools.search(step.query)            sources.extend(results)        elif step.type == "verify_information":            verified_data = await self.tools.fact_check(results)        elif step.type == "synthesize":            synthesis = await self.tools.analyze(verified_data)                    # Adaptive planning        if self.evaluate_progress() < self.quality_threshold:            plan = await self.revise_research_plan()        return self.compile_research(sources, synthesis)

This misconception is particularly important because:

1.  It affects system design expectations
2.  It influences how we evaluate AI system capabilities
3.  It impacts how we implement security and permissions
4.  It shapes user expectations and interaction patterns

These diagrams highlight the key differences between AI Assistants and AI Agents:

1.  **Architecture Complexity**
    
    *   Assistant: Simple, linear flow with reactive tool usage
    *   Agent: Complex system with multiple interacting components
2.  **Processing Flow**
    
    *   Assistant: Direct input â†’ response pattern
    *   Agent: Multi-step process with planning and feedback loops
3.  **Tool Integration**
    
    *   Assistant: Passive, explicitly requested tool usage
    *   Agent: Active, autonomous tool selection and execution
4.  **Memory Usage**
    
    *   Assistant: Basic conversation tracking
    *   Agent: Sophisticated memory system for context and learning
5.  **Decision Making**
    
    *   Assistant: Reactive decisions based on immediate input
    *   Agent: Proactive decisions based on goals and strategy</content>
</page>

<page>
  <title>Microservices and AI Agents | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/concepts-ai-agents/agent-comparison</url>
  <content>Fetch.ai Microservice Agent and AI Agent
----------------------------------------

Before diving into the world of AI agents, letâ€™s first explore the key differences between microservices and AI agents.

*   **Microservice**: If you want to fetch specific information or perform an isolated action, you typically use tools or APIsâ€”this is known as a microservice. Imagine you want to check available flights from London to New York. Youâ€™d use an API or tool to fetch the available options. Thatâ€™s a microservice in actionâ€”focused, specific, and task-oriented.
    
*   **AI Agent**: If you have a larger objective that requires coordination between multiple microservices or tools, thatâ€™s where an AI agent steps in. Now, letâ€™s say you want to book the best possible flight. This involves more than just checking flight options: youâ€™ll compare prices across different days, check your personal schedule, and evaluate additional services like baggage policies or meal preferences. To achieve this goal, you need reasoning and decision-making capabilitiesâ€”this is where an AI agent comes into play.
    

The Fetch.ai ecosystem offers a variety of tools to build both microservices and AI agents, enabling developers to cater to different use cases.

### Microservices with uAgents[â€‹](#microservices-with-uagents "Direct link to Microservices with uAgents")

uAgents is Fetch.ai's lightweight Python framework for creating agents (microservices). Itâ€™s designed to be simple and efficient, making it a great choice for building basic microservices.

### AI Agents with the Fetch.ai SDK[â€‹](#ai-agents-with-the-fetchai-sdk "Direct link to AI Agents with the Fetch.ai SDK")

Fetch.ai's SDK is built on top of the uAgents framework. It simplifies the creation of intelligent, dynamic AI agents that can interact with other agents and microservices seamlessly. The SDK allows for more flexibility, reasoning, and adaptability compared to uAgents.

Youâ€™ll learn in the upcoming sections how to decide between these tools based on the problem youâ€™re solving.

How Fetch.aiâ€™s Ecosystem Works[â€‹](#how-fetchais-ecosystem-works "Direct link to How Fetch.aiâ€™s Ecosystem Works")
----------------------------------------------------------------------------------------------------------------

Fetch.ai provides an integrated ecosystem to host and manage your agents, ensuring connectivity and scalability.

1.  **Agentverse Platform**:
    
    *   Agents can be hosted on the Agentverse platform, allowing them to stay active and accessible.
    *   Due to security constraints, Agentverse only supports a limited set of packages.
2.  **Mailbox Service**:
    
    *   For scenarios requiring custom environments (like virtual machines), you can use the Mailbox Service. This lets you host agents outside Agentverse while maintaining connectivity.
    *   The mailbox ensures that messages sent to an agent are queued, so the agent can process them once it comes online.
3.  **Agent Registration with the Almanac Contract**:
    
    *   When agents are registered on Agentverse, they are also listed in the Almanac Contract on Fetch.aiâ€™s blockchain. This registry allows other agents to discover your agent using its unique address.

Why Use the Fetch.ai SDK?[â€‹](#why-use-the-fetchai-sdk "Direct link to Why Use the Fetch.ai SDK?")
-------------------------------------------------------------------------------------------------

While the uAgents framework is sufficient for basic microservices, the Fetch.ai SDK offers additional capabilities suited for more complex AI agents. Hereâ€™s why the SDK exists:

*   Dynamic Messaging:
    *   uAgents require precise Data Models to identify and process messages. In contrast, the SDK enables agents to handle dynamic messages, making it more adaptable.
*   Handler Differences:
    *   uAgents rely on predefined handlers (e.g., interval-based or message-based handlers).
    *   SDK-based agents leverage Flask webhooks, enabling more flexible and scalable communication.

With these distinctions, the SDK empowers developers to build agents capable of reasoning, adaptability, and complex decision-making.</content>
</page>

<page>
  <title>Understanding Agentic Systems | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/concepts-ai-agents/understanding-agentic-systems</url>
  <content>### Common Misconceptions[â€‹](#common-misconceptions "Direct link to Common Misconceptions")

The term "agent" in AI is overused and often applied inconsistently, diminishing its meaning and creating confusion about the actual capabilities of AI systems. Some of the common misconceptions are explained below:

#### Misconception 1: "My application uses multiple LLM calls, so it's an agent"[â€‹](#misconception-1-my-application-uses-multiple-llm-calls-so-its-an-agent "Direct link to Misconception 1: \"My application uses multiple LLM calls, so it's an agent\"")

    # This is NOT an agent - it's a multi-step LLM applicationdef process_document(doc):    summary = llm.generate_summary(doc)    keywords = llm.extract_keywords(summary)    sentiment = llm.analyze_sentiment(summary)    return {        "summary": summary,        "keywords": keywords,        "sentiment": sentiment    }

#### Misconception 2: "I'm using tools and APIs, so it's an agent"[â€‹](#misconception-2-im-using-tools-and-apis-so-its-an-agent "Direct link to Misconception 2: \"I'm using tools and APIs, so it's an agent\"")

    # This is NOT an agent - it's an automated workflowdef analyze_stock(symbol):    price_data = stock_api.get_price(symbol)    news = news_api.get_recent_news(symbol)    analysis = llm.analyze(f"Price: {price_data}, News: {news}")    return analysis

#### Misconception 3: "Having memory makes it an agent"[â€‹](#misconception-3-having-memory-makes-it-an-agent "Direct link to Misconception 3: \"Having memory makes it an agent\"")

    # This is NOT an agent - just stateful LLM interactionclass ChatSystem:    def __init__(self):        self.conversation_history = []        def respond(self, user_input):        self.conversation_history.append(user_input)        response = llm.generate(context=self.conversation_history)        self.conversation_history.append(response)        return response

#### Misconception 4: "Using planning means its an agent"[â€‹](#misconception-4-using-planning-means-its-an-agent "Direct link to Misconception 4: \"Using planning means its an agent\"")

    # This is NOT an agent - it's structured task decompositiondef handle_task(task):    # Fixed planning template    steps = llm.break_down_task(task)    results = []    for step in steps:        result = execute_step(step)        results.append(result)    return combine_results(results)

#### Misconception 4: "Complex prompt engineering makes it an agent"[â€‹](#misconception-4-complex-prompt-engineering-makes-it-an-agent "Direct link to Misconception 4: \"Complex prompt engineering makes it an agent\"")

    # This is NOT an agent - just sophisticated promptingdef analyze_with_cot(query):    prompt = f"""    Step 1: Understand the query    {query}    Step 2: Break down the components    Step 3: Analyze each component    Step 4: Synthesize findings    """    return llm.generate(prompt)

#### Misconception 5: "Having a feedback loop makes it an agent"[â€‹](#misconception-5-having-a-feedback-loop-makes-it-an-agent "Direct link to Misconception 5: \"Having a feedback loop makes it an agent\"")

    # This is NOT an agent - just iterative refinementdef iterative_response(query, max_iterations=3):    response = initial_response(query)    for _ in range(max_iterations):        quality = evaluate_response(response)        if quality > threshold:            break        response = improve_response(response)    return response

#### Misconception 6: "The LLM performs the actions in an agent"[â€‹](#misconception-6-the-llm-performs-the-actions-in-an-agent "Direct link to Misconception 6: \"The LLM performs the actions in an agent\"")

    # Common MISCONCEPTION: People think this actually performs actionsdef incorrect_understanding():    llm_response = llm.generate("Please save this file to disk")    # The LLM can't actually save files!    # REALITY: Tools perform actions, LLM orchestratesclass PropertyAgent:    def __init__(self):        self.tools = {            'database': DatabaseTool(),            'email': EmailTool(),            'calendar': CalendarTool()        }        def handle_request(self, query):        # LLM determines what needs to be done        action_plan = llm.plan_actions(query)                # TOOLS actually perform the actions        for action in action_plan:            if action.type == "schedule_viewing":                # Calendar tool performs the actual scheduling                self.tools['calendar'].create_appointment(action.details)            elif action.type == "send_confirmation":                # Email tool performs the actual sending                self.tools['email'].send_message(action.details)

### Key Points About LLM's Role[â€‹](#key-points-about-llms-role "Direct link to Key Points About LLM's Role")

1.  **LLM's Actual Functions:**
    *   Planning and strategizing actions
    *   Reasoning about which tools to use
    *   Interpreting results from tools
    *   Generating natural language responses
2.  **Tools' Actual Functions:**
    *   File operations
    *   Database queries
    *   API calls
    *   Network requests
    *   System modifications
    *   Real-world interactions

    # Clear separation of responsibilitiesclass AgentSystem:    def process_task(self, task):        # LLM PLANS the action        plan = self.llm.create_execution_plan(task)                # TOOLS EXECUTE the action        for step in plan:            if step.requires_web_access:                result = self.web_tool.fetch_data(step.url)            elif step.requires_database:                result = self.db_tool.query(step.sql)            elif step.requires_file_operation:                result = self.file_tool.process(step.path)                            # LLM INTERPRETS results and plans next steps            next_actions = self.llm.analyze_results(result)

This misconception is particularly important because it helps explain:

*   Why tool integration is crucial for practical agent systems
*   Why agents need careful permission and capability management
*   The importance of proper tool abstraction and safety measures
*   Why LLM responses alone can't perform real-world actions

This diagram illustrate several key points:

1.  **Separation of Responsibilities**
    
    *   LLM handles planning, reasoning, and decision-making
    *   Tools perform actual real-world actions
    *   Clear boundaries between thinking and doing
2.  **Flow of Control**
    
    *   User requests flow through the LLM first
    *   LLM determines which tools to use
    *   Tools execute actions and return results
    *   LLM interprets results and plans next steps
3.  **Real World Impact**
    
    *   Only tools can affect the external world
    *   LLM provides intelligence but not execution
    *   Actions are constrained by available tools

This helps explain why:

*   Tool integration is crucial for practical agent systems
*   Security and permissions must be implemented at the tool level
*   LLM capabilities alone don't enable real-world actions
*   System design must carefully consider tool access and limitations

#### Misconception 7:"My AI Assistant/AI chatbot is an AI Agent"[â€‹](#misconception-7my-ai-assistantai-chatbot--is-an-ai-agent "Direct link to Misconception 7:\"My AI Assistant/AI chatbot  is an AI Agent\"")

    # This is NOT an agent - it's an AI Assistantclass BasicAIAssistant:    def chat(self, user_input):        response = llm.generate_response(user_input)        return response# This is CLOSER to an agentclass AIAgent:    def __init__(self):        self.tools = load_available_tools()        self.memory = AgentMemory()        self.planner = ActionPlanner()            def handle_task(self, task):        # Autonomous decision making        goal = self.planner.define_goal(task)        plan = self.planner.create_plan(goal)                # Dynamic tool selection and execution        while not goal.is_achieved():            next_action = self.planner.next_action(plan)            tool = self.select_tool(next_action)            result = tool.execute(next_action.parameters)                        # Adaptive behavior            if not result.is_successful():                plan = self.planner.revise_plan(result)                        self.memory.update(result)

However, sometimes as we discussed in chapter 1, AI assistants could have certain level of Agentic behavior depending on how they are implemented.

### Key Differences:[â€‹](#key-differences "Direct link to Key Differences:")

1.  **Autonomy Level**
    
    *   Assistant: Responds to direct commands and questions
    *   Agent: Makes autonomous decisions about how to achieve goals
2.  **Tool Usage**
    
    *   Assistant: May have access to tools but uses them as instructed
    *   Agent: Autonomously decides which tools to use and when
3.  **Goal Orientation**
    
    *   Assistant: Focuses on responding to immediate requests
    *   Agent: Maintains and works toward longer-term goals
4.  **Memory Usage**
    
    *   Assistant: May maintain conversation history
    *   Agent: Uses memory strategically for goal achievement
5.  **Decision Making**
    
    *   Assistant: Makes limited decisions within conversation scope
    *   Agent: Makes complex decisions about actions, strategy, and resource use

### Example Task Comparison:[â€‹](#example-task-comparison "Direct link to Example Task Comparison:")

    # Research Task Example# AI Assistant Approach:async def assistant_research(query):    """Responds to direct questions with available information"""    response = await llm.generate(        f"Please research about {query}"    )    return response# AI Agent Approach:async def agent_research(query):    """Autonomously conducts comprehensive research"""    plan = await self.create_research_plan(query)    sources = []        for step in plan:        if step.type == "web_search":            results = await self.tools.search(step.query)            sources.extend(results)        elif step.type == "verify_information":            verified_data = await self.tools.fact_check(results)        elif step.type == "synthesize":            synthesis = await self.tools.analyze(verified_data)                    # Adaptive planning        if self.evaluate_progress() < self.quality_threshold:            plan = await self.revise_research_plan()        return self.compile_research(sources, synthesis)

This misconception is particularly important because:

1.  It affects system design expectations
2.  It influences how we evaluate AI system capabilities
3.  It impacts how we implement security and permissions
4.  It shapes user expectations and interaction patterns

These diagrams highlight the key differences between AI Assistants and AI Agents:

1.  **Architecture Complexity**
    
    *   Assistant: Simple, linear flow with reactive tool usage
    *   Agent: Complex system with multiple interacting components
2.  **Processing Flow**
    
    *   Assistant: Direct input â†’ response pattern
    *   Agent: Multi-step process with planning and feedback loops
3.  **Tool Integration**
    
    *   Assistant: Passive, explicitly requested tool usage
    *   Agent: Active, autonomous tool selection and execution
4.  **Memory Usage**
    
    *   Assistant: Basic conversation tracking
    *   Agent: Sophisticated memory system for context and learning
5.  **Decision Making**
    
    *   Assistant: Reactive decisions based on immediate input
    *   Agent: Proactive decisions based on goals and strategy</content>
</page>

<page>
  <title>Microservices and AI Agents | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/concepts-ai-agents/agent-comparison</url>
  <content>Fetch.ai Microservice Agent and AI Agent
----------------------------------------

Before diving into the world of AI agents, letâ€™s first explore the key differences between microservices and AI agents.

*   **Microservice**: If you want to fetch specific information or perform an isolated action, you typically use tools or APIsâ€”this is known as a microservice. Imagine you want to check available flights from London to New York. Youâ€™d use an API or tool to fetch the available options. Thatâ€™s a microservice in actionâ€”focused, specific, and task-oriented.
    
*   **AI Agent**: If you have a larger objective that requires coordination between multiple microservices or tools, thatâ€™s where an AI agent steps in. Now, letâ€™s say you want to book the best possible flight. This involves more than just checking flight options: youâ€™ll compare prices across different days, check your personal schedule, and evaluate additional services like baggage policies or meal preferences. To achieve this goal, you need reasoning and decision-making capabilitiesâ€”this is where an AI agent comes into play.
    

The Fetch.ai ecosystem offers a variety of tools to build both microservices and AI agents, enabling developers to cater to different use cases.

### Microservices with uAgents[â€‹](#microservices-with-uagents "Direct link to Microservices with uAgents")

uAgents is Fetch.ai's lightweight Python framework for creating agents (microservices). Itâ€™s designed to be simple and efficient, making it a great choice for building basic microservices.

### AI Agents with the Fetch.ai SDK[â€‹](#ai-agents-with-the-fetchai-sdk "Direct link to AI Agents with the Fetch.ai SDK")

Fetch.ai's SDK is built on top of the uAgents framework. It simplifies the creation of intelligent, dynamic AI agents that can interact with other agents and microservices seamlessly. The SDK allows for more flexibility, reasoning, and adaptability compared to uAgents.

Youâ€™ll learn in the upcoming sections how to decide between these tools based on the problem youâ€™re solving.

How Fetch.aiâ€™s Ecosystem Works[â€‹](#how-fetchais-ecosystem-works "Direct link to How Fetch.aiâ€™s Ecosystem Works")
----------------------------------------------------------------------------------------------------------------

Fetch.ai provides an integrated ecosystem to host and manage your agents, ensuring connectivity and scalability.

1.  **Agentverse Platform**:
    
    *   Agents can be hosted on the Agentverse platform, allowing them to stay active and accessible.
    *   Due to security constraints, Agentverse only supports a limited set of packages.
2.  **Mailbox Service**:
    
    *   For scenarios requiring custom environments (like virtual machines), you can use the Mailbox Service. This lets you host agents outside Agentverse while maintaining connectivity.
    *   The mailbox ensures that messages sent to an agent are queued, so the agent can process them once it comes online.
3.  **Agent Registration with the Almanac Contract**:
    
    *   When agents are registered on Agentverse, they are also listed in the Almanac Contract on Fetch.aiâ€™s blockchain. This registry allows other agents to discover your agent using its unique address.

Why Use the Fetch.ai SDK?[â€‹](#why-use-the-fetchai-sdk "Direct link to Why Use the Fetch.ai SDK?")
-------------------------------------------------------------------------------------------------

While the uAgents framework is sufficient for basic microservices, the Fetch.ai SDK offers additional capabilities suited for more complex AI agents. Hereâ€™s why the SDK exists:

*   Dynamic Messaging:
    *   uAgents require precise Data Models to identify and process messages. In contrast, the SDK enables agents to handle dynamic messages, making it more adaptable.
*   Handler Differences:
    *   uAgents rely on predefined handlers (e.g., interval-based or message-based handlers).
    *   SDK-based agents leverage Flask webhooks, enabling more flexible and scalable communication.

With these distinctions, the SDK empowers developers to build agents capable of reasoning, adaptability, and complex decision-making.</content>
</page>

<page>
  <title>Team Based Artchitectures | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/concepts-ai-agents/team-based-architectures</url>
  <content>### Multi-agent Systems[â€‹](#multi-agent-systems "Direct link to Multi-agent Systems")

An agent uses an LLM to control application flow. However, as systems grow complex, using a single agent can become challenging. This is where multi-agent systems come in.

### Why Use Multiple Agents?[â€‹](#why-use-multiple-agents "Direct link to Why Use Multiple Agents?")

*   **Simplicity**: Break complex tasks into manageable pieces
*   **Expertise**: Create specialized agents for specific tasks
*   **Better Control**: Manage how agents work together

### Common Multi-agent Patterns[â€‹](#common-multi-agent-patterns "Direct link to Common Multi-agent Patterns")

1.  **Network Pattern**
    
    *   Agents can communicate freely with every other agent
    *   Any agent can decide which other agent to call next
    *   Flexible but potentially complex to manage
2.  **Supervisor Pattern**
    
    *   Central supervisor coordinates other agents
    *   Clear control flow through supervisor
    *   Better oversight and management
    *   Can be implemented through tool-calling
3.  **Hierarchical Pattern**
    
    *   Supervisors of supervisors
    *   Allows for more complex control flows
    *   Suitable for large-scale systems
4.  **Custom Workflow Pattern**
    
    *   Agents communicate with specific subset of agents
    *   Parts of flow are deterministic
    *   Limited decision-making about which agents to call next

We will build our agent using a supervisor based multi-agent pattern.

### Supervisor-Based Agent Architecture[â€‹](#supervisor-based-agent-architecture "Direct link to Supervisor-Based Agent Architecture")

A team-based architecture typically consists of three main components:

1.  **Supervisor Agent**
    
    *   Coordinates team activities
    *   Makes routing decisions
    *   Ensures task completion
2.  **Specialist Agents**
    
    *   Handle domain-specific tasks
    *   Maintain focused expertise
    *   Provide detailed analysis
3.  **State Management System**
    
    *   Maintains conversation context
    *   Tracks team progress
    *   Manages shared resources

### Implementation Pattern[â€‹](#implementation-pattern "Direct link to Implementation Pattern")

The code implements a team-based architecture using langgraph with three essential components that work together to process complex tasks:

#### 1\. Supervisor Agent[â€‹](#1-supervisor-agent "Direct link to 1. Supervisor Agent")

    def create_supervisor_agent(llm: ChatOpenAI):    """Creates a supervisor agent that orchestrates the team's activities.        The supervisor agent is the core decision-maker that:    1. Analyzes incoming queries to understand requirements    2. Determines which specialist agent is best suited for each subtask    3. Routes tasks to appropriate specialists    4. Monitors the overall progress of the task    5. Decides when enough information has been gathered        Args:        llm: Language model for decision making            Returns:        A supervisor agent configured with team coordination capabilities    """    supervisor_prompt = """    Core responsibilities:    1. Analyze incoming queries to break down complex tasks    2. Determine what specific information is needed    3. Select the most appropriate specialist for each subtask    4. Monitor progress and ensure task completion    5. Decide when sufficient information has been gathered    """        return create_team_supervisor(        llm=llm,        system_prompt=supervisor_prompt,        members=["SpecialistA", "SpecialistB"]    )

#### 2\. Specialist Agents[â€‹](#2-specialist-agents "Direct link to 2. Specialist Agents")

    def create_specialist_agent(    llm: ChatOpenAI,    domain: str,    tools: List[Tool]):    """Creates a specialist agent with domain-specific expertise.        Specialist agents are focused experts that:    1. Handle specific types of tasks within their domain    2. Use specialized tools for their domain    3. Provide structured analysis and insights    4. Request clarification when needed        Args:        llm: Language model for domain-specific processing        domain: Area of expertise (e.g., "financial analysis", "market research")        tools: List of domain-specific tools available to this specialist            Returns:        A specialist agent configured for its specific domain    """    system_prompt = f"""    You are specialized in {domain}.    When responding:    1. Use your domain-specific tools effectively    2. Provide clearly structured outputs    3. Explicitly request any missing information needed    """        return create_agent(        llm=llm,        tools=tools,        system_prompt=system_prompt    )

#### 3\. State Management[â€‹](#3-state-management "Direct link to 3. State Management")

    class TeamState(TypedDict):    """Manages the shared state and context for the entire team.        Attributes:        messages: List of all messages in the conversation history        team_members: List of available specialist agents        next: Identifier of the next agent to act        information_needed: List of missing information to be gathered        reasoning: Explanation for the current decision or action    """    messages: List[BaseMessage]    team_members: List[str]    next: str    information_needed: List[str]    reasoning: str

### Team Graph Implementation[â€‹](#team-graph-implementation "Direct link to Team Graph Implementation")

The team graph orchestrates how all components work together:

    def create_team_graph():    """Creates a coordinated team of agents with defined interaction patterns.        The graph defines:    1. How agents communicate with each other    2. The flow of information between agents    3. Decision points for task routing    4. Conditions for task completion        Process Flow:    1. Supervisor receives task and analyzes requirements    2. Tasks are routed to appropriate specialists    3. Specialists process tasks and return results    4. Supervisor evaluates results and decides next steps    5. Process continues until task is complete        Returns:        A compiled graph ready for task processing    """    # Initialize team members    specialist_a = create_specialist_agent(...)    specialist_b = create_specialist_agent(...)    supervisor = create_supervisor_agent(...)        # Create the coordination graph    graph = StateGraph(TeamState)        # Define team structure    graph.add_node("SpecialistA", specialist_a)    graph.add_node("SpecialistB", specialist_b)    graph.add_node("supervisor", supervisor)        # Define information flow    graph.add_edge("SpecialistA", "supervisor")    graph.add_edge("SpecialistB", "supervisor")        # Set up decision routing    graph.add_conditional_edges(        "supervisor",        lambda x: x["next"],        {            "SpecialistA": "SpecialistA",            "SpecialistB": "SpecialistB",            "FINISH": END        }    )        return graph.compile()</content>
</page>

<page>
  <title>Team Based Artchitectures | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/concepts-ai-agents/team-based-architectures</url>
  <content>### Multi-agent Systems[â€‹](#multi-agent-systems "Direct link to Multi-agent Systems")

An agent uses an LLM to control application flow. However, as systems grow complex, using a single agent can become challenging. This is where multi-agent systems come in.

### Why Use Multiple Agents?[â€‹](#why-use-multiple-agents "Direct link to Why Use Multiple Agents?")

*   **Simplicity**: Break complex tasks into manageable pieces
*   **Expertise**: Create specialized agents for specific tasks
*   **Better Control**: Manage how agents work together

### Common Multi-agent Patterns[â€‹](#common-multi-agent-patterns "Direct link to Common Multi-agent Patterns")

1.  **Network Pattern**
    
    *   Agents can communicate freely with every other agent
    *   Any agent can decide which other agent to call next
    *   Flexible but potentially complex to manage
2.  **Supervisor Pattern**
    
    *   Central supervisor coordinates other agents
    *   Clear control flow through supervisor
    *   Better oversight and management
    *   Can be implemented through tool-calling
3.  **Hierarchical Pattern**
    
    *   Supervisors of supervisors
    *   Allows for more complex control flows
    *   Suitable for large-scale systems
4.  **Custom Workflow Pattern**
    
    *   Agents communicate with specific subset of agents
    *   Parts of flow are deterministic
    *   Limited decision-making about which agents to call next

We will build our agent using a supervisor based multi-agent pattern.

### Supervisor-Based Agent Architecture[â€‹](#supervisor-based-agent-architecture "Direct link to Supervisor-Based Agent Architecture")

A team-based architecture typically consists of three main components:

1.  **Supervisor Agent**
    
    *   Coordinates team activities
    *   Makes routing decisions
    *   Ensures task completion
2.  **Specialist Agents**
    
    *   Handle domain-specific tasks
    *   Maintain focused expertise
    *   Provide detailed analysis
3.  **State Management System**
    
    *   Maintains conversation context
    *   Tracks team progress
    *   Manages shared resources

### Implementation Pattern[â€‹](#implementation-pattern "Direct link to Implementation Pattern")

The code implements a team-based architecture using langgraph with three essential components that work together to process complex tasks:

#### 1\. Supervisor Agent[â€‹](#1-supervisor-agent "Direct link to 1. Supervisor Agent")

    def create_supervisor_agent(llm: ChatOpenAI):    """Creates a supervisor agent that orchestrates the team's activities.        The supervisor agent is the core decision-maker that:    1. Analyzes incoming queries to understand requirements    2. Determines which specialist agent is best suited for each subtask    3. Routes tasks to appropriate specialists    4. Monitors the overall progress of the task    5. Decides when enough information has been gathered        Args:        llm: Language model for decision making            Returns:        A supervisor agent configured with team coordination capabilities    """    supervisor_prompt = """    Core responsibilities:    1. Analyze incoming queries to break down complex tasks    2. Determine what specific information is needed    3. Select the most appropriate specialist for each subtask    4. Monitor progress and ensure task completion    5. Decide when sufficient information has been gathered    """        return create_team_supervisor(        llm=llm,        system_prompt=supervisor_prompt,        members=["SpecialistA", "SpecialistB"]    )

#### 2\. Specialist Agents[â€‹](#2-specialist-agents "Direct link to 2. Specialist Agents")

    def create_specialist_agent(    llm: ChatOpenAI,    domain: str,    tools: List[Tool]):    """Creates a specialist agent with domain-specific expertise.        Specialist agents are focused experts that:    1. Handle specific types of tasks within their domain    2. Use specialized tools for their domain    3. Provide structured analysis and insights    4. Request clarification when needed        Args:        llm: Language model for domain-specific processing        domain: Area of expertise (e.g., "financial analysis", "market research")        tools: List of domain-specific tools available to this specialist            Returns:        A specialist agent configured for its specific domain    """    system_prompt = f"""    You are specialized in {domain}.    When responding:    1. Use your domain-specific tools effectively    2. Provide clearly structured outputs    3. Explicitly request any missing information needed    """        return create_agent(        llm=llm,        tools=tools,        system_prompt=system_prompt    )

#### 3\. State Management[â€‹](#3-state-management "Direct link to 3. State Management")

    class TeamState(TypedDict):    """Manages the shared state and context for the entire team.        Attributes:        messages: List of all messages in the conversation history        team_members: List of available specialist agents        next: Identifier of the next agent to act        information_needed: List of missing information to be gathered        reasoning: Explanation for the current decision or action    """    messages: List[BaseMessage]    team_members: List[str]    next: str    information_needed: List[str]    reasoning: str

### Team Graph Implementation[â€‹](#team-graph-implementation "Direct link to Team Graph Implementation")

The team graph orchestrates how all components work together:

    def create_team_graph():    """Creates a coordinated team of agents with defined interaction patterns.        The graph defines:    1. How agents communicate with each other    2. The flow of information between agents    3. Decision points for task routing    4. Conditions for task completion        Process Flow:    1. Supervisor receives task and analyzes requirements    2. Tasks are routed to appropriate specialists    3. Specialists process tasks and return results    4. Supervisor evaluates results and decides next steps    5. Process continues until task is complete        Returns:        A compiled graph ready for task processing    """    # Initialize team members    specialist_a = create_specialist_agent(...)    specialist_b = create_specialist_agent(...)    supervisor = create_supervisor_agent(...)        # Create the coordination graph    graph = StateGraph(TeamState)        # Define team structure    graph.add_node("SpecialistA", specialist_a)    graph.add_node("SpecialistB", specialist_b)    graph.add_node("supervisor", supervisor)        # Define information flow    graph.add_edge("SpecialistA", "supervisor")    graph.add_edge("SpecialistB", "supervisor")        # Set up decision routing    graph.add_conditional_edges(        "supervisor",        lambda x: x["next"],        {            "SpecialistA": "SpecialistA",            "SpecialistB": "SpecialistB",            "FINISH": END        }    )        return graph.compile()</content>
</page>

<page>
  <title>AI Agent to uAgent Communication | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/agent-communication/sdk-uagent-communication</url>
  <content>uAgent â†” AI Agent Communication Using Fetch.ai SDK and uAgents
--------------------------------------------------------------

This guide demonstrates how to enable communication between a Microservice Agent created using the uagents framework and an AI Agent created using the Fetch.ai SDK.

### uAgent Script (uagent.py)[â€‹](#uagent-script-uagentpy "Direct link to uAgent Script (uagent.py)")

Please remember to have the **uagents** and the **fetchai** package installed in the terminal in order to create and run the agents.

This uAgent will receive a message from the AI Agent and send a response back.

uagent.py

    from uagents import Agent, Context, Model# Define the request model the uAgent will handleclass Request(Model):    message: str# Define the response model the uAgent will send backclass Response(Model):    response: str# Initialize the uAgentuagent = Agent(    name="Sample uAgent",    port=8000,    endpoint=["http://localhost:8000/submit"])# Handle incoming messages with the Request model@uagent.on_message(model=Request)async def message_handler(ctx: Context, sender: str, msg: Request):    ctx.logger.info(f"Received message from {sender}: {msg.message}")    # Generate a response message    response = Response(response=f'Hello, AI Agent! I received your message:{msg.message}')        # Send the response back to the AI Agent    await ctx.send(sender, response)if __name__ == "__main__":    uagent.run()

### Explanation[â€‹](#explanation "Direct link to Explanation")

*   **Agent Initialization**: The uAgent listens on port 8000 for incoming messages.
*   **Message Handling**: When a message matching the `Request` model is received, the agent logs it and responds with a predefined message using the `Response` model.

Setting Up the AI Agent[â€‹](#setting-up-the-ai-agent "Direct link to Setting Up the AI Agent")
---------------------------------------------------------------------------------------------

The AI Agent will send messages to the uAgent and handle the response received from the uAgent.

### AI Agent Script (ai\_agent.py)[â€‹](#ai-agent-script-ai_agentpy "Direct link to AI Agent Script (ai_agent.py)")

ai\_agent.py

    import osfrom flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai.communication import send_message_to_agent, parse_message_from_agentimport loggingfrom dotenv import load_dotenv# Configure logginglogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)# Initialize Flask appapp = Flask(__name__)CORS(app)client_identity = Noneagent_response = Noneclass Request(Model):    message: str# Load environment variables from .env fileload_dotenv()def init_client():    """Initialize and register the client agent."""    global client_identity    try:        # Load the client identity from environment variables        client_identity = Identity.from_seed("Sample AI AGENT SEED PHRASE for communication", 0)        logger.info(f"Client agent started with address: {client_identity.address}")        readme = """        ![domain:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)        domain:domain-of-your-agent        <description>This Agent can send a message to a uAgent and receive a message from a uAgent in string format.</description>        <use_cases>        <use_case>Send and receive messages with another uAgent.</use_case>        </use_cases>        <payload_requirements>            <description>This agent can only send and receive messages in text format.</description>            <payload>                <requirement>                    <parameter>message</parameter>                    <description>The agent sends and receives messages in text format.</description>                </requirement>            </payload>        </payload_requirements>        """        # Register the agent with Agentverse        register_with_agentverse(            identity=client_identity,            url="http://localhost:5002/api/webhook",            agentverse_token = os.getenv("AGENTVERSE_API_KEY"),            agent_title="Sample AI Agent communication"            readme=readme        )        logger.info("Client agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise@app.route('/request', methods=['POST'])def send_data():    """Send payload to the selected agent based on provided address."""    global agent_response    agent_response = None    try:        # Parse the request payload        data = request.json        payload = data.get('payload')  # Extract the payload dictionary        uagent_address = "agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6" #run the uagent.py copy the address and paste here                # Build the Data Model digest for the Request model to ensure message format consistency between the uAgent and AI Agent        model_digest = Model.build_schema_digest(Request)        # Send the payload to the specified agent        send_message_to_agent(            client_identity,  # Frontend client identity            uagent_address,  # Agent address where we have to send the data            payload,  # Payload containing the data            model_digest=model_digest        )        return jsonify({"status": "request_sent", "payload": payload})    except Exception as e:        logger.error(f"Error sending data to agent: {e}")        return jsonify({"error": str(e)}), 500# app route to get recieve the messages on the agent@app.route('/api/webhook', methods=['POST'])def webhook():    """Handle incoming messages from the dashboard agent."""    global agent_response    try:        # Parse the incoming webhook message        data = request.get_data().decode("utf-8")        logger.info("Received response")        message = parse_message_from_agent(data)        agent_response = message.payload        logger.info(f"Processed response: {agent_response}")        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500if __name__ == "__main__":    load_dotenv()    init_client()    app.run(host="0.0.0.0", port=5002)

### Explanation[â€‹](#explanation-1 "Direct link to Explanation")

*   **Data Model**: The Request and Response models define the structure of messages exchanged between the AI Agent and the uAgent. A correctly defined data model is essential for sending a message to the uAgent from an SDK-based AI Agent.
*   **Schema Validation**: The model digest ensures that messages conform to the expected schema before transmission, preventing format mismatches.
*   **Sending Data**: The AI Agent sends a message to the uAgent using the /request endpoint.
*   **Handling Response**: The AI Agent listens for responses using the /api/webhook endpoint.

Environment Variables[â€‹](#environment-variables "Direct link to Environment Variables")
---------------------------------------------------------------------------------------

Create a .env file and add the following environment variables:

    AGENTVERSE_API_KEY="YOUR_AGENTVERSE_API_KEY"AGENT_SECRET_KEY="YOUR_SECRET_KEY_FOR_AI_AGENT"

Replace the placeholders with your actual API keys and agent secrets. Refer to this [guide](https://innovationlab.fetch.ai/resources/docs/next/agentverse/agentverse-api-key) to get your Agentverse API Key.

Testing the Communication[â€‹](#testing-the-communication "Direct link to Testing the Communication")
---------------------------------------------------------------------------------------------------

### Step 1: Running the uAgent[â€‹](#step-1-running-the-uagent "Direct link to Step 1: Running the uAgent")

Start the uAgent by running the following command in your terminal:

#### uAgent Logs[â€‹](#uagent-logs "Direct link to uAgent Logs")

    (venv) abhi@Fetchs-MacBook-Pro ILAgents % python3 uagent.pyINFO:     [Sample uAgent]: Starting agent with address: agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: My name is Sample uAgent and my address is agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8000&address=agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: Starting server on http://0.0.0.0:8000 (Press CTRL+C to quit)INFO:     [uagents.registration]: Registration on Almanac API successfulINFO:     [uagents.registration]: Registering on almanac contract...INFO:     [uagents.registration]: Registering on almanac contract...complete

#### Copy the uAgent Address[â€‹](#copy-the-uagent-address "Direct link to Copy the uAgent Address")

Look for the uAgent address in the logs, which appears in this format:

    agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6

Copy this uAgent address and paste it into the AI Agent script at the following line:

    uagent_address="PASTE YOUR UAGENT ADDRESS HERE"

### Step 2: Running the AI Agent[â€‹](#step-2-running-the-ai-agent "Direct link to Step 2: Running the AI Agent")

Start the AI Agent by running the following command in your terminal:

#### AI Agent Logs[â€‹](#ai-agent-logs "Direct link to AI Agent Logs")

    (venv) abhi@Fetchs-MacBook-Pro ILAgents % python3 ai_agent.pyINFO:__main__:Client agent started with address: agent1qw7u5sw63a88kmcn5j5kxf7q326u5hgmppvy2vpxlh3re6y0yp8253ec7xlINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Client agent registration complete! * Serving Flask app 'ai_agent' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5002 * Running on http://172.20.10.2:5002INFO:werkzeug:Press CTRL+C to quit

### Step 3: Sending a message from Agent2 to Agent1[â€‹](#step-3-sending-a-message-from-agent2-to-agent1 "Direct link to Step 3: Sending a message from Agent2 to Agent1")

We will use the following curl command to send a message from the AI Agent to the uAgent:

    curl -X POST http://localhost:5002/request \-H "Content-Type: application/json" \-d '{  "payload": {"message": "Hello uAgent!"}}'

#### Expected Logs on the uAgent Terminal[â€‹](#expected-logs-on-the-uagent-terminal "Direct link to Expected Logs on the uAgent Terminal")

    (venv) abhi@Fetchs-MacBook-Pro ILAgents % python3 uagent.pyINFO:     [Sample uAgent]: Starting agent with address: agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: My name is Sample uAgent and my address is agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8000&address=agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: Starting server on http://0.0.0.0:8000 (Press CTRL+C to quit)INFO:     [uagents.registration]: Registration on Almanac API successfulINFO:     [uagents.registration]: Registering on almanac contract...INFO:     [uagents.registration]: Registering on almanac contract...completeINFO:     [Sample uAgent]: Received message from agent1qw7u5sw63a88kmcn5j5kxf7q326u5hgmppvy2vpxlh3re6y0yp8253ec7xl: Hello uAgent!

#### Expected Logs on the AI Agent Terminal[â€‹](#expected-logs-on-the-ai-agent-terminal "Direct link to Expected Logs on the AI Agent Terminal")

    (venv) abhi@Fetchs-MacBook-Pro ILAgents % python3 ai_agent.pyINFO:__main__:Client agent started with address: agent1qw7u5sw63a88kmcn5j5kxf7q326u5hgmppvy2vpxlh3re6y0yp8253ec7xlINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Client agent registration complete! * Serving Flask app 'ai_agent' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5002 * Running on http://172.20.10.2:5002INFO:werkzeug:Press CTRL+C to quit{"version":1,"sender":"agent1qw7u5sw63a88kmcn5j5kxf7q326u5hgmppvy2vpxlh3re6y0yp8253ec7xl","target":"agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6","session":"c28d03fd-ad42-4c09-80b8-2bb8df9d7c3e","schema_digest":"model:14d760ab9a6127711e530c0f1bd84d5caa48c6bc6566ca489581d6918e6dff85","protocol_digest":"proto:a03398ea81d7aaaf67e72940937676eae0d019f8e1d8b5efbadfef9fd2e98bb2","payload":"eyJtZXNzYWdlIjoiaGV5In0=","expires":null,"nonce":null,"signature":"sig14ukfdrc98924wvx66xa2c32pk2wqyn69cepkvcpwahlygv3tc2t8hrzfkuzc9earscpnasdt8txpu99cyw24gmyspfdhqkxsn7a3lnq60mpaq"}INFO:fetchai:Got response looking up agent endpointhttp://localhost:8000/submitINFO:fetchai:Sent message to agentINFO:werkzeug:127.0.0.1 - - [30/Jan/2025 14:21:58] "POST /request HTTP/1.1" 200 -INFO:__main__:Received responseINFO:__main__:Processed response: {'response': 'Hello, AI Agent! I received your message: Hello uAgent!'}INFO:werkzeug:127.0.0.1 - - [30/Jan/2025 14:21:58] "POST /api/webhook HTTP/1.1" 200 -

Explanation of Communication Flow[â€‹](#explanation-of-communication-flow "Direct link to Explanation of Communication Flow")
---------------------------------------------------------------------------------------------------------------------------

*   The AI Agent sends a message using the **send\_message\_to\_agent** function.
*   The uAgent receives the message via the **@uagent.on\_message** handler.
*   The uAgent processes the message and responds using **await ctx.send()**.
*   The AI Agent receives the response through the **/api/webhook** endpoint.

Key Takeaways[â€‹](#key-takeaways "Direct link to Key Takeaways")
---------------------------------------------------------------

*   The **uAgent** handles structured messages using Fetch.aiâ€™s uAgents framework.
*   The **AI Agent** utilizes Fetch.aiâ€™s SDK for message transmission and parsing.
*   Communication between the two agents follows a request-response pattern using their respective handlers.

This setup can be extended to build more complex agent interactions involving dynamic data exchange, service orchestration, and autonomous decision-making.</content>
</page>

<page>
  <title>AI Agent to uAgent Communication | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/agent-communication/sdk-uagent-communication</url>
  <content>uAgent â†” AI Agent Communication Using Fetch.ai SDK and uAgents
--------------------------------------------------------------

This guide demonstrates how to enable communication between a Microservice Agent created using the uagents framework and an AI Agent created using the Fetch.ai SDK.

### uAgent Script (uagent.py)[â€‹](#uagent-script-uagentpy "Direct link to uAgent Script (uagent.py)")

Please remember to have the **uagents** and the **fetchai** package installed in the terminal in order to create and run the agents.

This uAgent will receive a message from the AI Agent and send a response back.

uagent.py

    from uagents import Agent, Context, Model# Define the request model the uAgent will handleclass Request(Model):    message: str# Define the response model the uAgent will send backclass Response(Model):    response: str# Initialize the uAgentuagent = Agent(    name="Sample uAgent",    port=8000,    endpoint=["http://localhost:8000/submit"])# Handle incoming messages with the Request model@uagent.on_message(model=Request)async def message_handler(ctx: Context, sender: str, msg: Request):    ctx.logger.info(f"Received message from {sender}: {msg.message}")    # Generate a response message    response = Response(response=f'Hello, AI Agent! I received your message:{msg.message}')        # Send the response back to the AI Agent    await ctx.send(sender, response)if __name__ == "__main__":    uagent.run()

### Explanation[â€‹](#explanation "Direct link to Explanation")

*   **Agent Initialization**: The uAgent listens on port 8000 for incoming messages.
*   **Message Handling**: When a message matching the `Request` model is received, the agent logs it and responds with a predefined message using the `Response` model.

Setting Up the AI Agent[â€‹](#setting-up-the-ai-agent "Direct link to Setting Up the AI Agent")
---------------------------------------------------------------------------------------------

The AI Agent will send messages to the uAgent and handle the response received from the uAgent.

### AI Agent Script (ai\_agent.py)[â€‹](#ai-agent-script-ai_agentpy "Direct link to AI Agent Script (ai_agent.py)")

ai\_agent.py

    import osfrom flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai.communication import send_message_to_agent, parse_message_from_agentimport loggingfrom dotenv import load_dotenv# Configure logginglogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)# Initialize Flask appapp = Flask(__name__)CORS(app)client_identity = Noneagent_response = Noneclass Request(Model):    message: str# Load environment variables from .env fileload_dotenv()def init_client():    """Initialize and register the client agent."""    global client_identity    try:        # Load the client identity from environment variables        client_identity = Identity.from_seed("Sample AI AGENT SEED PHRASE for communication", 0)        logger.info(f"Client agent started with address: {client_identity.address}")        readme = """        ![domain:innovation-lab](https://img.shields.io/badge/innovation--lab-3D8BD3)        domain:domain-of-your-agent        <description>This Agent can send a message to a uAgent and receive a message from a uAgent in string format.</description>        <use_cases>        <use_case>Send and receive messages with another uAgent.</use_case>        </use_cases>        <payload_requirements>            <description>This agent can only send and receive messages in text format.</description>            <payload>                <requirement>                    <parameter>message</parameter>                    <description>The agent sends and receives messages in text format.</description>                </requirement>            </payload>        </payload_requirements>        """        # Register the agent with Agentverse        register_with_agentverse(            identity=client_identity,            url="http://localhost:5002/api/webhook",            agentverse_token = os.getenv("AGENTVERSE_API_KEY"),            agent_title="Sample AI Agent communication"            readme=readme        )        logger.info("Client agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise@app.route('/request', methods=['POST'])def send_data():    """Send payload to the selected agent based on provided address."""    global agent_response    agent_response = None    try:        # Parse the request payload        data = request.json        payload = data.get('payload')  # Extract the payload dictionary        uagent_address = "agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6" #run the uagent.py copy the address and paste here                # Build the Data Model digest for the Request model to ensure message format consistency between the uAgent and AI Agent        model_digest = Model.build_schema_digest(Request)        # Send the payload to the specified agent        send_message_to_agent(            client_identity,  # Frontend client identity            uagent_address,  # Agent address where we have to send the data            payload,  # Payload containing the data            model_digest=model_digest        )        return jsonify({"status": "request_sent", "payload": payload})    except Exception as e:        logger.error(f"Error sending data to agent: {e}")        return jsonify({"error": str(e)}), 500# app route to get recieve the messages on the agent@app.route('/api/webhook', methods=['POST'])def webhook():    """Handle incoming messages from the dashboard agent."""    global agent_response    try:        # Parse the incoming webhook message        data = request.get_data().decode("utf-8")        logger.info("Received response")        message = parse_message_from_agent(data)        agent_response = message.payload        logger.info(f"Processed response: {agent_response}")        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500if __name__ == "__main__":    load_dotenv()    init_client()    app.run(host="0.0.0.0", port=5002)

### Explanation[â€‹](#explanation-1 "Direct link to Explanation")

*   **Data Model**: The Request and Response models define the structure of messages exchanged between the AI Agent and the uAgent. A correctly defined data model is essential for sending a message to the uAgent from an SDK-based AI Agent.
*   **Schema Validation**: The model digest ensures that messages conform to the expected schema before transmission, preventing format mismatches.
*   **Sending Data**: The AI Agent sends a message to the uAgent using the /request endpoint.
*   **Handling Response**: The AI Agent listens for responses using the /api/webhook endpoint.

Environment Variables[â€‹](#environment-variables "Direct link to Environment Variables")
---------------------------------------------------------------------------------------

Create a .env file and add the following environment variables:

    AGENTVERSE_API_KEY="YOUR_AGENTVERSE_API_KEY"AGENT_SECRET_KEY="YOUR_SECRET_KEY_FOR_AI_AGENT"

Replace the placeholders with your actual API keys and agent secrets. Refer to this [guide](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key) to get your Agentverse API Key.

Testing the Communication[â€‹](#testing-the-communication "Direct link to Testing the Communication")
---------------------------------------------------------------------------------------------------

### Step 1: Running the uAgent[â€‹](#step-1-running-the-uagent "Direct link to Step 1: Running the uAgent")

Start the uAgent by running the following command in your terminal:

#### uAgent Logs[â€‹](#uagent-logs "Direct link to uAgent Logs")

    (venv) abhi@Fetchs-MacBook-Pro ILAgents % python3 uagent.pyINFO:     [Sample uAgent]: Starting agent with address: agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: My name is Sample uAgent and my address is agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8000&address=agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: Starting server on http://0.0.0.0:8000 (Press CTRL+C to quit)INFO:     [uagents.registration]: Registration on Almanac API successfulINFO:     [uagents.registration]: Registering on almanac contract...INFO:     [uagents.registration]: Registering on almanac contract...complete

#### Copy the uAgent Address[â€‹](#copy-the-uagent-address "Direct link to Copy the uAgent Address")

Look for the uAgent address in the logs, which appears in this format:

    agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6

Copy this uAgent address and paste it into the AI Agent script at the following line:

    uagent_address="PASTE YOUR UAGENT ADDRESS HERE"

### Step 2: Running the AI Agent[â€‹](#step-2-running-the-ai-agent "Direct link to Step 2: Running the AI Agent")

Start the AI Agent by running the following command in your terminal:

#### AI Agent Logs[â€‹](#ai-agent-logs "Direct link to AI Agent Logs")

    (venv) abhi@Fetchs-MacBook-Pro ILAgents % python3 ai_agent.pyINFO:__main__:Client agent started with address: agent1qw7u5sw63a88kmcn5j5kxf7q326u5hgmppvy2vpxlh3re6y0yp8253ec7xlINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Client agent registration complete! * Serving Flask app 'ai_agent' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5002 * Running on http://172.20.10.2:5002INFO:werkzeug:Press CTRL+C to quit

### Step 3: Sending a message from Agent2 to Agent1[â€‹](#step-3-sending-a-message-from-agent2-to-agent1 "Direct link to Step 3: Sending a message from Agent2 to Agent1")

We will use the following curl command to send a message from the AI Agent to the uAgent:

    curl -X POST http://localhost:5002/request \-H "Content-Type: application/json" \-d '{  "payload": {"message": "Hello uAgent!"}}'

#### Expected Logs on the uAgent Terminal[â€‹](#expected-logs-on-the-uagent-terminal "Direct link to Expected Logs on the uAgent Terminal")

    (venv) abhi@Fetchs-MacBook-Pro ILAgents % python3 uagent.pyINFO:     [Sample uAgent]: Starting agent with address: agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: My name is Sample uAgent and my address is agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8000&address=agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6INFO:     [Sample uAgent]: Starting server on http://0.0.0.0:8000 (Press CTRL+C to quit)INFO:     [uagents.registration]: Registration on Almanac API successfulINFO:     [uagents.registration]: Registering on almanac contract...INFO:     [uagents.registration]: Registering on almanac contract...completeINFO:     [Sample uAgent]: Received message from agent1qw7u5sw63a88kmcn5j5kxf7q326u5hgmppvy2vpxlh3re6y0yp8253ec7xl: Hello uAgent!

#### Expected Logs on the AI Agent Terminal[â€‹](#expected-logs-on-the-ai-agent-terminal "Direct link to Expected Logs on the AI Agent Terminal")

    (venv) abhi@Fetchs-MacBook-Pro ILAgents % python3 ai_agent.pyINFO:__main__:Client agent started with address: agent1qw7u5sw63a88kmcn5j5kxf7q326u5hgmppvy2vpxlh3re6y0yp8253ec7xlINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Client agent registration complete! * Serving Flask app 'ai_agent' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5002 * Running on http://172.20.10.2:5002INFO:werkzeug:Press CTRL+C to quit{"version":1,"sender":"agent1qw7u5sw63a88kmcn5j5kxf7q326u5hgmppvy2vpxlh3re6y0yp8253ec7xl","target":"agent1qgd54rrq8ex4uhdxe6qg0sklz7h7dkacdk9rz4ec0l304wghw88sg35rfk6","session":"c28d03fd-ad42-4c09-80b8-2bb8df9d7c3e","schema_digest":"model:14d760ab9a6127711e530c0f1bd84d5caa48c6bc6566ca489581d6918e6dff85","protocol_digest":"proto:a03398ea81d7aaaf67e72940937676eae0d019f8e1d8b5efbadfef9fd2e98bb2","payload":"eyJtZXNzYWdlIjoiaGV5In0=","expires":null,"nonce":null,"signature":"sig14ukfdrc98924wvx66xa2c32pk2wqyn69cepkvcpwahlygv3tc2t8hrzfkuzc9earscpnasdt8txpu99cyw24gmyspfdhqkxsn7a3lnq60mpaq"}INFO:fetchai:Got response looking up agent endpointhttp://localhost:8000/submitINFO:fetchai:Sent message to agentINFO:werkzeug:127.0.0.1 - - [30/Jan/2025 14:21:58] "POST /request HTTP/1.1" 200 -INFO:__main__:Received responseINFO:__main__:Processed response: {'response': 'Hello, AI Agent! I received your message: Hello uAgent!'}INFO:werkzeug:127.0.0.1 - - [30/Jan/2025 14:21:58] "POST /api/webhook HTTP/1.1" 200 -

Explanation of Communication Flow[â€‹](#explanation-of-communication-flow "Direct link to Explanation of Communication Flow")
---------------------------------------------------------------------------------------------------------------------------

*   The AI Agent sends a message using the **send\_message\_to\_agent** function.
*   The uAgent receives the message via the **@uagent.on\_message** handler.
*   The uAgent processes the message and responds using **await ctx.send()**.
*   The AI Agent receives the response through the **/api/webhook** endpoint.

Key Takeaways[â€‹](#key-takeaways "Direct link to Key Takeaways")
---------------------------------------------------------------

*   The **uAgent** handles structured messages using Fetch.aiâ€™s uAgents framework.
*   The **AI Agent** utilizes Fetch.aiâ€™s SDK for message transmission and parsing.
*   Communication between the two agents follows a request-response pattern using their respective handlers.

This setup can be extended to build more complex agent interactions involving dynamic data exchange, service orchestration, and autonomous decision-making.</content>
</page>

<page>
  <title>LangGraph Adapter Example | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/adapters/langgraph-adapter-example</url>
  <content>LangGraph Adapter for uAgents
-----------------------------

This example demonstrates how to integrate a **LangGraph agent** with the **uAgents ecosystem** using the uAgents Adapter package. LangGraph provides powerful orchestration capabilities for LLM applications through directed graphs.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

The LangGraph adapter allows you to:

*   Wrap LangGraph agents as uAgents for seamless communication
*   Enable LangGraph agents to participate in the Agentverse ecosystem
*   Leverage advanced orchestration for complex reasoning while maintaining uAgent compatibility

Example Implementation[â€‹](#example-implementation "Direct link to Example Implementation")
------------------------------------------------------------------------------------------

*   Create an agent with file name `agent.py`

agent.py

    import osimport timefrom dotenv import load_dotenvfrom langchain_openai import ChatOpenAIfrom langchain_community.tools.tavily_search import TavilySearchResultsfrom langgraph.prebuilt import chat_agent_executorfrom langchain_core.messages import HumanMessagefrom uagents_adapter import LangchainRegisterTool, cleanup_uagent# Load environment variablesload_dotenv()# Set your API keys - for production, use environment variables instead of hardcodingOPENAI_API_KEY = os.environ["OPENAI_API_KEY"]TAVILY_API_KEY = os.environ["TAVILY_API_KEY"]# Get API token for AgentverseAPI_TOKEN = os.environ["AGENTVERSE_API_KEY"]if not API_TOKEN:    raise ValueError("Please set AGENTVERSE_API_KEY environment variable")# Set up tools and LLMtools = [TavilySearchResults(max_results=3)]model = ChatOpenAI(temperature=0)# LangGraph-based executorapp = chat_agent_executor.create_tool_calling_executor(model, tools)# Wrap LangGraph agent into a function for UAgentdef langgraph_agent_func(query):    # Handle input if it's a dict with 'input' key    if isinstance(query, dict) and 'input' in query:        query = query['input']        messages = {"messages": [HumanMessage(content=query)]}    final = None    for output in app.stream(messages):        final = list(output.values())[0]  # Get latest    return final["messages"][-1].content if final else "No response"# Register the LangGraph agent via uAgenttool = LangchainRegisterTool()agent_info = tool.invoke(    {        "agent_obj": langgraph_agent_func,        "name": "langgraph_tavily_agent",        "port": 8080,        "description": "A LangGraph-based Tavily-powered search agent",        "api_token": API_TOKEN,        "mailbox": True    })print(f"âœ… Registered LangGraph agent: {agent_info}")# Keep the agent alivetry:    while True:        time.sleep(1)except KeyboardInterrupt:    print("ğŸ›‘ Shutting down LangGraph agent...")    cleanup_uagent("langgraph_tavily_agent")    print("âœ… Agent stopped.")

Key Components[â€‹](#key-components "Direct link to Key Components")
------------------------------------------------------------------

1.  **LangGraph Setup**:
    
    *   Creates a tool-calling executor using LangGraph's prebuilt components
    *   Configures Tavily search as the tool for retrieving information
    *   Uses OpenAI's ChatGPT for LLM capabilities
2.  **Function Wrapper**:
    
    *   Wraps the LangGraph app in a function that accepts queries
    *   Handles input format conversion
    *   Processes streaming output from LangGraph
3.  **uAgent Registration**:
    
    *   Uses UAgentRegisterTool to register the LangGraph function as a uAgent
    *   Configures a port, description, and mailbox for persistence
    *   Generates a unique address for agent communication

Sample requirements.txt[â€‹](#sample-requirementstxt "Direct link to Sample requirements.txt")
--------------------------------------------------------------------------------------------

Here's a sample `requirements.txt` file you can use for this example:

    uagents==0.22.3uagents-adapter==0.2.1langchain-openai==0.3.14langchain-community==0.3.21langgraph==0.3.31dotenv==0.9.9

Interacting with the Agent[â€‹](#interacting-with-the-agent "Direct link to Interacting with the Agent")
------------------------------------------------------------------------------------------------------

You can interact with this LangGraph agent through any uAgent using the chat protocol. Here's a client implementation:

agent\_client.py

    from datetime import datetimefrom uuid import uuid4from uagents import Agent, Protocol, Context#import the necessary components from the chat protocolfrom uagents_core.contrib.protocols.chat import (    ChatAcknowledgement,    ChatMessage,    TextContent,    chat_protocol_spec,)# Initialise agent2agent2 = Agent(name="client_agent",               port = 8082,               mailbox=True,               seed="client agent testing seed"               )# Initialize the chat protocolchat_proto = Protocol(spec=chat_protocol_spec)langgraph_agent_address = "agent1q0zyxrneyaury3f5c7aj67hfa5w65cykzplxkst5f5mnyf4y3em3kplxn4t" # Update with your Langgraph Agent's address# Startup Handler - Print agent details@agent2.on_event("startup")async def startup_handler(ctx: Context):    # Print agent details    ctx.logger.info(f"My name is {ctx.agent.name} and my address is {ctx.agent.address}")    # Send initial message to agent2    initial_message = ChatMessage(        timestamp=datetime.utcnow(),        msg_id=uuid4(),        content=[TextContent(type="text", text="I want to send query to tavily agent that Give me a list of latest agentic AI trends")]    )    await ctx.send(langgraph_agent_address, initial_message)# Message Handler - Process received messages and send acknowledgements@chat_proto.on_message(ChatMessage)async def handle_message(ctx: Context, sender: str, msg: ChatMessage):    for item in msg.content:        if isinstance(item, TextContent):            # Log received message            ctx.logger.info(f"Received message from {sender}: {item.text}")                        # Send acknowledgment            ack = ChatAcknowledgement(                timestamp=datetime.utcnow(),                acknowledged_msg_id=msg.msg_id            )            await ctx.send(sender, ack)            # Acknowledgement Handler - Process received acknowledgements@chat_proto.on_message(ChatAcknowledgement)async def handle_acknowledgement(ctx: Context, sender: str, msg: ChatAcknowledgement):    ctx.logger.info(f"Received acknowledgement from {sender} for message: {msg.acknowledged_msg_id}")# Include the protocol in the agent to enable the chat functionality# This allows the agent to send/receive messages and handle acknowledgements using the chat protocolagent2.include(chat_proto, publish_manifest=True)if __name__ == '__main__':    agent2.run()

Why Use LangGraph with uAgents?[â€‹](#why-use-langgraph-with-uagents "Direct link to Why Use LangGraph with uAgents?")
--------------------------------------------------------------------------------------------------------------------

LangGraph offers several advantages when combined with uAgents:

*   **Advanced Orchestration**: Create complex reasoning flows using directed graphs
*   **State Management**: Handle complex multi-turn conversations with state persistence
*   **Tool Integration**: Easily connect to external services and APIs
*   **Debugging Capabilities**: Inspect and debug agent reasoning processes

By wrapping LangGraph with the uAgents adapter, you get the best of both worlds: sophisticated LLM orchestration with the decentralized communication capabilities of uAgents.

Getting Started[â€‹](#getting-started "Direct link to Getting Started")
---------------------------------------------------------------------

1.  In a directory get both the agents above `agent.py` and `agent_client.py`.
    
2.  Install required packages:
    
        pip install uagents>=0.22.3 uagents-adapter>=0.2.1 langchain-openai>=0.3.14 langchain-community>=0.3.21 langgraph>=0.3.31  dotenv>=0.9.9
    
3.  Set up your environment variables in `.env` file:
    
        OPENAI_API_KEY=your_openai_keyTAVILY_API_KEY=your_tavily_key  AGENTVERSE_API_KEY=your_agentverse_key
    
4.  Run the LangGraph agent:
    
5.  In a separate terminal, run the client agent:
    

Expected Outputs[â€‹](#expected-outputs "Direct link to Expected Outputs")
------------------------------------------------------------------------

When running the examples, you should expect to see outputs similar to these:

### LangGraph Agent[â€‹](#langgraph-agent "Direct link to LangGraph Agent")

When running the LangGraph agent:

    (venv) Fetchs-MacBook-Pro test examples % python3 agent.py INFO:     [langgraph_tavily_agent]: Starting agent with address: agent1q0zyxrneyaury3f5c7aj67hfa5w65cykzplxkst5f5mnyf4y3em3kplxn4tINFO:     [langgraph_tavily_agent]: Agent 'langgraph_tavily_agent' started with address: agent1q0zyxrneyaury3f5c7aj67hfa5w65cykzplxkst5f5mnyf4y3em3kplxn4tINFO:     [langgraph_tavily_agent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8080&address=agent1q0zyxrneyaury3f5c7aj67hfa5w65cykzplxkst5f5mnyf4y3em3kplxn4tINFO:     [langgraph_tavily_agent]: Starting server on http://0.0.0.0:8080 (Press CTRL+C to quit)INFO:     [langgraph_tavily_agent]: Starting mailbox client for https://agentverse.aiINFO:     [langgraph_tavily_agent]: Mailbox access token acquiredINFO:     [langgraph_tavily_agent]: Received structured output response from agent1qtlpfshtlcxekgrfcpmv7m9zpajuwu7d5jfyachvpa4u3dkt6k0uwwp2lct: Hello, Tavily Agent. Could you please provide a list of the latest trends in agentic AI? I am interested in understanding how agent-based artificial intelligence is evolving and what innovations or developments stand out in this field. Thank you!INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"âœ… Registered LangGraph agent: Created uAgent 'langgraph_tavily_agent' with address agent1q0zyxrneyaury3f5c7aj67hfa5w65cykzplxkst5f5mnyf4y3em3kplxn4t on port 8080INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"INFO:     [langgraph_tavily_agent]: Got a message from agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49INFO:     [langgraph_tavily_agent]: Got a text message from agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49: I want to send query to tavily agent that Give me a list of latest agentic AI trendsINFO:     [langgraph_tavily_agent]: Sending structured output prompt to {'title': 'QueryMessage', 'type': 'object', 'properties': {'query': {'title': 'Query', 'type': 'string'}}, 'required': ['query']}INFO:     [langgraph_tavily_agent]: Sent structured output prompt to agent1qtlpfshtlcxekgrfcpmv7m9zpajuwu7d5jfyachvpa4u3dkt6k0uwwp2lctINFO:     [langgraph_tavily_agent]: Got an acknowledgement from agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49 for 451f41aa-be41-471f-bddc-276caffb7d94Connecting agent 'langgraph_tavily_agent' to Agentverse...INFO:     [mailbox]: Successfully registered as mailbox agent in AgentverseSuccessfully connected agent 'langgraph_tavily_agent' to AgentverseUpdating agent 'langgraph_tavily_agent' README on Agentverse...Successfully updated agent 'langgraph_tavily_agent' README on AgentverseINFO:     [langgraph_tavily_agent]: Received structured output response from agent1qtlpfshtlcxekgrfcpmv7m9zpajuwu7d5jfyachvpa4u3dkt6k0uwwp2lct: Subject: Request for Information on Latest Agentic AI TrendsHi Tavily Agent,I hope this message finds you well. I am reaching out to inquire about the latest trends in agentic AI technology. As this area is rapidly evolving, I am keen to stay updated on the most recent developments and innovations.Could you please provide me with a comprehensive list of the latest trends in agentic AI? I'm particularly interested in understanding how these trends might impact various industries and potential future applications.Thank you for your assistance. I look forward to your response.---INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"

### Client Agent[â€‹](#client-agent "Direct link to Client Agent")

When running the client agent:

    (venv) Fetchs-MacBook-Pro test examples % python3 agent_client.pyINFO:     [client_agent]: Starting agent with address: agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49INFO:     [client_agent]: My name is client_agent and my address is agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49INFO:     [client_agent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A8082&address=agent1qwwng5d939vyaa6d2trnllyltgrndtfd6z44h8ey8a56hf4dcatsytgzm49INFO:     [client_agent]: Starting server on http://0.0.0.0:8082 (Press CTRL+C to quit)INFO:     [client_agent]: Starting mailbox client for https://agentverse.aiINFO:     [uagents.registration]: Registration on Almanac API successfulINFO:     [uagents.registration]: Almanac contract registration is up to date!INFO:     [client_agent]: Manifest published successfully: AgentChatProtocolINFO:     [client_agent]: Mailbox access token acquiredINFO:     [client_agent]: Received message from agent1q0zyxrneyaury3f5c7aj67hfa5w65cykzplxkst5f5mnyf4y3em3kplxn4t: Here are some of the latest trends in agentic AI:1. **The Next Frontier: The Rise of Agentic AI - Adams Street Partners**   - **Link:** [The Next Frontier: The Rise of Agentic AI - Adams Street Partners](https://www.adamsstreetpartners.com/insights/the-next-frontier-the-rise-of-agentic-ai/)   - **Summary:** Several converging trends have set the stage for agentic AI, including advances in Large Language Models, improved reasoning, planning, and multistep processes.2. **7 Agentic AI Trends To Watch for 2025 - ServiceNow**   - **Link:** [7 Agentic AI Trends To Watch for 2025 - ServiceNow](https://www.servicenow.com/products/ai-agents/agentic-ai-trends.html)   - **Summary:** Explore the latest agentic AI trends shaping the future of work, from hyperautomation to decision intelligence, and how it can transform businesses.3. **Agentic AI: Three themes to watch for 2025 - Constellation Research**   - **Link:** [Agentic AI: Three themes to watch for 2025 - Constellation Research](https://www.constellationr.com/blog-news/insights/agentic-ai-three-themes-watch-2025)   - **Summary:** This article discusses three themes to watch in agentic AI for 2025, including horizontal approaches vs. platform-specific strategies and the proliferation of agentic AI launches by various vendors.These sources provide insights into the evolving landscape of agentic AI and the key trends that are shaping the future of this field.INFO:     [client_agent]: Received acknowledgement from agent1q0zyxrneyaury3f5c7aj67hfa5w65cykzplxkst5f5mnyf4y3em3kplxn4t for message: 1cdda4bd-4597-42a9-b6f1-13c6ca67a0eaINFO:     [client_agent]: Received message from agent1q0zyxrneyaury3f5c7aj67hfa5w65cykzplxkst5f5mnyf4y3em3kplxn4t: ### Latest Trends in Agentic AI Technology:1. **[7 Agentic AI Trends To Watch for 2025 - ServiceNow](https://www.servicenow.com/products/ai-agents/agentic-ai-trends.html)**   - Explore the latest agentic AI trends shaping the future of work, from hyperautomation to decision intelligence, and how it can transform your business.2. **[Agentic AI: Three themes to watch for 2025 - Constellation Research](https://www.constellationr.com/blog-news/insights/agentic-ai-three-themes-watch-2025)**   - Three themes to watch in agentic AI, including horizontal approaches vs. platform-specific trends and the proliferation of agentic AI platforms across various vendors.3. **[The Top Customer Service Trends and Technologies for 2025](https://www.destinationcrm.com/Articles/Editorial/Magazine-Features/The-Top-Customer-Service-Trends-and-Technologies-for-2025-Agentic-AI-Is-Poised-to-Remake-Self-Service-168751.aspx)**   - Agentic AI is poised to remake self-service in customer service, with predictions that by 2030, 50% of service requests will be initiated by machine customers powered by agentic AI systems.4. **[Future Trends in Agentic AI Development: What's Next for Intelligent Automation](https://www.imbrace.co/future-trends-in-agentic-ai-development-whats-next-for-intelligent-automation/)**   - Trends include providing clear insights into decision-making, ensuring alignment with ethical guidelines, expanded applications across industries like logistics and healthcare, and the integration of Explainable AI (XAI).5. **[5 Reasons Why Agentic AI Will Transform Industries by 2030](https://hyperight.com/5-reasons-why-agentic-ai-will-transform-industries-by-2030/)**   - Agentic AI is expected to enhance productivity and efficiency, reshape industries by 2030, and be incorporated into 33% of enterprise software applications by 2028.6. **[Agentic AI Trends - What to expect in the near future - Atera](https://www.atera.com/blog/agentic-ai-trends/)**   - Agentic AI is set to revolutionize customer service, with researchers predicting a significant impact on customer service operations.These trends highlight the advancements and potential impacts of agentic AI technology across various industries and applications.

Try different queries to see how the LangGraph agent processes them and returns search-enhanced responses through the uAgents ecosystem!</content>
</page>

<page>
  <title>Frontend Web App Integration | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/integrations/frontend-integration</url>
  <content>Build a Frontend Web Application with uAgents and Open Food Facts API
---------------------------------------------------------------------

This guide demonstrates how to create a complete web application that integrates uAgents with external APIs using a Flask frontend. We'll build a food product discovery system using the Open Food Facts API.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

The Frontend Integration example provides:

*   **Two Specialized uAgents** - Search Agent and Info Agent
*   **REST API Endpoints** using uAgents framework
*   **External API Integration** with Open Food Facts
*   **Modern Web Interface** built with Flask and HTML/CSS/JavaScript
*   **Real-time Health Monitoring** of all services

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

Before you begin, ensure you have:

*   **Python 3.11+** installed
*   **Basic knowledge** of Flask and web development
*   **Understanding** of REST APIs and HTTP requests

Installation[â€‹](#installation "Direct link to Installation")
------------------------------------------------------------

### 1\. Clone the Complete Example[â€‹](#1-clone-the-complete-example "Direct link to 1. Clone the Complete Example")

    git clone https://github.com/fetchai/innovation-lab-examples.gitcd innovation-lab-examples/frontend-integration

### 2\. Set Up Virtual Environment[â€‹](#2-set-up-virtual-environment "Direct link to 2. Set Up Virtual Environment")

    # Create virtual environmentpython3 -m venv venv# Activate virtual environment# On macOS/Linux:source venv/bin/activate# On Windows:# venv\Scripts\activate

### 3\. Install Dependencies[â€‹](#3-install-dependencies "Direct link to 3. Install Dependencies")

    pip install -r requirements.txt

Architecture Overview[â€‹](#architecture-overview "Direct link to Architecture Overview")
---------------------------------------------------------------------------------------

### Two Specialized uAgents[â€‹](#two-specialized-uagents "Direct link to Two Specialized uAgents")

Our system consists of two specialized microservices:

1.  **Search Agent** (Port 8001): Handles product search queries
2.  **Info Agent** (Port 8002): Retrieves detailed product information

### Frontend Application[â€‹](#frontend-application "Direct link to Frontend Application")

*   **Flask Web App** (Port 5000): Modern web interface to interact with agents

Quick Start[â€‹](#quick-start "Direct link to Quick Start")
---------------------------------------------------------

### 1\. Start Search Agent[â€‹](#1-start-search-agent "Direct link to 1. Start Search Agent")

**Terminal 1:**

    source venv/bin/activate  # Activate venv if not already activepython3 product_search_agent.py

### 2\. Start Info Agent[â€‹](#2-start-info-agent "Direct link to 2. Start Info Agent")

**Terminal 2:**

    source venv/bin/activate  # Activate venv if not already activepython3 product_info_agent.py

### 3\. Start Frontend[â€‹](#3-start-frontend "Direct link to 3. Start Frontend")

**Terminal 3:**

    source venv/bin/activate  # Activate venv if not already activepython3 frontend_app.py

### 4\. Access the Web Interface[â€‹](#4-access-the-web-interface "Direct link to 4. Access the Web Interface")

Open your browser to: [http://127.0.0.1:5000](http://127.0.0.1:5000/)

Agent Implementation Details[â€‹](#agent-implementation-details "Direct link to Agent Implementation Details")
------------------------------------------------------------------------------------------------------------

### Search Agent (Port 8001)[â€‹](#search-agent-port-8001 "Direct link to Search Agent (Port 8001)")

The search agent handles product discovery using natural language queries:

    from uagents import Agent, Context, Modelimport openfoodfactsclass SearchRequest(Model):    query: strclass ProductInfo(Model):    code: str    product_name: str    brands: str    categories: str    image_url: strclass SearchResponse(Model):    query: str    count: int    products: List[ProductInfo]    error: Optional[str] = Nonesearch_agent = Agent(    name="product_search_agent",    port=8001,    endpoint=["http://127.0.0.1:8001/submit"],)@search_agent.on_rest_post("/search", SearchRequest, SearchResponse)async def search_products(ctx: Context, req: SearchRequest) -> SearchResponse:    try:        query = req.query        ctx.logger.info(f"Searching for products with query: {query}")                # Search products using Open Food Facts API        results = api.product.text_search(query, page_size=10)                # Extract relevant information        products = []        for product in results.get("products", [])[:10]:            product_info = ProductInfo(                code=product.get("code", "N/A"),                product_name=product.get("product_name", "N/A"),                brands=product.get("brands", "N/A"),                categories=product.get("categories", "N/A"),                image_url=product.get("image_url", "")            )            products.append(product_info)                return SearchResponse(            query=query,            count=results.get("count", 0),            products=products        )    except Exception as e:        return SearchResponse(            query=req.query,            count=0,            products=[],            error=f"Failed to search products: {str(e)}"        )

### Info Agent (Port 8002)[â€‹](#info-agent-port-8002 "Direct link to Info Agent (Port 8002)")

The info agent provides detailed product information using exact barcodes:

    from uagents import Agent, Context, Modelimport requestsclass ProductRequest(Model):    barcode: strclass ProductInfoResponse(Model):    barcode: str    product_name: str    brands: str    categories: str    ingredients_text: str    allergens: str    nutrition_grades: str    ecoscore_grade: str    image_url: str    countries: str    stores: str    packaging: str    quantity: str    energy_100g: str    fat_100g: str    sugars_100g: str    salt_100g: str    error: Optional[str] = Noneinfo_agent = Agent(    name="product_info_agent",    port=8002,    endpoint=["http://127.0.0.1:8002/submit"],)@info_agent.on_rest_post("/product", ProductRequest, ProductInfoResponse)async def get_product_info(ctx: Context, req: ProductRequest) -> ProductInfoResponse:    try:        barcode = req.barcode        ctx.logger.info(f"Getting product info for barcode: {barcode}")                # Use direct API call to Open Food Facts        url = f"https://world.openfoodfacts.org/api/v0/product/{barcode}.json"        headers = {            'User-Agent': 'uAgents-FoodInfo/1.0 (https://github.com/fetchai/uAgents)'        }                response = requests.get(url, headers=headers, timeout=10)        data = response.json()                if data.get('status') != 1 or "product" not in data:            return ProductInfoResponse(                barcode=barcode,                # ... other fields with "N/A"                error="Product not found"            )                product = data["product"]                # Extract comprehensive product information        return ProductInfoResponse(            barcode=barcode,            product_name=product.get("product_name", "N/A"),            brands=product.get("brands", "N/A"),            categories=product.get("categories", "N/A"),            ingredients_text=product.get("ingredients_text", "N/A"),            allergens=product.get("allergens", "N/A"),            nutrition_grades=product.get("nutrition_grades", "N/A"),            ecoscore_grade=product.get("ecoscore_grade", "N/A"),            image_url=product.get("image_url", ""),            countries=product.get("countries", "N/A"),            stores=product.get("stores", "N/A"),            packaging=product.get("packaging", "N/A"),            quantity=product.get("quantity", "N/A"),            energy_100g=str(product.get("nutriments", {}).get("energy_100g", "N/A")),            fat_100g=str(product.get("nutriments", {}).get("fat_100g", "N/A")),            sugars_100g=str(product.get("nutriments", {}).get("sugars_100g", "N/A")),            salt_100g=str(product.get("nutriments", {}).get("salt_100g", "N/A"))        )    except Exception as e:        return ProductInfoResponse(            barcode=req.barcode,            # ... other fields with "N/A"            error=f"Failed to get product info: {str(e)}"        )

Frontend Implementation[â€‹](#frontend-implementation "Direct link to Frontend Implementation")
---------------------------------------------------------------------------------------------

### Flask Backend[â€‹](#flask-backend "Direct link to Flask Backend")

The Flask application serves as a bridge between the web interface and uAgents:

    from flask import Flask, render_template, request, jsonifyimport requestsapp = Flask(__name__)# Agent endpointsAGENTS = {    "search": "http://127.0.0.1:8001",    "info": "http://127.0.0.1:8002"}@app.route('/search_products', methods=['POST'])def search_products():    """Search products via search agent"""    try:        query = request.form.get('query', '').strip()        if not query:            return jsonify({"error": "Please provide a search query"})                # Call search agent with POST request        payload = {"query": query}        response = requests.post(f"{AGENTS['search']}/search", json=payload)        response.raise_for_status()                result = response.json()                # Format the results for display        formatted_results = []        if result.get('products'):            for product in result['products'][:10]:                formatted_product = {                    'name': product.get('product_name', 'N/A'),                    'brands': product.get('brands', 'N/A'),                    'barcode': product.get('code', 'N/A'),                    'categories': product.get('categories', 'N/A'),                    'image_url': product.get('image_url', '')                }                formatted_results.append(formatted_product)                return jsonify({            "success": True,             "count": result.get('count', 0),            "query": query,            "products": formatted_results        })            except requests.RequestException as e:        return jsonify({"error": f"Failed to connect to search agent: {str(e)}"})    except Exception as e:        return jsonify({"error": f"Search failed: {str(e)}"})@app.route('/health')def health_check():    """Check health of all agents"""    health_status = {}        for agent_name, agent_url in AGENTS.items():        try:            response = requests.get(f"{agent_url}/health", timeout=5)            if response.status_code == 200:                health_data = response.json()                health_status[agent_name] = {                    "status": "healthy",                     "url": agent_url,                     "agent_info": health_data                }            else:                health_status[agent_name] = {"status": "unhealthy", "url": agent_url}        except:            health_status[agent_name] = {"status": "offline", "url": agent_url}        return jsonify(health_status)

Testing the Application[â€‹](#testing-the-application "Direct link to Testing the Application")
---------------------------------------------------------------------------------------------

### 1\. Product Search Testing[â€‹](#1-product-search-testing "Direct link to 1. Product Search Testing")

**Search Query: "chocolate"**

1.  Navigate to [http://127.0.0.1:5000](http://127.0.0.1:5000/)
2.  In the "Search Products" section, enter `chocolate` in the search field
3.  Click "Search Products" to see results

The search will return multiple chocolate products with product names, brands, categories, barcodes, and images.

### 2\. Product Details Testing[â€‹](#2-product-details-testing "Direct link to 2. Product Details Testing")

**Barcode Query: "3017624010701"**

1.  In the "Get Product Information" section, enter the barcode `3017624010701`
2.  Click "Get Product Info" to retrieve detailed information

This will display comprehensive information for Nutella including basic information, ingredients, nutrition facts, and quality scores.

### 3\. Health Status Monitoring[â€‹](#3-health-status-monitoring "Direct link to 3. Health Status Monitoring")

Click the "Check Agent Status" button to verify all services are running:

This displays the real-time status of both Search Agent (Port 8001) and Info Agent (Port 8002).

Key Features Demonstrated[â€‹](#key-features-demonstrated "Direct link to Key Features Demonstrated")
---------------------------------------------------------------------------------------------------

### 1\. Microservice Architecture[â€‹](#1-microservice-architecture "Direct link to 1. Microservice Architecture")

*   **Separation of Concerns**: Each agent has a specific responsibility
*   **Independent Scaling**: Agents can be scaled independently
*   **Fault Isolation**: Failure in one service doesn't affect others

### 2\. REST API Design[â€‹](#2-rest-api-design "Direct link to 2. REST API Design")

*   **Proper HTTP Methods**: POST for data operations, GET for health checks
*   **Pydantic Models**: Type-safe request/response validation
*   **Error Handling**: Comprehensive error responses

### 3\. Frontend Integration[â€‹](#3-frontend-integration "Direct link to 3. Frontend Integration")

*   **Ajax Requests**: Asynchronous communication with agents
*   **Real-time Updates**: Dynamic UI updates without page refresh
*   **Health Monitoring**: Live status checking of backend services

### 4\. External API Integration[â€‹](#4-external-api-integration "Direct link to 4. External API Integration")

*   **HTTP Client Usage**: Direct API calls to Open Food Facts
*   **Data Transformation**: Converting external API responses to internal models
*   **Error Handling**: Graceful handling of external API failures

GitHub Repository[â€‹](#github-repository "Direct link to GitHub Repository")
---------------------------------------------------------------------------

For the complete code and additional examples, visit the [Frontend Integration Example](https://github.com/fetchai/innovation-lab-examples/tree/main/frontend-integration) repository.

This repository includes:

*   âœ… Complete agent implementations
*   âœ… Flask web application
*   âœ… Modern responsive web interface
*   âœ… Docker configuration
*   âœ… Comprehensive documentation
*   âœ… Testing examples</content>
</page>

<page>
  <title>Langchain Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/examples/other-frameworks/langchain</url>
  <content>Getting a langchain tool on Agentverse Marketplace
--------------------------------------------------

This document is a comprehensive guide to help users integrate **LangChain** tools into **Fetch.ai's** ecosystem using the provided **Alphavantage Agent** and **User Agent scripts**. These agents work together to fetch stock prices and demonstrate how LangChain tools can interact seamlessly with Agentverse agents.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

### Purpose[â€‹](#purpose "Direct link to Purpose")

The purpose of this script is to:

*   **Use LangChain integration** to fetch stock prices from the Alpha Vantage API.
*   **Demonstrate communication** between agents within the Fetch.ai Agentverse.
*   **Provide** a clear example of webhook-based communication.

### Key Features[â€‹](#key-features "Direct link to Key Features")

*   The\_\_ Alphavantage Agent\_\_ handles requests for stock prices using the Alpha Vantage API.
*   The **User Agent** acts as a client to forward requests and retrieve responses.
*   Communication is done via **HTTP webhooks** registered with Fetch.aiâ€™s Agentverse.

### Architecture Diagram[â€‹](#architecture-diagram "Direct link to Architecture Diagram")

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

### Environment Setup[â€‹](#environment-setup "Direct link to Environment Setup")

*   **Python 3.8** or higher.
*   A **virtual environment** (recommended).
*   **Flask** and related libraries installed.

### Required Libraries[â€‹](#required-libraries "Direct link to Required Libraries")

Install the necessary libraries using:

    pip install flask flask-cors fetchai langchain â€œflask[async]â€

note

**Note:** For more advanced dependency management, consider creating a `requirements.txt` and using `pip install -r requirements.txt`.

### Environment Variables[â€‹](#environment-variables "Direct link to Environment Variables")

Create a `.env` file in the project directory with the following content:

    AV_STOCKPRICE_AI_KEY=<your_alpha_vantage_ai_key>AGENTVERSE_API_KEY=<your_agentverse_api_key>USER_STOCKPRICE_AI_KEY=<your_user_stockprice_ai_key>ALPHAVANTAGE_API_KEY=<your_agentverse_api_key>

Replace the placeholders with your respective API keys. Sign up for a free API key at [Alpha Vantage](https://www.alphavantage.co/).

Alphvantage Agent Script[â€‹](#alphvantage-agent-script "Direct link to Alphvantage Agent Script")
------------------------------------------------------------------------------------------------

The **Alphavantage Agent** Script focuses on:

*   **Registering** with Fetch.aiâ€™s Agentverse.
*   **Handling requests** to fetch stock prices.
*   **Responding** to the User Agent with stock price data.

### Script Breakdown (stockprice.py)[â€‹](#script-breakdown-stockpricepy "Direct link to Script Breakdown (stockprice.py)")

**Required Libraries**

The script imports the required modules for:

*   **Identity Management**: Fetch.aiâ€™s Identity and registration modules.
*   **Communication**: Flask for HTTP handling and Fetch.aiâ€™s communication utilities.
*   **Alpha Vantage API**: LangChainâ€™s `AlphaVantageAPIWrapper`.

    import osimport loggingimport asynciofrom uagents_core.crypto import Identityfrom flask import Flask, request, jsonifyfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentfrom langchain_community.utilities.alpha_vantage import AlphaVantageAPIWrapper

**Setting Up Logging**

Logging is configured to monitor the agentâ€™s activities:

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)

**Flask Webhook**

The Flask app is created to handle incoming webhook requests:

    flask_app = Flask(__name__)

**Alpha Vantage Integration**

An instance of the Alpha Vantage wrapper is created:

    alpha_vantage = AlphaVantageAPIWrapper()

**Fetch Stock Price Function**

The get\_stock\_price function fetches stock prices asynchronously from the Alpha Vantage API:

    async def get_stock_price(symbol):    response = alpha_vantage._get_quote_endpoint(symbol)    return response['Global Quote']['05. price']

**Webhook Endpoint**

The /webhook route handles requests sent by other agents:

    @flask_app.route('/webhook', methods=['POST'])async def webhook():    global response    global av_identity    try:        # Parse incoming message        data = request.get_data().decode('utf-8')        message = parse_message_from_agent(data)        symbol = message.payload.get("request", "")        agent_address = message.sender        if not symbol:            return jsonify({"status": "error", "message": "No data path provided"}), 400        price = await get_stock_price(symbol)        print(price)        payload = {'price': price}        send_message_to_agent(av_identity, agent_address, payload)        return jsonify({"status": "wiki_content_sent"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"status": "error", "message": str(e)}), 500

**Registering the Agent**

The init\_agent function registers the Alphavantage Agent with Fetch.aiâ€™s Agentverse:

    def init_agent():   global av_identity   try:       av_identity = Identity.from_seed(os.getenv("AV_STOCKPRICE_AI_KEY"), 0)       register_with_agentverse(           identity=av_identity,           url="http://localhost:5008/webhook",           agentverse_token=os.getenv("AGENTVERSE_API_KEY"),           agent_title="Alphavantage Stock Price Langchain tool",           # Define the client agent's metadata           readme = """               <description>This is langchain based alphavantage tool to get the latest stock price for the given symbol.</description>               <use_cases>                   <use_case>Gives you the stock price of given symbol.</use_case>               </use_cases>               <payload_requirements>               <description>Expects the symbol for which you want stock price for.</description>                   <payload>                       <requirement>                           <parameter>symbol</parameter>                           <description>Symbol for which you want to check the stock price?</description>                       </requirement>                   </payload>               </payload_requirements>           """       )       logger.info("Langchain Alphavantage tool agent registered successfully!")   except Exception as e:       logger.error(f"Error initializing agent: {e}")       raise

**Running the Agent**

The Flask server runs on port 5008 to handle requests:

    if __name__ == "__main__":    init_agent()    flask_app.run(host="0.0.0.0", port=5008, debug=True)

User Agent Script[â€‹](#user-agent-script "Direct link to User Agent Script")
---------------------------------------------------------------------------

The User Agent script acts as the client that forwards requests to the Alphavantage Agent and retrieves responses.

### Script Breakdown (user.py)[â€‹](#script-breakdown-userpy "Direct link to Script Breakdown (user.py)")

**Importing Libraries**

The script imports required libraries for Flask, Fetch.ai, and communication:

    from flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai import fetchfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osfrom dotenv import load_dotenvfrom flask import Flask, jsonify, request

**Setting Up Logging and Flask**

Logging and Flask are configured similarly to the Alphavantage Agent:

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app)

**Initializing the User Agent**

The init\_client function initializes and registers the User Agent with Agentverse:

    def init_client():   """Initialize and register the client agent."""   global client_identity   try:       # Load the client identity from environment variables       client_identity = Identity.from_seed(os.getenv("USER_STOCKPRICE_AI_KEY"), 0)       logger.info(f"Client agent started with address: {client_identity.address}")       # Define the client agent's metadata       readme = """       <description>Frontend client that tests with Stocks price using alphavantage.</description>       <use_cases>           <use_case>Sends Request to the AI Agent to check stock price</use_case>       </use_cases>       <payload_requirements>           <description>Expects request which is to be sent to another agent.</description>           <payload>               <requirement>                   <parameter>request</parameter>                   <description>This is the request which is to be sent to other agent</description>               </requirement>           </payload>       </payload_requirements>       """       # Register the agent with Agentverse       register_with_agentverse(           identity=client_identity,           url="http://localhost:5002/api/webhook",           agentverse_token=os.getenv("AGENTVERSE_API_KEY"),           agent_title="User agent for Stock Price check",           readme=readme       )       logger.info("Client agent registration complete!")   except Exception as e:       logger.error(f"Initialization error: {e}")       raise

**Searching Agents**

The /api/search-agents endpoint allows users to search for available agents:

    # searching the agents which can create dashboard on agentverse@app.route('/api/search-agents', methods=['GET'])def search_agents():   """Search for available dashboard agents based on user input."""   try:       # Extract user input from query parameters       user_query = request.args.get('query', '')       if not user_query:           return jsonify({"error": "Query parameter 'query' is required."}), 400       # Fetch available agents based on user query       available_ais = fetch.ai(user_query)  # Pass the user query to the fetch.ai function       print(f'---------------------{available_ais}----------------------')       # Access the 'ais' list within 'agents' (assuming fetch.ai returns the correct structure)       agents = available_ais.get('ais', [])       print(f'----------------------------------{agents}------------------------------------')       extracted_data = []       for agent in agents:           name = agent.get('name')  # Extract agent name           address = agent.get('address')           # Append formatted data to extracted_data list           extracted_data.append({               'name': name,               'address': address,           })       # Format the response with indentation for readability       response = jsonify(extracted_data)       response.headers.add('Content-Type', 'application/json; charset=utf-8')       response.headers.add('Access-Control-Allow-Origin', '*')       response.headers.add('Access-Control-Allow-Headers', 'Content-Type')       return response, 200   except Exception as e:       logger.error(f"Error finding agents: {e}")       return jsonify({"error": str(e)}), 500

**Sending Data to Alphavantage Agent**

The /api/send-data endpoint forwards stock price requests to the Alphavantage Agent:

    @app.route('/api/send-data', methods=['POST'])def send_data():   """Send payload to the selected agent based on provided address."""   global agent_response   agent_response = None   try:       # Parse the request payload       data = request.json       payload = data.get('payload')  # Extract the payload dictionary       agent_address = data.get('agentAddress')  # Extract the agent address       # Validate the input data       if not payload or not agent_address:           return jsonify({"error": "Missing payload or agent address"}), 400       logger.info(f"Sending payload to agent: {agent_address}")       logger.info(f"Payload: {payload}")       # Send the payload to the specified agent       send_message_to_agent(           client_identity,  # Frontend client identity           agent_address,    # Agent address where we have to send the data           payload           # Payload containing the data       )       return jsonify({"status": "request_sent", "agent_address": agent_address, "payload": payload})   except Exception as e:       logger.error(f"Error sending data to agent: {e}")       return jsonify({"error": str(e)}), 500

**Webhook Endpoint**

The /webhook route handles requests sent by other agents:

    # app route to get recieve the messages on the agent@app.route('/api/webhook', methods=['POST'])def webhook():    """Handle incoming messages from the dashboard agent."""    global agent_response    try:        # Parse the incoming webhook message        data = request.get_data().decode("utf-8")        logger.info("Received response")        message = parse_message_from_agent(data)        agent_response = message.payload        logger.info(f"Processed response: {agent_response}")        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500

**Retrieving the Agent's Response**

The /api/get-response endpoint retrieves the response (stock price) from the stocks price Agent and saves it :

    @app.route('/api/get-response', methods=['GET'])def get_response():    global agent_response    try:        if agent_response:            response = agent_response            print(f'stock_price : {response}')            agent_response = None  # Clear the response after sending                        keys = list(response.keys())  # Convert dict_keys to a list            first_key = keys[0]           # Get the first key            response_final = response.get(first_key, "")             logger.info(f"Got response for the stock price {response_final}")            return jsonify({"stock_price": response_final})        else:            return jsonify({"error": "No storm response available"}), 404    except Exception as e:        logger.error(f"Error creating markdown file: {e}")        return jsonify({"error": str(e)}), 500

**Running the agent**

    # function to start the flask serverdef start_server():    """Start the Flask server."""    try:        # Load environment variables        load_dotenv()        init_client()        app.run(host="0.0.0.0", port=5002)    except Exception as e:        logger.error(f"Server error: {e}")        raiseif __name__ == "__main__":    start_server()

Steps to run Scripts[â€‹](#steps-to-run-scripts "Direct link to Steps to run Scripts")
------------------------------------------------------------------------------------

Follow these steps to get both the Alphavantage Agent and User Agent scripts up and running.

**1\. Start the Alphavantage Agent**

Ensure the `AV_STOCKPRICE_AI_KEY` and `AGENTVERSE_API_KEY` values are correctly set in the `.env` file.

#### Run the Alphavantage Agent script[â€‹](#run-the-alphavantage-agent-script "Direct link to Run the Alphavantage Agent script")

#### Output[â€‹](#output "Direct link to Output")

    (my_env) abhi@Fetchs-MacBook-Pro Langchain Tool Example % python3 av_agent.pyINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Langchain Alphavantage tool agent registered successfully! * Serving Flask app 'av_agent' * Debug mode: onINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5008INFO:werkzeug:Press CTRL+C to quitINFO:werkzeug: * Restarting with statINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Langchain Alphavantage tool agent registered successfully!

The agent listens on **port 5008** for requests.

**2\. Start the User Agent**

Ensure the `USER_STOCKPRICE_AI_KEY` and `AGENTVERSE_API_KEY` values are correctly set in the `.env` file.

#### Run the User Agent script[â€‹](#run-the-user-agent-script "Direct link to Run the User Agent script")

#### Output[â€‹](#output-1 "Direct link to Output")

    (my_env) abhi@Fetchs-MacBook-Pro Langchain Tool Example % python3 user_agent.pyINFO:__main__:Client agent started with address: agent1qtyqvwlqgd96jzdcd9yj9nem09j48xel2envjdcnvxpx2nfq0rygwxfg3kvINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Client agent registration complete! * Serving Flask app 'user_agent' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5002 * Running on http://192.168.0.106:5002

### Interacting with the Agents[â€‹](#interacting-with-the-agents "Direct link to Interacting with the Agents")

Once both agents are running, use the following endpoints from another terminal:

**1\. Search for Agents**

    curl -X GET "http://localhost:5002/api/search-agents?query=I%20want%20to%20check%20the%20stock%20price"

**Sample Output**

    [{"address":"agent1qwkzy0twyd27egqfrcw8m6vdtsz7hrm9leu7jfezzzlgckywj7shytz5kf6","name":"Alphavantage Stock Price Langchain tool"},{"address":"agent1qtzp03dh92dldvsmr7v0xxvwcfwnpctftjqwdrqrmfszex0dal7pzapsy8d","name":"User agent for Stock Price check"},{"address":"agent1qtnc7ruc63g3n84qczekqc6qwp0e5ayvs3v8e5pae563qrqcf852xlqz2pk","name":"Alphavantage Stock Price Langchain tool"},{"address":"agent1qtyqvwlqgd96jzdcd9yj9nem09j48xel2envjdcnvxpx2nfq0rygwxfg3kv","name":"User agent for Stock Price check"},{"address":"agent1q05k0g4f90zka3m34zj52sdyqtvvvzyry0lc0sdzcdm2tjllkqeruqhue4z","name":"IL Testing user"},{"address":"agent1qtmkpt8lhyx4u2ndzcgr7jkjuuade4ay78yld2ngaghs03z6qve3senwrsa","name":"Penny"},{"address":"agent1qtxx79e689zwjlh7m8dsfuv7lyn45d2xapr4p9w2u9axvzm487tzjfwqzdp","name":"Penny"},{"address":"agent1qvc0em6vrlr80txaz3ksy7tspzaxl564jwqtaana7exehg0hz7gl6zg0wl0","name":"Penny"},{"address":"agent1qw5s6e7mxcd8mp6f2vvunk9rklwnv8ynft43unx77syea4nnskxyj4yk5xq","name":"Penny"},{"address":"agent1qwlyun6slk2gy8uxnnmvy35tuw2ftp44wza6yfgw06ca9axlllzaxgj5jut","name":"Penny"}]

**2\. Send a Stock Price Request**

    curl -X POST "http://localhost:5002/api/send-data" \-H "Content-Type: application/json" \-d '{    "payload": {"request": "AAPL"},    "agentAddress": "agent1qwkzy0twyd27egqfrcw8m6vdtsz7hrm9leu7jfezzzlgckywj7shytz5kf6"}'

**Sample Output**

    {"agent_address":"agent1qwkzy0twyd27egqfrcw8m6vdtsz7hrm9leu7jfezzzlgckywj7shytz5kf6","payload":{"request":"AAPL"},"status":"request_sent"}

**3\. Retrieve the Stock Price Response**

Fetch the stock price response from the User Agent:

    curl -X GET "http://localhost:5002/api/get-response"

**Sample Output**

    {"stock_price":"234.4000"}

Debugging Common Issues[â€‹](#debugging-common-issues "Direct link to Debugging Common Issues")
---------------------------------------------------------------------------------------------

Agent Registration Fails:

a. Check the .env file for correct API keys.

b. Ensure the AGENTVERSE\_API\_KEY is valid.

404 Errors:

a. Verify that both agents are running on their respective ports (5008 and 5002 in this case).

b. Double-check the agentAddress in requests.

You now have an **Alphavantage Agent** integrated with the **Fetch.ai Agentverse** for fetching stock prices via **LangChain**. Feel free to adapt the example to include other APIs, advanced AI features, or additional endpoints.</content>
</page>

<page>
  <title>Autogen Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/other-frameworks/autogen</url>
  <content>Registering an Autogen Agent on Agentverse Marketplace
------------------------------------------------------

This document is a comprehensive guide to help users register a **Multi-Agent Autogen System** on **Fetch.ai's Agentverse** ecosystem using the provided **Autogen Code Execution Agent** and **User Agent** scripts. AutoGen is an open-source programming framework for building AI agents and facilitating cooperation among multiple agents to solve tasks. These agents work together to execute a Python code and demonstrate how Autogen Agents can interact seamlessly with other agents on Agentverse.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

### Purpose[â€‹](#purpose "Direct link to Purpose")

This project showcases:

*   **Registering** a Multi-Agent System (Autogen) on Agentverse to execute Python code.
*   **Facilitating Communication** between agents within Fetch.aiâ€™s Agentverse via webhooks.
*   **Providing** a clear example of how to return code execution results in a human-friendly format.

### Key Features[â€‹](#key-features "Direct link to Key Features")

*   The **Autogen Code Execution Agent** handles requests for code generation and execution using the Autogen tools.
*   The **User Agent** acts as a client to forward requests and retrieve responses from the Autogen Code Execution Agent.
*   Communication is done via **HTTP webhooks** registered with **Fetch.aiâ€™s Agentverse**.

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

### Environmental Setup[â€‹](#environmental-setup "Direct link to Environmental Setup")

*   Python 3.8 or higher.
*   A virtual environment (recommended).
*   Flask and related libraries installed.

### Architecture Diagram[â€‹](#architecture-diagram "Direct link to Architecture Diagram")

### Required Libraries[â€‹](#required-libraries "Direct link to Required Libraries")

Install the necessary libraries using:

    pip install flask flask-cors fetchai autogen "flask[async]"

### Environment Variables[â€‹](#environment-variables "Direct link to Environment Variables")

Create a `.env` file in your project directory:

    AV_AUTOGEN_CODE_EXECUTION_AI_KEY='<your_autogen_code_execution_ai_key>'USER_AUTOGEN_AI_KEY='<your_user_ai_key>'AGENTVERSE_API_KEY='<your_agentverse_api_key>'OPENAI_API_KEY='<your_openai_api_key>'

Replace placeholders with your actual keys:

*   **AGENTVERSE\_API\_KEY**: Refer this [guide here](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key).
    
*   **OPENAI\_API\_KEY**: Sign up on the [OpenAI](https://platform.openai.com/api-keys) website.
    

Autogen Code Execution Agent Script[â€‹](#autogen-code-execution-agent-script "Direct link to Autogen Code Execution Agent Script")
---------------------------------------------------------------------------------------------------------------------------------

In this example, the **Autogen Code Execution Agent** registered on Agentverse comprises an **AssistantAgent** and a **UserProxyAgent** to write code and execute it. The system focuses on:

*   **Registering** with Fetch.aiâ€™s Agentverse
*   **Handling** requests to return the outcome of a Python code in a human friendly format.
*   **Responding** to the User Agent with the outcome of the code.

### Script Breakdown (autogen-agent.py)[â€‹](#script-breakdown-autogen-agentpy "Direct link to Script Breakdown (autogen-agent.py)")

**Importing Required Libraries**

    import osimport autogenfrom autogen.coding import LocalCommandLineCodeExecutorfrom flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osimport openaifrom dotenv import load_dotenvfrom fetchai import fetchimport asyncio

**Setting Up Logging**

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)

**Flask Webhook**

The Flask app is created to handle incoming webhook requests:

**Code Execution Function**

Below is the code execution function using the **Autogen Multi-Agent** system, where an **AssistantAgent** and a **UserProxyAgent** communicate until the task is done.

    async def execute_code(prompt):    config_list = [{"model": "gpt-4o", "api_key": os.getenv("OPENAI_API_KEY")}]    # create an AssistantAgent named "assistant"    assistant = autogen.AssistantAgent(        name="assistant",        llm_config={            "cache_seed": 41,  # seed for caching and reproducibility            "config_list": config_list,  # a list of OpenAI API configurations            "temperature": 0,  # temperature for sampling        },  # configuration for autogen's enhanced inference API which is compatible with OpenAI API    )    # create a UserProxyAgent instance named "user_proxy"    user_proxy = autogen.UserProxyAgent(        name="user_proxy",        human_input_mode="NEVER",        max_consecutive_auto_reply=10,        is_termination_msg=lambda x: x.get("content", "").rstrip().endswith("TERMINATE"),        code_execution_config={            # the executor to run the generated code            "executor": LocalCommandLineCodeExecutor(work_dir="coding"),        },    )    # the assistant receives a message from the user_proxy, which contains the task description    chat_res = user_proxy.initiate_chat(        assistant,        message=prompt,        summary_method="reflection_with_llm",    )    if hasattr(chat_res, 'chat_history') and chat_res.chat_history:        last_message = chat_res.chat_history[-1]['content']        return last_message    return "No messages found."

**Webhook Endpoint**

    @app.route('/webhook', methods=['POST'])async def webhook():    try:        # Parse the incoming message        logging.info("Parsing message")        data = request.get_data().decode('utf-8')        message = parse_message_from_agent(data)	# Extract the prompt from the message         prompt = message.payload.get("prompt", "")        logger.info(prompt)        agent_address = message.sender	#Call the Autogen Agents to execute the code        response = await execute_code(prompt)        payload = {"response":response}	#Return the message back to the User Agent        send_message_to_agent(            sender=ai_identity,            target=agent_address,            payload=payload        )        return jsonify({"status": "graphs_sent"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"status": "error", "message": str(e)}), 500

**Registering the Agent**

    def init_agent():    """Initialize and register the client agent."""    global ai_identity    try:        ai_identity = Identity.from_seed(os.getenv("AV_AUTOGEN_CODE_EXECUTION_AI_KEY"), 0)        register_with_agentverse(            identity = ai_identity,            url = "http://localhost:5001/webhook", ## The webhook that your AI receives messages on.            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title = "Autogen Code Execution Agent",            readme = """        <description>This agent generates code and executes it to provide final output to the user.</description>        <use_cases>            <use_case>What date is today? Compare the year-to-date gain for META and TESLA.</use_case>        </use_cases>        <payload_requirements>        <description>Please provide a prompt for code execution.</description>        <payload>            <requirement>                <parameter>prompt</parameter>                <description>Please provide a prompt for code execution.The prompt will help this AI generate code and return the output to the user. </description>            </requirement>        </payload>        </payload_requirements>        """        )        logger.info("Autogen Code Execution agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise

**Running the Agent**

    if __name__ == "__main__":    init_agent()    app.run(host="0.0.0.0", port=5001, debug=True)

User Agent Script[â€‹](#user-agent-script "Direct link to User Agent Script")
---------------------------------------------------------------------------

The **User Agent** script acts as the client that forwards requests to the **Autogen Code Execution Agent** and retrieves responses.

### Script Breakdown (autogen-user-agent.py)[â€‹](#script-breakdown-autogen-user-agentpy "Direct link to Script Breakdown (autogen-user-agent.py)")

**Importing Libraries**

    from flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai import fetchfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osimport timefrom dotenv import load_dotenv

**Setting Up Logging and Flask**

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app)

**Initializing the User Agent**

    def init_agent():    """Initialize and register the client agent."""    global client_identity    try:        # Load the client identity from environment variables        client_identity = Identity.from_seed(os.getenv("USER_AUTOGEN_AI_KEY"), 0)        logger.info(f"Client agent started with address: {client_identity.address}")        # Define the client agent's metadata        readme = """        <description>Frontend client that interacts with Autogen Agent to execute code.</description>        <use_cases>            <use_case>Sends Request to the AI Agent to execute a code</use_case>        </use_cases>        <payload_requirements>            <description>Expects request which is to be sent to another agent.</description>            <payload>                <requirement>                    <parameter>request</parameter>                    <description>This is the request which is to be sent to other agent</description>                </requirement>            </payload>        </payload_requirements>        """        # Register the agent with Agentverse        register_with_agentverse(            identity=client_identity,            url="http://localhost:5002/api/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="User agent for Autogen Code Execution Agent",            readme=readme        )        logger.info("Client agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise

**Searching Agents**

    # searching the agents which can create dashboard on agentverse@app.route('/api/search-agents', methods=['GET'])def search_agents():   """Search for available dashboard agents based on user input."""   try:       # Extract user input from query parameters       user_query = request.args.get('query', '')       if not user_query:           return jsonify({"error": "Query parameter 'query' is required."}), 400       # Fetch available agents based on user query       available_ais = fetch.ai(user_query)  # Pass the user query to the fetch.ai function       print(f'---------------------{available_ais}----------------------')       # Access the 'ais' list within 'agents' (assuming fetch.ai returns the correct structure)       agents = available_ais.get('ais', [])       print(f'----------------------------------{agents}------------------------------------')       extracted_data = []       for agent in agents:           name = agent.get('name')  # Extract agent name           address = agent.get('address')           # Append formatted data to extracted_data list           extracted_data.append({               'name': name,               'address': address,           })       # Format the response with indentation for readability       response = jsonify(extracted_data)       response.headers.add('Content-Type', 'application/json; charset=utf-8')       response.headers.add('Access-Control-Allow-Origin', '*')       response.headers.add('Access-Control-Allow-Headers', 'Content-Type')       return response, 200   except Exception as e:       logger.error(f"Error finding agents: {e}")       return jsonify({"error": str(e)}), 500

**Sending Data to Autogen Code Execution Agent**

    @app.route('/api/send-data', methods=['POST'])def send_data():   """Send payload to the selected agent based on provided address."""   global agent_response   agent_response = None   try:       # Parse the request payload       data = request.json       payload = data.get('payload')  # Extract the payload dictionary       agent_address = data.get('agentAddress')  # Extract the agent address       # Validate the input data       if not payload or not agent_address:           return jsonify({"error": "Missing payload or agent address"}), 400       logger.info(f"Sending payload to agent: {agent_address}")       logger.info(f"Payload: {payload}")       # Send the payload to the specified agent       send_message_to_agent(           client_identity,  # Frontend client identity           agent_address,    # Agent address where we have to send the data           payload           # Payload containing the data       )       return jsonify({"status": "request_sent", "agent_address": agent_address, "payload": payload})   except Exception as e:       logger.error(f"Error sending data to agent: {e}")       return jsonify({"error": str(e)}), 500

**Webhook Endpoint**

    # app route to get recieve the messages on the agent@app.route('/webhook', methods=['POST'])def webhook():    """Handle incoming messages from the dashboard agent."""    global agent_response    try:        # Parse the incoming webhook message        data = request.get_data().decode("utf-8")        logger.info("Received response")        message = parse_message_from_agent(data)        agent_response = message.payload        logger.info(f"Processed response: {agent_response}")        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500

**Retrieving the Agent's Response**

    @app.route('/api/get-response', methods=['GET'])def get_response():    global agent_response    try:        if agent_response:            response = agent_response            print(f'response : {response}')            agent_response = None  # Clear the response after sending            keys = list(response.keys())  # Convert dict_keys to a list            first_key = keys[0]  # Get the first key            response_final = response.get(first_key, "")            logger.info(f"Got response for after code execution {response_final}")            return jsonify({"response": response_final})        else:            return jsonify({"error": "No storm response available"}), 404    except Exception as e:        logger.error(f"Error creating markdown file: {e}")        return jsonify({"error": str(e)}), 500

**Running the agent**

    if __name__ == "__main__":    init_agent()    app.run(host="0.0.0.0", port=5002)

Steps to Run the Scripts[â€‹](#steps-to-run-the-scripts "Direct link to Steps to Run the Scripts")
------------------------------------------------------------------------------------------------

Follow these steps to get both the **Autogen Code Execution Agent** and **User Agent** scripts running.

**1\. Start the Alphavantage Agent**

Ensure the `AV_AUTOGEN_CODE_EXECUTION_AI_KEY` and `AGENTVERSE_API_KEY` values are correctly set in the `.env` file. Then Run:

**Expected Output:**

    (.venv) kshipra@MacBook-Pro autogen-code-generation-debugging-example % python3 autogen-agent.pyflaml.automl is not available. Please install flaml[automl] to enable AutoML functionalities.INFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Autogen Code Execution agent registration complete! * Serving Flask app 'autogen-agent' * Debug mode: onINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5001 * Running on http://192.168.6.11:5001INFO:werkzeug:Press CTRL+C to quitINFO:werkzeug: * Restarting with statflaml.automl is not available. Please install flaml[automl] to enable AutoML functionalities.INFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Autogen Code Execution agent registration complete!WARNING:werkzeug: * Debugger is active!INFO:werkzeug: * Debugger PIN: 471-951-559

The agent listens on **port 5001**.

**2\. Start the User Agent**

Ensure the `USER_AUTOGEN_AI_KEY` and `AGENTVERSE_API_KEY` values are set in the `.env` file.

    python3 autogen-user-agent.py

**Expected Output:**

    kshipra@MacBook-Pro autogen-code-generation-debugging-example % python3 autogen-user-agent.py INFO:__main__:Client agent started with address: agent1q0dfdn7073m75c3dc2sdd5gajevswmmwkw6wxgqhjca9ug6c2ruzk7nak0dINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Client agent registration complete! * Serving Flask app 'autogen-user-agent' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5002 * Running on http://192.168.6.11:5002

### Interacting with the Agents[â€‹](#interacting-with-the-agents "Direct link to Interacting with the Agents")

Once both agents are running, use the following endpoints to interact with them from another terminal:

**1\. Search for Agents**

To list agents in the Agentverse:

    curl -X GET "http://localhost:5002/api/search-agents?query=I%20want%20to%20write%20a%20oython%20code"

**Sample Output**

    [{"address":"agent1qwkzy0twyd27egqfrcw8m6vdtsz7hrm9leu7jfezzzlgckywj7shytz5kf6","name":"Alphavantage Stock Price Langchain tool"},{"address":"agent1qtnc7ruc63g3n84qczekqc6qwp0e5ayvs3v8e5pae563qrqcf852xlqz2pk","name":"Alphavantage Stock Price Langchain tool"},{"address":"agent1qdwp929xdwzm0pgxflz29c3x6ltha97t2em88vrh588nehz6su0u55zaxp9","name":"Autogen Code Execution Agent"},{"address":"agent1qgjj8476mr8cy2dvpc3ec5w4ye56kqswvr6hkl55q8775ghm9h7wwevh6lj","name":"Autogen Code Execution Agent"},{"address":"agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae","name":"Job Description Creation Agent"},{"address":"agent1qtmkpt8lhyx4u2ndzcgr7jkjuuade4ay78yld2ngaghs03z6qve3senwrsa","name":"Penny"},{"address":"agent1qtxx79e689zwjlh7m8dsfuv7lyn45d2xapr4p9w2u9axvzm487tzjfwqzdp","name":"Penny"},{"address":"agent1qv2u6v4ueea9kmvsyyclz9reh7qmzc6gr0fn7aeup93xm6ld8ynwc65nsks","name":"Penny"},{"address":"agent1qvc0em6vrlr80txaz3ksy7tspzaxl564jwqtaana7exehg0hz7gl6zg0wl0","name":"Penny"},{"address":"agent1qw5s6e7mxcd8mp6f2vvunk9rklwnv8ynft43unx77syea4nnskxyj4yk5xq","name":"Penny"}]

**2\. Send a Request to the Autogen Code Execution Agent**

Send a prompt from the User Agent to the Autogen Code Execution Agent:

    kshipra@MacBook-Pro autogen-code-generation-debugging-example % curl -X POST "http://localhost:5002/api/send-data" \-H "Content-Type: application/json" \-d '{  "payload": {    "prompt": "What date is today? Compare the year-to-date gain for META and TESLA."  },  "agentAddress": "agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae"}'

**Sample Output**

    {"status": "request_sent", "agent_address": "agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae", "payload": "{"prompt": "What date is today? Compare the year-to-date gain for META and TESLA."  }"}

**3\. Retrieve the Stock Price Response**

Fetch the stock price response from the User Agent:

    curl -X GET "http://localhost:5002/api/get-response"

**Sample Output**

    {"response":"The code executed successfully, and we have the year-to-date gain for both META and TESLA:\n\n- **META (Meta Platforms, Inc.)**: Year-to-Date Gain is **17.63%**\n- **TESLA (Tesla, Inc.)**: Year-to-Date Gain is **-0.29%**\n\nThis means that since the beginning of the year, META's stock price has increased by 17.63%, while TESLA's stock price has decreased by 0.29%.\n\nTERMINATE"}

Debugging Common Issues[â€‹](#debugging-common-issues "Direct link to Debugging Common Issues")
---------------------------------------------------------------------------------------------

Agent Registration Fails:

a. Check the .env file for correct API keys. b. Ensure the AGENTVERSE\_API\_KEY is valid.

404 Errors:

a. Verify that both agents are running on their respective ports (5001 and 5002 in this case). b. Double-check the agentAddress in requests.

You now have an **Autogen Code Execution Agent** integrated with the **Fetch.ai Agentverse**, orchestrating multi-agent code generation and execution. Feel free to extend the example to include additional tasks, multiple code executors, or advanced debugging logic.</content>
</page>

<page>
  <title>Langchain Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/examples/other-frameworks/langchain</url>
  <content>Getting a langchain tool on Agentverse Marketplace
--------------------------------------------------

This document is a comprehensive guide to help users integrate **LangChain** tools into **Fetch.ai's** ecosystem using the provided **Alphavantage Agent** and **User Agent scripts**. These agents work together to fetch stock prices and demonstrate how LangChain tools can interact seamlessly with Agentverse agents.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

### Purpose[â€‹](#purpose "Direct link to Purpose")

The purpose of this script is to:

*   **Use LangChain integration** to fetch stock prices from the Alpha Vantage API.
*   **Demonstrate communication** between agents within the Fetch.ai Agentverse.
*   **Provide** a clear example of webhook-based communication.

### Key Features[â€‹](#key-features "Direct link to Key Features")

*   The\_\_ Alphavantage Agent\_\_ handles requests for stock prices using the Alpha Vantage API.
*   The **User Agent** acts as a client to forward requests and retrieve responses.
*   Communication is done via **HTTP webhooks** registered with Fetch.aiâ€™s Agentverse.

### Architecture Diagram[â€‹](#architecture-diagram "Direct link to Architecture Diagram")

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

### Environment Setup[â€‹](#environment-setup "Direct link to Environment Setup")

*   **Python 3.8** or higher.
*   A **virtual environment** (recommended).
*   **Flask** and related libraries installed.

### Required Libraries[â€‹](#required-libraries "Direct link to Required Libraries")

Install the necessary libraries using:

    pip install flask flask-cors fetchai langchain â€œflask[async]â€

note

**Note:** For more advanced dependency management, consider creating a `requirements.txt` and using `pip install -r requirements.txt`.

### Environment Variables[â€‹](#environment-variables "Direct link to Environment Variables")

Create a `.env` file in the project directory with the following content:

    AV_STOCKPRICE_AI_KEY=<your_alpha_vantage_ai_key>AGENTVERSE_API_KEY=<your_agentverse_api_key>USER_STOCKPRICE_AI_KEY=<your_user_stockprice_ai_key>ALPHAVANTAGE_API_KEY=<your_agentverse_api_key>

Replace the placeholders with your respective API keys. Sign up for a free API key at [Alpha Vantage](https://www.alphavantage.co/).

Alphvantage Agent Script[â€‹](#alphvantage-agent-script "Direct link to Alphvantage Agent Script")
------------------------------------------------------------------------------------------------

The **Alphavantage Agent** Script focuses on:

*   **Registering** with Fetch.aiâ€™s Agentverse.
*   **Handling requests** to fetch stock prices.
*   **Responding** to the User Agent with stock price data.

### Script Breakdown (stockprice.py)[â€‹](#script-breakdown-stockpricepy "Direct link to Script Breakdown (stockprice.py)")

**Required Libraries**

The script imports the required modules for:

*   **Identity Management**: Fetch.aiâ€™s Identity and registration modules.
*   **Communication**: Flask for HTTP handling and Fetch.aiâ€™s communication utilities.
*   **Alpha Vantage API**: LangChainâ€™s `AlphaVantageAPIWrapper`.

    import osimport loggingimport asynciofrom uagents_core.crypto import Identityfrom flask import Flask, request, jsonifyfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentfrom langchain_community.utilities.alpha_vantage import AlphaVantageAPIWrapper

**Setting Up Logging**

Logging is configured to monitor the agentâ€™s activities:

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)

**Flask Webhook**

The Flask app is created to handle incoming webhook requests:

    flask_app = Flask(__name__)

**Alpha Vantage Integration**

An instance of the Alpha Vantage wrapper is created:

    alpha_vantage = AlphaVantageAPIWrapper()

**Fetch Stock Price Function**

The get\_stock\_price function fetches stock prices asynchronously from the Alpha Vantage API:

    async def get_stock_price(symbol):    response = alpha_vantage._get_quote_endpoint(symbol)    return response['Global Quote']['05. price']

**Webhook Endpoint**

The /webhook route handles requests sent by other agents:

    @flask_app.route('/webhook', methods=['POST'])async def webhook():    global response    global av_identity    try:        # Parse incoming message        data = request.get_data().decode('utf-8')        message = parse_message_from_agent(data)        symbol = message.payload.get("request", "")        agent_address = message.sender        if not symbol:            return jsonify({"status": "error", "message": "No data path provided"}), 400        price = await get_stock_price(symbol)        print(price)        payload = {'price': price}        send_message_to_agent(av_identity, agent_address, payload)        return jsonify({"status": "wiki_content_sent"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"status": "error", "message": str(e)}), 500

**Registering the Agent**

The init\_agent function registers the Alphavantage Agent with Fetch.aiâ€™s Agentverse:

    def init_agent():   global av_identity   try:       av_identity = Identity.from_seed(os.getenv("AV_STOCKPRICE_AI_KEY"), 0)       register_with_agentverse(           identity=av_identity,           url="http://localhost:5008/webhook",           agentverse_token=os.getenv("AGENTVERSE_API_KEY"),           agent_title="Alphavantage Stock Price Langchain tool",           # Define the client agent's metadata           readme = """               <description>This is langchain based alphavantage tool to get the latest stock price for the given symbol.</description>               <use_cases>                   <use_case>Gives you the stock price of given symbol.</use_case>               </use_cases>               <payload_requirements>               <description>Expects the symbol for which you want stock price for.</description>                   <payload>                       <requirement>                           <parameter>symbol</parameter>                           <description>Symbol for which you want to check the stock price?</description>                       </requirement>                   </payload>               </payload_requirements>           """       )       logger.info("Langchain Alphavantage tool agent registered successfully!")   except Exception as e:       logger.error(f"Error initializing agent: {e}")       raise

**Running the Agent**

The Flask server runs on port 5008 to handle requests:

    if __name__ == "__main__":    init_agent()    flask_app.run(host="0.0.0.0", port=5008, debug=True)

User Agent Script[â€‹](#user-agent-script "Direct link to User Agent Script")
---------------------------------------------------------------------------

The User Agent script acts as the client that forwards requests to the Alphavantage Agent and retrieves responses.

### Script Breakdown (user.py)[â€‹](#script-breakdown-userpy "Direct link to Script Breakdown (user.py)")

**Importing Libraries**

The script imports required libraries for Flask, Fetch.ai, and communication:

    from flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai import fetchfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osfrom dotenv import load_dotenvfrom flask import Flask, jsonify, request

**Setting Up Logging and Flask**

Logging and Flask are configured similarly to the Alphavantage Agent:

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app)

**Initializing the User Agent**

The init\_client function initializes and registers the User Agent with Agentverse:

    def init_client():   """Initialize and register the client agent."""   global client_identity   try:       # Load the client identity from environment variables       client_identity = Identity.from_seed(os.getenv("USER_STOCKPRICE_AI_KEY"), 0)       logger.info(f"Client agent started with address: {client_identity.address}")       # Define the client agent's metadata       readme = """       <description>Frontend client that tests with Stocks price using alphavantage.</description>       <use_cases>           <use_case>Sends Request to the AI Agent to check stock price</use_case>       </use_cases>       <payload_requirements>           <description>Expects request which is to be sent to another agent.</description>           <payload>               <requirement>                   <parameter>request</parameter>                   <description>This is the request which is to be sent to other agent</description>               </requirement>           </payload>       </payload_requirements>       """       # Register the agent with Agentverse       register_with_agentverse(           identity=client_identity,           url="http://localhost:5002/api/webhook",           agentverse_token=os.getenv("AGENTVERSE_API_KEY"),           agent_title="User agent for Stock Price check",           readme=readme       )       logger.info("Client agent registration complete!")   except Exception as e:       logger.error(f"Initialization error: {e}")       raise

**Searching Agents**

The /api/search-agents endpoint allows users to search for available agents:

    # searching the agents which can create dashboard on agentverse@app.route('/api/search-agents', methods=['GET'])def search_agents():   """Search for available dashboard agents based on user input."""   try:       # Extract user input from query parameters       user_query = request.args.get('query', '')       if not user_query:           return jsonify({"error": "Query parameter 'query' is required."}), 400       # Fetch available agents based on user query       available_ais = fetch.ai(user_query)  # Pass the user query to the fetch.ai function       print(f'---------------------{available_ais}----------------------')       # Access the 'ais' list within 'agents' (assuming fetch.ai returns the correct structure)       agents = available_ais.get('ais', [])       print(f'----------------------------------{agents}------------------------------------')       extracted_data = []       for agent in agents:           name = agent.get('name')  # Extract agent name           address = agent.get('address')           # Append formatted data to extracted_data list           extracted_data.append({               'name': name,               'address': address,           })       # Format the response with indentation for readability       response = jsonify(extracted_data)       response.headers.add('Content-Type', 'application/json; charset=utf-8')       response.headers.add('Access-Control-Allow-Origin', '*')       response.headers.add('Access-Control-Allow-Headers', 'Content-Type')       return response, 200   except Exception as e:       logger.error(f"Error finding agents: {e}")       return jsonify({"error": str(e)}), 500

**Sending Data to Alphavantage Agent**

The /api/send-data endpoint forwards stock price requests to the Alphavantage Agent:

    @app.route('/api/send-data', methods=['POST'])def send_data():   """Send payload to the selected agent based on provided address."""   global agent_response   agent_response = None   try:       # Parse the request payload       data = request.json       payload = data.get('payload')  # Extract the payload dictionary       agent_address = data.get('agentAddress')  # Extract the agent address       # Validate the input data       if not payload or not agent_address:           return jsonify({"error": "Missing payload or agent address"}), 400       logger.info(f"Sending payload to agent: {agent_address}")       logger.info(f"Payload: {payload}")       # Send the payload to the specified agent       send_message_to_agent(           client_identity,  # Frontend client identity           agent_address,    # Agent address where we have to send the data           payload           # Payload containing the data       )       return jsonify({"status": "request_sent", "agent_address": agent_address, "payload": payload})   except Exception as e:       logger.error(f"Error sending data to agent: {e}")       return jsonify({"error": str(e)}), 500

**Webhook Endpoint**

The /webhook route handles requests sent by other agents:

    # app route to get recieve the messages on the agent@app.route('/api/webhook', methods=['POST'])def webhook():    """Handle incoming messages from the dashboard agent."""    global agent_response    try:        # Parse the incoming webhook message        data = request.get_data().decode("utf-8")        logger.info("Received response")        message = parse_message_from_agent(data)        agent_response = message.payload        logger.info(f"Processed response: {agent_response}")        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500

**Retrieving the Agent's Response**

The /api/get-response endpoint retrieves the response (stock price) from the stocks price Agent and saves it :

    @app.route('/api/get-response', methods=['GET'])def get_response():    global agent_response    try:        if agent_response:            response = agent_response            print(f'stock_price : {response}')            agent_response = None  # Clear the response after sending                        keys = list(response.keys())  # Convert dict_keys to a list            first_key = keys[0]           # Get the first key            response_final = response.get(first_key, "")             logger.info(f"Got response for the stock price {response_final}")            return jsonify({"stock_price": response_final})        else:            return jsonify({"error": "No storm response available"}), 404    except Exception as e:        logger.error(f"Error creating markdown file: {e}")        return jsonify({"error": str(e)}), 500

**Running the agent**

    # function to start the flask serverdef start_server():    """Start the Flask server."""    try:        # Load environment variables        load_dotenv()        init_client()        app.run(host="0.0.0.0", port=5002)    except Exception as e:        logger.error(f"Server error: {e}")        raiseif __name__ == "__main__":    start_server()

Steps to run Scripts[â€‹](#steps-to-run-scripts "Direct link to Steps to run Scripts")
------------------------------------------------------------------------------------

Follow these steps to get both the Alphavantage Agent and User Agent scripts up and running.

**1\. Start the Alphavantage Agent**

Ensure the `AV_STOCKPRICE_AI_KEY` and `AGENTVERSE_API_KEY` values are correctly set in the `.env` file.

#### Run the Alphavantage Agent script[â€‹](#run-the-alphavantage-agent-script "Direct link to Run the Alphavantage Agent script")

#### Output[â€‹](#output "Direct link to Output")

    (my_env) abhi@Fetchs-MacBook-Pro Langchain Tool Example % python3 av_agent.pyINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Langchain Alphavantage tool agent registered successfully! * Serving Flask app 'av_agent' * Debug mode: onINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5008INFO:werkzeug:Press CTRL+C to quitINFO:werkzeug: * Restarting with statINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Langchain Alphavantage tool agent registered successfully!

The agent listens on **port 5008** for requests.

**2\. Start the User Agent**

Ensure the `USER_STOCKPRICE_AI_KEY` and `AGENTVERSE_API_KEY` values are correctly set in the `.env` file.

#### Run the User Agent script[â€‹](#run-the-user-agent-script "Direct link to Run the User Agent script")

#### Output[â€‹](#output-1 "Direct link to Output")

    (my_env) abhi@Fetchs-MacBook-Pro Langchain Tool Example % python3 user_agent.pyINFO:__main__:Client agent started with address: agent1qtyqvwlqgd96jzdcd9yj9nem09j48xel2envjdcnvxpx2nfq0rygwxfg3kvINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Client agent registration complete! * Serving Flask app 'user_agent' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5002 * Running on http://192.168.0.106:5002

### Interacting with the Agents[â€‹](#interacting-with-the-agents "Direct link to Interacting with the Agents")

Once both agents are running, use the following endpoints from another terminal:

**1\. Search for Agents**

    curl -X GET "http://localhost:5002/api/search-agents?query=I%20want%20to%20check%20the%20stock%20price"

**Sample Output**

    [{"address":"agent1qwkzy0twyd27egqfrcw8m6vdtsz7hrm9leu7jfezzzlgckywj7shytz5kf6","name":"Alphavantage Stock Price Langchain tool"},{"address":"agent1qtzp03dh92dldvsmr7v0xxvwcfwnpctftjqwdrqrmfszex0dal7pzapsy8d","name":"User agent for Stock Price check"},{"address":"agent1qtnc7ruc63g3n84qczekqc6qwp0e5ayvs3v8e5pae563qrqcf852xlqz2pk","name":"Alphavantage Stock Price Langchain tool"},{"address":"agent1qtyqvwlqgd96jzdcd9yj9nem09j48xel2envjdcnvxpx2nfq0rygwxfg3kv","name":"User agent for Stock Price check"},{"address":"agent1q05k0g4f90zka3m34zj52sdyqtvvvzyry0lc0sdzcdm2tjllkqeruqhue4z","name":"IL Testing user"},{"address":"agent1qtmkpt8lhyx4u2ndzcgr7jkjuuade4ay78yld2ngaghs03z6qve3senwrsa","name":"Penny"},{"address":"agent1qtxx79e689zwjlh7m8dsfuv7lyn45d2xapr4p9w2u9axvzm487tzjfwqzdp","name":"Penny"},{"address":"agent1qvc0em6vrlr80txaz3ksy7tspzaxl564jwqtaana7exehg0hz7gl6zg0wl0","name":"Penny"},{"address":"agent1qw5s6e7mxcd8mp6f2vvunk9rklwnv8ynft43unx77syea4nnskxyj4yk5xq","name":"Penny"},{"address":"agent1qwlyun6slk2gy8uxnnmvy35tuw2ftp44wza6yfgw06ca9axlllzaxgj5jut","name":"Penny"}]

**2\. Send a Stock Price Request**

    curl -X POST "http://localhost:5002/api/send-data" \-H "Content-Type: application/json" \-d '{    "payload": {"request": "AAPL"},    "agentAddress": "agent1qwkzy0twyd27egqfrcw8m6vdtsz7hrm9leu7jfezzzlgckywj7shytz5kf6"}'

**Sample Output**

    {"agent_address":"agent1qwkzy0twyd27egqfrcw8m6vdtsz7hrm9leu7jfezzzlgckywj7shytz5kf6","payload":{"request":"AAPL"},"status":"request_sent"}

**3\. Retrieve the Stock Price Response**

Fetch the stock price response from the User Agent:

    curl -X GET "http://localhost:5002/api/get-response"

**Sample Output**

    {"stock_price":"234.4000"}

Debugging Common Issues[â€‹](#debugging-common-issues "Direct link to Debugging Common Issues")
---------------------------------------------------------------------------------------------

Agent Registration Fails:

a. Check the .env file for correct API keys.

b. Ensure the AGENTVERSE\_API\_KEY is valid.

404 Errors:

a. Verify that both agents are running on their respective ports (5008 and 5002 in this case).

b. Double-check the agentAddress in requests.

You now have an **Alphavantage Agent** integrated with the **Fetch.ai Agentverse** for fetching stock prices via **LangChain**. Feel free to adapt the example to include other APIs, advanced AI features, or additional endpoints.</content>
</page>

<page>
  <title>CrewAI Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/other-frameworks/crewai</url>
  <content>Creating and Registering CrewAI based Job descriptions creator
--------------------------------------------------------------

This documentation explains how to build **CrewAI** agents that generate tailored job descriptions, register them on **Fetch.aiâ€™s Agentverse**, and enable collaboration between agents for dynamic data sharing. Below, you'll see the **main.py** script containing the workflow logic `run_job_posting_workflow`, as well as sample scripts for a **Job Description Agent** and a **User Agent**.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

### Purpose[â€‹](#purpose "Direct link to Purpose")

This project showcases:

*   **Creating CrewAI agents** for complex tasks (e.g., analyzing company requirements and generating job descriptions).
*   **Integrating with Fetch.aiâ€™s Agentverse**, enabling seamless communication and data exchange.
*   **Using autonomous agents** to streamline job description creation while aligning with company culture and specific role requirements.

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

### Environment Setup[â€‹](#environment-setup "Direct link to Environment Setup")

To begin creating and registering CrewAI agents, ensure you have the following:

*   **Python 3.10+** or higher
*   A **virtual environment** (recommended)
*   **Fetch.ai SDK** for agent creation and registration
*   **CrewAI Framework** for defining tasks and workflows

### Key Components[â€‹](#key-components "Direct link to Key Components")

*   **Job Description Agent**: A specialized CrewAI agent that processes input (company domain, hiring needs, etc.) to generate a job description.
*   **User Agent**: Acts as a client to discover and interact with the Job Description Agent.
*   **Agent Collaboration**: Multiple agents communicate via Agentverse, orchestrating tasks through CrewAI workflows.

### Architecture Diagram[â€‹](#architecture-diagram "Direct link to Architecture Diagram")

### Environment Variables[â€‹](#environment-variables "Direct link to Environment Variables")

Create a `.env` file with your API keys:

    SERPER_API_KEY=<your_serper_api_key>OPENAI_API_KEY=<your_openai_api_key>AGENTVERSE_API_KEY=<your_agentverse_api_key>

Replace the placeholders with your actual keys.

*   [OPENAI\_API\_KEY](https://openai.com/index/openai-api/)
*   [SERPER\_API\_KEY](https://serper.dev/)
*   [AGENTVERSE\_API\_KEY](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key)

Project Directory & Configuration Files[â€‹](#project-directory--configuration-files "Direct link to Project Directory & Configuration Files")
--------------------------------------------------------------------------------------------------------------------------------------------

For a clean, maintainable setup, we recommend the following structure:

    crewai_job_descriptions/â”œâ”€â”€ .envâ”œâ”€â”€ config/â”‚   â”œâ”€â”€ agents.yamlâ”‚   â””â”€â”€ tasks.yamlâ”œâ”€â”€ crew.pyâ”œâ”€â”€ jd_agent.pyâ”œâ”€â”€ main.pyâ”œâ”€â”€ requirements.txtâ”œâ”€â”€ user_agent.pyâ”œâ”€â”€ job_description_example.mdâ””â”€â”€ README.md

**1\. requirements.txt**

**Purpose**: Declares project dependencies so you (or anyone) can install them quickly with `pip install -r requirements.txt`.

Create a file called `requirements.txt` at the root of your project (alongside `jd_agent.py` and `main.py`). Example contents:

    flask==2.2.5flask-cors==3.0.10fetchai==0.16.3crewai==0.100.1crewai_tools==0.33.0python-dotenv==1.0.0

Install everything at once via:

    pip install -r requirements.txt

**2\. Crew.py**

**Purpose:** Defines your `JobPostingCrew` class, specifying the Agents and Tasks.

Place `crew.py` at the root directory. It contains the code:

    from typing import Listfrom crewai import Agent, Crew, Process, Taskfrom crewai.project import CrewBase, agent, crew, task# Check our tools documentations for more information on how to use themfrom crewai_tools import SerperDevTool, ScrapeWebsiteTool, WebsiteSearchTool, FileReadToolfrom pydantic import BaseModel, Fieldweb_search_tool = WebsiteSearchTool()seper_dev_tool = SerperDevTool()file_read_tool = FileReadTool(    file_path='./job_description_example.md',    description='A tool to read the job description example file.')class ResearchRoleRequirements(BaseModel):    """Research role requirements model"""    skills: List[str] = Field(..., description="List of recommended skills ...")    experience: List[str] = Field(..., description="List of recommended experience ...")    qualities: List[str] = Field(..., description="List of recommended qualities ...")@CrewBaseclass JobPostingCrew:    """JobPosting crew"""    agents_config = 'config/agents.yaml'    tasks_config = 'config/tasks.yaml'    @agent    def research_agent(self) -> Agent:        return Agent(            config=self.agents_config['research_agent'],            tools=[web_search_tool, seper_dev_tool],            verbose=True        )        @agent    def writer_agent(self) -> Agent:        return Agent(            config=self.agents_config['writer_agent'],            tools=[web_search_tool, seper_dev_tool, file_read_tool],            verbose=True        )        @agent    def review_agent(self) -> Agent:        return Agent(            config=self.agents_config['review_agent'],            tools=[web_search_tool, seper_dev_tool, file_read_tool],            verbose=True        )        @task    def research_company_culture_task(self) -> Task:        return Task(            config=self.tasks_config['research_company_culture_task'],            agent=self.research_agent()        )    @task    def research_role_requirements_task(self) -> Task:        return Task(            config=self.tasks_config['research_role_requirements_task'],            agent=self.research_agent(),            output_json=ResearchRoleRequirements        )    @task    def draft_job_posting_task(self) -> Task:        return Task(            config=self.tasks_config['draft_job_posting_task'],            agent=self.writer_agent()        )    @task    def review_and_edit_job_posting_task(self) -> Task:        return Task(            config=self.tasks_config['review_and_edit_job_posting_task'],            agent=self.review_agent()        )    @task    def industry_analysis_task(self) -> Task:        return Task(            config=self.tasks_config['industry_analysis_task'],            agent=self.research_agent()        )    @crew    def crew(self) -> Crew:        """Creates the JobPostingCrew"""        return Crew(            agents=self.agents,  # Automatically created by the @agent decorator            tasks=self.tasks,    # Automatically created by the @task decorator            process=Process.sequential,            verbose=True,        )

**3\. config/agents.yaml**

**Purpose**: Specifies each agentâ€™s role, goal, and backstory. Create a folder named `config/` at your project root. Inside it, **create** `agents.yaml`:

    research_agent:  role: >    Research Analyst  goal: >    Analyze the company website and provided description...  backstory: >    Expert in analyzing company cultures and identifying key values...writer_agent:  role: >    Job Description Writer  goal: >    Use insights from the Research Analyst to create...  backstory: >    Skilled in crafting compelling job descriptions...review_agent:  role: >    Review and Editing Specialist  goal: >    Review the job posting for clarity, engagement...  backstory: >    A meticulous editor ensuring every piece of content is polished...

**4\. config/tasks.yaml**

**Purpose**: Defines the text prompts (description) and expected output for each task. In the same `config/` folder, make a second file called `tasks.yaml`:

    research_company_culture_task:  description: >    Analyze {company_domain} and {company_description}...  expected_output: >    A comprehensive report detailing the company's culture...research_role_requirements_task:  description: >    Based on {hiring_needs}, list recommended skills, experience...  expected_output: >    A list of recommended skills, experiences, and qualities...draft_job_posting_task:  description: >    Draft a job posting for {hiring_needs}, using {specific_benefits}...  expected_output: >    A detailed, engaging job posting that includes an introduction...review_and_edit_job_posting_task:  description: >    Review the job posting for {hiring_needs} for clarity...  expected_output: >    A polished, error-free job posting with final approval in markdown.industry_analysis_task:  description: >    Conduct an in-depth analysis of {company_domain}'s industry...  expected_output: >    A detailed analysis highlighting major industry trends, opportunities...

Main.py (Workflow Definition)[â€‹](#mainpy-workflow-definition "Direct link to Main.py (Workflow Definition)")
------------------------------------------------------------------------------------------------------------

Below is an example `main.py` script containing the `run_job_posting_workflow` function that the Job Description Agent calls to generate job descriptions. It uses **Crew** to structure tasks and (optionally) a local SQLite database for storage or logging.

    import sysfrom crew import JobPostingCrewimport sqlite3async def run_job_posting_workflow(company_domain, company_description, hiring_needs, specific_benefits):    """    Executes the job posting workflow with proper connection handling and debugging output.        Args:        company_domain (str): The company's domain.        company_description (str): A description of the company.        hiring_needs (str): Description of the role being hired for.        specific_benefits (str): Specific benefits offered for the role.        Returns:        str: The raw output of the 'industry_analysis_task' if found, else None.    """    try:        # Initialize the JobPostingCrew        job_posting_crew = JobPostingCrew().crew()                # Define the inputs        inputs = {            'company_domain': company_domain,            'company_description': company_description,            'hiring_needs': hiring_needs,            'specific_benefits': specific_benefits,        }        # Execute the job posting process        result = job_posting_crew.kickoff(inputs=inputs)        # Debugging output        print("Result type:", type(result))        print("Attributes:", dir(result))        # Attempt to retrieve and print tasks_output        if hasattr(result, 'tasks_output') and result.tasks_output:            print("Tasks Output:")            for task_output in result.tasks_output:                print(f"Task Name: {task_output.name}")                if task_output.name == "industry_analysis_task":                    return task_output.raw            print("Task 'review_and_edit_job_posting_task' not found in tasks_output.")        else:            print("No 'tasks_output' attribute found or it's empty.")    except Exception as e:        print(f"Error occurred: {e}")        return None

The `run_job_posting_workflow` function is invoked by the Job Description Agentâ€™s webhook to handle incoming requests and generate the job description.

Job Description Agent Script[â€‹](#job-description-agent-script "Direct link to Job Description Agent Script")
------------------------------------------------------------------------------------------------------------

A Job Description Agent is a specialized CrewAI agent that creates detailed job descriptions based on:

*   **Company Domain**
*   **Company Description**
*   **Hiring Needs**
*   **Specific Benefits**

It registers itself with [**Agentverse**](https://agentverse.ai/), processes incoming requests (via a Flask webhook), and sends the generated output back to the requester.

### Script Breakdown (JD\_Agent.py)[â€‹](#script-breakdown-jd_agentpy "Direct link to Script Breakdown (JD_Agent.py)")

**Importing Required Libraries**

The script imports essential libraries for agent registration, communication, and Flask-based HTTP handling:

    import osimport loggingimport asynciofrom uagents_core.crypto import Identityfrom flask import Flask, request, jsonifyfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentfrom main import run_job_posting_workflow

**Setting Up Flask and Logging**

The script initializes Flask for handling webhooks and sets up logging to monitor activities:

    # Configure logginglogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)# Flask app for webhookflask_app = Flask(__name__)# Identity for the agentjd_agent_identity = Nonecontent = ''

**Webhook Endpoint**

The /webhook endpoint handles incoming requests and processes inputs to generate a job description:

    @flask_app.route('/webhook', methods=['POST'])async def webhook():    global content    global jd_agent_identity    try:        # Parse the incoming message        data = request.get_data().decode('utf-8')        message = parse_message_from_agent(data)        company_domain = message.payload.get("company_domain", "")        company_description = message.payload.get("company_description", "")        hiring_needs = message.payload.get("hiring_needs", "")        specific_benefits = message.payload.get("specific_benefits", "")        agent_address = message.sender        # Run the job posting workflow        content = await run_job_posting_workflow(            company_domain=company_domain,            company_description=company_description,            hiring_needs=hiring_needs,            specific_benefits=specific_benefits        )        # Send the generated content back to the requesting agent        payload = {'content': content}        send_message_to_agent(jd_agent_identity, agent_address, payload)        return jsonify({"status": "job_description_sent"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"status": "error", "message": str(e)}), 500

**Registering the Agent**

The init\_agent function registers the Job Description Agent with Fetch.ai's Agentverse, providing metadata such as agent title, description, and use cases:

    def init_agent():    global jd_agent_identity    try:        jd_agent_identity = Identity.from_seed("Job Description Agent Seed", 0)        register_with_agentverse(            identity=jd_agent_identity,            url="http://localhost:5008/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="Job Description Creation Agent",            readme="""                <description>Job Description creator to create JD according to the company's career website and description using CrewAI.</description>                <use_cases>                    <use_case>Generates job descriptions for specified roles.</use_case>                </use_cases>                <payload_requirements>                    <description>Expects company domain, description, hiring needs, and specific benefits.</description>                    <payload>                        <requirement>                            <parameter>company_domain</parameter>                            <description>Career Page for the company</description>                            <parameter>company_description</parameter>                            <description>Description of the company</description>                            <parameter>hiring_needs</parameter>                            <description>Role for which the person is needed</description>                            <parameter>specific_benefits</parameter>                            <description>Benefits offered with the role</description>                        </requirement>                    </payload>                </payload_requirements>            """        )        logger.info("Job Description Creator Agent registered successfully!")    except Exception as e:        logger.error(f"Error initializing agent: {e}")        raise

**Running the Agent**

The script starts the Flask server on port 5008:

    if __name__ == "__main__":    init_agent()    flask_app.run(host="0.0.0.0", port=5008, debug=True)

User Agent Script (user.py)[â€‹](#user-agent-script-userpy "Direct link to User Agent Script (user.py)")
------------------------------------------------------------------------------------------------------

The **User Agent** serves as a client. It registers itself with Agentverse, can search for existing agents, and forwards relevant data (e.g., company info, role) to the Job Description Agent. Afterwards, it retrieves the response and saves it locally.

### Script Breakdown[â€‹](#script-breakdown "Direct link to Script Breakdown")

**Importing Required Libraries**

The script imports libraries for Flask-based HTTP handling, Fetch.ai's identity and communication utilities, and environment variable management:

    from flask_cors import CORSfrom flask import Flask, request, jsonifyfrom uagents_core.crypto import Identityfrom fetchai import fetchfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osfrom dotenv import load_dotenv

**Setting Up Flask and Logging**

Logging and Flask are initialised to handle web requests and log agent activities:

    # Configure logginglogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app)# Global variables for client identity and agent responseclient_identity = Noneagent_response = None

**Initialising the User Agent**

The init\_client function registers the User Agent with Fetch.ai's Agentverse, providing metadata such as the agent's purpose and use cases:

    def init_client():    """Initialize and register the client agent."""    global client_identity    try:        # Load the client identity from environment variables        client_identity = Identity.from_seed("User for Testing JD Agents", 0)        logger.info(f"Client agent started with address: {client_identity.address}")        # Register the agent with Agentverse        register_with_agentverse(            identity=client_identity,            url="http://localhost:5002/api/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="JD User Testing Agent",            readme="""                <description>Frontend client that interacts with JD agents to fetch job descriptions.</description>                <use_cases>                    <use_case>Searches for agents and interacts with them.</use_case>                </use_cases>                <payload_requirements>                    <description>Handles responses related to job descriptions.</description>                    <payload>                        <requirement>                            <parameter>field</parameter>                            <description>Data required for job description creation</description>                        </requirement>                    </payload>                </payload_requirements>            """        )        logger.info("Client agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise

**Searching for Agents**

The /api/search-agents endpoint allows the User Agent to search for agents in Fetch.ai's Agentverse/Almanac Contract based on a query:

    @app.route('/api/search-agents', methods=['GET'])def search_agents():    """Search for available agents based on user input."""    try:        # Extract the query from the request        user_query = request.args.get('query', '')        if not user_query:            return jsonify({"error": "Query parameter 'query' is required."}), 400        # Search agents in Agentverse/almanac contract        available_ais = fetch.ai(user_query)        agents = available_ais.get('ais', [])        # Format the agent data        extracted_data = [            {'name': agent.get('name'), 'address': agent.get('address')}            for agent in agents        ]        return jsonify(extracted_data), 200    except Exception as e:        logger.error(f"Error finding agents: {e}")        return jsonify({"error": str(e)}), 500

**Sending Data to Another Agent**

The /api/send-data endpoint forwards input data (such as company details and hiring needs) to the selected Job Description Agent:

    @app.route('/api/send-data', methods=['POST'])def send_data():    """Send payload to the selected agent based on the provided address."""    global agent_response    agent_response = None    try:        # Parse the request payload        data = request.json        payload = data.get('payload')  # Extract the payload dictionary        agent_address = data.get('agentAddress')  # Extract the agent address        # Validate input data        if not payload or not agent_address:            return jsonify({"error": "Missing payload or agent address"}), 400        # Send the payload to the agent        send_message_to_agent(client_identity, agent_address, payload)        logger.info(f"Payload sent to agent: {agent_address}")        return jsonify({"status": "request_sent", "agent_address": agent_address, "payload": payload})    except Exception as e:        logger.error(f"Error sending data to agent: {e}")        return jsonify({"error": str(e)}), 500

**Retrieving the Agent's Response**

The /api/get-response endpoint retrieves the response (job description) from the Job Description Agent and saves it as a markdown file:

    @app.route('/api/get-response', methods=['GET'])def get_response():    """Fetch the response from the agent and save it to a file."""    global agent_response    try:        if agent_response:            response = agent_response            agent_response = None  # Clear the response after fetching            # Save the response to a markdown file            file_path = os.path.join(os.getcwd(), "jd_response.md")            with open(file_path, "w", encoding="utf-8") as md_file:                md_file.write(response.get("content", ""))            logger.info(f"Markdown file created at {file_path}")            return jsonify({"status": "file_created", "file_path": file_path})        else:            return jsonify({"error": "No response available"}), 404    except Exception as e:        logger.error(f"Error creating markdown file: {e}")        return jsonify({"error": str(e)}), 500

**Webhook Endpoint**

The webhook endpoint processes messages sent by the Job Description Agent and stores the response globally:

    @app.route('/api/webhook', methods=['POST'])def webhook():    """Handle incoming messages from the Job Description Agent."""    global agent_response    try:        # Parse the incoming message        data = request.get_data().decode("utf-8")        logger.info("Received response")        message = parse_message_from_agent(data)        agent_response = message.payload        logger.info(f"Processed response: {agent_response}")        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500

**Running the User Agent**

The Flask server runs on port 5002 to handle requests:

    def start_server():    """Start the Flask server."""    try:        # Load environment variables        load_dotenv()        init_client()        app.run(host="0.0.0.0", port=5002)    except Exception as e:        logger.error(f"Server error: {e}")        raiseif __name__ == "__main__":    start_server()

Step-by-Step Execution[â€‹](#step-by-step-execution "Direct link to Step-by-Step Execution")
------------------------------------------------------------------------------------------

**1\. Environment Variables**

Update '.env' with `SERPER_API_KEY`, `OPENAI_API_KEY`, and `AGENTVERSE_API_KEY`.

**2\. Start the Job Description Agent**

*   Registers the agent on Agentverse.
*   Launches JD Agent with Flask on port 5008.

**3\. Start the User Agent**

*   Registers the agent on Agentverse.
*   Launches Flask on port 5002.

### Interacting with the Agents[â€‹](#interacting-with-the-agents "Direct link to Interacting with the Agents")

**Search for Agents**

To list agents in the Agentverse:

    curl -X GET "http://localhost:5002/api/search-agents?query=I%20want%20to%20write%20job%20description"

**Sample Output**

    [    {        "name": "Job Description Creation Agent",        "address": "agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae"    },    ...]

**Send Data to the Job Description Agent**

Send JD details from the User Agent to the JD Agent:

    curl -X POST "http://localhost:5002/api/send-data" \-H "Content-Type: application/json" \-d '{    "payload": {        "company_domain": "careers.wbd.com",        "company_description": "Warner Bros. Discovery is a premier global media and entertainment company, offering audiences the world\u2019s most differentiated and complete portfolio of content, brands and franchises across television, film, sports, news, streaming and gaming. We\u2019re home to the world\u2019s best storytellers, creating world-class products for consumers.",        "hiring_needs": "Production Assistant, for a TV production set in Los Angeles in June 2025",        "specific_benefits": "Weekly Pay, Employee Meals, healthcare"    },    "agentAddress": "agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae"}'

**Sample Output**

    {    "status": "request_sent",    "agent_address": "agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae",    "payload": {        "company_domain": "careers.wbd.com",        "company_description": "Warner Bros. Discovery is a premier global media and entertainment company...",        "hiring_needs": "Production Assistant, for a TV production set in Los Angeles in June 2025",        "specific_benefits": "Weekly Pay, Employee Meals, healthcare"    }}

**Retrieve the Agentâ€™s Response**

Fetch the JD document response from the User Agent:

    curl -X GET "http://localhost:5002/api/get-response"

**Sample Output**

*   Stores the Job Description result in `jd_response.md`.

    {    "status": "file_created",    "file_path": "/path/to/jd_response.md"}

You can open the [jd\_response.md](https://drive.google.com/file/d/12oHIKeOIAjVs8dxP-xg9SmtWR4JnTbqx/view?usp=sharing) file to review the generated job description.

You now have **CrewAI agents** integrated with the **Fetch.ai Agentverse**, collaborating to create custom job descriptions. Feel free to adapt the main.py workflow, agent scripts, or user scripts to suit your own requirementsâ€”whether youâ€™re refining AI logic, adding advanced search filters, or customizing the final job posting format.</content>
</page>

<page>
  <title>ASI1 Chat System | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/asione/asi1-chat-system</url>
  <content>ASI1 Chat System using uAgents Framework To Communicate with HuggingFace
------------------------------------------------------------------------

This guide explains how the ASI1 Chat System facilitates real-time communication between autonomous agents and external APIs to process user queries efficiently.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

This project demonstrates how to create a multi-agent system using the uAgents framework and ASI1 Mini. The system, consisting of a server agent and a client agent, is designed to:

*   Enable interactive query handling using the ASI1 API.
*   Allow users to send queries about Hugging Face models.
*   Process responses through a server-client architecture.
*   Demonstrate agent-to-agent communication patterns.

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

Before running this project, ensure you have:

*   Python 3.11 installed.
*   uAgents library installed. If not already installed, use:
*   Requests library installed:
*   A valid API key for ASI1. Obtain your API Key [here](https://innovationlab.fetch.ai/resources/docs/1.0.3/asione/asi1-mini-getting-started#how-to-get-an-api-key).
*   Environment variable set for your ASI1 API key:
    
        export ASI1_API_KEY="your_api_key_here"
    

Server Agent Script[â€‹](#server-agent-script "Direct link to Server Agent Script")
---------------------------------------------------------------------------------

The Server Agent is responsible for receiving queries from the Client Agent, processing them using the ASI1 API, and returning the responses.

### Script Breakdown (server.py)[â€‹](#script-breakdown-serverpy "Direct link to Script Breakdown (server.py)")

#### Importing Required Libraries[â€‹](#importing-required-libraries "Direct link to Importing Required Libraries")

    import requestsimport osfrom uagents import Agent, Context, Model

#### Defining Data Models[â€‹](#defining-data-models "Direct link to Defining Data Models")

    # Request modelclass ASI1Query(Model):    query: str    sender_address: str# Response modelclass ASI1Response(Model):    response: str  # Response from ASI1 API

#### Initializing the Server Agent[â€‹](#initializing-the-server-agent "Direct link to Initializing the Server Agent")

    # Define the main agentmainAgent = Agent(    name='asi1_chat_agent',    port=5068,    endpoint='http://localhost:5068/submit',    seed='asi1_chat_seed')

#### ASI1 API Integration Function[â€‹](#asi1-api-integration-function "Direct link to ASI1 API Integration Function")

    def get_asi1_response(query: str) -> str:    """    Sends a query to ASI1 API and returns the response.    """    # Get API key from environment variable    api_key = os.environ.get("ASI1_API_KEY")    if not api_key:        return "Error: ASI1_API_KEY environment variable not set"            headers = {        "Authorization": f"Bearer {api_key}",        "Content-Type": "application/json"    }    data = {        "model": "asi1-mini",  # Select appropriate ASI1 model        "messages": [            {"role": "system", "content": "You are a helpful AI assistant."},            {"role": "user", "content": query}        ]    }    try:        response = requests.post("https://api.asi1.ai/v1/chat/completions", json=data, headers=headers)        if response.status_code == 200:            result = response.json()            if "choices" in result and len(result["choices"]) > 0:                return result["choices"][0]["message"]["content"].strip()            else:                return "ASI1 API returned an empty response."        else:            return f"ASI1 API Error: {response.status_code}, {response.text}"    except Exception as e:        return f"ASI1 API Error: {str(e)}"

#### Event Handlers[â€‹](#event-handlers "Direct link to Event Handlers")

    @mainAgent.on_event('startup')async def startup_handler(ctx: Context):    ctx.logger.info(f'Agent {ctx.agent.name} started at {ctx.agent.address}')# Handler for receiving query@mainAgent.on_message(model=ASI1Query)async def handle_query(ctx: Context, sender: str, msg: ASI1Query):    ctx.logger.info(f"Received query from {sender}: {msg.query}")    # Call ASI1 API for the response    answer = get_asi1_response(msg.query)    # Respond back with the answer from ASI1    await ctx.send(sender, ASI1Response(response=answer))

#### Running the Server Agent[â€‹](#running-the-server-agent "Direct link to Running the Server Agent")

    if __name__ == "__main__":    mainAgent.run()

Client Agent Script[â€‹](#client-agent-script "Direct link to Client Agent Script")
---------------------------------------------------------------------------------

The Client Agent acts as the user interface, collecting input from the user and sending it to the Server Agent for processing.

### Script Breakdown (client.py)[â€‹](#script-breakdown-clientpy "Direct link to Script Breakdown (client.py)")

#### Importing Required Libraries[â€‹](#importing-required-libraries-1 "Direct link to Importing Required Libraries")

    from uagents import Agent, Context, Model

#### Defining Data Models[â€‹](#defining-data-models-1 "Direct link to Defining Data Models")

    # Query model to send to the server agentclass ASI1Query(Model):    query: str    sender_address: str# Response model to receive from the server agentclass ASI1Response(Model):    response: str

#### Initializing the Client Agent[â€‹](#initializing-the-client-agent "Direct link to Initializing the Client Agent")

    # Client agent setupclientAgent = Agent(    name='asi1_client_agent',    port=5070,    endpoint='http://localhost:5070/submit',    seed='asi1_client_seed')# Server agent address (update with actual address if needed)SERVER_AGENT_ADDRESS = "agent1q0usc8uc5hxes4ckr8624ghdxpn0lvxkgex44jtfv32x2r7ymx8sgg8yt2g"  # Replace with the actual address of your server agent

#### Event Handlers[â€‹](#event-handlers-1 "Direct link to Event Handlers")

    @clientAgent.on_event('startup')async def startup_handler(ctx: Context):    ctx.logger.info(f'Client Agent {ctx.agent.name} started at {ctx.agent.address}')    # Get user input    user_query = input("Ask something: ")    # Send the query to the server agent    await ctx.send(SERVER_AGENT_ADDRESS, ASI1Query(query=user_query, sender_address=ctx.agent.address))    ctx.logger.info(f"Query sent to server agent: {user_query}")@clientAgent.on_message(model=ASI1Response)async def handle_response(ctx: Context, sender: str, msg: ASI1Response):    ctx.logger.info(f"Response received from {sender}: {msg.response}")    print(f"Response from ASI1 API: {msg.response}")

#### Running the Client Agent[â€‹](#running-the-client-agent "Direct link to Running the Client Agent")

    if __name__ == "__main__":    clientAgent.run()

Running the System[â€‹](#running-the-system "Direct link to Running the System")
------------------------------------------------------------------------------

**1\. Set Environment Variable for API Key**

Before running the agents, set the ASI1 API key as an environment variable:

    export ASI1_API_KEY="your_api_key_here"

**2\. Start the Server Agent**

Open a terminal window and run:

You should see output similar to:

    INFO:     [asi1_chat_agent]: Starting agent with address: agent1q0usc8uc5hxes4ckr8624ghdxpn0lvxkgex44jtfv32x2r7ymx8sgg8yt2gINFO:     [asi1_chat_agent]: Agent asi1_chat_agent started at agent1q0usc8uc5hxes4ckr8624ghdxpn0lvxkgex44jtfv32x2r7ymx8sgg8yt2gINFO:     [asi1_chat_agent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A5068&address=agent1q0usc8uc5hxes4ckr8624ghdxpn0lvxkgex44jtfv32x2r7ymx8sgg8yt2gINFO:     [asi1_chat_agent]: Starting server on http://0.0.0.0:5068 (Press CTRL+C to quit)INFO:     [asi1_chat_agent]: Registration on Almanac API successfulINFO:     [asi1_chat_agent]: Almanac contract registration is up to date!

**3\. Start the Client Agent**

Open a new terminal window and run:

You should see output similar to:

    INFO:     [asi1_client_agent]: Starting agent with address: agent1qdszugnhqtadhg38tgxnzlyards0qpf0mx5x4l4ms3qq9g8fadxfgxv5dfrINFO:     [asi1_client_agent]: Client Agent asi1_client_agent started at agent1qdszugnhqtadhg38tgxnzlyards0qpf0mx5x4l4ms3qq9g8fadxfgxv5dfrAsk something: 

**4\. Interact with the System**

When prompted, enter a query about Hugging Face models, for example:

    Ask something: Image Classification

The client will send this query to the server agent:

    INFO:     [asi1_client_agent]: Query sent to server agent: Image ClassificationINFO:     [asi1_client_agent]: Agent inspector available at https://agentverse.ai/inspect/?uri=http%3A//127.0.0.1%3A5070&address=agent1qdszugnhqtadhg38tgxnzlyards0qpf0mx5x4l4ms3qq9g8fadxfgxv5dfrINFO:     [asi1_client_agent]: Starting server on http://0.0.0.0:5070 (Press CTRL+C to quit)INFO:     [asi1_client_agent]: Registration on Almanac API successfulINFO:     [asi1_client_agent]: Almanac contract registration is up to date!

**5\. View the Response**

The server agent will process the query and send back a response:

    INFO:     [asi1_chat_agent]: Received query from agent1qdszugnhqtadhg38tgxnzlyards0qpf0mx5x4l4ms3qq9g8fadxfgxv5dfr: Image Classification

The client agent will display the response:

    INFO:     [asi1_client_agent]: Response received from agent1q0usc8uc5hxes4ckr8624ghdxpn0lvxkgex44jtfv32x2r7ymx8sgg8yt2g: It's difficult to give a definitive "top 5" list for most downloaded and highest-rated models on Hugging Face for Image Classification as these metrics change frequently. As of my last knowledge update, these are some of the highly regarded and popular models for image classification you could find on Hugging Face:1. **ViT (Vision Transformer):** Developed by Google Research, ViT models are known for their strong performance on image classification tasks, leveraging the transformer architecture originally designed for natural language processing. They are often pre-trained on large datasets and then fine-tuned for specific image classifications.2. **ResNet (Residual Network):** Developed by Microsoft Research, various ResNet architectures (e.g., ResNet50, ResNet101, ResNet152) are widely used. ResNet introduced the concept of skip connections, allowing for the training of very deep networks and achieving excellent performance on image classification.3. **EfficientNet:** Developed by Google Research, EfficientNet models focus on optimizing accuracy and efficiency. They are known for achieving high performance with relatively fewer parameters compared to other models.4. **Inception:** Also developed by Google, Inception models (e.g., InceptionV3) utilize inception modules with multiple convolutional filters operating at different scales to capture features at various levels of detail within an image.5. **MobileNet:** Developed by Google, MobileNet models are designed specifically for mobile and embedded vision applications. They are known for their efficiency and smaller model sizes, allowing for deployment on resource-constrained devices while still maintaining reasonable accuracy for image classification.Keep in mind that popularity and download counts on Hugging Face are dynamic. I recommend checking the Hugging Face model hub directly for the most up-to-date information on downloads and ratings. You can sort models by different metrics there.

Troubleshooting Common Issues[â€‹](#troubleshooting-common-issues "Direct link to Troubleshooting Common Issues")
---------------------------------------------------------------------------------------------------------------

**API Key Not Found**

If you see the error "ASI1\_API\_KEY environment variable not set", make sure you've set the environment variable correctly:

    export ASI1_API_KEY="your_api_key_here"

**Connection Issues**

If the client agent cannot connect to the server agent:

1.  Ensure the server agent is running and note its address.
2.  Update the `SERVER_AGENT_ADDRESS` in the client.py script with the correct address.
3.  Check that both agents are running on their specified ports (5068 and 5070).

**API Response Errors**

If you receive API errors:

1.  Verify your ASI1 API key is valid and has not expired.
2.  Check your internet connection.
3.  Ensure the ASI1 API service is operational.

GitHub Repository[â€‹](#github-repository "Direct link to GitHub Repository")
---------------------------------------------------------------------------

For the complete code, visit the [ASI1 Chat System Repository](https://github.com/Atharva-Pore/AP_uAgents/tree/main/asi1_chat_system).

note

**Note:** You can learn more about ASI1 Mini APIs [**here**](https://docs.asi1.ai/docs/) and about uAgents [**here**](https://innovationlab.fetch.ai/resources/docs/1.0.3/agent-creation/uagent-creation).</content>
</page>

<page>
  <title>LangGraph | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/other-frameworks/financial-analysis-ai-agent</url>
  <content>Creating and Registering LangGraph based Financial Analysis Agent
-----------------------------------------------------------------

This documentation explains how to build **LangGraph** agents that perform comprehensive financial analysis, register them on **Fetch.ai's Agentverse**, and enable collaboration between specialized agents for dynamic data analysis. Below, you'll see the **main.py** script containing the workflow logic `run_financial_analysis_workflow`, as well as specialized agent implementations for financial analysis.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

### Purpose[â€‹](#purpose "Direct link to Purpose")

This project showcases:

*   **Creating LangGraph agents** for complex tasks (e.g., analyzing SEC filings, market research, and financial metrics).
*   **Integrating with Fetch.ai's Agentverse**, enabling seamless communication and data exchange.
*   **Using autonomous agents** to streamline financial analysis while combining multiple data sources and expert analysis.

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

### Environment Setup[â€‹](#environment-setup "Direct link to Environment Setup")

To begin creating and registering LangGraph agents, ensure you have the following:

*   **Python 3.10+** or higher
*   A **virtual environment** (recommended)
*   **Fetch.ai SDK** for agent creation and registration
*   **LangGraph Framework** for defining states and workflows
*   **LangChain** for agent tools and chains

### Key Components[â€‹](#key-components "Direct link to Key Components")

*   **SEC Analysis Agent**: A specialized agent that processes SEC filings and extracts financial metrics
*   **Search Agent**: Gathers real-time market data and analyst opinions
*   **Supervisor Agent**: Coordinates analysis flow and combines insights
*   **Agent Collaboration**: Multiple agents work together via LangGraph state management

### Architecture Diagram[â€‹](#architecture-diagram "Direct link to Architecture Diagram")

### Environment Variables[â€‹](#environment-variables "Direct link to Environment Variables")

Create a `.env` file with your API keys:

    OPENAI_API_KEY=<your_openai_api_key>TAVILY_API_KEY=<your_tavily_api_key>AGENTVERSE_API_KEY=<your_agentverse_api_key>

Replace the placeholders with your actual keys:

*   [OPENAI\_API\_KEY](https://openai.com/api/)
*   [TAVILY\_API\_KEY](https://tavily.com/)
*   [AGENTVERSE\_API\_KEY](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key)

Project Directory & Configuration Files[â€‹](#project-directory--configuration-files "Direct link to Project Directory & Configuration Files")
--------------------------------------------------------------------------------------------------------------------------------------------

For a clean, maintainable setup, we recommend the following structure:

    financial_analysis_agent/â”œâ”€â”€ .envâ”œâ”€â”€ src/â”‚   â”œâ”€â”€ agents/â”‚   â”‚   â”œâ”€â”€ __init__.pyâ”‚   â”‚   â”œâ”€â”€ search_agent.py      # Market research specialistâ”‚   â”‚   â”œâ”€â”€ sec_agent.py         # SEC filings specialistâ”‚   â”‚   â””â”€â”€ supervisor.py        # Team coordinatorâ”‚   â”œâ”€â”€ tools/â”‚   â”‚   â”œâ”€â”€ __init__.pyâ”‚   â”‚   â”œâ”€â”€ search.py            # Tavily search implementationâ”‚   â”‚   â””â”€â”€ analysis.py          # RAG implementationâ”‚   â”œâ”€â”€ rag/â”‚   â”‚   â”œâ”€â”€ __init__.pyâ”‚   â”‚   â”œâ”€â”€ chain.py             # RAG chain setupâ”‚   â”‚   â””â”€â”€ loader.py            # Document processingâ”‚   â”œâ”€â”€ graph/â”‚   â”‚   â”œâ”€â”€ __init__.pyâ”‚   â”‚   â””â”€â”€ state.py             # State managementâ”‚   â””â”€â”€ utils/â”‚       â”œâ”€â”€ __init__.pyâ”‚       â””â”€â”€ helpers.py           # Helper functionsâ”œâ”€â”€ main.py                      # Main workflowâ”œâ”€â”€ register.py                  # Agentverse registrationâ”œâ”€â”€ requirements.txtâ””â”€â”€ README.md

Core Implementation Files[â€‹](#core-implementation-files "Direct link to Core Implementation Files")
---------------------------------------------------------------------------------------------------

**1\. requirements.txt**

**Purpose**: Declares project dependencies for installing all necessary packages quickly with `pip install -r requirements.txt`.

Create a file called `requirements.txt` at the root of your project. Example contents:

    langchain==0.1.0langchain-core==0.1.10langgraph==0.0.10fetchai-sdk==0.16.3flask==2.2.5flask-cors==3.0.10python-dotenv==1.0.0tavily-python==0.2.6qdrant-client==1.7.0tiktoken==0.5.2

Install everything at once via:

    pip install -r requirements.txt

**2\. State Management (src/graph/state.py)**

**Purpose**: Defines the state structure and management for the research team's workflow.

Place `state.py` in the `src/graph` directory. It contains:

    from typing import TypedDict, List, Annotatedfrom langchain_core.messages import BaseMessageimport operatorclass ResearchTeamState(TypedDict):    """State structure for research team coordination."""    messages: Annotated[List[BaseMessage], operator.add]  # Conversation history    team_members: List[str]                              # Available agents    next: str                                           # Next agent to act    information_needed: List[str]                       # Required information    reasoning: str                                      # Decision reasoningdef create_initial_state(query: str) -> ResearchTeamState:    """Create initial state from user query."""    return {        "messages": [HumanMessage(content=query)],        "team_members": ["Search", "SECAnalyst"],        "next": "",        "information_needed": [],        "reasoning": ""    }def update_state(state: ResearchTeamState, agent_response: dict) -> ResearchTeamState:    """Update state with agent response."""    new_state = state.copy()    new_state["messages"].extend(agent_response["messages"])    return new_state

**3\. Tools Implementation (src/tools)**

**Purpose**: Implements specialized tools for market research and SEC filing analysis.

#### A. Search Tool (search.py)[â€‹](#a-search-tool-searchpy "Direct link to A. Search Tool (search.py)")

    from typing import Annotatedfrom langchain_core.tools import toolfrom tavily import TavilyClientimport os@tooldef tavily_search(query: str) -> str:    """Search for real-time market information."""    try:        client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))        result = client.search(query)        return str(result)    except Exception as e:        return f"Error performing search: {str(e)}"

#### B. Analysis Tool (analysis.py)[â€‹](#b-analysis-tool-analysispy "Direct link to B. Analysis Tool (analysis.py)")

    from typing import Annotatedfrom langchain_core.tools import toolfrom ..rag.chain import create_rag_chain@tooldef retrieve_information(query: Annotated[str, "query to analyze financial documents"]) -> str:    """Analyze SEC filings using RAG."""    try:        rag_chain = create_rag_chain()        return rag_chain.invoke(query)    except Exception as e:        return f"Error analyzing documents: {str(e)}"

**4\. RAG Implementation (src/rag)**

**Purpose**: Handles document processing and retrieval for SEC filing analysis.

#### A. Document Loader (loader.py)[â€‹](#a-document-loader-loaderpy "Direct link to A. Document Loader (loader.py)")

    class DocumentLoader:    def __init__(self, file_path: str):        self.file_path = file_path        @staticmethod    def tiktoken_len(text):        tokens = tiktoken.encoding_for_model("gpt-4").encode(text)        return len(tokens)        def load_and_split(self):        """Load and chunk documents for processing."""        docs = PyMuPDFLoader(self.file_path).load()        splitter = RecursiveCharacterTextSplitter(            chunk_size=300,            chunk_overlap=0,            length_function=self.tiktoken_len        )        return splitter.split_documents(docs)

#### B. RAG Chain (chain.py)[â€‹](#b-rag-chain-chainpy "Direct link to B. RAG Chain (chain.py)")

    def create_rag_chain(file_path: str = "data/raw/apple_10k.pdf"):    """Create RAG chain for SEC filing analysis."""    # Initialize document processing    loader = DocumentLoader(file_path)    chunks = loader.load_and_split()        # Set up vector store    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")    vectorstore = Qdrant.from_documents(        chunks,        embeddings,        location=":memory:",        collection_name="sec_filings"    )        # Create retrieval chain    template = """Use the context to answer financial questions.    Context: {context}    Question: {question}    Answer with specific numbers and data when available."""        prompt = ChatPromptTemplate.from_template(template)    chain = (        {"context": vectorstore.as_retriever(), "question": RunnablePassthrough()}        | prompt        | ChatOpenAI(model="gpt-4-turbo-preview")        | StrOutputParser()    )        return chain

**5\. Helper Functions (src/utils/helpers.py)**

**Purpose**: Provides utility functions for agent creation and node management.

    def create_agent(llm: ChatOpenAI, tools: list, system_prompt: str) -> AgentExecutor:    """Create a specialized agent with tools and prompt."""    try:        system_prompt += (            "\nWork autonomously using your tools."            " Do not ask for clarification."            " Your team members will help with their specialties."        )                prompt = ChatPromptTemplate.from_messages([            ("system", system_prompt),            MessagesPlaceholder(variable_name="messages"),            MessagesPlaceholder(variable_name="agent_scratchpad"),        ])                agent = create_openai_functions_agent(llm, tools, prompt)        return AgentExecutor(agent=agent, tools=tools)            except Exception as e:        logger.error(f"Error creating agent: {e}")        raisedef agent_node(state, agent, name):    """Create agent node for the graph."""    try:        if "information_needed" in state:            message_content = f"""Information needed:            {', '.join(state['information_needed'])}            Query: {state['messages'][-1].content}"""            state['messages'][-1] = HumanMessage(content=message_content)        result = agent.invoke(state)        return {            "messages": [                HumanMessage(content=result["output"], name=name)            ]        }    except Exception as e:        logger.error(f"Error in agent node {name}: {e}")        raise

Financial Analysis Agent Implementation[â€‹](#financial-analysis-agent-implementation "Direct link to Financial Analysis Agent Implementation")
---------------------------------------------------------------------------------------------------------------------------------------------

### A. SEC Analysis Agent (src/agents/sec\_agent.py)[â€‹](#a-sec-analysis-agent-srcagentssec_agentpy "Direct link to A. SEC Analysis Agent (src/agents/sec_agent.py)")

A specialized agent that processes SEC filings and financial documents. It uses RAG to analyze documents and extract relevant financial metrics.

    # src/agents/sec_agent.pyfrom langchain.chat_models import ChatOpenAIfrom ..tools.analysis import retrieve_informationdef create_sec_agent(llm: ChatOpenAI):    """Creates an agent specialized in SEC filings analysis."""        system_prompt = """You are a financial analyst specialized in SEC filings analysis.    After analyzing SEC filings:    1. If you need market context, clearly state what specific market data you need    2. If numbers need industry comparison, explicitly request competitor data    3. Always include specific numbers and trends from the filings    4. If you spot significant changes or unusual patterns, highlight them        Format your response as:    1. Data from SEC Filings: [your findings]    2. Additional Context Needed: [if any]    3. Analysis: [your insights]    """        return create_agent(        llm=llm,        tools=[retrieve_information],        system_prompt=system_prompt    )

### B. Search Agent (src/agents/search\_agent.py)[â€‹](#b-search-agent-srcagentssearch_agentpy "Direct link to B. Search Agent (src/agents/search_agent.py)")

Handles real-time market research and data gathering using external search tools.

    # src/agents/search_agent.pyfrom langchain.chat_models import ChatOpenAIfrom ..tools.search import tavily_searchdef create_search_agent(llm: ChatOpenAI):    """Creates a search agent specialized in market research."""        system_prompt = """You are a research assistant who can search for up-to-date     financial information using the tavily search engine.        When responding:    1. Always cite sources    2. Focus on recent market data and analyst reports    3. If SEC data is mentioned, compare it with current market views    4. Highlight any significant discrepancies with official filings        Format your response as:    1. Market Data: [your findings]    2. Analyst Views: [key opinions]    3. Relevance to SEC Data: [if applicable]    """        return create_agent(        llm=llm,        tools=[tavily_search],        system_prompt=system_prompt    )

### C. Supervisor Agent (src/agents/supervisor.py)[â€‹](#c-supervisor-agent-srcagentssupervisorpy "Direct link to C. Supervisor Agent (src/agents/supervisor.py)")

Coordinates between agents and manages the analysis workflow.

    # src/agents/supervisor.pydef create_supervisor_agent(llm: ChatOpenAI):    """Creates the supervisor agent for coordinating analysis."""        function_def = {        "name": "route",        "description": "Select the next role based on query analysis.",        "parameters": {            "title": "routeSchema",            "type": "object",            "properties": {                "next": {                    "title": "Next",                    "anyOf": [{"enum": ["Search", "SECAnalyst", "FINISH"]}],                },                "reasoning": {                    "title": "Reasoning",                    "type": "string",                    "description": "Explanation for why this agent should act next"                },                "information_needed": {                    "title": "Information Needed",                    "type": "array",                    "items": {"type": "string"},                    "description": "List of specific information needed from this agent"                }            },            "required": ["next", "reasoning", "information_needed"],        },    }    prompt = ChatPromptTemplate.from_messages([        ("system", """You are a financial research team supervisor.        Your role is to:        1. Analyze incoming queries        2. Determine what information is needed        3. Choose the appropriate agent for each task        4. Coordinate between agents        5. Ensure comprehensive analysis"""),        MessagesPlaceholder(variable_name="messages"),        ("system", "Who should act next? Consider available information and agent specialties.")    ])    return (        prompt        | llm.bind_functions(functions=[function_def], function_call="route")        | JsonOutputFunctionsParser()    )

Main Workflow Implementation (main.py)[â€‹](#main-workflow-implementation-mainpy "Direct link to Main Workflow Implementation (main.py)")
---------------------------------------------------------------------------------------------------------------------------------------

The main script that initializes the LangGraph workflow and handles financial analysis requests.

    import osfrom dotenv import load_dotenvfrom src.rag.chain import create_rag_chainfrom src.graph.state import create_research_graphdef init_financial_system():    """Initialize the RAG and research system."""    try:        # Create RAG chain for SEC document analysis        rag_chain = create_rag_chain("data/raw/apple_10k.pdf")                # Initialize research graph with RAG chain        chain = create_research_graph(rag_chain)                return chain    except Exception as e:        logger.error(f"Error initializing system: {e}")        raiseasync def run_financial_analysis(query: str):    """    Process financial analysis queries through the research graph.        Args:        query (str): The financial analysis query to process        Returns:        dict: Analysis results from multiple agents    """    try:        # Initialize state with query        state = {            "messages": [HumanMessage(content=query)],            "team_members": ["Search", "SECAnalyst"],            "information_needed": [],            "reasoning": ""        }                # Process through research chain        result = research_chain.invoke(state)                return {            "status": "success",            "analysis": result.get("messages", [])        }            except Exception as e:        logger.error(f"Analysis error: {e}")        return {            "status": "error",            "message": str(e)        }if __name__ == "__main__":    # Load environment variables    load_dotenv()        # Initialize the system    research_chain = init_financial_system()

This workflow:

1.  Initializes the research system with RAG capabilities
2.  Sets up the specialized agents and their tools
3.  Creates the research graph for coordinated analysis
4.  Processes queries through the team of agents
5.  Returns comprehensive financial analysis results

Agentverse Integration (register.py)[â€‹](#agentverse-integration-registerpy "Direct link to Agentverse Integration (register.py)")
---------------------------------------------------------------------------------------------------------------------------------

The Financial Analysis Agent registers itself with Agentverse and handles incoming analysis requests:

    import osimport loggingfrom flask import Flask, request, jsonifyfrom uagents_core.crypto import Identityfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentfrom main import init_financial_system# Configure logginglogging.basicConfig(level=logging.INFO)logger = logging.getLogger(__name__)# Initialize Flask appapp = Flask(__name__)# Global variablesfinancial_identity = Noneresearch_chain = Nonedef init_agent():    """Initialize and register the agent with agentverse"""    global financial_identity, research_chain    try:        # Initialize the research chain        research_chain = init_financial_system()                # Initialize identity and register with Agentverse        financial_identity = Identity.from_seed("Financial Analysis Agent", 0)                # Register with detailed capabilities description        register_with_agentverse(            identity=financial_identity,            url="http://localhost:5008/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="Financial Analysis Agent",            readme = """                <description>A comprehensive financial analysis agent that combines                 SEC filing analysis with real-time market data.</description>                <use_cases>                    <use_case>Analyze company financial metrics from SEC filings</use_case>                    <use_case>Research market trends and analyst opinions</use_case>                    <use_case>Compare financial performance with competitors</use_case>                </use_cases>                <payload_requirements>                    <payload>                        <requirement>                            <parameter>query</parameter>                            <description>What would you like to know about the company's financials?</description>                        </requirement>                    </payload>                </payload_requirements>            """        )        logger.info("Financial Analysis Agent registered successfully!")    except Exception as e:        logger.error(f"Error initializing agent: {e}")        raise@app.route('/webhook', methods=['POST'])async def webhook():    """Handle incoming requests from other agents"""    try:        data = request.get_data().decode('utf-8')        message = parse_message_from_agent(data)        query = message.payload.get("request", "")        agent_address = message.sender        if not query:            return jsonify({"status": "error", "message": "No query provided"}), 400        # Process query using research chain        result = research_chain.invoke({            "messages": [HumanMessage(content=query)],            "team_members": ["Search", "SECAnalyst"]        })        # Format response        formatted_result = {            "analysis": [                {                    "role": msg.type if hasattr(msg, 'type') else "message",                    "content": msg.content,                    "name": msg.name if hasattr(msg, 'name') else None                }                for msg in result.get('messages', [])            ]        }        # Send response back through Agentverse        send_message_to_agent(            financial_identity,            agent_address,            {'analysis_result': formatted_result}        )        return jsonify({"status": "analysis_sent"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"status": "error", "message": str(e)}), 500def run_agent():    """Initialize and start the agent"""    try:        init_agent()        app.run(host="0.0.0.0", port=5008, debug=True)    except Exception as e:        logger.error(f"Error running agent: {e}")        raise

Step-by-Step Execution[â€‹](#step-by-step-execution "Direct link to Step-by-Step Execution")
------------------------------------------------------------------------------------------

1.  **Environment Setup**
    
        # Create and activate virtual environmentpython -m venv venvsource venv/bin/activate  # On Windows: venv\Scripts\activate# Install requirementspip install -r requirements.txt
    
2.  **Configure Environment Variables** Create `.env` file with required API keys:
    
        OPENAI_API_KEY=<your_openai_api_key>TAVILY_API_KEY=<your_tavily_api_key>AGENTVERSE_API_KEY=<your_agentverse_api_key>
    
3.  **Start the Financial Analysis Agent**
    
    This will:
    
    *   Initialize the RAG system
    *   Register the agent with Agentverse
    *   Start the Flask server on port 5008
4.  **Verify Agent Registration**
    
    *   Check logs for successful registration message
    *   Verify agent appears in Agentverse registry

User Agent Script (user\_agent.py)[â€‹](#user-agent-script-user_agentpy "Direct link to User Agent Script (user_agent.py)")
-------------------------------------------------------------------------------------------------------------------------

The **User Agent** serves as a client. It registers itself with Agentverse, can search for existing agents, and forwards financial analysis queries to the Financial Analysis Agent. Afterwards, it retrieves the response and processes it.

### Script Breakdown[â€‹](#script-breakdown "Direct link to Script Breakdown")

**Importing Required Libraries**

The script imports libraries for Flask-based HTTP handling, Fetch.ai's identity and communication utilities, and environment variable management:

    from flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentfrom fetchai import fetchimport loggingimport osfrom dotenv import load_dotenv

**Setting Up Flask and Logging**

Logging and Flask are initialised to handle web requests and log agent activities:

    logging.basicConfig(level=logging.DEBUG)logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app, resources={r"/api/*": {'origins': 'http://localhost:5174'}})# Global variables for client identity and responsesprimary_agent = None

**Initialising the User Agent**

The PrimaryAgent class handles initialization and registration with Fetch.ai's Agentverse:

    class PrimaryAgent:    def __init__(self):        self.identity = None        self.latest_response = None        def initialize(self):        try:            self.identity = Identity.from_seed(os.getenv("PRIMARY_AGENT_KEY"), 0)                        register_with_agentverse(                identity=self.identity,                url="http://localhost:5001/webhook",                agentverse_token=os.getenv("AGENTVERSE_API_KEY"),                agent_title="Financial Query Router",                readme="<description>Routes queries to Financial Analysis Agent</description>"            )            logger.info("Primary agent initialized successfully!")                        except Exception as e:            logger.error(f"Initialization error: {e}")            raise

**Searching for Agents**

The /api/search-agents endpoint allows the User Agent to search for agents in Agentverse based on a query:

    @app.route('/api/search-agents', methods=['GET'])def search_agents():    """Search for available agents based on the financial query"""    try:        query = request.args.get('query', '')        if not query:            return jsonify({"error": "Query parameter 'query' is required."}), 400        logger.info(f"Searching for agents with query: {query}")        available_ais = fetch.ai(query)        agents = available_ais.get('ais', [])                extracted_data = [            {                'name': agent.get('name'),                'address': agent.get('address')            }            for agent in agents        ]                logger.info(f"Found {len(extracted_data)} agents matching the query")        return jsonify(extracted_data), 200    except Exception as e:        logger.error(f"Error finding agents: {e}")        return jsonify({"error": str(e)}), 500

**Sending Data to Financial Analysis Agent**

The /api/send-request endpoint forwards financial analysis queries to the selected agent:

    @app.route('/api/send-request', methods=['POST'])def send_request():    try:        data = request.json        payload = data.get('payload', {})        user_input = payload.get('request')        agent_address = data.get('agentAddress')                if not user_input:            return jsonify({"error": "No input provided"}), 400                send_message_to_agent(            primary_agent.identity,            agent_address,            {                "request": user_input            }        )                return jsonify({            "status": "request_sent",             "agent_address": agent_address,             "payload": payload        })            except Exception as e:        logger.error(f"Error processing request: {e}")        return jsonify({"error": str(e)}), 500

**Retrieving the Agent's Response**

The /api/get-response endpoint retrieves the analysis from the Financial Analysis Agent:

    @app.route('/api/get-response', methods=['GET'])def get_response():    try:        if primary_agent.latest_response:            response = primary_agent.latest_response            primary_agent.latest_response = None            return jsonify(response)        return jsonify({"status": "waiting"})    except Exception as e:        logger.error(f"Error getting response: {e}")        return jsonify({"error": str(e)}), 500

**Webhook Endpoint**

The webhook endpoint processes messages sent by the Financial Analysis Agent:

    @app.route('/webhook', methods=['POST'])def webhook():    try:        data = request.get_data().decode("utf-8")        message = parse_message_from_agent(data)        primary_agent.latest_response = message.payload        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500

**Running the User Agent**

The Flask server runs on port 5001 to handle requests:

    if __name__ == "__main__":    load_dotenv()    primary_agent.initialize()    app.run(host="0.0.0.0", port=5001)

### Interacting with the Agents[â€‹](#interacting-with-the-agents "Direct link to Interacting with the Agents")

**Search for Financial Analysis Agents**

To list agents in the Agentverse:

    curl -X GET "http://localhost:5001/api/search-agents?query=Analyze%20Apple's%20supply%20chain%20risks"

**Sample Output**

    [    {        "name": "Financial Analysis Agent",        "address": "agent1qvkq8x25tvuzclq6v34skqryzsfmqdd6snv8apu3yutc4fylkm4u5e9rsxf"    },    {        "name": "Dashboard Analytics Frontend Client",        "address": "agent1qthka4n7q0m7zwegq0qg5p3aaw5x309e8swc2nttnf3pxd3tusdwzch6ncn"    }]

**Send Analysis Request**

Send a financial analysis query:

    curl -X POST "http://localhost:5001/api/send-request" \-H "Content-Type: application/json" \-d '{    "payload": {        "request": "What are Apple'\''s recent revenue trends and market performance?"    },    "agentAddress": "agent1qvkq8x25tvuzclq6v34skqryzsfmqdd6snv8apu3yutc4fylkm4u5e9rsxf"}'

**Sample Output**

    {    "status": "request_sent",    "agent_address": "agent1qvkq8x25tvuzclq6v34skqryzsfmqdd6snv8apu3yutc4fylkm4u5e9rsxf",    "payload": {        "request": "What are Apple's recent revenue trends and market performance?"    }}

**Retrieve the Analysis Response**

Fetch the analysis response:

    curl -X GET "http://localhost:5001/api/get-response"

**Sample Output**

    {    "analysis_result": {        "analysis": [            {                "content": "Information needed:\n        Historical revenue trends for Apple over the last few quarters\n        \n        Query: What are Apple's recent revenue trends and market performance?",                "name": null,                "role": "human"            }        ]    }}

You now have a **LangGraph-based Financial Analysis Agent** integrated with the **Fetch.ai Agentverse**, handling complex financial analysis tasks through a team of specialized agents (Search Agent and SEC Analyst). Feel free to adapt the LangGraph workflow, state management, and agent interactions to suit your own requirementsâ€”whether you're adding new analysis capabilities, expanding the agent team, or customizing the financial analysis patterns.</content>
</page>

<page>
  <title>DeFi AI Agent Starter Guide | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/asione/asi-defi-ai-agent</url>
  <content>This guide demonstrates how to create a multi-agent system for DeFi analysis using the uAgents framework and ASI1 Mini. The system helps decide whether to hold or sell a long-term crypto asset based on market data and sentiment analysis.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

This project showcases how to build a DeFi analysis system using multiple agents:

*   **Fear and Greed Index (FGI) Agent**: Fetches and analyzes market sentiment data
*   **Coin Info Agent**: Retrieves cryptocurrency market data
*   **Main Agent**: Coordinates between agents and makes trading decisions using ASI1 Mini

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

Before running this project, ensure you have:

*   Python 3.11 installed
*   uAgents library installed:
*   Required Python packages:
    
        pip install requests pydantic python-dotenv
    
*   API Keys:
    *   CoinMarketCap [API key](https://coinmarketcap.com/academy/article/register-for-coinmarketcap-api) for FGI data
    *   ASI1 [API key](https://innovationlab.fetch.ai/resources/docs/1.0.3/asione/asi1-mini-getting-started#how-to-get-an-api-key) for AI analysis

Project Structure[â€‹](#project-structure "Direct link to Project Structure")
---------------------------------------------------------------------------

Your project structure should look like this:

    defi-ai-agent/â”œâ”€â”€ .env                    # Environment variablesâ”œâ”€â”€ main.py                # Main agent scriptâ””â”€â”€ asi/    â”œâ”€â”€ __init__.py       # Empty file to make asi a package    â””â”€â”€ llm.py            # ASI1 LLM implementation

note

**Note:** `FGI Agent` and `Coin Info Agent` are Hosted on [Agentverse](https://agentverse.ai/).

FGI Agent Script[â€‹](#fgi-agent-script "Direct link to FGI Agent Script")
------------------------------------------------------------------------

The Fear and Greed Index Agent fetches market sentiment data from CoinMarketCap.

### Script Breakdown (fgi-agent/agent.py)[â€‹](#script-breakdown-fgi-agentagentpy "Direct link to Script Breakdown (fgi-agent/agent.py)")

#### Importing Required Libraries[â€‹](#importing-required-libraries "Direct link to Importing Required Libraries")

    import osfrom uagents import Agent, Context, Modelimport requestsfrom datetime import datetimefrom typing import Optional

#### Defining Data Models[â€‹](#defining-data-models "Direct link to Defining Data Models")

    class FGIRequest(Model):    limit: Optional[int] = 1class FearGreedData(Model):    value: float    value_classification: str    timestamp: strclass FGIResponse(Model):    data: list[FearGreedData]    status: str    timestamp: str

#### Initializing the FGI Agent[â€‹](#initializing-the-fgi-agent "Direct link to Initializing the FGI Agent")

#### API Integration Function[â€‹](#api-integration-function "Direct link to API Integration Function")

    def get_fear_and_greed_index(limit: int = 1) -> FGIResponse:    """Fetch Fear and Greed index data from CoinMarketCap API"""    url = "https://pro-api.coinmarketcap.com/v3/fear-and-greed/historical"    api_key = CMC_API_KEY        headers = {        "X-CMC_PRO_API_KEY": api_key    }        params = {        "limit": limit    }    response = requests.get(url, headers=headers, params=params)        if response.status_code == 200:        raw_data = response.json()        fear_greed_data = []                for entry in raw_data["data"]:            data = FearGreedData(                value=entry["value"],                value_classification=entry["value_classification"],                timestamp=entry["timestamp"]            )            fear_greed_data.append(data)                return FGIResponse(            data=fear_greed_data,            status="success",            timestamp=datetime.utcnow().isoformat()        )    else:        raise Exception(f"Error fetching data: {response.json()['status']['error_message']}")async def process_response(ctx: Context, msg: FGIRequest) -> FGIResponse:    """Process the request and return formatted response"""    fear_greed_data = get_fear_and_greed_index(msg.limit)        for entry in fear_greed_data.data:        ctx.logger.info(f"Fear and Greed Index: {entry.value}")        ctx.logger.info(f"Classification: {entry.value_classification}")        ctx.logger.info(f"Timestamp: {entry.timestamp}")        return fear_greed_data

#### Event Handlers[â€‹](#event-handlers "Direct link to Event Handlers")

    @agent.on_event("startup")async def startup(ctx: Context):    """Initialize agent with a test request"""    ctx.logger.info(f"Hello, I'm a Fear and Greed Index agent and my address is {ctx.agent.address}.")    dummy_request = FGIRequest(limit=1)    await process_response(ctx, dummy_request)@agent.on_message(model=FGIRequest)async def handle_message(ctx: Context, sender: str, msg: FGIRequest):    """Handle incoming messages requesting Fear and Greed index data"""    ctx.logger.info(f"Received message from {sender}: FGIRequest for {msg.limit} entries")        response = await process_response(ctx, msg)    await ctx.send(sender, response)        return response

note

**Note:** This is an hosted agent on [Agentverse](https://agentverse.ai/). This is the reason we have not provided `name, seed, endpoint` and `port` to the agent.

**Note:** Store `CMC_API_KEY` in the agent secret of your hosted Agent.

Coin Info Agent Script[â€‹](#coin-info-agent-script "Direct link to Coin Info Agent Script")
------------------------------------------------------------------------------------------

The Coin Info Agent fetches cryptocurrency market data from CoinGecko.

### Script Breakdown (coin-info-agent/agent.py)[â€‹](#script-breakdown-coin-info-agentagentpy "Direct link to Script Breakdown (coin-info-agent/agent.py)")

#### Importing Required Libraries[â€‹](#importing-required-libraries-1 "Direct link to Importing Required Libraries")

    import osfrom uagents import Agent, Context, Modelimport requests

#### Defining Data Models[â€‹](#defining-data-models-1 "Direct link to Defining Data Models")

    class CoinRequest(Model):    coin_id: strclass CoinResponse(Model):    name: str    symbol: str    current_price: float    market_cap: float    total_volume: float    price_change_24h: float

#### Initializing the Coin Info Agent[â€‹](#initializing-the-coin-info-agent "Direct link to Initializing the Coin Info Agent")

#### API Integration Function[â€‹](#api-integration-function-1 "Direct link to API Integration Function")

    def get_crypto_info(coin_id: str) -> CoinResponse:    """Fetch cryptocurrency information from CoinGecko API"""    url = f"https://api.coingecko.com/api/v3/coins/{coin_id}"    response = requests.get(url)        if response.status_code == 200:        data = response.json()                return CoinResponse(            name=data['name'],            symbol=data['symbol'].upper(),            current_price=data['market_data']['current_price']['usd'],            market_cap=data['market_data']['market_cap']['usd'],            total_volume=data['market_data']['total_volume']['usd'],            price_change_24h=data['market_data']['price_change_percentage_24h']        )    else:        raise Exception(f"Failed to get crypto info: {response.text}")async def process_response(ctx: Context, msg: CoinRequest) -> CoinResponse:    """Process the crypto request and return formatted response"""    crypto_data = get_crypto_info(msg.coin_id)    ctx.logger.info(f"Crypto data: {crypto_data}")    return crypto_data

#### Event Handlers[â€‹](#event-handlers-1 "Direct link to Event Handlers")

    @agent.on_event("startup")async def startup(ctx: Context):    """Initialize agent with a test request for Bitcoin data"""    ctx.logger.info(f"Hello, I'm a crypto agent and my address is {ctx.agent.address}.")@agent.on_message(model=CoinRequest)async def handle_message(ctx: Context, sender: str, msg: CoinRequest):    """Handle incoming messages requesting crypto information"""    ctx.logger.info(f"Received message from {sender}: {msg.coin_id}")        response = await process_response(ctx, msg)    await ctx.send(sender, response)        return response

note

**Note:** This is an hosted agent on [Agentverse](https://agentverse.ai/). This is the reason we have not provided `name, seed, endpoint` and `port` to the agent.

ASI1 LLM Implementation[â€‹](#asi1-llm-implementation "Direct link to ASI1 LLM Implementation")
---------------------------------------------------------------------------------------------

Create a new file called `asi/llm.py` with the following code:

    import requestsimport osfrom dotenv import load_dotenv# Load environment variables from a .env fileload_dotenv()# Retrieve the API key from environment variablesapi_key = os.getenv("ASI1_LLM_API_KEY")# ASI1-Mini LLM API endpointurl = "https://api.asi1.ai/v1/chat/completions"# Define headers for API requests, including authenticationheaders = {    "Content-Type": "application/json",    "Authorization": f"Bearer {api_key}"}def query_llm(query):    """    Queries the ASI1-Mini LLM with a given prompt and returns the model's response.    Parameters:        query (str): The input question or statement for the language model.    Returns:        str: The response from the LLM.        If an error occurs during the request, the function returns the exception object.    """    data = {        "messages": [{"role": "user", "content": query}],  # User input for the chat model        "conversationId": None,  # No conversation history tracking        "model": "asi1-mini"  # Specifies the model version to use    }    try:        # Send a POST request to the LLM API with the input query        with requests.post(url, headers=headers, json=data) as response:            output = response.json()  # Parse the JSON response            # Extract and return the generated message content            return output["choices"][0]["message"]["content"]        except requests.exceptions.RequestException as e:        # Handle and return any request-related exceptions (e.g., network errors)        return str(e)

Make sure to create the `asi` directory and include an empty `__init__.py` file to make it a Python package.

Main Agent Script[â€‹](#main-agent-script "Direct link to Main Agent Script")
---------------------------------------------------------------------------

The Main Agent coordinates between the FGI and Coin Info agents, using ASI1 Mini for decision-making.

### Script Breakdown (main.py)[â€‹](#script-breakdown-mainpy "Direct link to Script Breakdown (main.py)")

#### Importing Required Libraries[â€‹](#importing-required-libraries-2 "Direct link to Importing Required Libraries")

    from uagents import Agent, Context, Modelfrom typing import Optionalfrom asi.llm import query_llm

#### Agent Configuration and Data Models[â€‹](#agent-configuration-and-data-models "Direct link to Agent Configuration and Data Models")

    # Initialize the agent with a name and mailbox enabled for communicationagent = Agent(name="Sentiment-Based Crypto Sell Alerts Agent", mailbox=True,port = 8001)# Coin to monitorCOIN_ID = "bitcoin"# Agentverse agent addressesCOIN_AGENT = "agent1q0005tewegtmkruc9s3v2zz8p45dd68hu38zurgpwjrrj94h8sl5cz0qev5" #Update this with your Coin agent address on AgentverseFGI_AGENT = "agent1q2eln9t0c70ha0z8uz6q88mrdzsdkxfmk3zmjejecv7skf4844cpvzplp02" #Update this with your FGI agent address on Agentverse### AGENTVERSE INTERACTION CLASSES #### Request model for retrieving coin dataclass CoinRequest(Model):    coin_id: str# Response model for coin dataclass CoinResponse(Model):    name: str    symbol: str    current_price: float    market_cap: float    total_volume: float    price_change_24h: float# Request model for Fear Greed Index (FGI) dataclass FGIRequest(Model):    limit: Optional[int] = 1# Model for individual FGI data pointsclass FearGreedData(Model):    value: float    value_classification: str    timestamp: str# Response model for FGI dataclass FGIResponse(Model):    data: list[FearGreedData]    status: str    timestamp: str

#### Event Handlers[â€‹](#event-handlers-2 "Direct link to Event Handlers")

    @agent.on_event("startup")async def introduce_agent(ctx: Context):    """Introduces the agent when it starts running."""    print(f"Hello! I'm {agent.name} and my address is {agent.address}.")@agent.on_interval(period=24 * 60 * 60.0)  # Runs every 24 hoursasync def check_coin(ctx: Context):    """Requests market data for the monitored coin once a day."""    await ctx.send(COIN_AGENT, CoinRequest(coin_id=COIN_ID))@agent.on_message(model=CoinResponse)async def handle_coin_response(ctx: Context, sender: str, msg: CoinResponse):    """Handles incoming coin market data and requests FGI data if the price drop exceeds 10%."""    global market_data    market_data = msg        # Check if price has dropped by 10% or more before requesting FGI analysis    if msg.price_change_24h <= -10.0:        await ctx.send(FGI_AGENT, FGIRequest())@agent.on_message(model=FGIResponse)async def handle_fgi_response(ctx: Context, sender: str, msg: FGIResponse):    """Analyzes Fear Greed Index data and determines whether to issue a SELL alert."""    global fgi_analysis    fgi_analysis = msg        # Construct the AI prompt based on current market and sentiment analysis    prompt = f'''    Given the following information, respond with either SELL or HOLD for the coin {COIN_ID}.        Below is analysis on the Fear Greed Index:    {fgi_analysis}        Below is analysis on the coin:    {market_data}    '''        response = query_llm(prompt)  # Query ASI1 Mini for a decision        # Interpret the AI response and print decision    if "SELL" in response:        print("SELL")    else:        print("HOLD")

Running the System[â€‹](#running-the-system "Direct link to Running the System")
------------------------------------------------------------------------------

**1\. Start the FGI Agent**

Open a blank agent on AV and write your [script](#coin-info-agent-script) in it. Click on `Start Agent` button.

You should see output similar to:

    INFO: Hello, I'm a Fear and Greed Index agent and my address is agent1q2eln9t0c70ha0z8uz6q88mrdzsdkxfmk3zmjejecv7skf4844cpvzplp02

**2\. Start the Coin Info Agent**

Open another blank agent on AV and write your [script](#fgi-agent-script) in it. Click on `Start Agent` button.

You should see output similar to:

    INFO: Hello, I'm a crypto agent and my address is agent1q0005tewegtmkruc9s3v2zz8p45dd68hu38zurgpwjrrj94h8sl5cz0qev5

**3\. Start the Main Agent**

Open a terminal window and run:

You should see output similar to:

    Hello! I'm Sentiment-Based Crypto Sell Alerts Agent and my address is agent1qxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

**4\. Monitor the System**

The main agent will:

1.  Check cryptocurrency prices every 24 hours
2.  Request FGI analysis if price drops by 10% or more
3.  Use ASI1 Mini to analyze data and make SELL/HOLD recommendations

Troubleshooting Common Issues[â€‹](#troubleshooting-common-issues "Direct link to Troubleshooting Common Issues")
---------------------------------------------------------------------------------------------------------------

### API Key Issues[â€‹](#api-key-issues "Direct link to API Key Issues")

If you see API-related errors:

1.  Verify your API keys are correctly set in the `.env` files
2.  Check API rate limits and quotas
3.  Ensure the API services are operational

### Agent Communication Issues[â€‹](#agent-communication-issues "Direct link to Agent Communication Issues")

If agents aren't communicating:

1.  Verify all agents are running
2.  Check agent addresses in `main.py` match the actual addresses
3.  Ensure network connectivity

### ASI1 Mini Integration Issues[â€‹](#asi1-mini-integration-issues "Direct link to ASI1 Mini Integration Issues")

If AI analysis isn't working:

1.  Check ASI1\_API\_KEY is set correctly
2.  Verify ASI1 Mini service status
3.  Review prompt formatting in `main.py`

Benefits of DeFi AI Agent System[â€‹](#benefits-of-defi-ai-agent-system "Direct link to Benefits of DeFi AI Agent System")
------------------------------------------------------------------------------------------------------------------------

*   **Automated Monitoring**: 24/7 tracking of cryptocurrency prices
*   **Data-Driven Decisions**: Combines market data with sentiment analysis
*   **AI-Powered Analysis**: Leverages ASI1 Mini for objective decision-making
*   **Scalable Architecture**: Easy to add more data sources and analysis types
*   **Real-time Alerts**: Immediate notification of significant market events

GitHub Repository[â€‹](#github-repository "Direct link to GitHub Repository")
---------------------------------------------------------------------------

For the complete code, visit the [DeFi AI Agent Repository](https://github.com/RoyceBraden/DeFI-Agent-Starter/tree/main).

note

**Note:** You can learn more about ASI1 Mini APIs [**here**](https://docs.asi1.ai/docs/) and about uAgents [**here**](https://innovationlab.fetch.ai/resources/docs/1.0.3/agent-creation/uagent-creation).</content>
</page>

<page>
  <title>ASI1-mini LangChain & Tavily Search Integration Guide | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/asione/asi-langchain-tavily</url>
  <content>This guide demonstrates how to integrate the ASI1-mini API with LangChain and leverage the Tavily Search tool to process search queries in a streamlined manner. The project is encapsulated in a single file that implements a custom LangChain `LLM` and integrates it with an agent chain to combine API responses with dynamic search results.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

This project showcases an integration system built on the following key components:

*   **Custom LLM Integration:**  
    Implements a custom LangChain `LLM` that sends user prompts to the ASI1-mini API using a defined JSON payload.
    
*   **Tavily Search Tool:**  
    Uses the Tavily Search API to fetch search results, which are then incorporated into the agent chain to enhance the response.
    
*   **Agent Chain Execution:**  
    Sets up an agent chain that processes search queries, calls the ASI1-mini API, and returns a combined result.
    
*   **Environment-Based Configuration:**  
    Manages API keys and sensitive configurations through environment variables loaded from a `.env` file.
    

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

Before running this project, ensure you have the following:

*   **Python:** Version 3.8 or higher.
    
*   **Required Python Packages:**
    
        pip install requests pydantic python-dotenv langchain
    
*   **Environment Variables:**
    
    *   A valid API key for ASI1. Obtain your API Key [here](https://asi1.ai/dashboard/api-keys).
    *   A valid API key for Tavily. Obtain your API Key [here](https://app.tavily.com/home#).
    
    Create a `.env` file in the project directory with the following keys: `ASI_LLM_KEY=<asi1-api_key>` `TAVILY_API_KEY=<tavily_api_key>`
    

Project Structure[â€‹](#project-structure "Direct link to Project Structure")
---------------------------------------------------------------------------

The entire integration is contained within a single file:

`ASI_Langchain.py` # Contains the custom LLM class and the search handler integration

Script Breakdown[â€‹](#script-breakdown "Direct link to Script Breakdown")
------------------------------------------------------------------------

**1\. Importing Required Libraries**

The script begins by importing the necessary modules:

*   **os**: To get environment variables.
*   **requests:** To perform HTTP requests to the ASI-1 Mini API.
*   **typing:** For getting Python types.
*   **pydantic:** To define pydantic data models required by Langchain.
*   **langchain:** Imports required by Langchain.
*   **langchain\_community:** Imports required to use the TavilySearch tool.

    import osimport requestsfrom typing import Optional, Listfrom pydantic import Fieldfrom langchain.llms.base import LLMfrom langchain_community.utilities.tavily_search import TavilySearchAPIWrapperfrom langchain.agents import initialize_agent, AgentTypefrom langchain_community.tools.tavily_search.tool import TavilySearchResultsfrom dotenv import load_dotenv

**2\. Defining the ASI-1 Mini LLM Class**

Defines a custom LangChain LLM that sends prompts to the ASI1-mini API. It supports parameters such as temperature, fun mode, and web search, and handles API responses by extracting the relevant message content.

    class ASI1MINI(LLM):    api_key: str = Field(...)    api_url: str = Field(...)    model: str = Field(default="asi1-mini")    temperature: float = Field(default=0.7)    fun_mode: bool = Field(default=False)    web_search: bool = Field(default=False)    enable_stream: bool = Field(default=False)    max_tokens: int = Field(default=1024)    @property    def _llm_type(self) -> str:        return "custom_llm"    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:        headers = {            "Authorization": f"Bearer {self.api_key}",            "Content-Type": "application/json",        }        payload = {            "model": self.model,            "messages": [{"role": "user", "content": prompt}],            "temperature": self.temperature,            "fun_mode": self.fun_mode,            "web_search": self.web_search,            "stream": self.enable_stream,            "max_tokens": self.max_tokens,        }        if stop:            payload["stop"] = stop        response = requests.post(self.api_url, headers=headers, json=payload)        response.raise_for_status()        response_data = response.json()        return (            response_data.get("choices", [{}])[0].get("message", {}).get("content", "")        )

**3\. Initializing the Agent**

The agent is defined in the `custom_search_handler` function.

    def custom_search_handler(data):    """    Uses LangChain to process a search query with the custom LLM.    Expects a JSON payload with the key "search_query" and returns the result.    """    search_query = data.get("search_query")    if not search_query:        return {"error": "Missing search query"}    custom_api_key = os.getenv("ASI_LLM_KEY")    custom_api_url = "https://api.asi1.ai/v1/chat/completions"    tavily_api_key = os.getenv("TAVILY_API_KEY")    print("1: ", custom_api_key)    print("2: ", custom_api_url)    print("3: ", tavily_api_key)    if not custom_api_key or not custom_api_url or not tavily_api_key:        return {"error": "Missing API keys"}    try:        # Initialize your custom LLM        llm = ASI1MINI(api_key=custom_api_key, api_url=custom_api_url, temperature=0.7)        # Initialize the Tavily search tool        search = TavilySearchAPIWrapper()        tavily_tool = TavilySearchResults(            api_wrapper=search, tavily_api_key=tavily_api_key        )        # Initialize the agent with your custom LLM and Tavily search tool        agent_chain = initialize_agent(            [tavily_tool],            llm,            agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,            verbose=True,        )        # Run the agent chain with the search query        result = agent_chain.invoke({"input": search_query})        return {"result": result}    except Exception as e:        return {"error": str(e)}

Running the System
------------------

**1\. Activate Your Virtual Environment:**

    source venv/bin/activate   # On Windows: venv\Scripts\activate

**2\. Run the Script:**

Sample Outputs
--------------

1.  **Factual question known by the LLM(Does not use the Tavily tool)**
    
    **Input:** `How tall is the Eiffel tower?`
    
    **Final Output:**
    
        {'result': 'The Eiffel Tower is approximately 330 meters (1,083 feet) tall, including its antennas.'}
    
2.  **Current news not known by the LLM (Uses the Tavily tool)**
    
    **Input:** `Nvidia company news?`
    
    \*\* Final Output:\*\*
    
        {'result': "Here are some recent updates on NVIDIA:\n1. **GTC 2025 Announcement**: NVIDIAâ€™s premier AI conference will take place from March 17-21, 2025, in San Jose, California, featuring advancements in agentic AI and RTX AI tools.\n2. **New Product Launch**: The NVIDIA GeForce RTX 5070 Ti, built on the Blackwell architecture, is now available, boosting generative AI content creation and creative workflows.\n3. **AI Platform Advancements**: NVIDIA has unveiled the Rubin AI platform, set for 2026, and introduced the largest publicly available AI model for genomic data using DGX Cloud.\n4. **Stock Performance**: After a 27% decline over three weeks, Nvidia stock is attempting a rebound, supported by positive analyst reports.\nFor more details, you can visit NVIDIA's official newsroom or recent financial updates."}
    

Troubleshooting
---------------

1.  **Environment Variables**

Ensure that both `ASI_LLM_KEY` and `TAVILY_API_KEY` are correctly defined in your `.env` file.

Missing or incorrect API keys will lead to errors. API Connectivity

Verify that the ASI1-mini API endpoint ([https://api.asi1.ai/v1/chat/completions](https://api.asi1.ai/v1/chat/completions)) is accessible.

Confirm that the Tavily Search API is operational and that your API key is valid.

Debugging
---------

The code includes debug print statements (e.g., printing API responses) to help trace issues with API calls or response handling. Review the console output to diagnose any problems during execution.

Benefits of This Integration
----------------------------

1.  **Seamless API Communication:**
    
    Directly integrates with the ASI1-mini API via a custom LangChain LLM.
    
2.  **Enhanced Search Capabilities:**
    
    Enriches responses by combining LLM outputs with real-time search results using Tavily Search.
    
3.  **Configurable Parameters:**
    
    Offers flexibility through parameters like temperature, fun mode, and maximum tokens.
    
4.  **Simplified Deployment:**
    
    The single-file integration simplifies setup and deployment, making it easy to incorporate into larger projects.
    

Additional Resources
--------------------

*   **ASI1-mini API Documentation**([https://docs.asi1.ai](https://docs.asi1.ai/))
*   **LangChain GitHub Repository**([https://python.langchain.com/docs/introduction/](https://python.langchain.com/docs/introduction/))
*   **Tavily Search Tool Documentation**([https://docs.tavily.com/welcome](https://docs.tavily.com/welcome))

GitHub Repository[â€‹](#github-repository "Direct link to GitHub Repository")
---------------------------------------------------------------------------

For the complete code, visit the [ASI1 Chat System Repository](https://github.com/abhifetch/ASI-1_mini_Langchain).

note

**Note:** You can learn more about ASI1 Mini APIs [**here**](https://docs.asi1.ai/docs/).</content>
</page>

<page>
  <title>Image Analysis Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/chat-protocol/image-analysis-agent</url>
  <content>This guide demonstrates how to create an Image Analysis Agent that can process both text and image inputs using the chat protocol. The agent is compatible with the [Agentverse Chat Interface](http://chat.agentverse.ai/) and can process natural language queries about image analysis.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

In this example, you'll learn how to build a uAgent that can:

*   Accept both text and image inputs through the chat protocol
*   Process images using Claude's vision capabilities
*   Store and manage image resources using Agent storage
*   Respond to queries about image content

Message Flow[â€‹](#message-flow "Direct link to Message Flow")
------------------------------------------------------------

The communication between the User, Chat Interface, and Image Analyser Agent proceeds as follows:

1.  **User Query**
    
    *   The user submits a query along with an image through the [Chat Interface](http://chat.agentverse.ai/).
2.  **Image Upload & Query Forwarding**
    
    *   **2.1**: The Chat Interface uploads the image to the Agent Storage.
    *   **2.2**: The Chat Interface forwards the user's query with a reference to the uploaded image to the Image Analyser Agent as a `ChatMessage`.
3.  **Image Retrieval**
    
    *   The Image Analyser Agent retrieves the image from Agent Storage using the provided reference.
4.  **Image Analysis**
    
    *   **4.1**: The agent passes the query and image to the Image Analysis Function.
    *   **4.2**: The Image Analysis Function processes the image and returns a response.
5.  **Response & Acknowledgement**
    
    *   **5.1**: The agent sends the analysis result back to the Chat Interface as a `ChatMessage`.
    *   **5.2**: The agent also sends a `ChatAcknowledgement` to confirm receipt and processing of the message.
6.  **User Receives Response**
    
    *   The Chat Interface delivers the analysis result to the user.

Implementation[â€‹](#implementation "Direct link to Implementation")
------------------------------------------------------------------

In this example we will create an agent and its associated files on [Agentverse](https://agentverse.ai/) that communicate using the chat protocol with the [Chat Interface](https://agentverse.ai/) Refer to the [Hosted Agents](https://innovationlab.fetch.ai/resources/docs/agent-creation/uagent-creation#hosted-agents) section to understand the detailed steps for agent creation on Agentverse.

Create a new agent named "Image Analysis Agent" on Agentverse and create the following files:

    agent.py            # Main agent file image_analysis.py   # Image analysis functionchat_proto.py       # Chat protocol implementation for enabling text based communication 

To create a new file on Agentverse:

1.  Click on the New File icon
    
2.  Assign a name to the File
    
3.  Directory Structure
    

### 1\. Image Analysis Implementation[â€‹](#1-image-analysis-implementation "Direct link to 1. Image Analysis Implementation")

The `image_analysis.py` file implements the logic for passing both text and image inputs to Claude's vision model. It handles encoding images, constructing the appropriate request, and returning the AI-generated analysis of the image and query.

image\_analysis.py

    import jsonimport osfrom typing import Anyimport requestsCLAUDE_URL = "https://api.anthropic.com/v1/messages"MAX_TOKENS = int(os.getenv("MAX_TOKENS", "1024"))ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY", "YOUR_ANTHROPIC_API_KEY")if ANTHROPIC_API_KEY is None or ANTHROPIC_API_KEY == "YOUR_ANTHROPIC_API_KEY":    raise ValueError(        "You need to provide an API key: https://platform.openai.com/api-keys"    )MODEL_ENGINE = os.getenv("MODEL_ENGINE", "claude-3-5-haiku-latest")HEADERS = {    "x-api-key": ANTHROPIC_API_KEY,    "anthropic-version": "2023-06-01",    "content-type": "application/json",}def get_image_analysis(    content: list[dict[str, Any]], tool: dict[str, Any] | None = None) -> str | None:    processed_content = []    for item in content:        if item.get("type") == "text":            processed_content.append({"type": "text", "text": item["text"]})        elif item.get("type") == "resource":            mime_type = item["mime_type"]            if mime_type.startswith("image/"):                processed_content.append({                    "type": "image",                    "source": {                        "type": "base64",                        "media_type": mime_type,                        "data": item["contents"],                    }                })            else:                return f"Unsupported mime type: {mime_type}"    data = {        "model": MODEL_ENGINE,        "max_tokens": MAX_TOKENS,        "messages": [            {                "role": "user",                "content": processed_content,            }        ],    }    if tool:        data["tools"] = [tool]        data["tool_choice"] = {"type": "tool", "name": tool["name"]}    try:        response = requests.post(            CLAUDE_URL, headers=HEADERS, data=json.dumps(data), timeout=120        )        response.raise_for_status()    except requests.exceptions.Timeout:        return "The request timed out. Please try again."    except requests.exceptions.RequestException as e:        return f"An error occurred: {e}"    # Check if the response was successful    response_data = response.json()    # Handle error responses    if "error" in response_data:        return f"API Error: {response_data['error'].get('message', 'Unknown error')}"    if tool:        for item in response_data["content"]:            if item["type"] == "tool_use":                return item["input"]                messages = response_data["content"]    if messages:        return messages[0]["text"]    else:        return None

### 2\. Chat Protocol Integration[â€‹](#2-chat-protocol-integration "Direct link to 2. Chat Protocol Integration")

The `chat_proto.py` file is essential for enabling natural language communication between your agent and ASI1 LLM.

#### Message Flow[â€‹](#message-flow-1 "Direct link to Message Flow")

1.  **Session Initiation**
    
    *   When a user starts a chat session, the agent receives a `ChatMessage` containing a `StartSessionContent`.
    *   The agent responds with a `MetadataContent` message: `{"attachments": "true"}`. This signals to the chat UI that file attachments (such as images) are supported.
2.  **User Query**
    
    *   The user sends a query as a ChatMessage, which includes:
        *   `TextContent` (the user's question)
        *   `ResourceContent` (an image or other file attachment)
3.  **Message Processing**
    
    *   For each content item:
        *   If `TextContent`, the agent adds the text to the prompt for Claude.
        *   If `ResourceContent`, the agent downloads the image from Agent Storage and adds it to the prompt as an image input for Claude.
4.  **Image Analysis and AI Processing**
    
    *   The agent then analyses the image with the help of `image_analysis.py` and sends back a response with the analysis to the user.

To enable natural language communication with your agent add the following chat protocol in the `chat_proto.py` file created on Agentverse:

chat\_proto.py

    import osfrom datetime import datetimefrom uuid import uuid4from uagents import Context, Protocolfrom uagents_core.contrib.protocols.chat import (    ChatAcknowledgement,    ChatMessage,    MetadataContent,    ResourceContent,    StartSessionContent,    TextContent,    chat_protocol_spec,)from uagents_core.storage import ExternalStoragefrom image_analysis import get_image_analysisSTORAGE_URL = os.getenv("AGENTVERSE_URL", "https://agentverse.ai") + "/v1/storage"def create_text_chat(text: str) -> ChatMessage:    return ChatMessage(        timestamp=datetime.utcnow(),        msg_id=uuid4(),        content=[TextContent(type="text", text=text)],    )def create_metadata(metadata: dict[str, str]) -> ChatMessage:    return ChatMessage(        timestamp=datetime.utcnow(),        msg_id=uuid4(),        content=[MetadataContent(            type="metadata",            metadata=metadata,        )],    )chat_proto = Protocol(spec=chat_protocol_spec)@chat_proto.on_message(ChatMessage)async def handle_message(ctx: Context, sender: str, msg: ChatMessage):    ctx.logger.info(f"Got a message from {sender}")    await ctx.send(        sender,        ChatAcknowledgement(            timestamp=datetime.utcnow(), acknowledged_msg_id=msg.msg_id        ),    )    prompt_content = []    for item in msg.content:        if isinstance(item, StartSessionContent):            await ctx.send(sender, create_metadata({"attachments": "true"}))        elif isinstance(item, TextContent):            prompt_content.append({"text": item.text, "type": "text"})        elif isinstance(item, ResourceContent):            try:                external_storage = ExternalStorage(                    identity=ctx.agent.identity,                    storage_url=STORAGE_URL,                )                data = external_storage.download(str(item.resource_id))                prompt_content.append({                    "type": "resource",                    "mime_type": data["mime_type"],                    "contents": data["contents"],                })            except Exception as ex:                ctx.logger.error(f"Failed to download resource: {ex}")                await ctx.send(sender, create_text_chat("Failed to download resource."))        else:            ctx.logger.warning(f"Got unexpected content from {sender}")    if prompt_content:        response = get_image_analysis(prompt_content)        await ctx.send(sender, create_text_chat(response))@chat_proto.on_message(ChatAcknowledgement)async def handle_ack(ctx: Context, sender: str, msg: ChatAcknowledgement):    ctx.logger.info(        f"Got an acknowledgement from {sender} for {msg.acknowledged_msg_id}"    )

### 3\. Image Analysis Agent Setup[â€‹](#3-image-analysis-agent-setup "Direct link to 3. Image Analysis Agent Setup")

The `agent.py` file is the core of your application. Think of it as the main control center that:

*   Initialises your agent
*   Handles incoming requests

In this example, we focus on the essential setup for an image analysis agent.

**Note:** If you want to add advanced features such as rate limiting or agent health checks, you can refer to the[Football Team Agent section](https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/chat-protocol/asi1-compatible-uagents#3-football-team-agent-setup) in the ASI1 Compatible uAgent guide.

agent.py

    from uagents import Agentfrom chat_proto import chat_protoagent = Agent()#Include the chat protocol defined in the previous step to handle text and image contentsagent.include(chat_proto, publish_manifest=True)if __name__ == "__main__":    agent.run()

Adding a README to your Agent[â€‹](#adding-a-readme-to-your-agent "Direct link to Adding a README to your Agent")
---------------------------------------------------------------------------------------------------------------

1.  Go to the Overview section in the Editor.
    
2.  Click on Edit and add a good description for your Agent so that it can be easily searchable by the ASI1 LLM. Please refer the [Importance of Good Readme](https://innovationlab.fetch.ai/resources/docs/agentverse/searching#importance-of-good-readme) section for more details.
    
3.  Make sure the Agent has the right `AgentChatProtocol`.
    

Query your Agent[â€‹](#query-your-agent "Direct link to Query your Agent")
------------------------------------------------------------------------

1.  Start your Agent
    
2.  Navigate to the Overview tab and click on **Chat with Agent** to interact with the agent from the [Agentverse Chat Interface](https://chat.agentverse.ai/).
    

3.  Click on the **Attach** button to upload the image and type in your query for instance 'How many people are present in the image?'

> **Note**: Currently, the image upload feature for agents is supported via the Agentverse Chat Interface. Support for image uploads through ASI:One will be available soon.</content>
</page>

<page>
  <title>Image Generation Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/chat-protocol/image-generation-agent</url>
  <content>This guide demonstrates how to create an Image Generation Agent that can generate images based on text descriptions using the chat protocol. The agent is compatible with the [Agentverse Chat Interface](http://chat.agentverse.ai/) and can process natural language requests to generate images.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

In this example, you'll learn how to build a uAgent that can:

*   Accept text descriptions through the chat protocol
*   Generate images using DALL-E 3
*   Store and manage generated images using Agent storage
*   Send generated images back to the user.

Message Flow[â€‹](#message-flow "Direct link to Message Flow")
------------------------------------------------------------

The communication between the User, Chat Interface, and Image Generator Agent proceeds as follows:

1.  **User Query**
    
    *   **1**: The user submits a text description of the desired image through the [Chat Interface](http://chat.agentverse.ai/).
2.  **Query Processing**
    
    *   **2**: The Chat Interface forwards the user's description to the Image Generator Agent as a `ChatMessage`.
3.  **Image Generation**
    
    *   **3.1** and **3.2**: The agent processes the text description using DALL-E 3.
    *   **4.1** and **4.2**: The generated image is uploaded to External Storage.
4.  **Response & Resource Sharing**
    
    *   **5.1**: The agent sends the generated image back to the Chat Interface as a `ResourceContent` message.
    *   **5.2**: The agent also sends a `ChatAcknowledgement` to confirm receipt and processing of the message.
5.  **User Receives Image**
    
    *   **6**: The Chat Interface displays the generated image to the user.

Implementation[â€‹](#implementation "Direct link to Implementation")
------------------------------------------------------------------

In this example, we will create an agent and its associated files on our local machine that communicate using the chat protocol. The agent will be connected to [Agentverse](https://agentverse.ai/) via **Mailbox**, refer to the [Mailbox Agents](https://innovationlab.fetch.ai/resources/docs/agent-creation/uagent-creation#mailbox-agents) section to understand the detailed steps for connecting a local agent to Agentverse.

Create a new directory named "image-generation" and create the following files:

    mkdir image-generation   #Create a directorycd image-generation      #Navigate to the directorytouch agent.py            # Main agent file touch models.py           # Image generation models and functionstouch chat_proto.py       # Chat protocol implementation for enabling text-based communication 

### 1\. Image Generation Implementation[â€‹](#1-image-generation-implementation "Direct link to 1. Image Generation Implementation")

The `models.py` file implements the logic for generating images using DALL-E 3. It handles the API connection, image generation, and response processing.

models.py

    import osfrom uagents import Modelfrom openai import OpenAI, OpenAIErrorOPENAI_API_KEY = os.getenv("OPENAI_API_KEY")         #Make sure to set your OpenAI API Key in environment variablesif OPENAI_API_KEY is None:    raise ValueError("You need to provide an OpenAI API Key.")client = OpenAI(api_key=OPENAI_API_KEY)class ImageRequest(Model):    image_description: strclass ImageResponse(Model):    image_url: strdef generate_image(prompt: str) -> str:    try:        response = client.images.generate(            model="dall-e-3",            prompt=prompt,        )    except OpenAIError as e:        return f"An error occurred: {e}"    return response.data[0].url

### 2\. Chat Protocol Integration[â€‹](#2-chat-protocol-integration "Direct link to 2. Chat Protocol Integration")

The `chat_protocol.py` file is responsible for orchestrating the entire communication and image generation process when the agent receives a user's request. Here's how it works step by step:

#### i) Receiving the Message[â€‹](#i-receiving-the-message "Direct link to i) Receiving the Message")

    @chat_proto.on_message(ChatMessage)async def handle_message(ctx: Context, sender: str, msg: ChatMessage):

The agent listens for incoming `ChatMessage` instances.

#### ii) Acknowledging Receipt[â€‹](#ii-acknowledging-receipt "Direct link to ii) Acknowledging Receipt")

    await ctx.send(    sender,    ChatAcknowledgement(timestamp=datetime.utcnow(), acknowledged_msg_id=msg.msg_id),)

Once a message is received, the agent immediately sends a `ChatAcknowledgement` back to the sender.

#### iii) Parsing Content[â€‹](#iii-parsing-content "Direct link to iii) Parsing Content")

    for item in msg.content:    if isinstance(item, StartSessionContent):        ...    elif isinstance(item, TextContent):

The message content is iterated over.

*   If it contains `StartSessionContent`, the agent logs it and waits for further input.
*   If it contains `TextContent`, it's treated as the image prompt and passed for processing.

#### iv) Image Generation via DALLÂ·E 3[â€‹](#iv-image-generation-via-dalle-3 "Direct link to iv) Image Generation via DALLÂ·E 3")

    prompt = item.textimage_url = generate_image(prompt)

*   The prompt is extracted and passed to the `generate_image()` function from `models.py` to generate an image URL using DALLÂ·E 3.

#### v) Downloading the Image[â€‹](#v-downloading-the-image "Direct link to v) Downloading the Image")

    response = requests.get(image_url)if response.status_code == 200:    image_data = response.content    content_type = response.headers.get("Content-Type", "")

*   The generated image is downloaded using a direct HTTP request.
*   If successful, the image binary and MIME type are extracted for storage.

#### vi) Uploading to Agent Storage[â€‹](#vi-uploading-to-agent-storage "Direct link to vi) Uploading to Agent Storage")

    asset_id = external_storage.create_asset(    name=str(ctx.session),    content=image_data,    mime_type=content_type)

*   The image is uploaded to the Agent's `ExternalStorage` system.
*   A unique `asset_id` is returned to identify the uploaded image.

#### vii) Permission Management[â€‹](#vii-permission-management "Direct link to vii) Permission Management")

    external_storage.set_permissions(asset_id=asset_id, agent_address=sender)

*   The agent sets viewing permissions so that only the user who requested the image can access it.

#### viii) Responding with the Image[â€‹](#viii-responding-with-the-image "Direct link to viii) Responding with the Image")

    asset_uri = f"agent-storage://{external_storage.storage_url}/{asset_id}"await ctx.send(sender, create_resource_chat(asset_id, asset_uri))

*   The agent constructs a `ResourceContent` message containing the image asset.
*   This message is sent back to the user for viewing in the chat interface.

#### Whole script[â€‹](#whole-script "Direct link to Whole script")

This agent leverages external storage to securely upload, store, and share generated images. An Agentverse API key is required for authentication and to enable interaction with the external storage. You can obtain your API key from [Agentverse](https://agentverse.ai/); for detailed instructions, please refer to the [Agentverse API Key guide](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key).

chat\_proto.py

    import base64import osimport requestsfrom uuid import uuid4from datetime import datetimefrom pydantic.v1 import UUID4from uagents import Context, Protocolfrom uagents_core.contrib.protocols.chat import (    ChatAcknowledgement,    ChatMessage,    EndSessionContent,    Resource,    ResourceContent,    StartSessionContent,    TextContent,    chat_protocol_spec,)from uagents_core.storage import ExternalStoragefrom models import generate_imageAGENTVERSE_API_KEY = os.getenv("AGENTVERSE_API_KEY")STORAGE_URL = os.getenv("AGENTVERSE_URL", "https://agentverse.ai") + "/v1/storage"if AGENTVERSE_API_KEY is None:    raise ValueError("You need to provide an API_TOKEN.")external_storage = ExternalStorage(api_token=AGENTVERSE_API_KEY, storage_url=STORAGE_URL)def create_text_chat(text: str) -> ChatMessage:    return ChatMessage(        timestamp=datetime.utcnow(),        msg_id=uuid4(),        content=[TextContent(type="text", text=text)],    )def create_end_session_chat() -> ChatMessage:    return ChatMessage(        timestamp=datetime.utcnow(),        msg_id=uuid4(),        content=[EndSessionContent(type="end-session")],    )def create_resource_chat(asset_id: str, uri: str) -> ChatMessage:    return ChatMessage(        timestamp=datetime.utcnow(),        msg_id=uuid4(),        content=[            ResourceContent(                type="resource",                resource_id=UUID4(asset_id),                resource=Resource(                    uri=uri,                    metadata={                        "mime_type": "image/png",                        "role": "generated-image"                    }                )            )        ]    )chat_proto = Protocol(spec=chat_protocol_spec)@chat_proto.on_message(ChatMessage)async def handle_message(ctx: Context, sender: str, msg: ChatMessage):    await ctx.send(        sender,        ChatAcknowledgement(timestamp=datetime.utcnow(), acknowledged_msg_id=msg.msg_id),    )    for item in msg.content:        if isinstance(item, StartSessionContent):            ctx.logger.info(f"Got a start session message from {sender}")            continue        elif isinstance(item, TextContent):            ctx.logger.info(f"Got a message from {sender}: {item.text}")            prompt = msg.content[0].text            try:                image_url = generate_image(prompt)                response = requests.get(image_url)                if response.status_code == 200:                    content_type = response.headers.get("Content-Type", "")                    image_data = response.content                                         try:                        asset_id = external_storage.create_asset(                            name=str(ctx.session),                            content=image_data,                            mime_type=content_type                        )                        ctx.logger.info(f"Asset created with ID: {asset_id}")                    except RuntimeError as err:                        ctx.logger.error(f"Asset creation failed: {err}")                    external_storage.set_permissions(asset_id=asset_id, agent_address=sender)                    ctx.logger.info(f"Asset permissions set to: {sender}")                    asset_uri = f"agent-storage://{external_storage.storage_url}/{asset_id}"                    await ctx.send(sender, create_resource_chat(asset_id, asset_uri))                else:                    ctx.logger.error("Failed to download image")                    await ctx.send(                        sender,                        create_text_chat(                            "Sorry, I couldn't process your request. Please try again later."                        ),                    )                    return            except Exception as err:                ctx.logger.error(err)                await ctx.send(                    sender,                    create_text_chat(                        "Sorry, I couldn't process your request. Please try again later."                    ),                )                return            await ctx.send(sender, create_end_session_chat())        else:            ctx.logger.info(f"Got unexpected content from {sender}")@chat_proto.on_message(ChatAcknowledgement)async def handle_ack(ctx: Context, sender: str, msg: ChatAcknowledgement):    ctx.logger.info(f"Got an acknowledgement from {sender} for {msg.acknowledged_msg_id}")

### 3\. Image Generator Agent Setup[â€‹](#3-image-generator-agent-setup "Direct link to 3. Image Generator Agent Setup")

The `agent.py` file initializes your agent and includes necessary protocols for handling user requests.

> **Note:** If you want to add advanced features such as rate limiting or agent health checks, you can refer to the[Football Team Agent section](https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/chat-protocol/asi1-compatible-uagents#3-football-team-agent-setup) in the ASI1 Compatible uAgent guide.

agent.py

    import osfrom enum import Enumfrom uagents import Agent, Context, Modelfrom uagents.experimental.quota import QuotaProtocol, RateLimitfrom uagents_core.models import ErrorMessagefrom chat_proto import chat_protofrom models import ImageRequest, ImageResponse, generate_imageAGENT_SEED = os.getenv("AGENT_SEED", "image-generator-agent-seed-phrase")AGENT_NAME = os.getenv("AGENT_NAME", "Image Generator Agent")PORT = 8000agent = Agent(    name=AGENT_NAME,    seed=AGENT_SEED,    port=PORT,    mailbox=True,)# Include protocolagent.include(chat_proto, publish_manifest=True)if __name__ == "__main__":    agent.run()

### Setting up Environment Variables[â€‹](#setting-up-environment-variables "Direct link to Setting up Environment Variables")

Make sure to set the following environment variables:

*   `OPENAI_API_KEY`: Your [OpenAI API key](https://platform.openai.com/api-keys) for DALL-E 3 access
*   `AGENTVERSE_API_KEY`: Your [Agentverse API key](https://agentverse.ai/) for storage access
*   `AGENT_SEED`: (Optional) Custom seed for your agent
*   `AGENT_NAME`: (Optional) Custom name for your agent

Adding a README to your Agent[â€‹](#adding-a-readme-to-your-agent "Direct link to Adding a README to your Agent")
---------------------------------------------------------------------------------------------------------------

1.  Start your agent and connect to Agentverse using the Agent Inspector Link in the logs. Please refer to the [Mailbox Agents](https://innovationlab.fetch.ai/resources/docs/agent-creation/uagent-creation#mailbox-agents) section to understand the detailed steps for connecting a local agent to Agentverse.

**Agent Logs**

> Click on the link, it will open a new window in your browser, click on **Connect** and then select **Mailbox**, this will connect your agent to Agentverse.

2.  Once you connect your Agent via Mailbox, click on Agent Profile and navigate to the Overview section of the Agent. Your Agent will appear under local agents on Agentverse.

2.  Click on Edit and add a good description and name for your Agent so that it can be easily searchable by the ASI1 LLM. Please refer to the [Importance of Good Readme](https://innovationlab.fetch.ai/resources/docs/agentverse/searching#importance-of-good-readme) section for more details.
    
3.  Make sure the Agent has the right `AgentChatProtocol`.
    

Query your Agent[â€‹](#query-your-agent "Direct link to Query your Agent")
------------------------------------------------------------------------

1.  Look for your agent under local agents on Agentverse.
    
2.  Navigate to the Overview tab of the agent and click on **Chat with Agent** to interact with the agent from the [Agentverse Chat Interface](https://chat.agentverse.ai/).
    

3.  Type in your image description, for example: "A serene landscape with mountains and a lake at sunset"
    
4.  The agent will generate an image based on your description and send it back through the chat interface.
    

> **Note**: Currently, the image sharing feature for agents is supported via the Agentverse Chat Interface. Support for image sharing through ASI:One will be available soon.</content>
</page>

<page>
  <title>Solana Wallet Balance Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/chat-protocol/solana-wallet-agent</url>
  <content>This guide demonstrates how to create a Solana Wallet Balance Agent that can check wallet balances using the Solana RPC API. The agent is compatible with ASI1 LLM and can process natural language queries about Solana wallet balances.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

The Solana Wallet Balance Agent allows users to query wallet balances using natural language. It uses the Solana RPC API to fetch real-time balance information and provides formatted responses with both SOL and lamports values.

Message Flow[â€‹](#message-flow "Direct link to Message Flow")
------------------------------------------------------------

The communication flow between ASI1 LLM, the Solana Wallet Agent, and OpenAI Agent follows this sequence:

1.  **Query Initiation (1.1)**
    
    *   ASI1 LLM sends a natural language query (e.g., "What's the balance of wallet address AtTjQKXo1CYTa2MuxPARtr382ZyhPU5YX4wMMpvaa1oy") as a `ChatMessage` to the Solana Wallet Agent.
2.  **Parameter Extraction (2, 3)**
    
    *   The Solana Wallet Agent forwards the query to OpenAI Agent for parameter extraction
    *   OpenAI Agent processes the natural language and extracts the wallet address
    *   The address is returned in a Pydantic Model format as `StructuredOutputResponse`
3.  **Balance Query (4, 5)**
    
    *   The Solana Wallet Agent calls the `get_balance_from_address` function with the extracted address
    *   The function queries the Solana RPC API and returns the balance information
4.  **Agent Response (6.1)**
    
    *   The Solana Wallet Agent sends the formatted response back as a `ChatMessage` to ASI1 LLM
5.  **Message Acknowledgements (1.2, 6.2)**
    
    *   Each message is acknowledged using `ChatAcknowledgement`

Here's a visual representation of the flow:

Implementation[â€‹](#implementation "Direct link to Implementation")
------------------------------------------------------------------

In this example we will create an agent and its associated files on [Agentverse](https://agentverse.ai/) that communicate using the chat protocol with the ASI1 LLM. Refer to the [Hosted Agents](https://innovationlab.fetch.ai/resources/docs/agent-creation/uagent-creation#hosted-agents) section to understand the detailed steps for agent creation on Agentverse.

Create a new agent named "SolanaWalletAgent" on Agentverse and create the following files:

    agent.py            # Main agent filesolana_service.py   # Solana RPC API integrationchat_proto.py       # Chat protocol implementation

To create a new file on Agentverse:

1.  Click on the New File icon

2.  Assign a name to the File

3.  Directory Structure

### 1\. Solana Service Implementation[â€‹](#1-solana-service-implementation "Direct link to 1. Solana Service Implementation")

The `solana_service.py` file handles the interaction with the Solana RPC API:

solana\_service.py

    import osimport loggingimport requestsimport jsonfrom uagents import Model, Field# Configure logginglogging.basicConfig(level=logging.INFO)logger = logging.getLogger(__name__)# Solana RPC endpointSOLANA_RPC_URL = "https://api.mainnet-beta.solana.com"class SolanaRequest(Model):    address: str = Field(        description="Solana wallet address to check",    )class SolanaResponse(Model):    balance: str = Field(        description="Formatted Solana wallet balance",    )async def get_balance_from_address(address: str) -> str:    """    Get the balance for a Solana address using the Solana RPC API        Args:        address: Solana wallet address            Returns:        Formatted balance string    """    try:        logger.info(f"Getting balance for address: {address}")                # Prepare the request payload        payload = {            "jsonrpc": "2.0",            "id": 1,            "method": "getBalance",            "params": [address]        }                # Set headers        headers = {            "Content-Type": "application/json"        }                # Make the API request        response = requests.post(SOLANA_RPC_URL, headers=headers, json=payload)        response.raise_for_status()                # Parse the response        result = response.json()                if "error" in result:            error_msg = f"Error: {result['error']['message']}"            logger.error(error_msg)            return error_msg                    if "result" in result and "value" in result["result"]:            # Convert lamports to SOL (1 SOL = 1,000,000,000 lamports)            lamports = result["result"]["value"]            sol_balance = lamports / 1_000_000_000                        # Format the result            result_str = f"{sol_balance:.9f} SOL ({lamports} lamports)"            logger.info(f"Balance for {address}: {result_str}")            return result_str        else:            error_msg = "No balance information found"            logger.error(error_msg)            return error_msg                except requests.exceptions.RequestException as e:        error_msg = f"Request error: {str(e)}"        logger.error(error_msg)        return error_msg    except json.JSONDecodeError as e:        error_msg = f"JSON decode error: {str(e)}"        logger.error(error_msg)        return error_msg    except Exception as e:        error_msg = f"Unexpected error: {str(e)}"        logger.error(error_msg)        return error_msg

### 2\. Chat Protocol Integration[â€‹](#2-chat-protocol-integration "Direct link to 2. Chat Protocol Integration")

The `chat_proto.py` file is essential for enabling natural language communication between your agent and ASI1 LLM.

#### LLM Integration[â€‹](#llm-integration "Direct link to LLM Integration")

To extract the important parameters from a user query passed by the LLM, you can use any of the following LLMs that support structured output:

*   OpenAI Agent: `agent1q0h70caed8ax769shpemapzkyk65uscw4xwk6dc4t3emvp5jdcvqs9xs32y`
*   Claude.ai Agent: `agent1qvk7q2av3e2y5gf5s90nfzkc8a48q3wdqeevwrtgqfdl0k78rspd6f2l4dx`

> **Note**: To ensure fair usage, each agent is limited to 6 requests per hour. Please implement appropriate rate limiting in your application.

#### Message Flow[â€‹](#message-flow-1 "Direct link to Message Flow")

1.  When a user sends a query (e.g., "What's the balance of wallet address AtTjQKXo1CYTa2MuxPARtr382ZyhPU5YX4wMMpvaa1oy"):
    
    *   The `ChatMessage` handler receives the message
    *   Acknowledges receipt to the sender
    *   Forwards the query to the chosen LLM for processing
2.  The LLM processes the query and returns a `StructuredOutputResponse`:
    
    *   Extracts relevant parameters (e.g., address="AtTjQKXo1CYTa2MuxPARtr382ZyhPU5YX4wMMpvaa1oy")
    *   Returns data in the format specified by your agent's schema
3.  Your agent processes the structured data:
    
    *   Calls appropriate functions (e.g., `get_balance_from_address`)
    *   Formats the response and sends it back.

#### Customizing the Handlers[â€‹](#customizing-the-handlers "Direct link to Customizing the Handlers")

When implementing a new agent for a different use case, you'll need to:

*   Update the schema in `StructuredOutputPrompt` to match your agent's data model
*   Modify the response handling in `handle_structured_output_response` function for your use case.

To enable natural language communication with your agent add the following chat protocol in the `chat_proto.py` file created on Agentverse:

chat\_proto.py

    from datetime import datetimefrom uuid import uuid4from typing import Anyfrom uagents import Context, Model, Protocol# Import the necessary components of the chat protocolfrom uagents_core.contrib.protocols.chat import (    ChatAcknowledgement,    ChatMessage,    EndSessionContent,    StartSessionContent,    TextContent,    chat_protocol_spec,)from solana_service import get_balance_from_address, SolanaRequest# AI Agent Address for structured output processingAI_AGENT_ADDRESS = 'agent1q0h70caed8ax769shpemapzkyk65uscw4xwk6dc4t3emvp5jdcvqs9xs32y'if not AI_AGENT_ADDRESS:    raise ValueError("AI_AGENT_ADDRESS not set")def create_text_chat(text: str, end_session: bool = True) -> ChatMessage:    content = [TextContent(type="text", text=text)]    if end_session:        content.append(EndSessionContent(type="end-session"))    return ChatMessage(        timestamp=datetime.utcnow(),        msg_id=uuid4(),        content=content,    )chat_proto = Protocol(spec=chat_protocol_spec)struct_output_client_proto = Protocol(    name="StructuredOutputClientProtocol", version="0.1.0")class StructuredOutputPrompt(Model):    prompt: str    output_schema: dict[str, Any]class StructuredOutputResponse(Model):    output: dict[str, Any]@chat_proto.on_message(ChatMessage)async def handle_message(ctx: Context, sender: str, msg: ChatMessage):    ctx.logger.info(f"Got a message from {sender}: {msg}")    ctx.storage.set(str(ctx.session), sender)    await ctx.send(        sender,        ChatAcknowledgement(timestamp=datetime.utcnow(), acknowledged_msg_id=msg.msg_id),    )    for item in msg.content:        if isinstance(item, StartSessionContent):            ctx.logger.info(f"Got a start session message from {sender}")            continue        elif isinstance(item, TextContent):            ctx.logger.info(f"Got a message from {sender}: {item.text}")            ctx.storage.set(str(ctx.session), sender)            await ctx.send(                AI_AGENT_ADDRESS,                StructuredOutputPrompt(                    prompt=item.text, output_schema=SolanaRequest.schema()                ),            )        else:            ctx.logger.info(f"Got unexpected content from {sender}")@chat_proto.on_message(ChatAcknowledgement)async def handle_ack(ctx: Context, sender: str, msg: ChatAcknowledgement):    ctx.logger.info(        f"Got an acknowledgement from {sender} for {msg.acknowledged_msg_id}"    )@struct_output_client_proto.on_message(StructuredOutputResponse)async def handle_structured_output_response(    ctx: Context, sender: str, msg: StructuredOutputResponse):    session_sender = ctx.storage.get(str(ctx.session))    if session_sender is None:        ctx.logger.error(            "Discarding message because no session sender found in storage"        )        return    if "<UNKNOWN>" in str(msg.output):        await ctx.send(            session_sender,            create_text_chat(                "Sorry, I couldn't process your request. Please include a valid Solana wallet address."            ),        )        return    try:        # Parse the structured output to get the address        wallet_request = SolanaRequest.parse_obj(msg.output)        address = wallet_request.address                if not address:            await ctx.send(                session_sender,                create_text_chat(                    "Sorry, I couldn't find a valid Solana wallet address in your query."                ),            )            return                # Get the balance for this address        balance = await get_balance_from_address(address)                # Create a nicely formatted response        response_text = f"Wallet Balance for `{address}`:\n{balance}\n\n[View on Solana Explorer](https://explorer.solana.com/address/{address})"                # Send the response back to the user        await ctx.send(session_sender, create_text_chat(response_text))            except Exception as err:        ctx.logger.error(err)        await ctx.send(            session_sender,            create_text_chat(                "Sorry, I couldn't check the wallet balance. Please try again later."            ),        )        return

### 3\. Solana Wallet Agent Setup[â€‹](#3-solana-wallet-agent-setup "Direct link to 3. Solana Wallet Agent Setup")

The `agent.py` file is the core of your application. Think of it as the main control center that:

*   Sets up your agent
*   Handles incoming requests
*   Manages rate limiting
*   Monitors the agent's health

Let's break down each component:

#### Basic Setup[â€‹](#basic-setup "Direct link to Basic Setup")

    from uagents import Agent, Context, Modelfrom uagents.experimental.quota import QuotaProtocol, RateLimitagent = Agent()  # Create a new agent instance

Import the necessary packages and initialise your agent.

#### Rate Limiting Setup[â€‹](#rate-limiting-setup "Direct link to Rate Limiting Setup")

    proto = QuotaProtocol(    storage_reference=agent.storage,    name="Solana-Wallet-Protocol",    version="0.1.0",    default_rate_limit=RateLimit(window_size_minutes=60, max_requests=30),)

To ensure fair usage and prevent API abuse, we recommend implementing the QuotaProtocol. While optional, this protocol helps manage service usage by limiting requests. In this example, we've set a limit of 30 requests per hour per user, but you can adjust this threshold based on your agent's functionality. This helps maintain service reliability and protects your agent from excessive requests.

#### Request Handler[â€‹](#request-handler "Direct link to Request Handler")

    @proto.on_message(    SolanaRequest, replies={SolanaResponse, ErrorMessage})async def handle_request(ctx: Context, sender: str, msg: SolanaRequest):    ctx.logger.info(f"Received wallet balance request for address: {msg.address}")    try:        balance = await get_balance_from_address(msg.address)        ctx.logger.info(f"Successfully fetched wallet balance for {msg.address}")        await ctx.send(sender, SolanaResponse(balance=balance))    except Exception as err:        ctx.logger.error(err)        await ctx.send(sender, ErrorMessage(error=str(err)))

This message handler allows your agent to receive direct requests from other agents in a structured format. While we're using it with ASI1 LLM in this example, it's versatile enough to work with agents that don't have the chat protocol enabled. This makes it particularly useful in multi-agent systems, where other agents can directly request information directly.

#### Health Monitoring[â€‹](#health-monitoring "Direct link to Health Monitoring")

    ### Health check related codedef agent_is_healthy() -> bool:    """    Implement the actual health check logic here.    For example, check if the agent can connect to the Solana RPC API.    """    try:        import asyncio        asyncio.run(get_balance_from_address("AtTjQKXo1CYTa2MuxPARtr382ZyhPU5YX4wMMpvaa1oy"))        return True    except Exception:        return Falseclass HealthCheck(Model):    passclass HealthStatus(str, Enum):    HEALTHY = "healthy"    UNHEALTHY = "unhealthy"class AgentHealth(Model):    agent_name: str    status: HealthStatushealth_protocol = QuotaProtocol(    storage_reference=agent.storage, name="HealthProtocol", version="0.1.0")@health_protocol.on_message(HealthCheck, replies={AgentHealth})async def handle_health_check(ctx: Context, sender: str, msg: HealthCheck):    status = HealthStatus.UNHEALTHY    try:        if agent_is_healthy():            status = HealthStatus.HEALTHY    except Exception as err:        ctx.logger.error(err)    finally:        await ctx.send(sender, AgentHealth(agent_name="solana_wallet_agent", status=status))

The HealthProtocol, while optional, provides a periodic health check system to monitor your agent's performance and reliability. It tests the agent's ability to fetch information about a wallet balance and verifies that all components are working correctly. This monitoring helps quickly identify and address any issues that might affect your agent's service, ensuring reliable operation for your users.

#### Protocol Registration[â€‹](#protocol-registration "Direct link to Protocol Registration")

    agent.include(proto, publish_manifest=True)agent.include(health_protocol, publish_manifest=True)agent.include(chat_proto, publish_manifest=True)agent.include(struct_output_client_proto, publish_manifest=True)

This registers all the necessary protocols with your agent:

*   Main protocol for handling wallet balance requests
*   Health monitoring protocol
*   Chat protocol for natural language communication
*   Structured output protocol for formatted responses

Here's the complete implementation:

agent.py

    import osfrom enum import Enumfrom uagents import Agent, Context, Modelfrom uagents.experimental.quota import QuotaProtocol, RateLimitfrom uagents_core.models import ErrorMessagefrom chat_proto import chat_proto, struct_output_client_protofrom solana_service import get_balance_from_address, SolanaRequest, SolanaResponseagent = Agent()proto = QuotaProtocol(    storage_reference=agent.storage,    name="Solana-Wallet-Protocol",    version="0.1.0",    default_rate_limit=RateLimit(window_size_minutes=60, max_requests=30),)@proto.on_message(    SolanaRequest, replies={SolanaResponse, ErrorMessage})async def handle_request(ctx: Context, sender: str, msg: SolanaRequest):    ctx.logger.info(f"Received wallet balance request for address: {msg.address}")    try:        balance = await get_balance_from_address(msg.address)        ctx.logger.info(f"Successfully fetched wallet balance for {msg.address}")        await ctx.send(sender, SolanaResponse(balance=balance))    except Exception as err:        ctx.logger.error(err)        await ctx.send(sender, ErrorMessage(error=str(err)))agent.include(proto, publish_manifest=True)### Health check related codedef agent_is_healthy() -> bool:    """    Implement the actual health check logic here.    For example, check if the agent can connect to the Solana RPC API.    """    try:        import asyncio        asyncio.run(get_balance_from_address("AtTjQKXo1CYTa2MuxPARtr382ZyhPU5YX4wMMpvaa1oy"))        return True    except Exception:        return Falseclass HealthCheck(Model):    passclass HealthStatus(str, Enum):    HEALTHY = "healthy"    UNHEALTHY = "unhealthy"class AgentHealth(Model):    agent_name: str    status: HealthStatushealth_protocol = QuotaProtocol(    storage_reference=agent.storage, name="HealthProtocol", version="0.1.0")@health_protocol.on_message(HealthCheck, replies={AgentHealth})async def handle_health_check(ctx: Context, sender: str, msg: HealthCheck):    status = HealthStatus.UNHEALTHY    try:        if agent_is_healthy():            status = HealthStatus.HEALTHY    except Exception as err:        ctx.logger.error(err)    finally:        await ctx.send(sender, AgentHealth(agent_name="solana_wallet_agent", status=status))agent.include(health_protocol, publish_manifest=True)agent.include(chat_proto, publish_manifest=True)agent.include(struct_output_client_proto, publish_manifest=True)if __name__ == "__main__":    agent.run()

Adding a README to your Agent[â€‹](#adding-a-readme-to-your-agent "Direct link to Adding a README to your Agent")
---------------------------------------------------------------------------------------------------------------

1.  Go to the Overview section in the Editor.
    
2.  Click on Edit and add a good description for your Agent so that it can be easily searchable by the ASI1 LLM. Please refer the [Importance of Good Readme](https://innovationlab.fetch.ai/resources/docs/agentverse/searching#importance-of-good-readme) section for more details.
    
3.  Make sure the Agent has the right `AgentChatProtocol`.
    

Query your Agent from ASI1 LLM[â€‹](#query-your-agent-from-asi1-llm "Direct link to Query your Agent from ASI1 LLM")
------------------------------------------------------------------------------------------------------------------

1.  Login to the [ASI1 LLM](https://asi1.ai/), either using your Google Account or the ASI1 Wallet and Start a New Chat.
    
2.  Toggle the "Agents" switch to enable ASI1 to connect with Agents on Agentverse.
    

3.  Type in a query to ask your Agent for instance 'What's the balance of wallet address 6wFKPxNToSnggrZr4P4s1r4zRxuJX2nSA7iTdQDPpgHc'.

> **Note**: The ASI1 LLM may not always select your agent for answering the query as it is designed to pick the best agent for a task based on a number of parameters.</content>
</page>

<page>
  <title>ASI-1 Mini Examples | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/examples/asi-mini-example</url>
  <content>ASI1-Mini Example Repositories
------------------------------

Below is a **brief overview** of various open-source repositories demonstrating how to integrate and use **ASI1-Mini** in different scenarios, from simple scripts to advanced multi-agent systems.

**GitHub**: [ASI-1-Mini-simple-Examples](https://github.com/abhifetch/ASI-1-Mini-simple-Examples)  
A **collection of simple agents** built with the uAgents framework. Each script focuses on a specialized task (language tutor, LeetCode solver, fun fact generator, etc.) and is powered by **ASI1-Mini**.

2\. langchain-asi[â€‹](#2-langchain-asi "Direct link to 2. langchain-asi")
------------------------------------------------------------------------

**GitHub**: [langchain-asi integration](https://github.com/rajashekarcs2023/langchain-asi)  
A **lightweight integration package** connecting ASI1â€™s API with LangChain. Ideal if you want to easily swap out other LLM providers for **ASI1-Mini** while retaining multi-turn conversations, agent support, system messages, and more.

3\. ASI-1\_mini\_Langchain[â€‹](#3-asi-1_mini_langchain "Direct link to 3. ASI-1_mini_Langchain")
-----------------------------------------------------------------------------------------------

**GitHub**: [ASI-1\_mini\_Langchain and tavily](https://github.com/abhifetch/ASI-1_mini_Langchain)  
Shows a **custom LLM integration** with LangChain and **Tavily Search**. Learn how to build a custom LangChain `LLM` class that calls the ASI1-Mini API and fetches external information for more sophisticated query handling.

4\. DeFI-Agent-Starter[â€‹](#4-defi-agent-starter "Direct link to 4. DeFI-Agent-Starter")
---------------------------------------------------------------------------------------

**GitHub**: [DeFI-Agent-Starter](https://github.com/RoyceBraden/DeFI-Agent-Starter)  
A **multi-agent** system focusing on **DeFi (Decentralized Finance)** use cases. It leverages **AgentVerse** and ASI1-Mini to determine whether to hold or sell a crypto asset based on Fear and Greed Index and sentiment analysis.

5\. ASI1-Mini-Chat-System[â€‹](#5-asi1-mini-chat-system "Direct link to 5. ASI1-Mini-Chat-System")
------------------------------------------------------------------------------------------------

**GitHub**: [ASI1-Mini-Chat-System](https://github.com/abhifetch/ASI1-Mini-Chat-System)  
A **modular, agent-based chat system** powered by uAgents. It features a **Client Agent** and a **Server Agent** that communicate in real time, with queries relayed to the ASI1 API for intelligent responses.

Getting Started[â€‹](#getting-started "Direct link to Getting Started")
---------------------------------------------------------------------

1.  **Clone any repository** that fits your interests.
2.  **Install dependencies** and create a `.env` file (or set environment variables) with your `ASI1_API_KEY`. Get your API Key [here](https://asi1.ai/dashboard/api-keys).
3.  **Run the sample scripts** or follow the instructions in each repoâ€™s README.
4.  **Experiment & Customize** â€“ tweak prompts, add tools, or integrate more data sources.

note

**Note:** You can learn more about ASI-1 Mini APIs [**here**](https://docs.asi1.ai/docs/).

With **ASI1-Mini**, you can rapidly build anything from simple chatbots to complex, multi-agent DeFi applications.</content>
</page>

<page>
  <title>ASI-1 Mini Examples | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/examples/asi-mini-example</url>
  <content>ASI1-Mini Example Repositories
------------------------------

Below is a **brief overview** of various open-source repositories demonstrating how to integrate and use **ASI1-Mini** in different scenarios, from simple scripts to advanced multi-agent systems.

**GitHub**: [ASI-1-Mini-simple-Examples](https://github.com/abhifetch/ASI-1-Mini-simple-Examples)  
A **collection of simple agents** built with the uAgents framework. Each script focuses on a specialized task (language tutor, LeetCode solver, fun fact generator, etc.) and is powered by **ASI1-Mini**.

2\. langchain-asi[â€‹](#2-langchain-asi "Direct link to 2. langchain-asi")
------------------------------------------------------------------------

**GitHub**: [langchain-asi integration](https://github.com/rajashekarcs2023/langchain-asi)  
A **lightweight integration package** connecting ASI1â€™s API with LangChain. Ideal if you want to easily swap out other LLM providers for **ASI1-Mini** while retaining multi-turn conversations, agent support, system messages, and more.

3\. ASI-1\_mini\_Langchain[â€‹](#3-asi-1_mini_langchain "Direct link to 3. ASI-1_mini_Langchain")
-----------------------------------------------------------------------------------------------

**GitHub**: [ASI-1\_mini\_Langchain and tavily](https://github.com/abhifetch/ASI-1_mini_Langchain)  
Shows a **custom LLM integration** with LangChain and **Tavily Search**. Learn how to build a custom LangChain `LLM` class that calls the ASI1-Mini API and fetches external information for more sophisticated query handling.

4\. DeFI-Agent-Starter[â€‹](#4-defi-agent-starter "Direct link to 4. DeFI-Agent-Starter")
---------------------------------------------------------------------------------------

**GitHub**: [DeFI-Agent-Starter](https://github.com/RoyceBraden/DeFI-Agent-Starter)  
A **multi-agent** system focusing on **DeFi (Decentralized Finance)** use cases. It leverages **AgentVerse** and ASI1-Mini to determine whether to hold or sell a crypto asset based on Fear and Greed Index and sentiment analysis.

5\. ASI1-Mini-Chat-System[â€‹](#5-asi1-mini-chat-system "Direct link to 5. ASI1-Mini-Chat-System")
------------------------------------------------------------------------------------------------

**GitHub**: [ASI1-Mini-Chat-System](https://github.com/abhifetch/ASI1-Mini-Chat-System)  
A **modular, agent-based chat system** powered by uAgents. It features a **Client Agent** and a **Server Agent** that communicate in real time, with queries relayed to the ASI1 API for intelligent responses.

Getting Started[â€‹](#getting-started "Direct link to Getting Started")
---------------------------------------------------------------------

1.  **Clone any repository** that fits your interests.
2.  **Install dependencies** and create a `.env` file (or set environment variables) with your `ASI1_API_KEY`. Get your API Key [here](https://asi1.ai/dashboard/api-keys).
3.  **Run the sample scripts** or follow the instructions in each repoâ€™s README.
4.  **Experiment & Customize** â€“ tweak prompts, add tools, or integrate more data sources.

note

**Note:** You can learn more about ASI-1 Mini APIs [**here**](https://docs.asi1.ai/docs/).

With **ASI1-Mini**, you can rapidly build anything from simple chatbots to complex, multi-agent DeFi applications.</content>
</page>

<page>
  <title>ASI-1 Mini Examples | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.3/examples/asi-mini-example</url>
  <content>ASI1-Mini Example Repositories
------------------------------

Below is a **brief overview** of various open-source repositories demonstrating how to integrate and use **ASI1-Mini** in different scenarios, from simple scripts to advanced multi-agent systems.

**GitHub**: [ASI-1-Mini-simple-Examples](https://github.com/abhifetch/ASI-1-Mini-simple-Examples)  
A **collection of simple agents** built with the uAgents framework. Each script focuses on a specialized task (language tutor, LeetCode solver, fun fact generator, etc.) and is powered by **ASI1-Mini**.

2\. langchain-asi[â€‹](#2-langchain-asi "Direct link to 2. langchain-asi")
------------------------------------------------------------------------

**GitHub**: [langchain-asi integration](https://github.com/rajashekarcs2023/langchain-asi)  
A **lightweight integration package** connecting ASI1â€™s API with LangChain. Ideal if you want to easily swap out other LLM providers for **ASI1-Mini** while retaining multi-turn conversations, agent support, system messages, and more.

3\. ASI-1\_mini\_Langchain[â€‹](#3-asi-1_mini_langchain "Direct link to 3. ASI-1_mini_Langchain")
-----------------------------------------------------------------------------------------------

**GitHub**: [ASI-1\_mini\_Langchain and tavily](https://github.com/abhifetch/ASI-1_mini_Langchain)  
Shows a **custom LLM integration** with LangChain and **Tavily Search**. Learn how to build a custom LangChain `LLM` class that calls the ASI1-Mini API and fetches external information for more sophisticated query handling.

4\. DeFI-Agent-Starter[â€‹](#4-defi-agent-starter "Direct link to 4. DeFI-Agent-Starter")
---------------------------------------------------------------------------------------

**GitHub**: [DeFI-Agent-Starter](https://github.com/RoyceBraden/DeFI-Agent-Starter)  
A **multi-agent** system focusing on **DeFi (Decentralized Finance)** use cases. It leverages **AgentVerse** and ASI1-Mini to determine whether to hold or sell a crypto asset based on Fear and Greed Index and sentiment analysis.

5\. ASI1-Mini-Chat-System[â€‹](#5-asi1-mini-chat-system "Direct link to 5. ASI1-Mini-Chat-System")
------------------------------------------------------------------------------------------------

**GitHub**: [ASI1-Mini-Chat-System](https://github.com/abhifetch/ASI1-Mini-Chat-System)  
A **modular, agent-based chat system** powered by uAgents. It features a **Client Agent** and a **Server Agent** that communicate in real time, with queries relayed to the ASI1 API for intelligent responses.

Getting Started[â€‹](#getting-started "Direct link to Getting Started")
---------------------------------------------------------------------

1.  **Clone any repository** that fits your interests.
2.  **Install dependencies** and create a `.env` file (or set environment variables) with your `ASI1_API_KEY`. Get your API Key [here](https://asi1.ai/dashboard/api-keys).
3.  **Run the sample scripts** or follow the instructions in each repoâ€™s README.
4.  **Experiment & Customize** â€“ tweak prompts, add tools, or integrate more data sources.

note

**Note:** You can learn more about ASI-1 Mini APIs [**here**](https://docs.asi1.ai/docs/).

With **ASI1-Mini**, you can rapidly build anything from simple chatbots to complex, multi-agent DeFi applications.</content>
</page>

<page>
  <title>LangGraph | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.2/examples/other-frameworks/financial-analysis-ai-agent</url>
  <content>Creating and Registering LangGraph based Financial Analysis Agent
-----------------------------------------------------------------

This documentation explains how to build **LangGraph** agents that perform comprehensive financial analysis, register them on **Fetch.ai's Agentverse**, and enable collaboration between specialized agents for dynamic data analysis. Below, you'll see the **main.py** script containing the workflow logic `run_financial_analysis_workflow`, as well as specialized agent implementations for financial analysis.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

### Purpose[â€‹](#purpose "Direct link to Purpose")

This project showcases:

*   **Creating LangGraph agents** for complex tasks (e.g., analyzing SEC filings, market research, and financial metrics).
*   **Integrating with Fetch.ai's Agentverse**, enabling seamless communication and data exchange.
*   **Using autonomous agents** to streamline financial analysis while combining multiple data sources and expert analysis.

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

### Environment Setup[â€‹](#environment-setup "Direct link to Environment Setup")

To begin creating and registering LangGraph agents, ensure you have the following:

*   **Python 3.10+** or higher
*   A **virtual environment** (recommended)
*   **Fetch.ai SDK** for agent creation and registration
*   **LangGraph Framework** for defining states and workflows
*   **LangChain** for agent tools and chains

### Key Components[â€‹](#key-components "Direct link to Key Components")

*   **SEC Analysis Agent**: A specialized agent that processes SEC filings and extracts financial metrics
*   **Search Agent**: Gathers real-time market data and analyst opinions
*   **Supervisor Agent**: Coordinates analysis flow and combines insights
*   **Agent Collaboration**: Multiple agents work together via LangGraph state management

### Architecture Diagram[â€‹](#architecture-diagram "Direct link to Architecture Diagram")

### Environment Variables[â€‹](#environment-variables "Direct link to Environment Variables")

Create a `.env` file with your API keys:

    OPENAI_API_KEY=<your_openai_api_key>TAVILY_API_KEY=<your_tavily_api_key>AGENTVERSE_API_KEY=<your_agentverse_api_key>

Replace the placeholders with your actual keys:

*   [OPENAI\_API\_KEY](https://openai.com/api/)
*   [TAVILY\_API\_KEY](https://tavily.com/)
*   [AGENTVERSE\_API\_KEY](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key)

Project Directory & Configuration Files[â€‹](#project-directory--configuration-files "Direct link to Project Directory & Configuration Files")
--------------------------------------------------------------------------------------------------------------------------------------------

For a clean, maintainable setup, we recommend the following structure:

    financial_analysis_agent/â”œâ”€â”€ .envâ”œâ”€â”€ src/â”‚   â”œâ”€â”€ agents/â”‚   â”‚   â”œâ”€â”€ __init__.pyâ”‚   â”‚   â”œâ”€â”€ search_agent.py      # Market research specialistâ”‚   â”‚   â”œâ”€â”€ sec_agent.py         # SEC filings specialistâ”‚   â”‚   â””â”€â”€ supervisor.py        # Team coordinatorâ”‚   â”œâ”€â”€ tools/â”‚   â”‚   â”œâ”€â”€ __init__.pyâ”‚   â”‚   â”œâ”€â”€ search.py            # Tavily search implementationâ”‚   â”‚   â””â”€â”€ analysis.py          # RAG implementationâ”‚   â”œâ”€â”€ rag/â”‚   â”‚   â”œâ”€â”€ __init__.pyâ”‚   â”‚   â”œâ”€â”€ chain.py             # RAG chain setupâ”‚   â”‚   â””â”€â”€ loader.py            # Document processingâ”‚   â”œâ”€â”€ graph/â”‚   â”‚   â”œâ”€â”€ __init__.pyâ”‚   â”‚   â””â”€â”€ state.py             # State managementâ”‚   â””â”€â”€ utils/â”‚       â”œâ”€â”€ __init__.pyâ”‚       â””â”€â”€ helpers.py           # Helper functionsâ”œâ”€â”€ main.py                      # Main workflowâ”œâ”€â”€ register.py                  # Agentverse registrationâ”œâ”€â”€ requirements.txtâ””â”€â”€ README.md

Core Implementation Files[â€‹](#core-implementation-files "Direct link to Core Implementation Files")
---------------------------------------------------------------------------------------------------

**1\. requirements.txt**

**Purpose**: Declares project dependencies for installing all necessary packages quickly with `pip install -r requirements.txt`.

Create a file called `requirements.txt` at the root of your project. Example contents:

    langchain==0.1.0langchain-core==0.1.10langgraph==0.0.10fetchai-sdk==0.16.3flask==2.2.5flask-cors==3.0.10python-dotenv==1.0.0tavily-python==0.2.6qdrant-client==1.7.0tiktoken==0.5.2

Install everything at once via:

    pip install -r requirements.txt

**2\. State Management (src/graph/state.py)**

**Purpose**: Defines the state structure and management for the research team's workflow.

Place `state.py` in the `src/graph` directory. It contains:

    from typing import TypedDict, List, Annotatedfrom langchain_core.messages import BaseMessageimport operatorclass ResearchTeamState(TypedDict):    """State structure for research team coordination."""    messages: Annotated[List[BaseMessage], operator.add]  # Conversation history    team_members: List[str]                              # Available agents    next: str                                           # Next agent to act    information_needed: List[str]                       # Required information    reasoning: str                                      # Decision reasoningdef create_initial_state(query: str) -> ResearchTeamState:    """Create initial state from user query."""    return {        "messages": [HumanMessage(content=query)],        "team_members": ["Search", "SECAnalyst"],        "next": "",        "information_needed": [],        "reasoning": ""    }def update_state(state: ResearchTeamState, agent_response: dict) -> ResearchTeamState:    """Update state with agent response."""    new_state = state.copy()    new_state["messages"].extend(agent_response["messages"])    return new_state

**3\. Tools Implementation (src/tools)**

**Purpose**: Implements specialized tools for market research and SEC filing analysis.

#### A. Search Tool (search.py)[â€‹](#a-search-tool-searchpy "Direct link to A. Search Tool (search.py)")

    from typing import Annotatedfrom langchain_core.tools import toolfrom tavily import TavilyClientimport os@tooldef tavily_search(query: str) -> str:    """Search for real-time market information."""    try:        client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))        result = client.search(query)        return str(result)    except Exception as e:        return f"Error performing search: {str(e)}"

#### B. Analysis Tool (analysis.py)[â€‹](#b-analysis-tool-analysispy "Direct link to B. Analysis Tool (analysis.py)")

    from typing import Annotatedfrom langchain_core.tools import toolfrom ..rag.chain import create_rag_chain@tooldef retrieve_information(query: Annotated[str, "query to analyze financial documents"]) -> str:    """Analyze SEC filings using RAG."""    try:        rag_chain = create_rag_chain()        return rag_chain.invoke(query)    except Exception as e:        return f"Error analyzing documents: {str(e)}"

**4\. RAG Implementation (src/rag)**

**Purpose**: Handles document processing and retrieval for SEC filing analysis.

#### A. Document Loader (loader.py)[â€‹](#a-document-loader-loaderpy "Direct link to A. Document Loader (loader.py)")

    class DocumentLoader:    def __init__(self, file_path: str):        self.file_path = file_path        @staticmethod    def tiktoken_len(text):        tokens = tiktoken.encoding_for_model("gpt-4").encode(text)        return len(tokens)        def load_and_split(self):        """Load and chunk documents for processing."""        docs = PyMuPDFLoader(self.file_path).load()        splitter = RecursiveCharacterTextSplitter(            chunk_size=300,            chunk_overlap=0,            length_function=self.tiktoken_len        )        return splitter.split_documents(docs)

#### B. RAG Chain (chain.py)[â€‹](#b-rag-chain-chainpy "Direct link to B. RAG Chain (chain.py)")

    def create_rag_chain(file_path: str = "data/raw/apple_10k.pdf"):    """Create RAG chain for SEC filing analysis."""    # Initialize document processing    loader = DocumentLoader(file_path)    chunks = loader.load_and_split()        # Set up vector store    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")    vectorstore = Qdrant.from_documents(        chunks,        embeddings,        location=":memory:",        collection_name="sec_filings"    )        # Create retrieval chain    template = """Use the context to answer financial questions.    Context: {context}    Question: {question}    Answer with specific numbers and data when available."""        prompt = ChatPromptTemplate.from_template(template)    chain = (        {"context": vectorstore.as_retriever(), "question": RunnablePassthrough()}        | prompt        | ChatOpenAI(model="gpt-4-turbo-preview")        | StrOutputParser()    )        return chain

**5\. Helper Functions (src/utils/helpers.py)**

**Purpose**: Provides utility functions for agent creation and node management.

    def create_agent(llm: ChatOpenAI, tools: list, system_prompt: str) -> AgentExecutor:    """Create a specialized agent with tools and prompt."""    try:        system_prompt += (            "\nWork autonomously using your tools."            " Do not ask for clarification."            " Your team members will help with their specialties."        )                prompt = ChatPromptTemplate.from_messages([            ("system", system_prompt),            MessagesPlaceholder(variable_name="messages"),            MessagesPlaceholder(variable_name="agent_scratchpad"),        ])                agent = create_openai_functions_agent(llm, tools, prompt)        return AgentExecutor(agent=agent, tools=tools)            except Exception as e:        logger.error(f"Error creating agent: {e}")        raisedef agent_node(state, agent, name):    """Create agent node for the graph."""    try:        if "information_needed" in state:            message_content = f"""Information needed:            {', '.join(state['information_needed'])}            Query: {state['messages'][-1].content}"""            state['messages'][-1] = HumanMessage(content=message_content)        result = agent.invoke(state)        return {            "messages": [                HumanMessage(content=result["output"], name=name)            ]        }    except Exception as e:        logger.error(f"Error in agent node {name}: {e}")        raise

Financial Analysis Agent Implementation[â€‹](#financial-analysis-agent-implementation "Direct link to Financial Analysis Agent Implementation")
---------------------------------------------------------------------------------------------------------------------------------------------

### A. SEC Analysis Agent (src/agents/sec\_agent.py)[â€‹](#a-sec-analysis-agent-srcagentssec_agentpy "Direct link to A. SEC Analysis Agent (src/agents/sec_agent.py)")

A specialized agent that processes SEC filings and financial documents. It uses RAG to analyze documents and extract relevant financial metrics.

    # src/agents/sec_agent.pyfrom langchain.chat_models import ChatOpenAIfrom ..tools.analysis import retrieve_informationdef create_sec_agent(llm: ChatOpenAI):    """Creates an agent specialized in SEC filings analysis."""        system_prompt = """You are a financial analyst specialized in SEC filings analysis.    After analyzing SEC filings:    1. If you need market context, clearly state what specific market data you need    2. If numbers need industry comparison, explicitly request competitor data    3. Always include specific numbers and trends from the filings    4. If you spot significant changes or unusual patterns, highlight them        Format your response as:    1. Data from SEC Filings: [your findings]    2. Additional Context Needed: [if any]    3. Analysis: [your insights]    """        return create_agent(        llm=llm,        tools=[retrieve_information],        system_prompt=system_prompt    )

### B. Search Agent (src/agents/search\_agent.py)[â€‹](#b-search-agent-srcagentssearch_agentpy "Direct link to B. Search Agent (src/agents/search_agent.py)")

Handles real-time market research and data gathering using external search tools.

    # src/agents/search_agent.pyfrom langchain.chat_models import ChatOpenAIfrom ..tools.search import tavily_searchdef create_search_agent(llm: ChatOpenAI):    """Creates a search agent specialized in market research."""        system_prompt = """You are a research assistant who can search for up-to-date     financial information using the tavily search engine.        When responding:    1. Always cite sources    2. Focus on recent market data and analyst reports    3. If SEC data is mentioned, compare it with current market views    4. Highlight any significant discrepancies with official filings        Format your response as:    1. Market Data: [your findings]    2. Analyst Views: [key opinions]    3. Relevance to SEC Data: [if applicable]    """        return create_agent(        llm=llm,        tools=[tavily_search],        system_prompt=system_prompt    )

### C. Supervisor Agent (src/agents/supervisor.py)[â€‹](#c-supervisor-agent-srcagentssupervisorpy "Direct link to C. Supervisor Agent (src/agents/supervisor.py)")

Coordinates between agents and manages the analysis workflow.

    # src/agents/supervisor.pydef create_supervisor_agent(llm: ChatOpenAI):    """Creates the supervisor agent for coordinating analysis."""        function_def = {        "name": "route",        "description": "Select the next role based on query analysis.",        "parameters": {            "title": "routeSchema",            "type": "object",            "properties": {                "next": {                    "title": "Next",                    "anyOf": [{"enum": ["Search", "SECAnalyst", "FINISH"]}],                },                "reasoning": {                    "title": "Reasoning",                    "type": "string",                    "description": "Explanation for why this agent should act next"                },                "information_needed": {                    "title": "Information Needed",                    "type": "array",                    "items": {"type": "string"},                    "description": "List of specific information needed from this agent"                }            },            "required": ["next", "reasoning", "information_needed"],        },    }    prompt = ChatPromptTemplate.from_messages([        ("system", """You are a financial research team supervisor.        Your role is to:        1. Analyze incoming queries        2. Determine what information is needed        3. Choose the appropriate agent for each task        4. Coordinate between agents        5. Ensure comprehensive analysis"""),        MessagesPlaceholder(variable_name="messages"),        ("system", "Who should act next? Consider available information and agent specialties.")    ])    return (        prompt        | llm.bind_functions(functions=[function_def], function_call="route")        | JsonOutputFunctionsParser()    )

Main Workflow Implementation (main.py)[â€‹](#main-workflow-implementation-mainpy "Direct link to Main Workflow Implementation (main.py)")
---------------------------------------------------------------------------------------------------------------------------------------

The main script that initializes the LangGraph workflow and handles financial analysis requests.

    import osfrom dotenv import load_dotenvfrom src.rag.chain import create_rag_chainfrom src.graph.state import create_research_graphdef init_financial_system():    """Initialize the RAG and research system."""    try:        # Create RAG chain for SEC document analysis        rag_chain = create_rag_chain("data/raw/apple_10k.pdf")                # Initialize research graph with RAG chain        chain = create_research_graph(rag_chain)                return chain    except Exception as e:        logger.error(f"Error initializing system: {e}")        raiseasync def run_financial_analysis(query: str):    """    Process financial analysis queries through the research graph.        Args:        query (str): The financial analysis query to process        Returns:        dict: Analysis results from multiple agents    """    try:        # Initialize state with query        state = {            "messages": [HumanMessage(content=query)],            "team_members": ["Search", "SECAnalyst"],            "information_needed": [],            "reasoning": ""        }                # Process through research chain        result = research_chain.invoke(state)                return {            "status": "success",            "analysis": result.get("messages", [])        }            except Exception as e:        logger.error(f"Analysis error: {e}")        return {            "status": "error",            "message": str(e)        }if __name__ == "__main__":    # Load environment variables    load_dotenv()        # Initialize the system    research_chain = init_financial_system()

This workflow:

1.  Initializes the research system with RAG capabilities
2.  Sets up the specialized agents and their tools
3.  Creates the research graph for coordinated analysis
4.  Processes queries through the team of agents
5.  Returns comprehensive financial analysis results

Agentverse Integration (register.py)[â€‹](#agentverse-integration-registerpy "Direct link to Agentverse Integration (register.py)")
---------------------------------------------------------------------------------------------------------------------------------

The Financial Analysis Agent registers itself with Agentverse and handles incoming analysis requests:

    import osimport loggingfrom flask import Flask, request, jsonifyfrom uagents_core.crypto import Identityfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentfrom main import init_financial_system# Configure logginglogging.basicConfig(level=logging.INFO)logger = logging.getLogger(__name__)# Initialize Flask appapp = Flask(__name__)# Global variablesfinancial_identity = Noneresearch_chain = Nonedef init_agent():    """Initialize and register the agent with agentverse"""    global financial_identity, research_chain    try:        # Initialize the research chain        research_chain = init_financial_system()                # Initialize identity and register with Agentverse        financial_identity = Identity.from_seed("Financial Analysis Agent", 0)                # Register with detailed capabilities description        register_with_agentverse(            identity=financial_identity,            url="http://localhost:5008/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="Financial Analysis Agent",            readme = """                <description>A comprehensive financial analysis agent that combines                 SEC filing analysis with real-time market data.</description>                <use_cases>                    <use_case>Analyze company financial metrics from SEC filings</use_case>                    <use_case>Research market trends and analyst opinions</use_case>                    <use_case>Compare financial performance with competitors</use_case>                </use_cases>                <payload_requirements>                    <payload>                        <requirement>                            <parameter>query</parameter>                            <description>What would you like to know about the company's financials?</description>                        </requirement>                    </payload>                </payload_requirements>            """        )        logger.info("Financial Analysis Agent registered successfully!")    except Exception as e:        logger.error(f"Error initializing agent: {e}")        raise@app.route('/webhook', methods=['POST'])async def webhook():    """Handle incoming requests from other agents"""    try:        data = request.get_data().decode('utf-8')        message = parse_message_from_agent(data)        query = message.payload.get("request", "")        agent_address = message.sender        if not query:            return jsonify({"status": "error", "message": "No query provided"}), 400        # Process query using research chain        result = research_chain.invoke({            "messages": [HumanMessage(content=query)],            "team_members": ["Search", "SECAnalyst"]        })        # Format response        formatted_result = {            "analysis": [                {                    "role": msg.type if hasattr(msg, 'type') else "message",                    "content": msg.content,                    "name": msg.name if hasattr(msg, 'name') else None                }                for msg in result.get('messages', [])            ]        }        # Send response back through Agentverse        send_message_to_agent(            financial_identity,            agent_address,            {'analysis_result': formatted_result}        )        return jsonify({"status": "analysis_sent"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"status": "error", "message": str(e)}), 500def run_agent():    """Initialize and start the agent"""    try:        init_agent()        app.run(host="0.0.0.0", port=5008, debug=True)    except Exception as e:        logger.error(f"Error running agent: {e}")        raise

Step-by-Step Execution[â€‹](#step-by-step-execution "Direct link to Step-by-Step Execution")
------------------------------------------------------------------------------------------

1.  **Environment Setup**
    
        # Create and activate virtual environmentpython -m venv venvsource venv/bin/activate  # On Windows: venv\Scripts\activate# Install requirementspip install -r requirements.txt
    
2.  **Configure Environment Variables** Create `.env` file with required API keys:
    
        OPENAI_API_KEY=<your_openai_api_key>TAVILY_API_KEY=<your_tavily_api_key>AGENTVERSE_API_KEY=<your_agentverse_api_key>
    
3.  **Start the Financial Analysis Agent**
    
    This will:
    
    *   Initialize the RAG system
    *   Register the agent with Agentverse
    *   Start the Flask server on port 5008
4.  **Verify Agent Registration**
    
    *   Check logs for successful registration message
    *   Verify agent appears in Agentverse registry

User Agent Script (user\_agent.py)[â€‹](#user-agent-script-user_agentpy "Direct link to User Agent Script (user_agent.py)")
-------------------------------------------------------------------------------------------------------------------------

The **User Agent** serves as a client. It registers itself with Agentverse, can search for existing agents, and forwards financial analysis queries to the Financial Analysis Agent. Afterwards, it retrieves the response and processes it.

### Script Breakdown[â€‹](#script-breakdown "Direct link to Script Breakdown")

**Importing Required Libraries**

The script imports libraries for Flask-based HTTP handling, Fetch.ai's identity and communication utilities, and environment variable management:

    from flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentfrom fetchai import fetchimport loggingimport osfrom dotenv import load_dotenv

**Setting Up Flask and Logging**

Logging and Flask are initialised to handle web requests and log agent activities:

    logging.basicConfig(level=logging.DEBUG)logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app, resources={r"/api/*": {'origins': 'http://localhost:5174'}})# Global variables for client identity and responsesprimary_agent = None

**Initialising the User Agent**

The PrimaryAgent class handles initialization and registration with Fetch.ai's Agentverse:

    class PrimaryAgent:    def __init__(self):        self.identity = None        self.latest_response = None        def initialize(self):        try:            self.identity = Identity.from_seed(os.getenv("PRIMARY_AGENT_KEY"), 0)                        register_with_agentverse(                identity=self.identity,                url="http://localhost:5001/webhook",                agentverse_token=os.getenv("AGENTVERSE_API_KEY"),                agent_title="Financial Query Router",                readme="<description>Routes queries to Financial Analysis Agent</description>"            )            logger.info("Primary agent initialized successfully!")                        except Exception as e:            logger.error(f"Initialization error: {e}")            raise

**Searching for Agents**

The /api/search-agents endpoint allows the User Agent to search for agents in Agentverse based on a query:

    @app.route('/api/search-agents', methods=['GET'])def search_agents():    """Search for available agents based on the financial query"""    try:        query = request.args.get('query', '')        if not query:            return jsonify({"error": "Query parameter 'query' is required."}), 400        logger.info(f"Searching for agents with query: {query}")        available_ais = fetch.ai(query)        agents = available_ais.get('ais', [])                extracted_data = [            {                'name': agent.get('name'),                'address': agent.get('address')            }            for agent in agents        ]                logger.info(f"Found {len(extracted_data)} agents matching the query")        return jsonify(extracted_data), 200    except Exception as e:        logger.error(f"Error finding agents: {e}")        return jsonify({"error": str(e)}), 500

**Sending Data to Financial Analysis Agent**

The /api/send-request endpoint forwards financial analysis queries to the selected agent:

    @app.route('/api/send-request', methods=['POST'])def send_request():    try:        data = request.json        payload = data.get('payload', {})        user_input = payload.get('request')        agent_address = data.get('agentAddress')                if not user_input:            return jsonify({"error": "No input provided"}), 400                send_message_to_agent(            primary_agent.identity,            agent_address,            {                "request": user_input            }        )                return jsonify({            "status": "request_sent",             "agent_address": agent_address,             "payload": payload        })            except Exception as e:        logger.error(f"Error processing request: {e}")        return jsonify({"error": str(e)}), 500

**Retrieving the Agent's Response**

The /api/get-response endpoint retrieves the analysis from the Financial Analysis Agent:

    @app.route('/api/get-response', methods=['GET'])def get_response():    try:        if primary_agent.latest_response:            response = primary_agent.latest_response            primary_agent.latest_response = None            return jsonify(response)        return jsonify({"status": "waiting"})    except Exception as e:        logger.error(f"Error getting response: {e}")        return jsonify({"error": str(e)}), 500

**Webhook Endpoint**

The webhook endpoint processes messages sent by the Financial Analysis Agent:

    @app.route('/webhook', methods=['POST'])def webhook():    try:        data = request.get_data().decode("utf-8")        message = parse_message_from_agent(data)        primary_agent.latest_response = message.payload        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500

**Running the User Agent**

The Flask server runs on port 5001 to handle requests:

    if __name__ == "__main__":    load_dotenv()    primary_agent.initialize()    app.run(host="0.0.0.0", port=5001)

### Interacting with the Agents[â€‹](#interacting-with-the-agents "Direct link to Interacting with the Agents")

**Search for Financial Analysis Agents**

To list agents in the Agentverse:

    curl -X GET "http://localhost:5001/api/search-agents?query=Analyze%20Apple's%20supply%20chain%20risks"

**Sample Output**

    [    {        "name": "Financial Analysis Agent",        "address": "agent1qvkq8x25tvuzclq6v34skqryzsfmqdd6snv8apu3yutc4fylkm4u5e9rsxf"    },    {        "name": "Dashboard Analytics Frontend Client",        "address": "agent1qthka4n7q0m7zwegq0qg5p3aaw5x309e8swc2nttnf3pxd3tusdwzch6ncn"    }]

**Send Analysis Request**

Send a financial analysis query:

    curl -X POST "http://localhost:5001/api/send-request" \-H "Content-Type: application/json" \-d '{    "payload": {        "request": "What are Apple'\''s recent revenue trends and market performance?"    },    "agentAddress": "agent1qvkq8x25tvuzclq6v34skqryzsfmqdd6snv8apu3yutc4fylkm4u5e9rsxf"}'

**Sample Output**

    {    "status": "request_sent",    "agent_address": "agent1qvkq8x25tvuzclq6v34skqryzsfmqdd6snv8apu3yutc4fylkm4u5e9rsxf",    "payload": {        "request": "What are Apple's recent revenue trends and market performance?"    }}

**Retrieve the Analysis Response**

Fetch the analysis response:

    curl -X GET "http://localhost:5001/api/get-response"

**Sample Output**

    {    "analysis_result": {        "analysis": [            {                "content": "Information needed:\n        Historical revenue trends for Apple over the last few quarters\n        \n        Query: What are Apple's recent revenue trends and market performance?",                "name": null,                "role": "human"            }        ]    }}

You now have a **LangGraph-based Financial Analysis Agent** integrated with the **Fetch.ai Agentverse**, handling complex financial analysis tasks through a team of specialized agents (Search Agent and SEC Analyst). Feel free to adapt the LangGraph workflow, state management, and agent interactions to suit your own requirementsâ€”whether you're adding new analysis capabilities, expanding the agent team, or customizing the financial analysis patterns.</content>
</page>

<page>
  <title>Autogen Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.2/examples/other-frameworks/autogen</url>
  <content>Registering an Autogen Agent on Agentverse Marketplace
------------------------------------------------------

This document is a comprehensive guide to help users register a **Multi-Agent Autogen System** on **Fetch.ai's Agentverse** ecosystem using the provided **Autogen Code Execution Agent** and **User Agent** scripts. AutoGen is an open-source programming framework for building AI agents and facilitating cooperation among multiple agents to solve tasks. These agents work together to execute a Python code and demonstrate how Autogen Agents can interact seamlessly with other agents on Agentverse.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

### Purpose[â€‹](#purpose "Direct link to Purpose")

This project showcases:

*   **Registering** a Multi-Agent System (Autogen) on Agentverse to execute Python code.
*   **Facilitating Communication** between agents within Fetch.aiâ€™s Agentverse via webhooks.
*   **Providing** a clear example of how to return code execution results in a human-friendly format.

### Key Features[â€‹](#key-features "Direct link to Key Features")

*   The **Autogen Code Execution Agent** handles requests for code generation and execution using the Autogen tools.
*   The **User Agent** acts as a client to forward requests and retrieve responses from the Autogen Code Execution Agent.
*   Communication is done via **HTTP webhooks** registered with **Fetch.aiâ€™s Agentverse**.

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

### Environmental Setup[â€‹](#environmental-setup "Direct link to Environmental Setup")

*   Python 3.8 or higher.
*   A virtual environment (recommended).
*   Flask and related libraries installed.

### Architecture Diagram[â€‹](#architecture-diagram "Direct link to Architecture Diagram")

### Required Libraries[â€‹](#required-libraries "Direct link to Required Libraries")

Install the necessary libraries using:

    pip install flask flask-cors fetchai autogen "flask[async]"

### Environment Variables[â€‹](#environment-variables "Direct link to Environment Variables")

Create a `.env` file in your project directory:

    AV_AUTOGEN_CODE_EXECUTION_AI_KEY='<your_autogen_code_execution_ai_key>'USER_AUTOGEN_AI_KEY='<your_user_ai_key>'AGENTVERSE_API_KEY='<your_agentverse_api_key>'OPENAI_API_KEY='<your_openai_api_key>'

Replace placeholders with your actual keys:

*   **AGENTVERSE\_API\_KEY**: Refer this [guide here](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key).
    
*   **OPENAI\_API\_KEY**: Sign up on the [OpenAI](https://platform.openai.com/api-keys) website.
    

Autogen Code Execution Agent Script[â€‹](#autogen-code-execution-agent-script "Direct link to Autogen Code Execution Agent Script")
---------------------------------------------------------------------------------------------------------------------------------

In this example, the **Autogen Code Execution Agent** registered on Agentverse comprises an **AssistantAgent** and a **UserProxyAgent** to write code and execute it. The system focuses on:

*   **Registering** with Fetch.aiâ€™s Agentverse
*   **Handling** requests to return the outcome of a Python code in a human friendly format.
*   **Responding** to the User Agent with the outcome of the code.

### Script Breakdown (autogen-agent.py)[â€‹](#script-breakdown-autogen-agentpy "Direct link to Script Breakdown (autogen-agent.py)")

**Importing Required Libraries**

    import osimport autogenfrom autogen.coding import LocalCommandLineCodeExecutorfrom flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osimport openaifrom dotenv import load_dotenvfrom fetchai import fetchimport asyncio

**Setting Up Logging**

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)

**Flask Webhook**

The Flask app is created to handle incoming webhook requests:

**Code Execution Function**

Below is the code execution function using the **Autogen Multi-Agent** system, where an **AssistantAgent** and a **UserProxyAgent** communicate until the task is done.

    async def execute_code(prompt):    config_list = [{"model": "gpt-4o", "api_key": os.getenv("OPENAI_API_KEY")}]    # create an AssistantAgent named "assistant"    assistant = autogen.AssistantAgent(        name="assistant",        llm_config={            "cache_seed": 41,  # seed for caching and reproducibility            "config_list": config_list,  # a list of OpenAI API configurations            "temperature": 0,  # temperature for sampling        },  # configuration for autogen's enhanced inference API which is compatible with OpenAI API    )    # create a UserProxyAgent instance named "user_proxy"    user_proxy = autogen.UserProxyAgent(        name="user_proxy",        human_input_mode="NEVER",        max_consecutive_auto_reply=10,        is_termination_msg=lambda x: x.get("content", "").rstrip().endswith("TERMINATE"),        code_execution_config={            # the executor to run the generated code            "executor": LocalCommandLineCodeExecutor(work_dir="coding"),        },    )    # the assistant receives a message from the user_proxy, which contains the task description    chat_res = user_proxy.initiate_chat(        assistant,        message=prompt,        summary_method="reflection_with_llm",    )    if hasattr(chat_res, 'chat_history') and chat_res.chat_history:        last_message = chat_res.chat_history[-1]['content']        return last_message    return "No messages found."

**Webhook Endpoint**

    @app.route('/webhook', methods=['POST'])async def webhook():    try:        # Parse the incoming message        logging.info("Parsing message")        data = request.get_data().decode('utf-8')        message = parse_message_from_agent(data)	# Extract the prompt from the message         prompt = message.payload.get("prompt", "")        logger.info(prompt)        agent_address = message.sender	#Call the Autogen Agents to execute the code        response = await execute_code(prompt)        payload = {"response":response}	#Return the message back to the User Agent        send_message_to_agent(            sender=ai_identity,            target=agent_address,            payload=payload        )        return jsonify({"status": "graphs_sent"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"status": "error", "message": str(e)}), 500

**Registering the Agent**

    def init_agent():    """Initialize and register the client agent."""    global ai_identity    try:        ai_identity = Identity.from_seed(os.getenv("AV_AUTOGEN_CODE_EXECUTION_AI_KEY"), 0)        register_with_agentverse(            identity = ai_identity,            url = "http://localhost:5001/webhook", ## The webhook that your AI receives messages on.            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title = "Autogen Code Execution Agent",            readme = """        <description>This agent generates code and executes it to provide final output to the user.</description>        <use_cases>            <use_case>What date is today? Compare the year-to-date gain for META and TESLA.</use_case>        </use_cases>        <payload_requirements>        <description>Please provide a prompt for code execution.</description>        <payload>            <requirement>                <parameter>prompt</parameter>                <description>Please provide a prompt for code execution.The prompt will help this AI generate code and return the output to the user. </description>            </requirement>        </payload>        </payload_requirements>        """        )        logger.info("Autogen Code Execution agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise

**Running the Agent**

    if __name__ == "__main__":    init_agent()    app.run(host="0.0.0.0", port=5001, debug=True)

User Agent Script[â€‹](#user-agent-script "Direct link to User Agent Script")
---------------------------------------------------------------------------

The **User Agent** script acts as the client that forwards requests to the **Autogen Code Execution Agent** and retrieves responses.

### Script Breakdown (autogen-user-agent.py)[â€‹](#script-breakdown-autogen-user-agentpy "Direct link to Script Breakdown (autogen-user-agent.py)")

**Importing Libraries**

    from flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai import fetchfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osimport timefrom dotenv import load_dotenv

**Setting Up Logging and Flask**

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app)

**Initializing the User Agent**

    def init_agent():    """Initialize and register the client agent."""    global client_identity    try:        # Load the client identity from environment variables        client_identity = Identity.from_seed(os.getenv("USER_AUTOGEN_AI_KEY"), 0)        logger.info(f"Client agent started with address: {client_identity.address}")        # Define the client agent's metadata        readme = """        <description>Frontend client that interacts with Autogen Agent to execute code.</description>        <use_cases>            <use_case>Sends Request to the AI Agent to execute a code</use_case>        </use_cases>        <payload_requirements>            <description>Expects request which is to be sent to another agent.</description>            <payload>                <requirement>                    <parameter>request</parameter>                    <description>This is the request which is to be sent to other agent</description>                </requirement>            </payload>        </payload_requirements>        """        # Register the agent with Agentverse        register_with_agentverse(            identity=client_identity,            url="http://localhost:5002/api/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="User agent for Autogen Code Execution Agent",            readme=readme        )        logger.info("Client agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise

**Searching Agents**

    # searching the agents which can create dashboard on agentverse@app.route('/api/search-agents', methods=['GET'])def search_agents():   """Search for available dashboard agents based on user input."""   try:       # Extract user input from query parameters       user_query = request.args.get('query', '')       if not user_query:           return jsonify({"error": "Query parameter 'query' is required."}), 400       # Fetch available agents based on user query       available_ais = fetch.ai(user_query)  # Pass the user query to the fetch.ai function       print(f'---------------------{available_ais}----------------------')       # Access the 'ais' list within 'agents' (assuming fetch.ai returns the correct structure)       agents = available_ais.get('ais', [])       print(f'----------------------------------{agents}------------------------------------')       extracted_data = []       for agent in agents:           name = agent.get('name')  # Extract agent name           address = agent.get('address')           # Append formatted data to extracted_data list           extracted_data.append({               'name': name,               'address': address,           })       # Format the response with indentation for readability       response = jsonify(extracted_data)       response.headers.add('Content-Type', 'application/json; charset=utf-8')       response.headers.add('Access-Control-Allow-Origin', '*')       response.headers.add('Access-Control-Allow-Headers', 'Content-Type')       return response, 200   except Exception as e:       logger.error(f"Error finding agents: {e}")       return jsonify({"error": str(e)}), 500

**Sending Data to Autogen Code Execution Agent**

    @app.route('/api/send-data', methods=['POST'])def send_data():   """Send payload to the selected agent based on provided address."""   global agent_response   agent_response = None   try:       # Parse the request payload       data = request.json       payload = data.get('payload')  # Extract the payload dictionary       agent_address = data.get('agentAddress')  # Extract the agent address       # Validate the input data       if not payload or not agent_address:           return jsonify({"error": "Missing payload or agent address"}), 400       logger.info(f"Sending payload to agent: {agent_address}")       logger.info(f"Payload: {payload}")       # Send the payload to the specified agent       send_message_to_agent(           client_identity,  # Frontend client identity           agent_address,    # Agent address where we have to send the data           payload           # Payload containing the data       )       return jsonify({"status": "request_sent", "agent_address": agent_address, "payload": payload})   except Exception as e:       logger.error(f"Error sending data to agent: {e}")       return jsonify({"error": str(e)}), 500

**Webhook Endpoint**

    # app route to get recieve the messages on the agent@app.route('/webhook', methods=['POST'])def webhook():    """Handle incoming messages from the dashboard agent."""    global agent_response    try:        # Parse the incoming webhook message        data = request.get_data().decode("utf-8")        logger.info("Received response")        message = parse_message_from_agent(data)        agent_response = message.payload        logger.info(f"Processed response: {agent_response}")        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500

**Retrieving the Agent's Response**

    @app.route('/api/get-response', methods=['GET'])def get_response():    global agent_response    try:        if agent_response:            response = agent_response            print(f'response : {response}')            agent_response = None  # Clear the response after sending            keys = list(response.keys())  # Convert dict_keys to a list            first_key = keys[0]  # Get the first key            response_final = response.get(first_key, "")            logger.info(f"Got response for after code execution {response_final}")            return jsonify({"response": response_final})        else:            return jsonify({"error": "No storm response available"}), 404    except Exception as e:        logger.error(f"Error creating markdown file: {e}")        return jsonify({"error": str(e)}), 500

**Running the agent**

    if __name__ == "__main__":    init_agent()    app.run(host="0.0.0.0", port=5002)

Steps to Run the Scripts[â€‹](#steps-to-run-the-scripts "Direct link to Steps to Run the Scripts")
------------------------------------------------------------------------------------------------

Follow these steps to get both the **Autogen Code Execution Agent** and **User Agent** scripts running.

**1\. Start the Alphavantage Agent**

Ensure the `AV_AUTOGEN_CODE_EXECUTION_AI_KEY` and `AGENTVERSE_API_KEY` values are correctly set in the `.env` file. Then Run:

**Expected Output:**

    (.venv) kshipra@MacBook-Pro autogen-code-generation-debugging-example % python3 autogen-agent.pyflaml.automl is not available. Please install flaml[automl] to enable AutoML functionalities.INFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Autogen Code Execution agent registration complete! * Serving Flask app 'autogen-agent' * Debug mode: onINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5001 * Running on http://192.168.6.11:5001INFO:werkzeug:Press CTRL+C to quitINFO:werkzeug: * Restarting with statflaml.automl is not available. Please install flaml[automl] to enable AutoML functionalities.INFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Autogen Code Execution agent registration complete!WARNING:werkzeug: * Debugger is active!INFO:werkzeug: * Debugger PIN: 471-951-559

The agent listens on **port 5001**.

**2\. Start the User Agent**

Ensure the `USER_AUTOGEN_AI_KEY` and `AGENTVERSE_API_KEY` values are set in the `.env` file.

    python3 autogen-user-agent.py

**Expected Output:**

    kshipra@MacBook-Pro autogen-code-generation-debugging-example % python3 autogen-user-agent.py INFO:__main__:Client agent started with address: agent1q0dfdn7073m75c3dc2sdd5gajevswmmwkw6wxgqhjca9ug6c2ruzk7nak0dINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Client agent registration complete! * Serving Flask app 'autogen-user-agent' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5002 * Running on http://192.168.6.11:5002

### Interacting with the Agents[â€‹](#interacting-with-the-agents "Direct link to Interacting with the Agents")

Once both agents are running, use the following endpoints to interact with them from another terminal:

**1\. Search for Agents**

To list agents in the Agentverse:

    curl -X GET "http://localhost:5002/api/search-agents?query=I%20want%20to%20write%20a%20oython%20code"

**Sample Output**

    [{"address":"agent1qwkzy0twyd27egqfrcw8m6vdtsz7hrm9leu7jfezzzlgckywj7shytz5kf6","name":"Alphavantage Stock Price Langchain tool"},{"address":"agent1qtnc7ruc63g3n84qczekqc6qwp0e5ayvs3v8e5pae563qrqcf852xlqz2pk","name":"Alphavantage Stock Price Langchain tool"},{"address":"agent1qdwp929xdwzm0pgxflz29c3x6ltha97t2em88vrh588nehz6su0u55zaxp9","name":"Autogen Code Execution Agent"},{"address":"agent1qgjj8476mr8cy2dvpc3ec5w4ye56kqswvr6hkl55q8775ghm9h7wwevh6lj","name":"Autogen Code Execution Agent"},{"address":"agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae","name":"Job Description Creation Agent"},{"address":"agent1qtmkpt8lhyx4u2ndzcgr7jkjuuade4ay78yld2ngaghs03z6qve3senwrsa","name":"Penny"},{"address":"agent1qtxx79e689zwjlh7m8dsfuv7lyn45d2xapr4p9w2u9axvzm487tzjfwqzdp","name":"Penny"},{"address":"agent1qv2u6v4ueea9kmvsyyclz9reh7qmzc6gr0fn7aeup93xm6ld8ynwc65nsks","name":"Penny"},{"address":"agent1qvc0em6vrlr80txaz3ksy7tspzaxl564jwqtaana7exehg0hz7gl6zg0wl0","name":"Penny"},{"address":"agent1qw5s6e7mxcd8mp6f2vvunk9rklwnv8ynft43unx77syea4nnskxyj4yk5xq","name":"Penny"}]

**2\. Send a Request to the Autogen Code Execution Agent**

Send a prompt from the User Agent to the Autogen Code Execution Agent:

    kshipra@MacBook-Pro autogen-code-generation-debugging-example % curl -X POST "http://localhost:5002/api/send-data" \-H "Content-Type: application/json" \-d '{  "payload": {    "prompt": "What date is today? Compare the year-to-date gain for META and TESLA."  },  "agentAddress": "agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae"}'

**Sample Output**

    {"status": "request_sent", "agent_address": "agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae", "payload": "{"prompt": "What date is today? Compare the year-to-date gain for META and TESLA."  }"}

**3\. Retrieve the Stock Price Response**

Fetch the stock price response from the User Agent:

    curl -X GET "http://localhost:5002/api/get-response"

**Sample Output**

    {"response":"The code executed successfully, and we have the year-to-date gain for both META and TESLA:\n\n- **META (Meta Platforms, Inc.)**: Year-to-Date Gain is **17.63%**\n- **TESLA (Tesla, Inc.)**: Year-to-Date Gain is **-0.29%**\n\nThis means that since the beginning of the year, META's stock price has increased by 17.63%, while TESLA's stock price has decreased by 0.29%.\n\nTERMINATE"}

Debugging Common Issues[â€‹](#debugging-common-issues "Direct link to Debugging Common Issues")
---------------------------------------------------------------------------------------------

Agent Registration Fails:

a. Check the .env file for correct API keys. b. Ensure the AGENTVERSE\_API\_KEY is valid.

404 Errors:

a. Verify that both agents are running on their respective ports (5001 and 5002 in this case). b. Double-check the agentAddress in requests.

You now have an **Autogen Code Execution Agent** integrated with the **Fetch.ai Agentverse**, orchestrating multi-agent code generation and execution. Feel free to extend the example to include additional tasks, multiple code executors, or advanced debugging logic.</content>
</page>

<page>
  <title>CrewAI Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/1.0.2/examples/other-frameworks/crewai</url>
  <content>Creating and Registering CrewAI based Job descriptions creator
--------------------------------------------------------------

This documentation explains how to build **CrewAI** agents that generate tailored job descriptions, register them on **Fetch.aiâ€™s Agentverse**, and enable collaboration between agents for dynamic data sharing. Below, you'll see the **main.py** script containing the workflow logic `run_job_posting_workflow`, as well as sample scripts for a **Job Description Agent** and a **User Agent**.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

### Purpose[â€‹](#purpose "Direct link to Purpose")

This project showcases:

*   **Creating CrewAI agents** for complex tasks (e.g., analyzing company requirements and generating job descriptions).
*   **Integrating with Fetch.aiâ€™s Agentverse**, enabling seamless communication and data exchange.
*   **Using autonomous agents** to streamline job description creation while aligning with company culture and specific role requirements.

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

### Environment Setup[â€‹](#environment-setup "Direct link to Environment Setup")

To begin creating and registering CrewAI agents, ensure you have the following:

*   **Python 3.10+** or higher
*   A **virtual environment** (recommended)
*   **Fetch.ai SDK** for agent creation and registration
*   **CrewAI Framework** for defining tasks and workflows

### Key Components[â€‹](#key-components "Direct link to Key Components")

*   **Job Description Agent**: A specialized CrewAI agent that processes input (company domain, hiring needs, etc.) to generate a job description.
*   **User Agent**: Acts as a client to discover and interact with the Job Description Agent.
*   **Agent Collaboration**: Multiple agents communicate via Agentverse, orchestrating tasks through CrewAI workflows.

### Architecture Diagram[â€‹](#architecture-diagram "Direct link to Architecture Diagram")

### Environment Variables[â€‹](#environment-variables "Direct link to Environment Variables")

Create a `.env` file with your API keys:

    SERPER_API_KEY=<your_serper_api_key>OPENAI_API_KEY=<your_openai_api_key>AGENTVERSE_API_KEY=<your_agentverse_api_key>

Replace the placeholders with your actual keys.

*   [OPENAI\_API\_KEY](https://openai.com/index/openai-api/)
*   [SERPER\_API\_KEY](https://serper.dev/)
*   [AGENTVERSE\_API\_KEY](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key)

Project Directory & Configuration Files[â€‹](#project-directory--configuration-files "Direct link to Project Directory & Configuration Files")
--------------------------------------------------------------------------------------------------------------------------------------------

For a clean, maintainable setup, we recommend the following structure:

    crewai_job_descriptions/â”œâ”€â”€ .envâ”œâ”€â”€ config/â”‚   â”œâ”€â”€ agents.yamlâ”‚   â””â”€â”€ tasks.yamlâ”œâ”€â”€ crew.pyâ”œâ”€â”€ jd_agent.pyâ”œâ”€â”€ main.pyâ”œâ”€â”€ requirements.txtâ”œâ”€â”€ user_agent.pyâ”œâ”€â”€ job_description_example.mdâ””â”€â”€ README.md

**1\. requirements.txt**

**Purpose**: Declares project dependencies so you (or anyone) can install them quickly with `pip install -r requirements.txt`.

Create a file called `requirements.txt` at the root of your project (alongside `jd_agent.py` and `main.py`). Example contents:

    flask==2.2.5flask-cors==3.0.10fetchai==0.16.3crewai==0.100.1crewai_tools==0.33.0python-dotenv==1.0.0

Install everything at once via:

    pip install -r requirements.txt

**2\. Crew.py**

**Purpose:** Defines your `JobPostingCrew` class, specifying the Agents and Tasks.

Place `crew.py` at the root directory. It contains the code:

    from typing import Listfrom crewai import Agent, Crew, Process, Taskfrom crewai.project import CrewBase, agent, crew, task# Check our tools documentations for more information on how to use themfrom crewai_tools import SerperDevTool, ScrapeWebsiteTool, WebsiteSearchTool, FileReadToolfrom pydantic import BaseModel, Fieldweb_search_tool = WebsiteSearchTool()seper_dev_tool = SerperDevTool()file_read_tool = FileReadTool(    file_path='./job_description_example.md',    description='A tool to read the job description example file.')class ResearchRoleRequirements(BaseModel):    """Research role requirements model"""    skills: List[str] = Field(..., description="List of recommended skills ...")    experience: List[str] = Field(..., description="List of recommended experience ...")    qualities: List[str] = Field(..., description="List of recommended qualities ...")@CrewBaseclass JobPostingCrew:    """JobPosting crew"""    agents_config = 'config/agents.yaml'    tasks_config = 'config/tasks.yaml'    @agent    def research_agent(self) -> Agent:        return Agent(            config=self.agents_config['research_agent'],            tools=[web_search_tool, seper_dev_tool],            verbose=True        )        @agent    def writer_agent(self) -> Agent:        return Agent(            config=self.agents_config['writer_agent'],            tools=[web_search_tool, seper_dev_tool, file_read_tool],            verbose=True        )        @agent    def review_agent(self) -> Agent:        return Agent(            config=self.agents_config['review_agent'],            tools=[web_search_tool, seper_dev_tool, file_read_tool],            verbose=True        )        @task    def research_company_culture_task(self) -> Task:        return Task(            config=self.tasks_config['research_company_culture_task'],            agent=self.research_agent()        )    @task    def research_role_requirements_task(self) -> Task:        return Task(            config=self.tasks_config['research_role_requirements_task'],            agent=self.research_agent(),            output_json=ResearchRoleRequirements        )    @task    def draft_job_posting_task(self) -> Task:        return Task(            config=self.tasks_config['draft_job_posting_task'],            agent=self.writer_agent()        )    @task    def review_and_edit_job_posting_task(self) -> Task:        return Task(            config=self.tasks_config['review_and_edit_job_posting_task'],            agent=self.review_agent()        )    @task    def industry_analysis_task(self) -> Task:        return Task(            config=self.tasks_config['industry_analysis_task'],            agent=self.research_agent()        )    @crew    def crew(self) -> Crew:        """Creates the JobPostingCrew"""        return Crew(            agents=self.agents,  # Automatically created by the @agent decorator            tasks=self.tasks,    # Automatically created by the @task decorator            process=Process.sequential,            verbose=True,        )

**3\. config/agents.yaml**

**Purpose**: Specifies each agentâ€™s role, goal, and backstory. Create a folder named `config/` at your project root. Inside it, **create** `agents.yaml`:

    research_agent:  role: >    Research Analyst  goal: >    Analyze the company website and provided description...  backstory: >    Expert in analyzing company cultures and identifying key values...writer_agent:  role: >    Job Description Writer  goal: >    Use insights from the Research Analyst to create...  backstory: >    Skilled in crafting compelling job descriptions...review_agent:  role: >    Review and Editing Specialist  goal: >    Review the job posting for clarity, engagement...  backstory: >    A meticulous editor ensuring every piece of content is polished...

**4\. config/tasks.yaml**

**Purpose**: Defines the text prompts (description) and expected output for each task. In the same `config/` folder, make a second file called `tasks.yaml`:

    research_company_culture_task:  description: >    Analyze {company_domain} and {company_description}...  expected_output: >    A comprehensive report detailing the company's culture...research_role_requirements_task:  description: >    Based on {hiring_needs}, list recommended skills, experience...  expected_output: >    A list of recommended skills, experiences, and qualities...draft_job_posting_task:  description: >    Draft a job posting for {hiring_needs}, using {specific_benefits}...  expected_output: >    A detailed, engaging job posting that includes an introduction...review_and_edit_job_posting_task:  description: >    Review the job posting for {hiring_needs} for clarity...  expected_output: >    A polished, error-free job posting with final approval in markdown.industry_analysis_task:  description: >    Conduct an in-depth analysis of {company_domain}'s industry...  expected_output: >    A detailed analysis highlighting major industry trends, opportunities...

Main.py (Workflow Definition)[â€‹](#mainpy-workflow-definition "Direct link to Main.py (Workflow Definition)")
------------------------------------------------------------------------------------------------------------

Below is an example `main.py` script containing the `run_job_posting_workflow` function that the Job Description Agent calls to generate job descriptions. It uses **Crew** to structure tasks and (optionally) a local SQLite database for storage or logging.

    import sysfrom crew import JobPostingCrewimport sqlite3async def run_job_posting_workflow(company_domain, company_description, hiring_needs, specific_benefits):    """    Executes the job posting workflow with proper connection handling and debugging output.        Args:        company_domain (str): The company's domain.        company_description (str): A description of the company.        hiring_needs (str): Description of the role being hired for.        specific_benefits (str): Specific benefits offered for the role.        Returns:        str: The raw output of the 'industry_analysis_task' if found, else None.    """    try:        # Initialize the JobPostingCrew        job_posting_crew = JobPostingCrew().crew()                # Define the inputs        inputs = {            'company_domain': company_domain,            'company_description': company_description,            'hiring_needs': hiring_needs,            'specific_benefits': specific_benefits,        }        # Execute the job posting process        result = job_posting_crew.kickoff(inputs=inputs)        # Debugging output        print("Result type:", type(result))        print("Attributes:", dir(result))        # Attempt to retrieve and print tasks_output        if hasattr(result, 'tasks_output') and result.tasks_output:            print("Tasks Output:")            for task_output in result.tasks_output:                print(f"Task Name: {task_output.name}")                if task_output.name == "industry_analysis_task":                    return task_output.raw            print("Task 'review_and_edit_job_posting_task' not found in tasks_output.")        else:            print("No 'tasks_output' attribute found or it's empty.")    except Exception as e:        print(f"Error occurred: {e}")        return None

The `run_job_posting_workflow` function is invoked by the Job Description Agentâ€™s webhook to handle incoming requests and generate the job description.

Job Description Agent Script[â€‹](#job-description-agent-script "Direct link to Job Description Agent Script")
------------------------------------------------------------------------------------------------------------

A Job Description Agent is a specialized CrewAI agent that creates detailed job descriptions based on:

*   **Company Domain**
*   **Company Description**
*   **Hiring Needs**
*   **Specific Benefits**

It registers itself with [**Agentverse**](https://agentverse.ai/), processes incoming requests (via a Flask webhook), and sends the generated output back to the requester.

### Script Breakdown (JD\_Agent.py)[â€‹](#script-breakdown-jd_agentpy "Direct link to Script Breakdown (JD_Agent.py)")

**Importing Required Libraries**

The script imports essential libraries for agent registration, communication, and Flask-based HTTP handling:

    import osimport loggingimport asynciofrom uagents_core.crypto import Identityfrom flask import Flask, request, jsonifyfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentfrom main import run_job_posting_workflow

**Setting Up Flask and Logging**

The script initializes Flask for handling webhooks and sets up logging to monitor activities:

    # Configure logginglogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)# Flask app for webhookflask_app = Flask(__name__)# Identity for the agentjd_agent_identity = Nonecontent = ''

**Webhook Endpoint**

The /webhook endpoint handles incoming requests and processes inputs to generate a job description:

    @flask_app.route('/webhook', methods=['POST'])async def webhook():    global content    global jd_agent_identity    try:        # Parse the incoming message        data = request.get_data().decode('utf-8')        message = parse_message_from_agent(data)        company_domain = message.payload.get("company_domain", "")        company_description = message.payload.get("company_description", "")        hiring_needs = message.payload.get("hiring_needs", "")        specific_benefits = message.payload.get("specific_benefits", "")        agent_address = message.sender        # Run the job posting workflow        content = await run_job_posting_workflow(            company_domain=company_domain,            company_description=company_description,            hiring_needs=hiring_needs,            specific_benefits=specific_benefits        )        # Send the generated content back to the requesting agent        payload = {'content': content}        send_message_to_agent(jd_agent_identity, agent_address, payload)        return jsonify({"status": "job_description_sent"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"status": "error", "message": str(e)}), 500

**Registering the Agent**

The init\_agent function registers the Job Description Agent with Fetch.ai's Agentverse, providing metadata such as agent title, description, and use cases:

    def init_agent():    global jd_agent_identity    try:        jd_agent_identity = Identity.from_seed("Job Description Agent Seed", 0)        register_with_agentverse(            identity=jd_agent_identity,            url="http://localhost:5008/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="Job Description Creation Agent",            readme="""                <description>Job Description creator to create JD according to the company's career website and description using CrewAI.</description>                <use_cases>                    <use_case>Generates job descriptions for specified roles.</use_case>                </use_cases>                <payload_requirements>                    <description>Expects company domain, description, hiring needs, and specific benefits.</description>                    <payload>                        <requirement>                            <parameter>company_domain</parameter>                            <description>Career Page for the company</description>                            <parameter>company_description</parameter>                            <description>Description of the company</description>                            <parameter>hiring_needs</parameter>                            <description>Role for which the person is needed</description>                            <parameter>specific_benefits</parameter>                            <description>Benefits offered with the role</description>                        </requirement>                    </payload>                </payload_requirements>            """        )        logger.info("Job Description Creator Agent registered successfully!")    except Exception as e:        logger.error(f"Error initializing agent: {e}")        raise

**Running the Agent**

The script starts the Flask server on port 5008:

    if __name__ == "__main__":    init_agent()    flask_app.run(host="0.0.0.0", port=5008, debug=True)

User Agent Script (user.py)[â€‹](#user-agent-script-userpy "Direct link to User Agent Script (user.py)")
------------------------------------------------------------------------------------------------------

The **User Agent** serves as a client. It registers itself with Agentverse, can search for existing agents, and forwards relevant data (e.g., company info, role) to the Job Description Agent. Afterwards, it retrieves the response and saves it locally.

### Script Breakdown[â€‹](#script-breakdown "Direct link to Script Breakdown")

**Importing Required Libraries**

The script imports libraries for Flask-based HTTP handling, Fetch.ai's identity and communication utilities, and environment variable management:

    from flask_cors import CORSfrom flask import Flask, request, jsonifyfrom uagents_core.crypto import Identityfrom fetchai import fetchfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osfrom dotenv import load_dotenv

**Setting Up Flask and Logging**

Logging and Flask are initialised to handle web requests and log agent activities:

    # Configure logginglogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app)# Global variables for client identity and agent responseclient_identity = Noneagent_response = None

**Initialising the User Agent**

The init\_client function registers the User Agent with Fetch.ai's Agentverse, providing metadata such as the agent's purpose and use cases:

    def init_client():    """Initialize and register the client agent."""    global client_identity    try:        # Load the client identity from environment variables        client_identity = Identity.from_seed("User for Testing JD Agents", 0)        logger.info(f"Client agent started with address: {client_identity.address}")        # Register the agent with Agentverse        register_with_agentverse(            identity=client_identity,            url="http://localhost:5002/api/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="JD User Testing Agent",            readme="""                <description>Frontend client that interacts with JD agents to fetch job descriptions.</description>                <use_cases>                    <use_case>Searches for agents and interacts with them.</use_case>                </use_cases>                <payload_requirements>                    <description>Handles responses related to job descriptions.</description>                    <payload>                        <requirement>                            <parameter>field</parameter>                            <description>Data required for job description creation</description>                        </requirement>                    </payload>                </payload_requirements>            """        )        logger.info("Client agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise

**Searching for Agents**

The /api/search-agents endpoint allows the User Agent to search for agents in Fetch.ai's Agentverse/Almanac Contract based on a query:

    @app.route('/api/search-agents', methods=['GET'])def search_agents():    """Search for available agents based on user input."""    try:        # Extract the query from the request        user_query = request.args.get('query', '')        if not user_query:            return jsonify({"error": "Query parameter 'query' is required."}), 400        # Search agents in Agentverse/almanac contract        available_ais = fetch.ai(user_query)        agents = available_ais.get('ais', [])        # Format the agent data        extracted_data = [            {'name': agent.get('name'), 'address': agent.get('address')}            for agent in agents        ]        return jsonify(extracted_data), 200    except Exception as e:        logger.error(f"Error finding agents: {e}")        return jsonify({"error": str(e)}), 500

**Sending Data to Another Agent**

The /api/send-data endpoint forwards input data (such as company details and hiring needs) to the selected Job Description Agent:

    @app.route('/api/send-data', methods=['POST'])def send_data():    """Send payload to the selected agent based on the provided address."""    global agent_response    agent_response = None    try:        # Parse the request payload        data = request.json        payload = data.get('payload')  # Extract the payload dictionary        agent_address = data.get('agentAddress')  # Extract the agent address        # Validate input data        if not payload or not agent_address:            return jsonify({"error": "Missing payload or agent address"}), 400        # Send the payload to the agent        send_message_to_agent(client_identity, agent_address, payload)        logger.info(f"Payload sent to agent: {agent_address}")        return jsonify({"status": "request_sent", "agent_address": agent_address, "payload": payload})    except Exception as e:        logger.error(f"Error sending data to agent: {e}")        return jsonify({"error": str(e)}), 500

**Retrieving the Agent's Response**

The /api/get-response endpoint retrieves the response (job description) from the Job Description Agent and saves it as a markdown file:

    @app.route('/api/get-response', methods=['GET'])def get_response():    """Fetch the response from the agent and save it to a file."""    global agent_response    try:        if agent_response:            response = agent_response            agent_response = None  # Clear the response after fetching            # Save the response to a markdown file            file_path = os.path.join(os.getcwd(), "jd_response.md")            with open(file_path, "w", encoding="utf-8") as md_file:                md_file.write(response.get("content", ""))            logger.info(f"Markdown file created at {file_path}")            return jsonify({"status": "file_created", "file_path": file_path})        else:            return jsonify({"error": "No response available"}), 404    except Exception as e:        logger.error(f"Error creating markdown file: {e}")        return jsonify({"error": str(e)}), 500

**Webhook Endpoint**

The webhook endpoint processes messages sent by the Job Description Agent and stores the response globally:

    @app.route('/api/webhook', methods=['POST'])def webhook():    """Handle incoming messages from the Job Description Agent."""    global agent_response    try:        # Parse the incoming message        data = request.get_data().decode("utf-8")        logger.info("Received response")        message = parse_message_from_agent(data)        agent_response = message.payload        logger.info(f"Processed response: {agent_response}")        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500

**Running the User Agent**

The Flask server runs on port 5002 to handle requests:

    def start_server():    """Start the Flask server."""    try:        # Load environment variables        load_dotenv()        init_client()        app.run(host="0.0.0.0", port=5002)    except Exception as e:        logger.error(f"Server error: {e}")        raiseif __name__ == "__main__":    start_server()

Step-by-Step Execution[â€‹](#step-by-step-execution "Direct link to Step-by-Step Execution")
------------------------------------------------------------------------------------------

**1\. Environment Variables**

Update '.env' with `SERPER_API_KEY`, `OPENAI_API_KEY`, and `AGENTVERSE_API_KEY`.

**2\. Start the Job Description Agent**

*   Registers the agent on Agentverse.
*   Launches JD Agent with Flask on port 5008.

**3\. Start the User Agent**

*   Registers the agent on Agentverse.
*   Launches Flask on port 5002.

### Interacting with the Agents[â€‹](#interacting-with-the-agents "Direct link to Interacting with the Agents")

**Search for Agents**

To list agents in the Agentverse:

    curl -X GET "http://localhost:5002/api/search-agents?query=I%20want%20to%20write%20job%20description"

**Sample Output**

    [    {        "name": "Job Description Creation Agent",        "address": "agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae"    },    ...]

**Send Data to the Job Description Agent**

Send JD details from the User Agent to the JD Agent:

    curl -X POST "http://localhost:5002/api/send-data" \-H "Content-Type: application/json" \-d '{    "payload": {        "company_domain": "careers.wbd.com",        "company_description": "Warner Bros. Discovery is a premier global media and entertainment company, offering audiences the world\u2019s most differentiated and complete portfolio of content, brands and franchises across television, film, sports, news, streaming and gaming. We\u2019re home to the world\u2019s best storytellers, creating world-class products for consumers.",        "hiring_needs": "Production Assistant, for a TV production set in Los Angeles in June 2025",        "specific_benefits": "Weekly Pay, Employee Meals, healthcare"    },    "agentAddress": "agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae"}'

**Sample Output**

    {    "status": "request_sent",    "agent_address": "agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae",    "payload": {        "company_domain": "careers.wbd.com",        "company_description": "Warner Bros. Discovery is a premier global media and entertainment company...",        "hiring_needs": "Production Assistant, for a TV production set in Los Angeles in June 2025",        "specific_benefits": "Weekly Pay, Employee Meals, healthcare"    }}

**Retrieve the Agentâ€™s Response**

Fetch the JD document response from the User Agent:

    curl -X GET "http://localhost:5002/api/get-response"

**Sample Output**

*   Stores the Job Description result in `jd_response.md`.

    {    "status": "file_created",    "file_path": "/path/to/jd_response.md"}

You can open the [jd\_response.md](https://drive.google.com/file/d/12oHIKeOIAjVs8dxP-xg9SmtWR4JnTbqx/view?usp=sharing) file to review the generated job description.

You now have **CrewAI agents** integrated with the **Fetch.ai Agentverse**, collaborating to create custom job descriptions. Feel free to adapt the main.py workflow, agent scripts, or user scripts to suit your own requirementsâ€”whether youâ€™re refining AI logic, adding advanced search filters, or customizing the final job posting format.</content>
</page>

<page>
  <title>Autogen Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/examples/other-frameworks/autogen</url>
  <content>Registering an Autogen Agent on Agentverse Marketplace
------------------------------------------------------

This document is a comprehensive guide to help users register a **Multi-Agent Autogen System** on **Fetch.ai's Agentverse** ecosystem using the provided **Autogen Code Execution Agent** and **User Agent** scripts. AutoGen is an open-source programming framework for building AI agents and facilitating cooperation among multiple agents to solve tasks. These agents work together to execute a Python code and demonstrate how Autogen Agents can interact seamlessly with other agents on Agentverse.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

### Purpose[â€‹](#purpose "Direct link to Purpose")

This project showcases:

*   **Registering** a Multi-Agent System (Autogen) on Agentverse to execute Python code.
*   **Facilitating Communication** between agents within Fetch.aiâ€™s Agentverse via webhooks.
*   **Providing** a clear example of how to return code execution results in a human-friendly format.

### Key Features[â€‹](#key-features "Direct link to Key Features")

*   The **Autogen Code Execution Agent** handles requests for code generation and execution using the Autogen tools.
*   The **User Agent** acts as a client to forward requests and retrieve responses from the Autogen Code Execution Agent.
*   Communication is done via **HTTP webhooks** registered with **Fetch.aiâ€™s Agentverse**.

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

### Environmental Setup[â€‹](#environmental-setup "Direct link to Environmental Setup")

*   Python 3.8 or higher.
*   A virtual environment (recommended).
*   Flask and related libraries installed.

### Architecture Diagram[â€‹](#architecture-diagram "Direct link to Architecture Diagram")

### Required Libraries[â€‹](#required-libraries "Direct link to Required Libraries")

Install the necessary libraries using:

    pip install flask flask-cors fetchai autogen "flask[async]"

### Environment Variables[â€‹](#environment-variables "Direct link to Environment Variables")

Create a `.env` file in your project directory:

    AV_AUTOGEN_CODE_EXECUTION_AI_KEY='<your_autogen_code_execution_ai_key>'USER_AUTOGEN_AI_KEY='<your_user_ai_key>'AGENTVERSE_API_KEY='<your_agentverse_api_key>'OPENAI_API_KEY='<your_openai_api_key>'

Replace placeholders with your actual keys:

*   **AGENTVERSE\_API\_KEY**: Refer this [guide here](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key).
    
*   **OPENAI\_API\_KEY**: Sign up on the [OpenAI](https://platform.openai.com/api-keys) website.
    

Autogen Code Execution Agent Script[â€‹](#autogen-code-execution-agent-script "Direct link to Autogen Code Execution Agent Script")
---------------------------------------------------------------------------------------------------------------------------------

In this example, the **Autogen Code Execution Agent** registered on Agentverse comprises an **AssistantAgent** and a **UserProxyAgent** to write code and execute it. The system focuses on:

*   **Registering** with Fetch.aiâ€™s Agentverse
*   **Handling** requests to return the outcome of a Python code in a human friendly format.
*   **Responding** to the User Agent with the outcome of the code.

### Script Breakdown (autogen-agent.py)[â€‹](#script-breakdown-autogen-agentpy "Direct link to Script Breakdown (autogen-agent.py)")

**Importing Required Libraries**

    import osimport autogenfrom autogen.coding import LocalCommandLineCodeExecutorfrom flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osimport openaifrom dotenv import load_dotenvfrom fetchai import fetchimport asyncio

**Setting Up Logging**

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)

**Flask Webhook**

The Flask app is created to handle incoming webhook requests:

**Code Execution Function**

Below is the code execution function using the **Autogen Multi-Agent** system, where an **AssistantAgent** and a **UserProxyAgent** communicate until the task is done.

    async def execute_code(prompt):    config_list = [{"model": "gpt-4o", "api_key": os.getenv("OPENAI_API_KEY")}]    # create an AssistantAgent named "assistant"    assistant = autogen.AssistantAgent(        name="assistant",        llm_config={            "cache_seed": 41,  # seed for caching and reproducibility            "config_list": config_list,  # a list of OpenAI API configurations            "temperature": 0,  # temperature for sampling        },  # configuration for autogen's enhanced inference API which is compatible with OpenAI API    )    # create a UserProxyAgent instance named "user_proxy"    user_proxy = autogen.UserProxyAgent(        name="user_proxy",        human_input_mode="NEVER",        max_consecutive_auto_reply=10,        is_termination_msg=lambda x: x.get("content", "").rstrip().endswith("TERMINATE"),        code_execution_config={            # the executor to run the generated code            "executor": LocalCommandLineCodeExecutor(work_dir="coding"),        },    )    # the assistant receives a message from the user_proxy, which contains the task description    chat_res = user_proxy.initiate_chat(        assistant,        message=prompt,        summary_method="reflection_with_llm",    )    if hasattr(chat_res, 'chat_history') and chat_res.chat_history:        last_message = chat_res.chat_history[-1]['content']        return last_message    return "No messages found."

**Webhook Endpoint**

    @app.route('/webhook', methods=['POST'])async def webhook():    try:        # Parse the incoming message        logging.info("Parsing message")        data = request.get_data().decode('utf-8')        message = parse_message_from_agent(data)	# Extract the prompt from the message         prompt = message.payload.get("prompt", "")        logger.info(prompt)        agent_address = message.sender	#Call the Autogen Agents to execute the code        response = await execute_code(prompt)        payload = {"response":response}	#Return the message back to the User Agent        send_message_to_agent(            sender=ai_identity,            target=agent_address,            payload=payload        )        return jsonify({"status": "graphs_sent"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"status": "error", "message": str(e)}), 500

**Registering the Agent**

    def init_agent():    """Initialize and register the client agent."""    global ai_identity    try:        ai_identity = Identity.from_seed(os.getenv("AV_AUTOGEN_CODE_EXECUTION_AI_KEY"), 0)        register_with_agentverse(            identity = ai_identity,            url = "http://localhost:5001/webhook", ## The webhook that your AI receives messages on.            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title = "Autogen Code Execution Agent",            readme = """        <description>This agent generates code and executes it to provide final output to the user.</description>        <use_cases>            <use_case>What date is today? Compare the year-to-date gain for META and TESLA.</use_case>        </use_cases>        <payload_requirements>        <description>Please provide a prompt for code execution.</description>        <payload>            <requirement>                <parameter>prompt</parameter>                <description>Please provide a prompt for code execution.The prompt will help this AI generate code and return the output to the user. </description>            </requirement>        </payload>        </payload_requirements>        """        )        logger.info("Autogen Code Execution agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise

**Running the Agent**

    if __name__ == "__main__":    init_agent()    app.run(host="0.0.0.0", port=5001, debug=True)

User Agent Script[â€‹](#user-agent-script "Direct link to User Agent Script")
---------------------------------------------------------------------------

The **User Agent** script acts as the client that forwards requests to the **Autogen Code Execution Agent** and retrieves responses.

### Script Breakdown (autogen-user-agent.py)[â€‹](#script-breakdown-autogen-user-agentpy "Direct link to Script Breakdown (autogen-user-agent.py)")

**Importing Libraries**

    from flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai import fetchfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osimport timefrom dotenv import load_dotenv

**Setting Up Logging and Flask**

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app)

**Initializing the User Agent**

    def init_agent():    """Initialize and register the client agent."""    global client_identity    try:        # Load the client identity from environment variables        client_identity = Identity.from_seed(os.getenv("USER_AUTOGEN_AI_KEY"), 0)        logger.info(f"Client agent started with address: {client_identity.address}")        # Define the client agent's metadata        readme = """        <description>Frontend client that interacts with Autogen Agent to execute code.</description>        <use_cases>            <use_case>Sends Request to the AI Agent to execute a code</use_case>        </use_cases>        <payload_requirements>            <description>Expects request which is to be sent to another agent.</description>            <payload>                <requirement>                    <parameter>request</parameter>                    <description>This is the request which is to be sent to other agent</description>                </requirement>            </payload>        </payload_requirements>        """        # Register the agent with Agentverse        register_with_agentverse(            identity=client_identity,            url="http://localhost:5002/api/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="User agent for Autogen Code Execution Agent",            readme=readme        )        logger.info("Client agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise

**Searching Agents**

    # searching the agents which can create dashboard on agentverse@app.route('/api/search-agents', methods=['GET'])def search_agents():   """Search for available dashboard agents based on user input."""   try:       # Extract user input from query parameters       user_query = request.args.get('query', '')       if not user_query:           return jsonify({"error": "Query parameter 'query' is required."}), 400       # Fetch available agents based on user query       available_ais = fetch.ai(user_query)  # Pass the user query to the fetch.ai function       print(f'---------------------{available_ais}----------------------')       # Access the 'ais' list within 'agents' (assuming fetch.ai returns the correct structure)       agents = available_ais.get('ais', [])       print(f'----------------------------------{agents}------------------------------------')       extracted_data = []       for agent in agents:           name = agent.get('name')  # Extract agent name           address = agent.get('address')           # Append formatted data to extracted_data list           extracted_data.append({               'name': name,               'address': address,           })       # Format the response with indentation for readability       response = jsonify(extracted_data)       response.headers.add('Content-Type', 'application/json; charset=utf-8')       response.headers.add('Access-Control-Allow-Origin', '*')       response.headers.add('Access-Control-Allow-Headers', 'Content-Type')       return response, 200   except Exception as e:       logger.error(f"Error finding agents: {e}")       return jsonify({"error": str(e)}), 500

**Sending Data to Autogen Code Execution Agent**

    @app.route('/api/send-data', methods=['POST'])def send_data():   """Send payload to the selected agent based on provided address."""   global agent_response   agent_response = None   try:       # Parse the request payload       data = request.json       payload = data.get('payload')  # Extract the payload dictionary       agent_address = data.get('agentAddress')  # Extract the agent address       # Validate the input data       if not payload or not agent_address:           return jsonify({"error": "Missing payload or agent address"}), 400       logger.info(f"Sending payload to agent: {agent_address}")       logger.info(f"Payload: {payload}")       # Send the payload to the specified agent       send_message_to_agent(           client_identity,  # Frontend client identity           agent_address,    # Agent address where we have to send the data           payload           # Payload containing the data       )       return jsonify({"status": "request_sent", "agent_address": agent_address, "payload": payload})   except Exception as e:       logger.error(f"Error sending data to agent: {e}")       return jsonify({"error": str(e)}), 500

**Webhook Endpoint**

    # app route to get recieve the messages on the agent@app.route('/webhook', methods=['POST'])def webhook():    """Handle incoming messages from the dashboard agent."""    global agent_response    try:        # Parse the incoming webhook message        data = request.get_data().decode("utf-8")        logger.info("Received response")        message = parse_message_from_agent(data)        agent_response = message.payload        logger.info(f"Processed response: {agent_response}")        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500

**Retrieving the Agent's Response**

    @app.route('/api/get-response', methods=['GET'])def get_response():    global agent_response    try:        if agent_response:            response = agent_response            print(f'response : {response}')            agent_response = None  # Clear the response after sending            keys = list(response.keys())  # Convert dict_keys to a list            first_key = keys[0]  # Get the first key            response_final = response.get(first_key, "")            logger.info(f"Got response for after code execution {response_final}")            return jsonify({"response": response_final})        else:            return jsonify({"error": "No storm response available"}), 404    except Exception as e:        logger.error(f"Error creating markdown file: {e}")        return jsonify({"error": str(e)}), 500

**Running the agent**

    if __name__ == "__main__":    init_agent()    app.run(host="0.0.0.0", port=5002)

Steps to Run the Scripts[â€‹](#steps-to-run-the-scripts "Direct link to Steps to Run the Scripts")
------------------------------------------------------------------------------------------------

Follow these steps to get both the **Autogen Code Execution Agent** and **User Agent** scripts running.

**1\. Start the Alphavantage Agent**

Ensure the `AV_AUTOGEN_CODE_EXECUTION_AI_KEY` and `AGENTVERSE_API_KEY` values are correctly set in the `.env` file. Then Run:

**Expected Output:**

    (.venv) kshipra@MacBook-Pro autogen-code-generation-debugging-example % python3 autogen-agent.pyflaml.automl is not available. Please install flaml[automl] to enable AutoML functionalities.INFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Autogen Code Execution agent registration complete! * Serving Flask app 'autogen-agent' * Debug mode: onINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5001 * Running on http://192.168.6.11:5001INFO:werkzeug:Press CTRL+C to quitINFO:werkzeug: * Restarting with statflaml.automl is not available. Please install flaml[automl] to enable AutoML functionalities.INFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Autogen Code Execution agent registration complete!WARNING:werkzeug: * Debugger is active!INFO:werkzeug: * Debugger PIN: 471-951-559

The agent listens on **port 5001**.

**2\. Start the User Agent**

Ensure the `USER_AUTOGEN_AI_KEY` and `AGENTVERSE_API_KEY` values are set in the `.env` file.

    python3 autogen-user-agent.py

**Expected Output:**

    kshipra@MacBook-Pro autogen-code-generation-debugging-example % python3 autogen-user-agent.py INFO:__main__:Client agent started with address: agent1q0dfdn7073m75c3dc2sdd5gajevswmmwkw6wxgqhjca9ug6c2ruzk7nak0dINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Client agent registration complete! * Serving Flask app 'autogen-user-agent' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5002 * Running on http://192.168.6.11:5002

### Interacting with the Agents[â€‹](#interacting-with-the-agents "Direct link to Interacting with the Agents")

Once both agents are running, use the following endpoints to interact with them from another terminal:

**1\. Search for Agents**

To list agents in the Agentverse:

    curl -X GET "http://localhost:5002/api/search-agents?query=I%20want%20to%20write%20a%20oython%20code"

**Sample Output**

    [{"address":"agent1qwkzy0twyd27egqfrcw8m6vdtsz7hrm9leu7jfezzzlgckywj7shytz5kf6","name":"Alphavantage Stock Price Langchain tool"},{"address":"agent1qtnc7ruc63g3n84qczekqc6qwp0e5ayvs3v8e5pae563qrqcf852xlqz2pk","name":"Alphavantage Stock Price Langchain tool"},{"address":"agent1qdwp929xdwzm0pgxflz29c3x6ltha97t2em88vrh588nehz6su0u55zaxp9","name":"Autogen Code Execution Agent"},{"address":"agent1qgjj8476mr8cy2dvpc3ec5w4ye56kqswvr6hkl55q8775ghm9h7wwevh6lj","name":"Autogen Code Execution Agent"},{"address":"agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae","name":"Job Description Creation Agent"},{"address":"agent1qtmkpt8lhyx4u2ndzcgr7jkjuuade4ay78yld2ngaghs03z6qve3senwrsa","name":"Penny"},{"address":"agent1qtxx79e689zwjlh7m8dsfuv7lyn45d2xapr4p9w2u9axvzm487tzjfwqzdp","name":"Penny"},{"address":"agent1qv2u6v4ueea9kmvsyyclz9reh7qmzc6gr0fn7aeup93xm6ld8ynwc65nsks","name":"Penny"},{"address":"agent1qvc0em6vrlr80txaz3ksy7tspzaxl564jwqtaana7exehg0hz7gl6zg0wl0","name":"Penny"},{"address":"agent1qw5s6e7mxcd8mp6f2vvunk9rklwnv8ynft43unx77syea4nnskxyj4yk5xq","name":"Penny"}]

**2\. Send a Request to the Autogen Code Execution Agent**

Send a prompt from the User Agent to the Autogen Code Execution Agent:

    kshipra@MacBook-Pro autogen-code-generation-debugging-example % curl -X POST "http://localhost:5002/api/send-data" \-H "Content-Type: application/json" \-d '{  "payload": {    "prompt": "What date is today? Compare the year-to-date gain for META and TESLA."  },  "agentAddress": "agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae"}'

**Sample Output**

    {"status": "request_sent", "agent_address": "agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae", "payload": "{"prompt": "What date is today? Compare the year-to-date gain for META and TESLA."  }"}

**3\. Retrieve the Stock Price Response**

Fetch the stock price response from the User Agent:

    curl -X GET "http://localhost:5002/api/get-response"

**Sample Output**

    {"response":"The code executed successfully, and we have the year-to-date gain for both META and TESLA:\n\n- **META (Meta Platforms, Inc.)**: Year-to-Date Gain is **17.63%**\n- **TESLA (Tesla, Inc.)**: Year-to-Date Gain is **-0.29%**\n\nThis means that since the beginning of the year, META's stock price has increased by 17.63%, while TESLA's stock price has decreased by 0.29%.\n\nTERMINATE"}

Debugging Common Issues[â€‹](#debugging-common-issues "Direct link to Debugging Common Issues")
---------------------------------------------------------------------------------------------

Agent Registration Fails:

a. Check the .env file for correct API keys. b. Ensure the AGENTVERSE\_API\_KEY is valid.

404 Errors:

a. Verify that both agents are running on their respective ports (5001 and 5002 in this case). b. Double-check the agentAddress in requests.

You now have an **Autogen Code Execution Agent** integrated with the **Fetch.ai Agentverse**, orchestrating multi-agent code generation and execution. Feel free to extend the example to include additional tasks, multiple code executors, or advanced debugging logic.</content>
</page>

<page>
  <title>Autogen Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/examples/other-frameworks/autogen</url>
  <content>Registering an Autogen Agent on Agentverse Marketplace
------------------------------------------------------

This document is a comprehensive guide to help users register a **Multi-Agent Autogen System** on **Fetch.ai's Agentverse** ecosystem using the provided **Autogen Code Execution Agent** and **User Agent** scripts. AutoGen is an open-source programming framework for building AI agents and facilitating cooperation among multiple agents to solve tasks. These agents work together to execute a Python code and demonstrate how Autogen Agents can interact seamlessly with other agents on Agentverse.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

### Purpose[â€‹](#purpose "Direct link to Purpose")

This project showcases:

*   **Registering** a Multi-Agent System (Autogen) on Agentverse to execute Python code.
*   **Facilitating Communication** between agents within Fetch.aiâ€™s Agentverse via webhooks.
*   **Providing** a clear example of how to return code execution results in a human-friendly format.

### Key Features[â€‹](#key-features "Direct link to Key Features")

*   The **Autogen Code Execution Agent** handles requests for code generation and execution using the Autogen tools.
*   The **User Agent** acts as a client to forward requests and retrieve responses from the Autogen Code Execution Agent.
*   Communication is done via **HTTP webhooks** registered with **Fetch.aiâ€™s Agentverse**.

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

### Environmental Setup[â€‹](#environmental-setup "Direct link to Environmental Setup")

*   Python 3.8 or higher.
*   A virtual environment (recommended).
*   Flask and related libraries installed.

### Architecture Diagram[â€‹](#architecture-diagram "Direct link to Architecture Diagram")

### Required Libraries[â€‹](#required-libraries "Direct link to Required Libraries")

Install the necessary libraries using:

    pip install flask flask-cors fetchai autogen "flask[async]"

### Environment Variables[â€‹](#environment-variables "Direct link to Environment Variables")

Create a `.env` file in your project directory:

    AV_AUTOGEN_CODE_EXECUTION_AI_KEY='<your_autogen_code_execution_ai_key>'USER_AUTOGEN_AI_KEY='<your_user_ai_key>'AGENTVERSE_API_KEY='<your_agentverse_api_key>'OPENAI_API_KEY='<your_openai_api_key>'

Replace placeholders with your actual keys:

*   **AGENTVERSE\_API\_KEY**: Refer this [guide here](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key).
    
*   **OPENAI\_API\_KEY**: Sign up on the [OpenAI](https://platform.openai.com/api-keys) website.
    

Autogen Code Execution Agent Script[â€‹](#autogen-code-execution-agent-script "Direct link to Autogen Code Execution Agent Script")
---------------------------------------------------------------------------------------------------------------------------------

In this example, the **Autogen Code Execution Agent** registered on Agentverse comprises an **AssistantAgent** and a **UserProxyAgent** to write code and execute it. The system focuses on:

*   **Registering** with Fetch.aiâ€™s Agentverse
*   **Handling** requests to return the outcome of a Python code in a human friendly format.
*   **Responding** to the User Agent with the outcome of the code.

### Script Breakdown (autogen-agent.py)[â€‹](#script-breakdown-autogen-agentpy "Direct link to Script Breakdown (autogen-agent.py)")

**Importing Required Libraries**

    import osimport autogenfrom autogen.coding import LocalCommandLineCodeExecutorfrom flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osimport openaifrom dotenv import load_dotenvfrom fetchai import fetchimport asyncio

**Setting Up Logging**

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)

**Flask Webhook**

The Flask app is created to handle incoming webhook requests:

**Code Execution Function**

Below is the code execution function using the **Autogen Multi-Agent** system, where an **AssistantAgent** and a **UserProxyAgent** communicate until the task is done.

    async def execute_code(prompt):    config_list = [{"model": "gpt-4o", "api_key": os.getenv("OPENAI_API_KEY")}]    # create an AssistantAgent named "assistant"    assistant = autogen.AssistantAgent(        name="assistant",        llm_config={            "cache_seed": 41,  # seed for caching and reproducibility            "config_list": config_list,  # a list of OpenAI API configurations            "temperature": 0,  # temperature for sampling        },  # configuration for autogen's enhanced inference API which is compatible with OpenAI API    )    # create a UserProxyAgent instance named "user_proxy"    user_proxy = autogen.UserProxyAgent(        name="user_proxy",        human_input_mode="NEVER",        max_consecutive_auto_reply=10,        is_termination_msg=lambda x: x.get("content", "").rstrip().endswith("TERMINATE"),        code_execution_config={            # the executor to run the generated code            "executor": LocalCommandLineCodeExecutor(work_dir="coding"),        },    )    # the assistant receives a message from the user_proxy, which contains the task description    chat_res = user_proxy.initiate_chat(        assistant,        message=prompt,        summary_method="reflection_with_llm",    )    if hasattr(chat_res, 'chat_history') and chat_res.chat_history:        last_message = chat_res.chat_history[-1]['content']        return last_message    return "No messages found."

**Webhook Endpoint**

    @app.route('/webhook', methods=['POST'])async def webhook():    try:        # Parse the incoming message        logging.info("Parsing message")        data = request.get_data().decode('utf-8')        message = parse_message_from_agent(data)	# Extract the prompt from the message         prompt = message.payload.get("prompt", "")        logger.info(prompt)        agent_address = message.sender	#Call the Autogen Agents to execute the code        response = await execute_code(prompt)        payload = {"response":response}	#Return the message back to the User Agent        send_message_to_agent(            sender=ai_identity,            target=agent_address,            payload=payload        )        return jsonify({"status": "graphs_sent"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"status": "error", "message": str(e)}), 500

**Registering the Agent**

    def init_agent():    """Initialize and register the client agent."""    global ai_identity    try:        ai_identity = Identity.from_seed(os.getenv("AV_AUTOGEN_CODE_EXECUTION_AI_KEY"), 0)        register_with_agentverse(            identity = ai_identity,            url = "http://localhost:5001/webhook", ## The webhook that your AI receives messages on.            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title = "Autogen Code Execution Agent",            readme = """        <description>This agent generates code and executes it to provide final output to the user.</description>        <use_cases>            <use_case>What date is today? Compare the year-to-date gain for META and TESLA.</use_case>        </use_cases>        <payload_requirements>        <description>Please provide a prompt for code execution.</description>        <payload>            <requirement>                <parameter>prompt</parameter>                <description>Please provide a prompt for code execution.The prompt will help this AI generate code and return the output to the user. </description>            </requirement>        </payload>        </payload_requirements>        """        )        logger.info("Autogen Code Execution agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise

**Running the Agent**

    if __name__ == "__main__":    init_agent()    app.run(host="0.0.0.0", port=5001, debug=True)

User Agent Script[â€‹](#user-agent-script "Direct link to User Agent Script")
---------------------------------------------------------------------------

The **User Agent** script acts as the client that forwards requests to the **Autogen Code Execution Agent** and retrieves responses.

### Script Breakdown (autogen-user-agent.py)[â€‹](#script-breakdown-autogen-user-agentpy "Direct link to Script Breakdown (autogen-user-agent.py)")

**Importing Libraries**

    from flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai import fetchfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osimport timefrom dotenv import load_dotenv

**Setting Up Logging and Flask**

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app)

**Initializing the User Agent**

    def init_agent():    """Initialize and register the client agent."""    global client_identity    try:        # Load the client identity from environment variables        client_identity = Identity.from_seed(os.getenv("USER_AUTOGEN_AI_KEY"), 0)        logger.info(f"Client agent started with address: {client_identity.address}")        # Define the client agent's metadata        readme = """        <description>Frontend client that interacts with Autogen Agent to execute code.</description>        <use_cases>            <use_case>Sends Request to the AI Agent to execute a code</use_case>        </use_cases>        <payload_requirements>            <description>Expects request which is to be sent to another agent.</description>            <payload>                <requirement>                    <parameter>request</parameter>                    <description>This is the request which is to be sent to other agent</description>                </requirement>            </payload>        </payload_requirements>        """        # Register the agent with Agentverse        register_with_agentverse(            identity=client_identity,            url="http://localhost:5002/api/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="User agent for Autogen Code Execution Agent",            readme=readme        )        logger.info("Client agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise

**Searching Agents**

    # searching the agents which can create dashboard on agentverse@app.route('/api/search-agents', methods=['GET'])def search_agents():   """Search for available dashboard agents based on user input."""   try:       # Extract user input from query parameters       user_query = request.args.get('query', '')       if not user_query:           return jsonify({"error": "Query parameter 'query' is required."}), 400       # Fetch available agents based on user query       available_ais = fetch.ai(user_query)  # Pass the user query to the fetch.ai function       print(f'---------------------{available_ais}----------------------')       # Access the 'ais' list within 'agents' (assuming fetch.ai returns the correct structure)       agents = available_ais.get('ais', [])       print(f'----------------------------------{agents}------------------------------------')       extracted_data = []       for agent in agents:           name = agent.get('name')  # Extract agent name           address = agent.get('address')           # Append formatted data to extracted_data list           extracted_data.append({               'name': name,               'address': address,           })       # Format the response with indentation for readability       response = jsonify(extracted_data)       response.headers.add('Content-Type', 'application/json; charset=utf-8')       response.headers.add('Access-Control-Allow-Origin', '*')       response.headers.add('Access-Control-Allow-Headers', 'Content-Type')       return response, 200   except Exception as e:       logger.error(f"Error finding agents: {e}")       return jsonify({"error": str(e)}), 500

**Sending Data to Autogen Code Execution Agent**

    @app.route('/api/send-data', methods=['POST'])def send_data():   """Send payload to the selected agent based on provided address."""   global agent_response   agent_response = None   try:       # Parse the request payload       data = request.json       payload = data.get('payload')  # Extract the payload dictionary       agent_address = data.get('agentAddress')  # Extract the agent address       # Validate the input data       if not payload or not agent_address:           return jsonify({"error": "Missing payload or agent address"}), 400       logger.info(f"Sending payload to agent: {agent_address}")       logger.info(f"Payload: {payload}")       # Send the payload to the specified agent       send_message_to_agent(           client_identity,  # Frontend client identity           agent_address,    # Agent address where we have to send the data           payload           # Payload containing the data       )       return jsonify({"status": "request_sent", "agent_address": agent_address, "payload": payload})   except Exception as e:       logger.error(f"Error sending data to agent: {e}")       return jsonify({"error": str(e)}), 500

**Webhook Endpoint**

    # app route to get recieve the messages on the agent@app.route('/webhook', methods=['POST'])def webhook():    """Handle incoming messages from the dashboard agent."""    global agent_response    try:        # Parse the incoming webhook message        data = request.get_data().decode("utf-8")        logger.info("Received response")        message = parse_message_from_agent(data)        agent_response = message.payload        logger.info(f"Processed response: {agent_response}")        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500

**Retrieving the Agent's Response**

    @app.route('/api/get-response', methods=['GET'])def get_response():    global agent_response    try:        if agent_response:            response = agent_response            print(f'response : {response}')            agent_response = None  # Clear the response after sending            keys = list(response.keys())  # Convert dict_keys to a list            first_key = keys[0]  # Get the first key            response_final = response.get(first_key, "")            logger.info(f"Got response for after code execution {response_final}")            return jsonify({"response": response_final})        else:            return jsonify({"error": "No storm response available"}), 404    except Exception as e:        logger.error(f"Error creating markdown file: {e}")        return jsonify({"error": str(e)}), 500

**Running the agent**

    if __name__ == "__main__":    init_agent()    app.run(host="0.0.0.0", port=5002)

Steps to Run the Scripts[â€‹](#steps-to-run-the-scripts "Direct link to Steps to Run the Scripts")
------------------------------------------------------------------------------------------------

Follow these steps to get both the **Autogen Code Execution Agent** and **User Agent** scripts running.

**1\. Start the Alphavantage Agent**

Ensure the `AV_AUTOGEN_CODE_EXECUTION_AI_KEY` and `AGENTVERSE_API_KEY` values are correctly set in the `.env` file. Then Run:

**Expected Output:**

    (.venv) kshipra@MacBook-Pro autogen-code-generation-debugging-example % python3 autogen-agent.pyflaml.automl is not available. Please install flaml[automl] to enable AutoML functionalities.INFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Autogen Code Execution agent registration complete! * Serving Flask app 'autogen-agent' * Debug mode: onINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5001 * Running on http://192.168.6.11:5001INFO:werkzeug:Press CTRL+C to quitINFO:werkzeug: * Restarting with statflaml.automl is not available. Please install flaml[automl] to enable AutoML functionalities.INFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Autogen Code Execution agent registration complete!WARNING:werkzeug: * Debugger is active!INFO:werkzeug: * Debugger PIN: 471-951-559

The agent listens on **port 5001**.

**2\. Start the User Agent**

Ensure the `USER_AUTOGEN_AI_KEY` and `AGENTVERSE_API_KEY` values are set in the `.env` file.

    python3 autogen-user-agent.py

**Expected Output:**

    kshipra@MacBook-Pro autogen-code-generation-debugging-example % python3 autogen-user-agent.py INFO:__main__:Client agent started with address: agent1q0dfdn7073m75c3dc2sdd5gajevswmmwkw6wxgqhjca9ug6c2ruzk7nak0dINFO:fetchai:Registering with Almanac APIINFO:fetchai:Completed registering agent with AgentverseINFO:__main__:Client agent registration complete! * Serving Flask app 'autogen-user-agent' * Debug mode: offINFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5002 * Running on http://192.168.6.11:5002

### Interacting with the Agents[â€‹](#interacting-with-the-agents "Direct link to Interacting with the Agents")

Once both agents are running, use the following endpoints to interact with them from another terminal:

**1\. Search for Agents**

To list agents in the Agentverse:

    curl -X GET "http://localhost:5002/api/search-agents?query=I%20want%20to%20write%20a%20oython%20code"

**Sample Output**

    [{"address":"agent1qwkzy0twyd27egqfrcw8m6vdtsz7hrm9leu7jfezzzlgckywj7shytz5kf6","name":"Alphavantage Stock Price Langchain tool"},{"address":"agent1qtnc7ruc63g3n84qczekqc6qwp0e5ayvs3v8e5pae563qrqcf852xlqz2pk","name":"Alphavantage Stock Price Langchain tool"},{"address":"agent1qdwp929xdwzm0pgxflz29c3x6ltha97t2em88vrh588nehz6su0u55zaxp9","name":"Autogen Code Execution Agent"},{"address":"agent1qgjj8476mr8cy2dvpc3ec5w4ye56kqswvr6hkl55q8775ghm9h7wwevh6lj","name":"Autogen Code Execution Agent"},{"address":"agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae","name":"Job Description Creation Agent"},{"address":"agent1qtmkpt8lhyx4u2ndzcgr7jkjuuade4ay78yld2ngaghs03z6qve3senwrsa","name":"Penny"},{"address":"agent1qtxx79e689zwjlh7m8dsfuv7lyn45d2xapr4p9w2u9axvzm487tzjfwqzdp","name":"Penny"},{"address":"agent1qv2u6v4ueea9kmvsyyclz9reh7qmzc6gr0fn7aeup93xm6ld8ynwc65nsks","name":"Penny"},{"address":"agent1qvc0em6vrlr80txaz3ksy7tspzaxl564jwqtaana7exehg0hz7gl6zg0wl0","name":"Penny"},{"address":"agent1qw5s6e7mxcd8mp6f2vvunk9rklwnv8ynft43unx77syea4nnskxyj4yk5xq","name":"Penny"}]

**2\. Send a Request to the Autogen Code Execution Agent**

Send a prompt from the User Agent to the Autogen Code Execution Agent:

    kshipra@MacBook-Pro autogen-code-generation-debugging-example % curl -X POST "http://localhost:5002/api/send-data" \-H "Content-Type: application/json" \-d '{  "payload": {    "prompt": "What date is today? Compare the year-to-date gain for META and TESLA."  },  "agentAddress": "agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae"}'

**Sample Output**

    {"status": "request_sent", "agent_address": "agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae", "payload": "{"prompt": "What date is today? Compare the year-to-date gain for META and TESLA."  }"}

**3\. Retrieve the Stock Price Response**

Fetch the stock price response from the User Agent:

    curl -X GET "http://localhost:5002/api/get-response"

**Sample Output**

    {"response":"The code executed successfully, and we have the year-to-date gain for both META and TESLA:\n\n- **META (Meta Platforms, Inc.)**: Year-to-Date Gain is **17.63%**\n- **TESLA (Tesla, Inc.)**: Year-to-Date Gain is **-0.29%**\n\nThis means that since the beginning of the year, META's stock price has increased by 17.63%, while TESLA's stock price has decreased by 0.29%.\n\nTERMINATE"}

Debugging Common Issues[â€‹](#debugging-common-issues "Direct link to Debugging Common Issues")
---------------------------------------------------------------------------------------------

Agent Registration Fails:

a. Check the .env file for correct API keys. b. Ensure the AGENTVERSE\_API\_KEY is valid.

404 Errors:

a. Verify that both agents are running on their respective ports (5001 and 5002 in this case). b. Double-check the agentAddress in requests.

You now have an **Autogen Code Execution Agent** integrated with the **Fetch.ai Agentverse**, orchestrating multi-agent code generation and execution. Feel free to extend the example to include additional tasks, multiple code executors, or advanced debugging logic.</content>
</page>

<page>
  <title>CrewAI Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/examples/other-frameworks/crewai</url>
  <content>Creating and Registering CrewAI based Job descriptions creator
--------------------------------------------------------------

This documentation explains how to build **CrewAI** agents that generate tailored job descriptions, register them on **Fetch.aiâ€™s Agentverse**, and enable collaboration between agents for dynamic data sharing. Below, you'll see the **main.py** script containing the workflow logic `run_job_posting_workflow`, as well as sample scripts for a **Job Description Agent** and a **User Agent**.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

### Purpose[â€‹](#purpose "Direct link to Purpose")

This project showcases:

*   **Creating CrewAI agents** for complex tasks (e.g., analyzing company requirements and generating job descriptions).
*   **Integrating with Fetch.aiâ€™s Agentverse**, enabling seamless communication and data exchange.
*   **Using autonomous agents** to streamline job description creation while aligning with company culture and specific role requirements.

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

### Environment Setup[â€‹](#environment-setup "Direct link to Environment Setup")

To begin creating and registering CrewAI agents, ensure you have the following:

*   **Python 3.10+** or higher
*   A **virtual environment** (recommended)
*   **Fetch.ai SDK** for agent creation and registration
*   **CrewAI Framework** for defining tasks and workflows

### Key Components[â€‹](#key-components "Direct link to Key Components")

*   **Job Description Agent**: A specialized CrewAI agent that processes input (company domain, hiring needs, etc.) to generate a job description.
*   **User Agent**: Acts as a client to discover and interact with the Job Description Agent.
*   **Agent Collaboration**: Multiple agents communicate via Agentverse, orchestrating tasks through CrewAI workflows.

### Architecture Diagram[â€‹](#architecture-diagram "Direct link to Architecture Diagram")

### Environment Variables[â€‹](#environment-variables "Direct link to Environment Variables")

Create a `.env` file with your API keys:

    SERPER_API_KEY=<your_serper_api_key>OPENAI_API_KEY=<your_openai_api_key>AGENTVERSE_API_KEY=<your_agentverse_api_key>

Replace the placeholders with your actual keys.

*   [OPENAI\_API\_KEY](https://openai.com/index/openai-api/)
*   [SERPER\_API\_KEY](https://serper.dev/)
*   [AGENTVERSE\_API\_KEY](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key)

Project Directory & Configuration Files[â€‹](#project-directory--configuration-files "Direct link to Project Directory & Configuration Files")
--------------------------------------------------------------------------------------------------------------------------------------------

For a clean, maintainable setup, we recommend the following structure:

    crewai_job_descriptions/â”œâ”€â”€ .envâ”œâ”€â”€ config/â”‚   â”œâ”€â”€ agents.yamlâ”‚   â””â”€â”€ tasks.yamlâ”œâ”€â”€ crew.pyâ”œâ”€â”€ jd_agent.pyâ”œâ”€â”€ main.pyâ”œâ”€â”€ requirements.txtâ”œâ”€â”€ user_agent.pyâ”œâ”€â”€ job_description_example.mdâ””â”€â”€ README.md

**1\. requirements.txt**

**Purpose**: Declares project dependencies so you (or anyone) can install them quickly with `pip install -r requirements.txt`.

Create a file called `requirements.txt` at the root of your project (alongside `jd_agent.py` and `main.py`). Example contents:

    flask==2.2.5flask-cors==3.0.10fetchai==0.16.3crewai==0.100.1crewai_tools==0.33.0python-dotenv==1.0.0

Install everything at once via:

    pip install -r requirements.txt

**2\. Crew.py**

**Purpose:** Defines your `JobPostingCrew` class, specifying the Agents and Tasks.

Place `crew.py` at the root directory. It contains the code:

    from typing import Listfrom crewai import Agent, Crew, Process, Taskfrom crewai.project import CrewBase, agent, crew, task# Check our tools documentations for more information on how to use themfrom crewai_tools import SerperDevTool, ScrapeWebsiteTool, WebsiteSearchTool, FileReadToolfrom pydantic import BaseModel, Fieldweb_search_tool = WebsiteSearchTool()seper_dev_tool = SerperDevTool()file_read_tool = FileReadTool(    file_path='./job_description_example.md',    description='A tool to read the job description example file.')class ResearchRoleRequirements(BaseModel):    """Research role requirements model"""    skills: List[str] = Field(..., description="List of recommended skills ...")    experience: List[str] = Field(..., description="List of recommended experience ...")    qualities: List[str] = Field(..., description="List of recommended qualities ...")@CrewBaseclass JobPostingCrew:    """JobPosting crew"""    agents_config = 'config/agents.yaml'    tasks_config = 'config/tasks.yaml'    @agent    def research_agent(self) -> Agent:        return Agent(            config=self.agents_config['research_agent'],            tools=[web_search_tool, seper_dev_tool],            verbose=True        )        @agent    def writer_agent(self) -> Agent:        return Agent(            config=self.agents_config['writer_agent'],            tools=[web_search_tool, seper_dev_tool, file_read_tool],            verbose=True        )        @agent    def review_agent(self) -> Agent:        return Agent(            config=self.agents_config['review_agent'],            tools=[web_search_tool, seper_dev_tool, file_read_tool],            verbose=True        )        @task    def research_company_culture_task(self) -> Task:        return Task(            config=self.tasks_config['research_company_culture_task'],            agent=self.research_agent()        )    @task    def research_role_requirements_task(self) -> Task:        return Task(            config=self.tasks_config['research_role_requirements_task'],            agent=self.research_agent(),            output_json=ResearchRoleRequirements        )    @task    def draft_job_posting_task(self) -> Task:        return Task(            config=self.tasks_config['draft_job_posting_task'],            agent=self.writer_agent()        )    @task    def review_and_edit_job_posting_task(self) -> Task:        return Task(            config=self.tasks_config['review_and_edit_job_posting_task'],            agent=self.review_agent()        )    @task    def industry_analysis_task(self) -> Task:        return Task(            config=self.tasks_config['industry_analysis_task'],            agent=self.research_agent()        )    @crew    def crew(self) -> Crew:        """Creates the JobPostingCrew"""        return Crew(            agents=self.agents,  # Automatically created by the @agent decorator            tasks=self.tasks,    # Automatically created by the @task decorator            process=Process.sequential,            verbose=True,        )

**3\. config/agents.yaml**

**Purpose**: Specifies each agentâ€™s role, goal, and backstory. Create a folder named `config/` at your project root. Inside it, **create** `agents.yaml`:

    research_agent:  role: >    Research Analyst  goal: >    Analyze the company website and provided description...  backstory: >    Expert in analyzing company cultures and identifying key values...writer_agent:  role: >    Job Description Writer  goal: >    Use insights from the Research Analyst to create...  backstory: >    Skilled in crafting compelling job descriptions...review_agent:  role: >    Review and Editing Specialist  goal: >    Review the job posting for clarity, engagement...  backstory: >    A meticulous editor ensuring every piece of content is polished...

**4\. config/tasks.yaml**

**Purpose**: Defines the text prompts (description) and expected output for each task. In the same `config/` folder, make a second file called `tasks.yaml`:

    research_company_culture_task:  description: >    Analyze {company_domain} and {company_description}...  expected_output: >    A comprehensive report detailing the company's culture...research_role_requirements_task:  description: >    Based on {hiring_needs}, list recommended skills, experience...  expected_output: >    A list of recommended skills, experiences, and qualities...draft_job_posting_task:  description: >    Draft a job posting for {hiring_needs}, using {specific_benefits}...  expected_output: >    A detailed, engaging job posting that includes an introduction...review_and_edit_job_posting_task:  description: >    Review the job posting for {hiring_needs} for clarity...  expected_output: >    A polished, error-free job posting with final approval in markdown.industry_analysis_task:  description: >    Conduct an in-depth analysis of {company_domain}'s industry...  expected_output: >    A detailed analysis highlighting major industry trends, opportunities...

Main.py (Workflow Definition)[â€‹](#mainpy-workflow-definition "Direct link to Main.py (Workflow Definition)")
------------------------------------------------------------------------------------------------------------

Below is an example `main.py` script containing the `run_job_posting_workflow` function that the Job Description Agent calls to generate job descriptions. It uses **Crew** to structure tasks and (optionally) a local SQLite database for storage or logging.

    import sysfrom crew import JobPostingCrewimport sqlite3async def run_job_posting_workflow(company_domain, company_description, hiring_needs, specific_benefits):    """    Executes the job posting workflow with proper connection handling and debugging output.        Args:        company_domain (str): The company's domain.        company_description (str): A description of the company.        hiring_needs (str): Description of the role being hired for.        specific_benefits (str): Specific benefits offered for the role.        Returns:        str: The raw output of the 'industry_analysis_task' if found, else None.    """    try:        # Initialize the JobPostingCrew        job_posting_crew = JobPostingCrew().crew()                # Define the inputs        inputs = {            'company_domain': company_domain,            'company_description': company_description,            'hiring_needs': hiring_needs,            'specific_benefits': specific_benefits,        }        # Execute the job posting process        result = job_posting_crew.kickoff(inputs=inputs)        # Debugging output        print("Result type:", type(result))        print("Attributes:", dir(result))        # Attempt to retrieve and print tasks_output        if hasattr(result, 'tasks_output') and result.tasks_output:            print("Tasks Output:")            for task_output in result.tasks_output:                print(f"Task Name: {task_output.name}")                if task_output.name == "industry_analysis_task":                    return task_output.raw            print("Task 'review_and_edit_job_posting_task' not found in tasks_output.")        else:            print("No 'tasks_output' attribute found or it's empty.")    except Exception as e:        print(f"Error occurred: {e}")        return None

The `run_job_posting_workflow` function is invoked by the Job Description Agentâ€™s webhook to handle incoming requests and generate the job description.

Job Description Agent Script[â€‹](#job-description-agent-script "Direct link to Job Description Agent Script")
------------------------------------------------------------------------------------------------------------

A Job Description Agent is a specialized CrewAI agent that creates detailed job descriptions based on:

*   **Company Domain**
*   **Company Description**
*   **Hiring Needs**
*   **Specific Benefits**

It registers itself with [**Agentverse**](https://agentverse.ai/), processes incoming requests (via a Flask webhook), and sends the generated output back to the requester.

### Script Breakdown (JD\_Agent.py)[â€‹](#script-breakdown-jd_agentpy "Direct link to Script Breakdown (JD_Agent.py)")

**Importing Required Libraries**

The script imports essential libraries for agent registration, communication, and Flask-based HTTP handling:

    import osimport loggingimport asynciofrom uagents_core.crypto import Identityfrom flask import Flask, request, jsonifyfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentfrom main import run_job_posting_workflow

**Setting Up Flask and Logging**

The script initializes Flask for handling webhooks and sets up logging to monitor activities:

    # Configure logginglogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)# Flask app for webhookflask_app = Flask(__name__)# Identity for the agentjd_agent_identity = Nonecontent = ''

**Webhook Endpoint**

The /webhook endpoint handles incoming requests and processes inputs to generate a job description:

    @flask_app.route('/webhook', methods=['POST'])async def webhook():    global content    global jd_agent_identity    try:        # Parse the incoming message        data = request.get_data().decode('utf-8')        message = parse_message_from_agent(data)        company_domain = message.payload.get("company_domain", "")        company_description = message.payload.get("company_description", "")        hiring_needs = message.payload.get("hiring_needs", "")        specific_benefits = message.payload.get("specific_benefits", "")        agent_address = message.sender        # Run the job posting workflow        content = await run_job_posting_workflow(            company_domain=company_domain,            company_description=company_description,            hiring_needs=hiring_needs,            specific_benefits=specific_benefits        )        # Send the generated content back to the requesting agent        payload = {'content': content}        send_message_to_agent(jd_agent_identity, agent_address, payload)        return jsonify({"status": "job_description_sent"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"status": "error", "message": str(e)}), 500

**Registering the Agent**

The init\_agent function registers the Job Description Agent with Fetch.ai's Agentverse, providing metadata such as agent title, description, and use cases:

    def init_agent():    global jd_agent_identity    try:        jd_agent_identity = Identity.from_seed("Job Description Agent Seed", 0)        register_with_agentverse(            identity=jd_agent_identity,            url="http://localhost:5008/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="Job Description Creation Agent",            readme="""                <description>Job Description creator to create JD according to the company's career website and description using CrewAI.</description>                <use_cases>                    <use_case>Generates job descriptions for specified roles.</use_case>                </use_cases>                <payload_requirements>                    <description>Expects company domain, description, hiring needs, and specific benefits.</description>                    <payload>                        <requirement>                            <parameter>company_domain</parameter>                            <description>Career Page for the company</description>                            <parameter>company_description</parameter>                            <description>Description of the company</description>                            <parameter>hiring_needs</parameter>                            <description>Role for which the person is needed</description>                            <parameter>specific_benefits</parameter>                            <description>Benefits offered with the role</description>                        </requirement>                    </payload>                </payload_requirements>            """        )        logger.info("Job Description Creator Agent registered successfully!")    except Exception as e:        logger.error(f"Error initializing agent: {e}")        raise

**Running the Agent**

The script starts the Flask server on port 5008:

    if __name__ == "__main__":    init_agent()    flask_app.run(host="0.0.0.0", port=5008, debug=True)

User Agent Script (user.py)[â€‹](#user-agent-script-userpy "Direct link to User Agent Script (user.py)")
------------------------------------------------------------------------------------------------------

The **User Agent** serves as a client. It registers itself with Agentverse, can search for existing agents, and forwards relevant data (e.g., company info, role) to the Job Description Agent. Afterwards, it retrieves the response and saves it locally.

### Script Breakdown[â€‹](#script-breakdown "Direct link to Script Breakdown")

**Importing Required Libraries**

The script imports libraries for Flask-based HTTP handling, Fetch.ai's identity and communication utilities, and environment variable management:

    from flask_cors import CORSfrom flask import Flask, request, jsonifyfrom uagents_core.crypto import Identityfrom fetchai import fetchfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osfrom dotenv import load_dotenv

**Setting Up Flask and Logging**

Logging and Flask are initialised to handle web requests and log agent activities:

    # Configure logginglogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app)# Global variables for client identity and agent responseclient_identity = Noneagent_response = None

**Initialising the User Agent**

The init\_client function registers the User Agent with Fetch.ai's Agentverse, providing metadata such as the agent's purpose and use cases:

    def init_client():    """Initialize and register the client agent."""    global client_identity    try:        # Load the client identity from environment variables        client_identity = Identity.from_seed("User for Testing JD Agents", 0)        logger.info(f"Client agent started with address: {client_identity.address}")        # Register the agent with Agentverse        register_with_agentverse(            identity=client_identity,            url="http://localhost:5002/api/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="JD User Testing Agent",            readme="""                <description>Frontend client that interacts with JD agents to fetch job descriptions.</description>                <use_cases>                    <use_case>Searches for agents and interacts with them.</use_case>                </use_cases>                <payload_requirements>                    <description>Handles responses related to job descriptions.</description>                    <payload>                        <requirement>                            <parameter>field</parameter>                            <description>Data required for job description creation</description>                        </requirement>                    </payload>                </payload_requirements>            """        )        logger.info("Client agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise

**Searching for Agents**

The /api/search-agents endpoint allows the User Agent to search for agents in Fetch.ai's Agentverse/Almanac Contract based on a query:

    @app.route('/api/search-agents', methods=['GET'])def search_agents():    """Search for available agents based on user input."""    try:        # Extract the query from the request        user_query = request.args.get('query', '')        if not user_query:            return jsonify({"error": "Query parameter 'query' is required."}), 400        # Search agents in Agentverse/almanac contract        available_ais = fetch.ai(user_query)        agents = available_ais.get('ais', [])        # Format the agent data        extracted_data = [            {'name': agent.get('name'), 'address': agent.get('address')}            for agent in agents        ]        return jsonify(extracted_data), 200    except Exception as e:        logger.error(f"Error finding agents: {e}")        return jsonify({"error": str(e)}), 500

**Sending Data to Another Agent**

The /api/send-data endpoint forwards input data (such as company details and hiring needs) to the selected Job Description Agent:

    @app.route('/api/send-data', methods=['POST'])def send_data():    """Send payload to the selected agent based on the provided address."""    global agent_response    agent_response = None    try:        # Parse the request payload        data = request.json        payload = data.get('payload')  # Extract the payload dictionary        agent_address = data.get('agentAddress')  # Extract the agent address        # Validate input data        if not payload or not agent_address:            return jsonify({"error": "Missing payload or agent address"}), 400        # Send the payload to the agent        send_message_to_agent(client_identity, agent_address, payload)        logger.info(f"Payload sent to agent: {agent_address}")        return jsonify({"status": "request_sent", "agent_address": agent_address, "payload": payload})    except Exception as e:        logger.error(f"Error sending data to agent: {e}")        return jsonify({"error": str(e)}), 500

**Retrieving the Agent's Response**

The /api/get-response endpoint retrieves the response (job description) from the Job Description Agent and saves it as a markdown file:

    @app.route('/api/get-response', methods=['GET'])def get_response():    """Fetch the response from the agent and save it to a file."""    global agent_response    try:        if agent_response:            response = agent_response            agent_response = None  # Clear the response after fetching            # Save the response to a markdown file            file_path = os.path.join(os.getcwd(), "jd_response.md")            with open(file_path, "w", encoding="utf-8") as md_file:                md_file.write(response.get("content", ""))            logger.info(f"Markdown file created at {file_path}")            return jsonify({"status": "file_created", "file_path": file_path})        else:            return jsonify({"error": "No response available"}), 404    except Exception as e:        logger.error(f"Error creating markdown file: {e}")        return jsonify({"error": str(e)}), 500

**Webhook Endpoint**

The webhook endpoint processes messages sent by the Job Description Agent and stores the response globally:

    @app.route('/api/webhook', methods=['POST'])def webhook():    """Handle incoming messages from the Job Description Agent."""    global agent_response    try:        # Parse the incoming message        data = request.get_data().decode("utf-8")        logger.info("Received response")        message = parse_message_from_agent(data)        agent_response = message.payload        logger.info(f"Processed response: {agent_response}")        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500

**Running the User Agent**

The Flask server runs on port 5002 to handle requests:

    def start_server():    """Start the Flask server."""    try:        # Load environment variables        load_dotenv()        init_client()        app.run(host="0.0.0.0", port=5002)    except Exception as e:        logger.error(f"Server error: {e}")        raiseif __name__ == "__main__":    start_server()

Step-by-Step Execution[â€‹](#step-by-step-execution "Direct link to Step-by-Step Execution")
------------------------------------------------------------------------------------------

**1\. Environment Variables**

Update '.env' with `SERPER_API_KEY`, `OPENAI_API_KEY`, and `AGENTVERSE_API_KEY`.

**2\. Start the Job Description Agent**

*   Registers the agent on Agentverse.
*   Launches JD Agent with Flask on port 5008.

**3\. Start the User Agent**

*   Registers the agent on Agentverse.
*   Launches Flask on port 5002.

### Interacting with the Agents[â€‹](#interacting-with-the-agents "Direct link to Interacting with the Agents")

**Search for Agents**

To list agents in the Agentverse:

    curl -X GET "http://localhost:5002/api/search-agents?query=I%20want%20to%20write%20job%20description"

**Sample Output**

    [    {        "name": "Job Description Creation Agent",        "address": "agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae"    },    ...]

**Send Data to the Job Description Agent**

Send JD details from the User Agent to the JD Agent:

    curl -X POST "http://localhost:5002/api/send-data" \-H "Content-Type: application/json" \-d '{    "payload": {        "company_domain": "careers.wbd.com",        "company_description": "Warner Bros. Discovery is a premier global media and entertainment company, offering audiences the world\u2019s most differentiated and complete portfolio of content, brands and franchises across television, film, sports, news, streaming and gaming. We\u2019re home to the world\u2019s best storytellers, creating world-class products for consumers.",        "hiring_needs": "Production Assistant, for a TV production set in Los Angeles in June 2025",        "specific_benefits": "Weekly Pay, Employee Meals, healthcare"    },    "agentAddress": "agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae"}'

**Sample Output**

    {    "status": "request_sent",    "agent_address": "agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae",    "payload": {        "company_domain": "careers.wbd.com",        "company_description": "Warner Bros. Discovery is a premier global media and entertainment company...",        "hiring_needs": "Production Assistant, for a TV production set in Los Angeles in June 2025",        "specific_benefits": "Weekly Pay, Employee Meals, healthcare"    }}

**Retrieve the Agentâ€™s Response**

Fetch the JD document response from the User Agent:

    curl -X GET "http://localhost:5002/api/get-response"

**Sample Output**

*   Stores the Job Description result in `jd_response.md`.

    {    "status": "file_created",    "file_path": "/path/to/jd_response.md"}

You can open the [jd\_response.md](https://drive.google.com/file/d/12oHIKeOIAjVs8dxP-xg9SmtWR4JnTbqx/view?usp=sharing) file to review the generated job description.

You now have **CrewAI agents** integrated with the **Fetch.ai Agentverse**, collaborating to create custom job descriptions. Feel free to adapt the main.py workflow, agent scripts, or user scripts to suit your own requirementsâ€”whether youâ€™re refining AI logic, adding advanced search filters, or customizing the final job posting format.</content>
</page>

<page>
  <title>CrewAI Agent | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/examples/other-frameworks/crewai</url>
  <content>Creating and Registering CrewAI based Job descriptions creator
--------------------------------------------------------------

This documentation explains how to build **CrewAI** agents that generate tailored job descriptions, register them on **Fetch.aiâ€™s Agentverse**, and enable collaboration between agents for dynamic data sharing. Below, you'll see the **main.py** script containing the workflow logic `run_job_posting_workflow`, as well as sample scripts for a **Job Description Agent** and a **User Agent**.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

### Purpose[â€‹](#purpose "Direct link to Purpose")

This project showcases:

*   **Creating CrewAI agents** for complex tasks (e.g., analyzing company requirements and generating job descriptions).
*   **Integrating with Fetch.aiâ€™s Agentverse**, enabling seamless communication and data exchange.
*   **Using autonomous agents** to streamline job description creation while aligning with company culture and specific role requirements.

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

### Environment Setup[â€‹](#environment-setup "Direct link to Environment Setup")

To begin creating and registering CrewAI agents, ensure you have the following:

*   **Python 3.10+** or higher
*   A **virtual environment** (recommended)
*   **Fetch.ai SDK** for agent creation and registration
*   **CrewAI Framework** for defining tasks and workflows

### Key Components[â€‹](#key-components "Direct link to Key Components")

*   **Job Description Agent**: A specialized CrewAI agent that processes input (company domain, hiring needs, etc.) to generate a job description.
*   **User Agent**: Acts as a client to discover and interact with the Job Description Agent.
*   **Agent Collaboration**: Multiple agents communicate via Agentverse, orchestrating tasks through CrewAI workflows.

### Architecture Diagram[â€‹](#architecture-diagram "Direct link to Architecture Diagram")

### Environment Variables[â€‹](#environment-variables "Direct link to Environment Variables")

Create a `.env` file with your API keys:

    SERPER_API_KEY=<your_serper_api_key>OPENAI_API_KEY=<your_openai_api_key>AGENTVERSE_API_KEY=<your_agentverse_api_key>

Replace the placeholders with your actual keys.

*   [OPENAI\_API\_KEY](https://openai.com/index/openai-api/)
*   [SERPER\_API\_KEY](https://serper.dev/)
*   [AGENTVERSE\_API\_KEY](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key)

Project Directory & Configuration Files[â€‹](#project-directory--configuration-files "Direct link to Project Directory & Configuration Files")
--------------------------------------------------------------------------------------------------------------------------------------------

For a clean, maintainable setup, we recommend the following structure:

    crewai_job_descriptions/â”œâ”€â”€ .envâ”œâ”€â”€ config/â”‚   â”œâ”€â”€ agents.yamlâ”‚   â””â”€â”€ tasks.yamlâ”œâ”€â”€ crew.pyâ”œâ”€â”€ jd_agent.pyâ”œâ”€â”€ main.pyâ”œâ”€â”€ requirements.txtâ”œâ”€â”€ user_agent.pyâ”œâ”€â”€ job_description_example.mdâ””â”€â”€ README.md

**1\. requirements.txt**

**Purpose**: Declares project dependencies so you (or anyone) can install them quickly with `pip install -r requirements.txt`.

Create a file called `requirements.txt` at the root of your project (alongside `jd_agent.py` and `main.py`). Example contents:

    flask==2.2.5flask-cors==3.0.10fetchai==0.16.3crewai==0.100.1crewai_tools==0.33.0python-dotenv==1.0.0

Install everything at once via:

    pip install -r requirements.txt

**2\. Crew.py**

**Purpose:** Defines your `JobPostingCrew` class, specifying the Agents and Tasks.

Place `crew.py` at the root directory. It contains the code:

    from typing import Listfrom crewai import Agent, Crew, Process, Taskfrom crewai.project import CrewBase, agent, crew, task# Check our tools documentations for more information on how to use themfrom crewai_tools import SerperDevTool, ScrapeWebsiteTool, WebsiteSearchTool, FileReadToolfrom pydantic import BaseModel, Fieldweb_search_tool = WebsiteSearchTool()seper_dev_tool = SerperDevTool()file_read_tool = FileReadTool(    file_path='./job_description_example.md',    description='A tool to read the job description example file.')class ResearchRoleRequirements(BaseModel):    """Research role requirements model"""    skills: List[str] = Field(..., description="List of recommended skills ...")    experience: List[str] = Field(..., description="List of recommended experience ...")    qualities: List[str] = Field(..., description="List of recommended qualities ...")@CrewBaseclass JobPostingCrew:    """JobPosting crew"""    agents_config = 'config/agents.yaml'    tasks_config = 'config/tasks.yaml'    @agent    def research_agent(self) -> Agent:        return Agent(            config=self.agents_config['research_agent'],            tools=[web_search_tool, seper_dev_tool],            verbose=True        )        @agent    def writer_agent(self) -> Agent:        return Agent(            config=self.agents_config['writer_agent'],            tools=[web_search_tool, seper_dev_tool, file_read_tool],            verbose=True        )        @agent    def review_agent(self) -> Agent:        return Agent(            config=self.agents_config['review_agent'],            tools=[web_search_tool, seper_dev_tool, file_read_tool],            verbose=True        )        @task    def research_company_culture_task(self) -> Task:        return Task(            config=self.tasks_config['research_company_culture_task'],            agent=self.research_agent()        )    @task    def research_role_requirements_task(self) -> Task:        return Task(            config=self.tasks_config['research_role_requirements_task'],            agent=self.research_agent(),            output_json=ResearchRoleRequirements        )    @task    def draft_job_posting_task(self) -> Task:        return Task(            config=self.tasks_config['draft_job_posting_task'],            agent=self.writer_agent()        )    @task    def review_and_edit_job_posting_task(self) -> Task:        return Task(            config=self.tasks_config['review_and_edit_job_posting_task'],            agent=self.review_agent()        )    @task    def industry_analysis_task(self) -> Task:        return Task(            config=self.tasks_config['industry_analysis_task'],            agent=self.research_agent()        )    @crew    def crew(self) -> Crew:        """Creates the JobPostingCrew"""        return Crew(            agents=self.agents,  # Automatically created by the @agent decorator            tasks=self.tasks,    # Automatically created by the @task decorator            process=Process.sequential,            verbose=True,        )

**3\. config/agents.yaml**

**Purpose**: Specifies each agentâ€™s role, goal, and backstory. Create a folder named `config/` at your project root. Inside it, **create** `agents.yaml`:

    research_agent:  role: >    Research Analyst  goal: >    Analyze the company website and provided description...  backstory: >    Expert in analyzing company cultures and identifying key values...writer_agent:  role: >    Job Description Writer  goal: >    Use insights from the Research Analyst to create...  backstory: >    Skilled in crafting compelling job descriptions...review_agent:  role: >    Review and Editing Specialist  goal: >    Review the job posting for clarity, engagement...  backstory: >    A meticulous editor ensuring every piece of content is polished...

**4\. config/tasks.yaml**

**Purpose**: Defines the text prompts (description) and expected output for each task. In the same `config/` folder, make a second file called `tasks.yaml`:

    research_company_culture_task:  description: >    Analyze {company_domain} and {company_description}...  expected_output: >    A comprehensive report detailing the company's culture...research_role_requirements_task:  description: >    Based on {hiring_needs}, list recommended skills, experience...  expected_output: >    A list of recommended skills, experiences, and qualities...draft_job_posting_task:  description: >    Draft a job posting for {hiring_needs}, using {specific_benefits}...  expected_output: >    A detailed, engaging job posting that includes an introduction...review_and_edit_job_posting_task:  description: >    Review the job posting for {hiring_needs} for clarity...  expected_output: >    A polished, error-free job posting with final approval in markdown.industry_analysis_task:  description: >    Conduct an in-depth analysis of {company_domain}'s industry...  expected_output: >    A detailed analysis highlighting major industry trends, opportunities...

Main.py (Workflow Definition)[â€‹](#mainpy-workflow-definition "Direct link to Main.py (Workflow Definition)")
------------------------------------------------------------------------------------------------------------

Below is an example `main.py` script containing the `run_job_posting_workflow` function that the Job Description Agent calls to generate job descriptions. It uses **Crew** to structure tasks and (optionally) a local SQLite database for storage or logging.

    import sysfrom crew import JobPostingCrewimport sqlite3async def run_job_posting_workflow(company_domain, company_description, hiring_needs, specific_benefits):    """    Executes the job posting workflow with proper connection handling and debugging output.        Args:        company_domain (str): The company's domain.        company_description (str): A description of the company.        hiring_needs (str): Description of the role being hired for.        specific_benefits (str): Specific benefits offered for the role.        Returns:        str: The raw output of the 'industry_analysis_task' if found, else None.    """    try:        # Initialize the JobPostingCrew        job_posting_crew = JobPostingCrew().crew()                # Define the inputs        inputs = {            'company_domain': company_domain,            'company_description': company_description,            'hiring_needs': hiring_needs,            'specific_benefits': specific_benefits,        }        # Execute the job posting process        result = job_posting_crew.kickoff(inputs=inputs)        # Debugging output        print("Result type:", type(result))        print("Attributes:", dir(result))        # Attempt to retrieve and print tasks_output        if hasattr(result, 'tasks_output') and result.tasks_output:            print("Tasks Output:")            for task_output in result.tasks_output:                print(f"Task Name: {task_output.name}")                if task_output.name == "industry_analysis_task":                    return task_output.raw            print("Task 'review_and_edit_job_posting_task' not found in tasks_output.")        else:            print("No 'tasks_output' attribute found or it's empty.")    except Exception as e:        print(f"Error occurred: {e}")        return None

The `run_job_posting_workflow` function is invoked by the Job Description Agentâ€™s webhook to handle incoming requests and generate the job description.

Job Description Agent Script[â€‹](#job-description-agent-script "Direct link to Job Description Agent Script")
------------------------------------------------------------------------------------------------------------

A Job Description Agent is a specialized CrewAI agent that creates detailed job descriptions based on:

*   **Company Domain**
*   **Company Description**
*   **Hiring Needs**
*   **Specific Benefits**

It registers itself with [**Agentverse**](https://agentverse.ai/), processes incoming requests (via a Flask webhook), and sends the generated output back to the requester.

### Script Breakdown (JD\_Agent.py)[â€‹](#script-breakdown-jd_agentpy "Direct link to Script Breakdown (JD_Agent.py)")

**Importing Required Libraries**

The script imports essential libraries for agent registration, communication, and Flask-based HTTP handling:

    import osimport loggingimport asynciofrom uagents_core.crypto import Identityfrom flask import Flask, request, jsonifyfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentfrom main import run_job_posting_workflow

**Setting Up Flask and Logging**

The script initializes Flask for handling webhooks and sets up logging to monitor activities:

    # Configure logginglogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)# Flask app for webhookflask_app = Flask(__name__)# Identity for the agentjd_agent_identity = Nonecontent = ''

**Webhook Endpoint**

The /webhook endpoint handles incoming requests and processes inputs to generate a job description:

    @flask_app.route('/webhook', methods=['POST'])async def webhook():    global content    global jd_agent_identity    try:        # Parse the incoming message        data = request.get_data().decode('utf-8')        message = parse_message_from_agent(data)        company_domain = message.payload.get("company_domain", "")        company_description = message.payload.get("company_description", "")        hiring_needs = message.payload.get("hiring_needs", "")        specific_benefits = message.payload.get("specific_benefits", "")        agent_address = message.sender        # Run the job posting workflow        content = await run_job_posting_workflow(            company_domain=company_domain,            company_description=company_description,            hiring_needs=hiring_needs,            specific_benefits=specific_benefits        )        # Send the generated content back to the requesting agent        payload = {'content': content}        send_message_to_agent(jd_agent_identity, agent_address, payload)        return jsonify({"status": "job_description_sent"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"status": "error", "message": str(e)}), 500

**Registering the Agent**

The init\_agent function registers the Job Description Agent with Fetch.ai's Agentverse, providing metadata such as agent title, description, and use cases:

    def init_agent():    global jd_agent_identity    try:        jd_agent_identity = Identity.from_seed("Job Description Agent Seed", 0)        register_with_agentverse(            identity=jd_agent_identity,            url="http://localhost:5008/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="Job Description Creation Agent",            readme="""                <description>Job Description creator to create JD according to the company's career website and description using CrewAI.</description>                <use_cases>                    <use_case>Generates job descriptions for specified roles.</use_case>                </use_cases>                <payload_requirements>                    <description>Expects company domain, description, hiring needs, and specific benefits.</description>                    <payload>                        <requirement>                            <parameter>company_domain</parameter>                            <description>Career Page for the company</description>                            <parameter>company_description</parameter>                            <description>Description of the company</description>                            <parameter>hiring_needs</parameter>                            <description>Role for which the person is needed</description>                            <parameter>specific_benefits</parameter>                            <description>Benefits offered with the role</description>                        </requirement>                    </payload>                </payload_requirements>            """        )        logger.info("Job Description Creator Agent registered successfully!")    except Exception as e:        logger.error(f"Error initializing agent: {e}")        raise

**Running the Agent**

The script starts the Flask server on port 5008:

    if __name__ == "__main__":    init_agent()    flask_app.run(host="0.0.0.0", port=5008, debug=True)

User Agent Script (user.py)[â€‹](#user-agent-script-userpy "Direct link to User Agent Script (user.py)")
------------------------------------------------------------------------------------------------------

The **User Agent** serves as a client. It registers itself with Agentverse, can search for existing agents, and forwards relevant data (e.g., company info, role) to the Job Description Agent. Afterwards, it retrieves the response and saves it locally.

### Script Breakdown[â€‹](#script-breakdown "Direct link to Script Breakdown")

**Importing Required Libraries**

The script imports libraries for Flask-based HTTP handling, Fetch.ai's identity and communication utilities, and environment variable management:

    from flask_cors import CORSfrom flask import Flask, request, jsonifyfrom uagents_core.crypto import Identityfrom fetchai import fetchfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentimport loggingimport osfrom dotenv import load_dotenv

**Setting Up Flask and Logging**

Logging and Flask are initialised to handle web requests and log agent activities:

    # Configure logginglogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app)# Global variables for client identity and agent responseclient_identity = Noneagent_response = None

**Initialising the User Agent**

The init\_client function registers the User Agent with Fetch.ai's Agentverse, providing metadata such as the agent's purpose and use cases:

    def init_client():    """Initialize and register the client agent."""    global client_identity    try:        # Load the client identity from environment variables        client_identity = Identity.from_seed("User for Testing JD Agents", 0)        logger.info(f"Client agent started with address: {client_identity.address}")        # Register the agent with Agentverse        register_with_agentverse(            identity=client_identity,            url="http://localhost:5002/api/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="JD User Testing Agent",            readme="""                <description>Frontend client that interacts with JD agents to fetch job descriptions.</description>                <use_cases>                    <use_case>Searches for agents and interacts with them.</use_case>                </use_cases>                <payload_requirements>                    <description>Handles responses related to job descriptions.</description>                    <payload>                        <requirement>                            <parameter>field</parameter>                            <description>Data required for job description creation</description>                        </requirement>                    </payload>                </payload_requirements>            """        )        logger.info("Client agent registration complete!")    except Exception as e:        logger.error(f"Initialization error: {e}")        raise

**Searching for Agents**

The /api/search-agents endpoint allows the User Agent to search for agents in Fetch.ai's Agentverse/Almanac Contract based on a query:

    @app.route('/api/search-agents', methods=['GET'])def search_agents():    """Search for available agents based on user input."""    try:        # Extract the query from the request        user_query = request.args.get('query', '')        if not user_query:            return jsonify({"error": "Query parameter 'query' is required."}), 400        # Search agents in Agentverse/almanac contract        available_ais = fetch.ai(user_query)        agents = available_ais.get('ais', [])        # Format the agent data        extracted_data = [            {'name': agent.get('name'), 'address': agent.get('address')}            for agent in agents        ]        return jsonify(extracted_data), 200    except Exception as e:        logger.error(f"Error finding agents: {e}")        return jsonify({"error": str(e)}), 500

**Sending Data to Another Agent**

The /api/send-data endpoint forwards input data (such as company details and hiring needs) to the selected Job Description Agent:

    @app.route('/api/send-data', methods=['POST'])def send_data():    """Send payload to the selected agent based on the provided address."""    global agent_response    agent_response = None    try:        # Parse the request payload        data = request.json        payload = data.get('payload')  # Extract the payload dictionary        agent_address = data.get('agentAddress')  # Extract the agent address        # Validate input data        if not payload or not agent_address:            return jsonify({"error": "Missing payload or agent address"}), 400        # Send the payload to the agent        send_message_to_agent(client_identity, agent_address, payload)        logger.info(f"Payload sent to agent: {agent_address}")        return jsonify({"status": "request_sent", "agent_address": agent_address, "payload": payload})    except Exception as e:        logger.error(f"Error sending data to agent: {e}")        return jsonify({"error": str(e)}), 500

**Retrieving the Agent's Response**

The /api/get-response endpoint retrieves the response (job description) from the Job Description Agent and saves it as a markdown file:

    @app.route('/api/get-response', methods=['GET'])def get_response():    """Fetch the response from the agent and save it to a file."""    global agent_response    try:        if agent_response:            response = agent_response            agent_response = None  # Clear the response after fetching            # Save the response to a markdown file            file_path = os.path.join(os.getcwd(), "jd_response.md")            with open(file_path, "w", encoding="utf-8") as md_file:                md_file.write(response.get("content", ""))            logger.info(f"Markdown file created at {file_path}")            return jsonify({"status": "file_created", "file_path": file_path})        else:            return jsonify({"error": "No response available"}), 404    except Exception as e:        logger.error(f"Error creating markdown file: {e}")        return jsonify({"error": str(e)}), 500

**Webhook Endpoint**

The webhook endpoint processes messages sent by the Job Description Agent and stores the response globally:

    @app.route('/api/webhook', methods=['POST'])def webhook():    """Handle incoming messages from the Job Description Agent."""    global agent_response    try:        # Parse the incoming message        data = request.get_data().decode("utf-8")        logger.info("Received response")        message = parse_message_from_agent(data)        agent_response = message.payload        logger.info(f"Processed response: {agent_response}")        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500

**Running the User Agent**

The Flask server runs on port 5002 to handle requests:

    def start_server():    """Start the Flask server."""    try:        # Load environment variables        load_dotenv()        init_client()        app.run(host="0.0.0.0", port=5002)    except Exception as e:        logger.error(f"Server error: {e}")        raiseif __name__ == "__main__":    start_server()

Step-by-Step Execution[â€‹](#step-by-step-execution "Direct link to Step-by-Step Execution")
------------------------------------------------------------------------------------------

**1\. Environment Variables**

Update '.env' with `SERPER_API_KEY`, `OPENAI_API_KEY`, and `AGENTVERSE_API_KEY`.

**2\. Start the Job Description Agent**

*   Registers the agent on Agentverse.
*   Launches JD Agent with Flask on port 5008.

**3\. Start the User Agent**

*   Registers the agent on Agentverse.
*   Launches Flask on port 5002.

### Interacting with the Agents[â€‹](#interacting-with-the-agents "Direct link to Interacting with the Agents")

**Search for Agents**

To list agents in the Agentverse:

    curl -X GET "http://localhost:5002/api/search-agents?query=I%20want%20to%20write%20job%20description"

**Sample Output**

    [    {        "name": "Job Description Creation Agent",        "address": "agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae"    },    ...]

**Send Data to the Job Description Agent**

Send JD details from the User Agent to the JD Agent:

    curl -X POST "http://localhost:5002/api/send-data" \-H "Content-Type: application/json" \-d '{    "payload": {        "company_domain": "careers.wbd.com",        "company_description": "Warner Bros. Discovery is a premier global media and entertainment company, offering audiences the world\u2019s most differentiated and complete portfolio of content, brands and franchises across television, film, sports, news, streaming and gaming. We\u2019re home to the world\u2019s best storytellers, creating world-class products for consumers.",        "hiring_needs": "Production Assistant, for a TV production set in Los Angeles in June 2025",        "specific_benefits": "Weekly Pay, Employee Meals, healthcare"    },    "agentAddress": "agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae"}'

**Sample Output**

    {    "status": "request_sent",    "agent_address": "agent1qdjqpsusql3nlvr6hnrhhru5tmytz3yxsephvwxgjerx7qetdv5usuevcae",    "payload": {        "company_domain": "careers.wbd.com",        "company_description": "Warner Bros. Discovery is a premier global media and entertainment company...",        "hiring_needs": "Production Assistant, for a TV production set in Los Angeles in June 2025",        "specific_benefits": "Weekly Pay, Employee Meals, healthcare"    }}

**Retrieve the Agentâ€™s Response**

Fetch the JD document response from the User Agent:

    curl -X GET "http://localhost:5002/api/get-response"

**Sample Output**

*   Stores the Job Description result in `jd_response.md`.

    {    "status": "file_created",    "file_path": "/path/to/jd_response.md"}

You can open the [jd\_response.md](https://drive.google.com/file/d/12oHIKeOIAjVs8dxP-xg9SmtWR4JnTbqx/view?usp=sharing) file to review the generated job description.

You now have **CrewAI agents** integrated with the **Fetch.ai Agentverse**, collaborating to create custom job descriptions. Feel free to adapt the main.py workflow, agent scripts, or user scripts to suit your own requirementsâ€”whether youâ€™re refining AI logic, adding advanced search filters, or customizing the final job posting format.</content>
</page>

<page>
  <title>LangGraph | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/next/examples/other-frameworks/financial-analysis-ai-agent</url>
  <content>Creating and Registering LangGraph based Financial Analysis Agent
-----------------------------------------------------------------

This documentation explains how to build **LangGraph** agents that perform comprehensive financial analysis, register them on **Fetch.ai's Agentverse**, and enable collaboration between specialized agents for dynamic data analysis. Below, you'll see the **main.py** script containing the workflow logic `run_financial_analysis_workflow`, as well as specialized agent implementations for financial analysis.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

### Purpose[â€‹](#purpose "Direct link to Purpose")

This project showcases:

*   **Creating LangGraph agents** for complex tasks (e.g., analyzing SEC filings, market research, and financial metrics).
*   **Integrating with Fetch.ai's Agentverse**, enabling seamless communication and data exchange.
*   **Using autonomous agents** to streamline financial analysis while combining multiple data sources and expert analysis.

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

### Environment Setup[â€‹](#environment-setup "Direct link to Environment Setup")

To begin creating and registering LangGraph agents, ensure you have the following:

*   **Python 3.10+** or higher
*   A **virtual environment** (recommended)
*   **Fetch.ai SDK** for agent creation and registration
*   **LangGraph Framework** for defining states and workflows
*   **LangChain** for agent tools and chains

### Key Components[â€‹](#key-components "Direct link to Key Components")

*   **SEC Analysis Agent**: A specialized agent that processes SEC filings and extracts financial metrics
*   **Search Agent**: Gathers real-time market data and analyst opinions
*   **Supervisor Agent**: Coordinates analysis flow and combines insights
*   **Agent Collaboration**: Multiple agents work together via LangGraph state management

### Architecture Diagram[â€‹](#architecture-diagram "Direct link to Architecture Diagram")

### Environment Variables[â€‹](#environment-variables "Direct link to Environment Variables")

Create a `.env` file with your API keys:

    OPENAI_API_KEY=<your_openai_api_key>TAVILY_API_KEY=<your_tavily_api_key>AGENTVERSE_API_KEY=<your_agentverse_api_key>

Replace the placeholders with your actual keys:

*   [OPENAI\_API\_KEY](https://openai.com/api/)
*   [TAVILY\_API\_KEY](https://tavily.com/)
*   [AGENTVERSE\_API\_KEY](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key)

Project Directory & Configuration Files[â€‹](#project-directory--configuration-files "Direct link to Project Directory & Configuration Files")
--------------------------------------------------------------------------------------------------------------------------------------------

For a clean, maintainable setup, we recommend the following structure:

    financial_analysis_agent/â”œâ”€â”€ .envâ”œâ”€â”€ src/â”‚   â”œâ”€â”€ agents/â”‚   â”‚   â”œâ”€â”€ __init__.pyâ”‚   â”‚   â”œâ”€â”€ search_agent.py      # Market research specialistâ”‚   â”‚   â”œâ”€â”€ sec_agent.py         # SEC filings specialistâ”‚   â”‚   â””â”€â”€ supervisor.py        # Team coordinatorâ”‚   â”œâ”€â”€ tools/â”‚   â”‚   â”œâ”€â”€ __init__.pyâ”‚   â”‚   â”œâ”€â”€ search.py            # Tavily search implementationâ”‚   â”‚   â””â”€â”€ analysis.py          # RAG implementationâ”‚   â”œâ”€â”€ rag/â”‚   â”‚   â”œâ”€â”€ __init__.pyâ”‚   â”‚   â”œâ”€â”€ chain.py             # RAG chain setupâ”‚   â”‚   â””â”€â”€ loader.py            # Document processingâ”‚   â”œâ”€â”€ graph/â”‚   â”‚   â”œâ”€â”€ __init__.pyâ”‚   â”‚   â””â”€â”€ state.py             # State managementâ”‚   â””â”€â”€ utils/â”‚       â”œâ”€â”€ __init__.pyâ”‚       â””â”€â”€ helpers.py           # Helper functionsâ”œâ”€â”€ main.py                      # Main workflowâ”œâ”€â”€ register.py                  # Agentverse registrationâ”œâ”€â”€ requirements.txtâ””â”€â”€ README.md

Core Implementation Files[â€‹](#core-implementation-files "Direct link to Core Implementation Files")
---------------------------------------------------------------------------------------------------

**1\. requirements.txt**

**Purpose**: Declares project dependencies for installing all necessary packages quickly with `pip install -r requirements.txt`.

Create a file called `requirements.txt` at the root of your project. Example contents:

    langchain==0.1.0langchain-core==0.1.10langgraph==0.0.10fetchai-sdk==0.16.3flask==2.2.5flask-cors==3.0.10python-dotenv==1.0.0tavily-python==0.2.6qdrant-client==1.7.0tiktoken==0.5.2

Install everything at once via:

    pip install -r requirements.txt

**2\. State Management (src/graph/state.py)**

**Purpose**: Defines the state structure and management for the research team's workflow.

Place `state.py` in the `src/graph` directory. It contains:

    from typing import TypedDict, List, Annotatedfrom langchain_core.messages import BaseMessageimport operatorclass ResearchTeamState(TypedDict):    """State structure for research team coordination."""    messages: Annotated[List[BaseMessage], operator.add]  # Conversation history    team_members: List[str]                              # Available agents    next: str                                           # Next agent to act    information_needed: List[str]                       # Required information    reasoning: str                                      # Decision reasoningdef create_initial_state(query: str) -> ResearchTeamState:    """Create initial state from user query."""    return {        "messages": [HumanMessage(content=query)],        "team_members": ["Search", "SECAnalyst"],        "next": "",        "information_needed": [],        "reasoning": ""    }def update_state(state: ResearchTeamState, agent_response: dict) -> ResearchTeamState:    """Update state with agent response."""    new_state = state.copy()    new_state["messages"].extend(agent_response["messages"])    return new_state

**3\. Tools Implementation (src/tools)**

**Purpose**: Implements specialized tools for market research and SEC filing analysis.

#### A. Search Tool (search.py)[â€‹](#a-search-tool-searchpy "Direct link to A. Search Tool (search.py)")

    from typing import Annotatedfrom langchain_core.tools import toolfrom tavily import TavilyClientimport os@tooldef tavily_search(query: str) -> str:    """Search for real-time market information."""    try:        client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))        result = client.search(query)        return str(result)    except Exception as e:        return f"Error performing search: {str(e)}"

#### B. Analysis Tool (analysis.py)[â€‹](#b-analysis-tool-analysispy "Direct link to B. Analysis Tool (analysis.py)")

    from typing import Annotatedfrom langchain_core.tools import toolfrom ..rag.chain import create_rag_chain@tooldef retrieve_information(query: Annotated[str, "query to analyze financial documents"]) -> str:    """Analyze SEC filings using RAG."""    try:        rag_chain = create_rag_chain()        return rag_chain.invoke(query)    except Exception as e:        return f"Error analyzing documents: {str(e)}"

**4\. RAG Implementation (src/rag)**

**Purpose**: Handles document processing and retrieval for SEC filing analysis.

#### A. Document Loader (loader.py)[â€‹](#a-document-loader-loaderpy "Direct link to A. Document Loader (loader.py)")

    class DocumentLoader:    def __init__(self, file_path: str):        self.file_path = file_path        @staticmethod    def tiktoken_len(text):        tokens = tiktoken.encoding_for_model("gpt-4").encode(text)        return len(tokens)        def load_and_split(self):        """Load and chunk documents for processing."""        docs = PyMuPDFLoader(self.file_path).load()        splitter = RecursiveCharacterTextSplitter(            chunk_size=300,            chunk_overlap=0,            length_function=self.tiktoken_len        )        return splitter.split_documents(docs)

#### B. RAG Chain (chain.py)[â€‹](#b-rag-chain-chainpy "Direct link to B. RAG Chain (chain.py)")

    def create_rag_chain(file_path: str = "data/raw/apple_10k.pdf"):    """Create RAG chain for SEC filing analysis."""    # Initialize document processing    loader = DocumentLoader(file_path)    chunks = loader.load_and_split()        # Set up vector store    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")    vectorstore = Qdrant.from_documents(        chunks,        embeddings,        location=":memory:",        collection_name="sec_filings"    )        # Create retrieval chain    template = """Use the context to answer financial questions.    Context: {context}    Question: {question}    Answer with specific numbers and data when available."""        prompt = ChatPromptTemplate.from_template(template)    chain = (        {"context": vectorstore.as_retriever(), "question": RunnablePassthrough()}        | prompt        | ChatOpenAI(model="gpt-4-turbo-preview")        | StrOutputParser()    )        return chain

**5\. Helper Functions (src/utils/helpers.py)**

**Purpose**: Provides utility functions for agent creation and node management.

    def create_agent(llm: ChatOpenAI, tools: list, system_prompt: str) -> AgentExecutor:    """Create a specialized agent with tools and prompt."""    try:        system_prompt += (            "\nWork autonomously using your tools."            " Do not ask for clarification."            " Your team members will help with their specialties."        )                prompt = ChatPromptTemplate.from_messages([            ("system", system_prompt),            MessagesPlaceholder(variable_name="messages"),            MessagesPlaceholder(variable_name="agent_scratchpad"),        ])                agent = create_openai_functions_agent(llm, tools, prompt)        return AgentExecutor(agent=agent, tools=tools)            except Exception as e:        logger.error(f"Error creating agent: {e}")        raisedef agent_node(state, agent, name):    """Create agent node for the graph."""    try:        if "information_needed" in state:            message_content = f"""Information needed:            {', '.join(state['information_needed'])}            Query: {state['messages'][-1].content}"""            state['messages'][-1] = HumanMessage(content=message_content)        result = agent.invoke(state)        return {            "messages": [                HumanMessage(content=result["output"], name=name)            ]        }    except Exception as e:        logger.error(f"Error in agent node {name}: {e}")        raise

Financial Analysis Agent Implementation[â€‹](#financial-analysis-agent-implementation "Direct link to Financial Analysis Agent Implementation")
---------------------------------------------------------------------------------------------------------------------------------------------

### A. SEC Analysis Agent (src/agents/sec\_agent.py)[â€‹](#a-sec-analysis-agent-srcagentssec_agentpy "Direct link to A. SEC Analysis Agent (src/agents/sec_agent.py)")

A specialized agent that processes SEC filings and financial documents. It uses RAG to analyze documents and extract relevant financial metrics.

    # src/agents/sec_agent.pyfrom langchain.chat_models import ChatOpenAIfrom ..tools.analysis import retrieve_informationdef create_sec_agent(llm: ChatOpenAI):    """Creates an agent specialized in SEC filings analysis."""        system_prompt = """You are a financial analyst specialized in SEC filings analysis.    After analyzing SEC filings:    1. If you need market context, clearly state what specific market data you need    2. If numbers need industry comparison, explicitly request competitor data    3. Always include specific numbers and trends from the filings    4. If you spot significant changes or unusual patterns, highlight them        Format your response as:    1. Data from SEC Filings: [your findings]    2. Additional Context Needed: [if any]    3. Analysis: [your insights]    """        return create_agent(        llm=llm,        tools=[retrieve_information],        system_prompt=system_prompt    )

### B. Search Agent (src/agents/search\_agent.py)[â€‹](#b-search-agent-srcagentssearch_agentpy "Direct link to B. Search Agent (src/agents/search_agent.py)")

Handles real-time market research and data gathering using external search tools.

    # src/agents/search_agent.pyfrom langchain.chat_models import ChatOpenAIfrom ..tools.search import tavily_searchdef create_search_agent(llm: ChatOpenAI):    """Creates a search agent specialized in market research."""        system_prompt = """You are a research assistant who can search for up-to-date     financial information using the tavily search engine.        When responding:    1. Always cite sources    2. Focus on recent market data and analyst reports    3. If SEC data is mentioned, compare it with current market views    4. Highlight any significant discrepancies with official filings        Format your response as:    1. Market Data: [your findings]    2. Analyst Views: [key opinions]    3. Relevance to SEC Data: [if applicable]    """        return create_agent(        llm=llm,        tools=[tavily_search],        system_prompt=system_prompt    )

### C. Supervisor Agent (src/agents/supervisor.py)[â€‹](#c-supervisor-agent-srcagentssupervisorpy "Direct link to C. Supervisor Agent (src/agents/supervisor.py)")

Coordinates between agents and manages the analysis workflow.

    # src/agents/supervisor.pydef create_supervisor_agent(llm: ChatOpenAI):    """Creates the supervisor agent for coordinating analysis."""        function_def = {        "name": "route",        "description": "Select the next role based on query analysis.",        "parameters": {            "title": "routeSchema",            "type": "object",            "properties": {                "next": {                    "title": "Next",                    "anyOf": [{"enum": ["Search", "SECAnalyst", "FINISH"]}],                },                "reasoning": {                    "title": "Reasoning",                    "type": "string",                    "description": "Explanation for why this agent should act next"                },                "information_needed": {                    "title": "Information Needed",                    "type": "array",                    "items": {"type": "string"},                    "description": "List of specific information needed from this agent"                }            },            "required": ["next", "reasoning", "information_needed"],        },    }    prompt = ChatPromptTemplate.from_messages([        ("system", """You are a financial research team supervisor.        Your role is to:        1. Analyze incoming queries        2. Determine what information is needed        3. Choose the appropriate agent for each task        4. Coordinate between agents        5. Ensure comprehensive analysis"""),        MessagesPlaceholder(variable_name="messages"),        ("system", "Who should act next? Consider available information and agent specialties.")    ])    return (        prompt        | llm.bind_functions(functions=[function_def], function_call="route")        | JsonOutputFunctionsParser()    )

Main Workflow Implementation (main.py)[â€‹](#main-workflow-implementation-mainpy "Direct link to Main Workflow Implementation (main.py)")
---------------------------------------------------------------------------------------------------------------------------------------

The main script that initializes the LangGraph workflow and handles financial analysis requests.

    import osfrom dotenv import load_dotenvfrom src.rag.chain import create_rag_chainfrom src.graph.state import create_research_graphdef init_financial_system():    """Initialize the RAG and research system."""    try:        # Create RAG chain for SEC document analysis        rag_chain = create_rag_chain("data/raw/apple_10k.pdf")                # Initialize research graph with RAG chain        chain = create_research_graph(rag_chain)                return chain    except Exception as e:        logger.error(f"Error initializing system: {e}")        raiseasync def run_financial_analysis(query: str):    """    Process financial analysis queries through the research graph.        Args:        query (str): The financial analysis query to process        Returns:        dict: Analysis results from multiple agents    """    try:        # Initialize state with query        state = {            "messages": [HumanMessage(content=query)],            "team_members": ["Search", "SECAnalyst"],            "information_needed": [],            "reasoning": ""        }                # Process through research chain        result = research_chain.invoke(state)                return {            "status": "success",            "analysis": result.get("messages", [])        }            except Exception as e:        logger.error(f"Analysis error: {e}")        return {            "status": "error",            "message": str(e)        }if __name__ == "__main__":    # Load environment variables    load_dotenv()        # Initialize the system    research_chain = init_financial_system()

This workflow:

1.  Initializes the research system with RAG capabilities
2.  Sets up the specialized agents and their tools
3.  Creates the research graph for coordinated analysis
4.  Processes queries through the team of agents
5.  Returns comprehensive financial analysis results

Agentverse Integration (register.py)[â€‹](#agentverse-integration-registerpy "Direct link to Agentverse Integration (register.py)")
---------------------------------------------------------------------------------------------------------------------------------

The Financial Analysis Agent registers itself with Agentverse and handles incoming analysis requests:

    import osimport loggingfrom flask import Flask, request, jsonifyfrom uagents_core.crypto import Identityfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentfrom main import init_financial_system# Configure logginglogging.basicConfig(level=logging.INFO)logger = logging.getLogger(__name__)# Initialize Flask appapp = Flask(__name__)# Global variablesfinancial_identity = Noneresearch_chain = Nonedef init_agent():    """Initialize and register the agent with agentverse"""    global financial_identity, research_chain    try:        # Initialize the research chain        research_chain = init_financial_system()                # Initialize identity and register with Agentverse        financial_identity = Identity.from_seed("Financial Analysis Agent", 0)                # Register with detailed capabilities description        register_with_agentverse(            identity=financial_identity,            url="http://localhost:5008/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="Financial Analysis Agent",            readme = """                <description>A comprehensive financial analysis agent that combines                 SEC filing analysis with real-time market data.</description>                <use_cases>                    <use_case>Analyze company financial metrics from SEC filings</use_case>                    <use_case>Research market trends and analyst opinions</use_case>                    <use_case>Compare financial performance with competitors</use_case>                </use_cases>                <payload_requirements>                    <payload>                        <requirement>                            <parameter>query</parameter>                            <description>What would you like to know about the company's financials?</description>                        </requirement>                    </payload>                </payload_requirements>            """        )        logger.info("Financial Analysis Agent registered successfully!")    except Exception as e:        logger.error(f"Error initializing agent: {e}")        raise@app.route('/webhook', methods=['POST'])async def webhook():    """Handle incoming requests from other agents"""    try:        data = request.get_data().decode('utf-8')        message = parse_message_from_agent(data)        query = message.payload.get("request", "")        agent_address = message.sender        if not query:            return jsonify({"status": "error", "message": "No query provided"}), 400        # Process query using research chain        result = research_chain.invoke({            "messages": [HumanMessage(content=query)],            "team_members": ["Search", "SECAnalyst"]        })        # Format response        formatted_result = {            "analysis": [                {                    "role": msg.type if hasattr(msg, 'type') else "message",                    "content": msg.content,                    "name": msg.name if hasattr(msg, 'name') else None                }                for msg in result.get('messages', [])            ]        }        # Send response back through Agentverse        send_message_to_agent(            financial_identity,            agent_address,            {'analysis_result': formatted_result}        )        return jsonify({"status": "analysis_sent"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"status": "error", "message": str(e)}), 500def run_agent():    """Initialize and start the agent"""    try:        init_agent()        app.run(host="0.0.0.0", port=5008, debug=True)    except Exception as e:        logger.error(f"Error running agent: {e}")        raise

Step-by-Step Execution[â€‹](#step-by-step-execution "Direct link to Step-by-Step Execution")
------------------------------------------------------------------------------------------

1.  **Environment Setup**
    
        # Create and activate virtual environmentpython -m venv venvsource venv/bin/activate  # On Windows: venv\Scripts\activate# Install requirementspip install -r requirements.txt
    
2.  **Configure Environment Variables** Create `.env` file with required API keys:
    
        OPENAI_API_KEY=<your_openai_api_key>TAVILY_API_KEY=<your_tavily_api_key>AGENTVERSE_API_KEY=<your_agentverse_api_key>
    
3.  **Start the Financial Analysis Agent**
    
    This will:
    
    *   Initialize the RAG system
    *   Register the agent with Agentverse
    *   Start the Flask server on port 5008
4.  **Verify Agent Registration**
    
    *   Check logs for successful registration message
    *   Verify agent appears in Agentverse registry

User Agent Script (user\_agent.py)[â€‹](#user-agent-script-user_agentpy "Direct link to User Agent Script (user_agent.py)")
-------------------------------------------------------------------------------------------------------------------------

The **User Agent** serves as a client. It registers itself with Agentverse, can search for existing agents, and forwards financial analysis queries to the Financial Analysis Agent. Afterwards, it retrieves the response and processes it.

### Script Breakdown[â€‹](#script-breakdown "Direct link to Script Breakdown")

**Importing Required Libraries**

The script imports libraries for Flask-based HTTP handling, Fetch.ai's identity and communication utilities, and environment variable management:

    from flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentfrom fetchai import fetchimport loggingimport osfrom dotenv import load_dotenv

**Setting Up Flask and Logging**

Logging and Flask are initialised to handle web requests and log agent activities:

    logging.basicConfig(level=logging.DEBUG)logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app, resources={r"/api/*": {'origins': 'http://localhost:5174'}})# Global variables for client identity and responsesprimary_agent = None

**Initialising the User Agent**

The PrimaryAgent class handles initialization and registration with Fetch.ai's Agentverse:

    class PrimaryAgent:    def __init__(self):        self.identity = None        self.latest_response = None        def initialize(self):        try:            self.identity = Identity.from_seed(os.getenv("PRIMARY_AGENT_KEY"), 0)                        register_with_agentverse(                identity=self.identity,                url="http://localhost:5001/webhook",                agentverse_token=os.getenv("AGENTVERSE_API_KEY"),                agent_title="Financial Query Router",                readme="<description>Routes queries to Financial Analysis Agent</description>"            )            logger.info("Primary agent initialized successfully!")                        except Exception as e:            logger.error(f"Initialization error: {e}")            raise

**Searching for Agents**

The /api/search-agents endpoint allows the User Agent to search for agents in Agentverse based on a query:

    @app.route('/api/search-agents', methods=['GET'])def search_agents():    """Search for available agents based on the financial query"""    try:        query = request.args.get('query', '')        if not query:            return jsonify({"error": "Query parameter 'query' is required."}), 400        logger.info(f"Searching for agents with query: {query}")        available_ais = fetch.ai(query)        agents = available_ais.get('ais', [])                extracted_data = [            {                'name': agent.get('name'),                'address': agent.get('address')            }            for agent in agents        ]                logger.info(f"Found {len(extracted_data)} agents matching the query")        return jsonify(extracted_data), 200    except Exception as e:        logger.error(f"Error finding agents: {e}")        return jsonify({"error": str(e)}), 500

**Sending Data to Financial Analysis Agent**

The /api/send-request endpoint forwards financial analysis queries to the selected agent:

    @app.route('/api/send-request', methods=['POST'])def send_request():    try:        data = request.json        payload = data.get('payload', {})        user_input = payload.get('request')        agent_address = data.get('agentAddress')                if not user_input:            return jsonify({"error": "No input provided"}), 400                send_message_to_agent(            primary_agent.identity,            agent_address,            {                "request": user_input            }        )                return jsonify({            "status": "request_sent",             "agent_address": agent_address,             "payload": payload        })            except Exception as e:        logger.error(f"Error processing request: {e}")        return jsonify({"error": str(e)}), 500

**Retrieving the Agent's Response**

The /api/get-response endpoint retrieves the analysis from the Financial Analysis Agent:

    @app.route('/api/get-response', methods=['GET'])def get_response():    try:        if primary_agent.latest_response:            response = primary_agent.latest_response            primary_agent.latest_response = None            return jsonify(response)        return jsonify({"status": "waiting"})    except Exception as e:        logger.error(f"Error getting response: {e}")        return jsonify({"error": str(e)}), 500

**Webhook Endpoint**

The webhook endpoint processes messages sent by the Financial Analysis Agent:

    @app.route('/webhook', methods=['POST'])def webhook():    try:        data = request.get_data().decode("utf-8")        message = parse_message_from_agent(data)        primary_agent.latest_response = message.payload        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500

**Running the User Agent**

The Flask server runs on port 5001 to handle requests:

    if __name__ == "__main__":    load_dotenv()    primary_agent.initialize()    app.run(host="0.0.0.0", port=5001)

### Interacting with the Agents[â€‹](#interacting-with-the-agents "Direct link to Interacting with the Agents")

**Search for Financial Analysis Agents**

To list agents in the Agentverse:

    curl -X GET "http://localhost:5001/api/search-agents?query=Analyze%20Apple's%20supply%20chain%20risks"

**Sample Output**

    [    {        "name": "Financial Analysis Agent",        "address": "agent1qvkq8x25tvuzclq6v34skqryzsfmqdd6snv8apu3yutc4fylkm4u5e9rsxf"    },    {        "name": "Dashboard Analytics Frontend Client",        "address": "agent1qthka4n7q0m7zwegq0qg5p3aaw5x309e8swc2nttnf3pxd3tusdwzch6ncn"    }]

**Send Analysis Request**

Send a financial analysis query:

    curl -X POST "http://localhost:5001/api/send-request" \-H "Content-Type: application/json" \-d '{    "payload": {        "request": "What are Apple'\''s recent revenue trends and market performance?"    },    "agentAddress": "agent1qvkq8x25tvuzclq6v34skqryzsfmqdd6snv8apu3yutc4fylkm4u5e9rsxf"}'

**Sample Output**

    {    "status": "request_sent",    "agent_address": "agent1qvkq8x25tvuzclq6v34skqryzsfmqdd6snv8apu3yutc4fylkm4u5e9rsxf",    "payload": {        "request": "What are Apple's recent revenue trends and market performance?"    }}

**Retrieve the Analysis Response**

Fetch the analysis response:

    curl -X GET "http://localhost:5001/api/get-response"

**Sample Output**

    {    "analysis_result": {        "analysis": [            {                "content": "Information needed:\n        Historical revenue trends for Apple over the last few quarters\n        \n        Query: What are Apple's recent revenue trends and market performance?",                "name": null,                "role": "human"            }        ]    }}

You now have a **LangGraph-based Financial Analysis Agent** integrated with the **Fetch.ai Agentverse**, handling complex financial analysis tasks through a team of specialized agents (Search Agent and SEC Analyst). Feel free to adapt the LangGraph workflow, state management, and agent interactions to suit your own requirementsâ€”whether you're adding new analysis capabilities, expanding the agent team, or customizing the financial analysis patterns.</content>
</page>

<page>
  <title>LangGraph | Innovation Lab Resources</title>
  <url>https://innovationlab.fetch.ai/resources/docs/examples/other-frameworks/financial-analysis-ai-agent</url>
  <content>Creating and Registering LangGraph based Financial Analysis Agent
-----------------------------------------------------------------

This documentation explains how to build **LangGraph** agents that perform comprehensive financial analysis, register them on **Fetch.ai's Agentverse**, and enable collaboration between specialized agents for dynamic data analysis. Below, you'll see the **main.py** script containing the workflow logic `run_financial_analysis_workflow`, as well as specialized agent implementations for financial analysis.

Overview[â€‹](#overview "Direct link to Overview")
------------------------------------------------

### Purpose[â€‹](#purpose "Direct link to Purpose")

This project showcases:

*   **Creating LangGraph agents** for complex tasks (e.g., analyzing SEC filings, market research, and financial metrics).
*   **Integrating with Fetch.ai's Agentverse**, enabling seamless communication and data exchange.
*   **Using autonomous agents** to streamline financial analysis while combining multiple data sources and expert analysis.

Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")
---------------------------------------------------------------

### Environment Setup[â€‹](#environment-setup "Direct link to Environment Setup")

To begin creating and registering LangGraph agents, ensure you have the following:

*   **Python 3.10+** or higher
*   A **virtual environment** (recommended)
*   **Fetch.ai SDK** for agent creation and registration
*   **LangGraph Framework** for defining states and workflows
*   **LangChain** for agent tools and chains

### Key Components[â€‹](#key-components "Direct link to Key Components")

*   **SEC Analysis Agent**: A specialized agent that processes SEC filings and extracts financial metrics
*   **Search Agent**: Gathers real-time market data and analyst opinions
*   **Supervisor Agent**: Coordinates analysis flow and combines insights
*   **Agent Collaboration**: Multiple agents work together via LangGraph state management

### Architecture Diagram[â€‹](#architecture-diagram "Direct link to Architecture Diagram")

### Environment Variables[â€‹](#environment-variables "Direct link to Environment Variables")

Create a `.env` file with your API keys:

    OPENAI_API_KEY=<your_openai_api_key>TAVILY_API_KEY=<your_tavily_api_key>AGENTVERSE_API_KEY=<your_agentverse_api_key>

Replace the placeholders with your actual keys:

*   [OPENAI\_API\_KEY](https://openai.com/api/)
*   [TAVILY\_API\_KEY](https://tavily.com/)
*   [AGENTVERSE\_API\_KEY](https://innovationlab.fetch.ai/resources/docs/agentverse/agentverse-api-key)

Project Directory & Configuration Files[â€‹](#project-directory--configuration-files "Direct link to Project Directory & Configuration Files")
--------------------------------------------------------------------------------------------------------------------------------------------

For a clean, maintainable setup, we recommend the following structure:

    financial_analysis_agent/â”œâ”€â”€ .envâ”œâ”€â”€ src/â”‚   â”œâ”€â”€ agents/â”‚   â”‚   â”œâ”€â”€ __init__.pyâ”‚   â”‚   â”œâ”€â”€ search_agent.py      # Market research specialistâ”‚   â”‚   â”œâ”€â”€ sec_agent.py         # SEC filings specialistâ”‚   â”‚   â””â”€â”€ supervisor.py        # Team coordinatorâ”‚   â”œâ”€â”€ tools/â”‚   â”‚   â”œâ”€â”€ __init__.pyâ”‚   â”‚   â”œâ”€â”€ search.py            # Tavily search implementationâ”‚   â”‚   â””â”€â”€ analysis.py          # RAG implementationâ”‚   â”œâ”€â”€ rag/â”‚   â”‚   â”œâ”€â”€ __init__.pyâ”‚   â”‚   â”œâ”€â”€ chain.py             # RAG chain setupâ”‚   â”‚   â””â”€â”€ loader.py            # Document processingâ”‚   â”œâ”€â”€ graph/â”‚   â”‚   â”œâ”€â”€ __init__.pyâ”‚   â”‚   â””â”€â”€ state.py             # State managementâ”‚   â””â”€â”€ utils/â”‚       â”œâ”€â”€ __init__.pyâ”‚       â””â”€â”€ helpers.py           # Helper functionsâ”œâ”€â”€ main.py                      # Main workflowâ”œâ”€â”€ register.py                  # Agentverse registrationâ”œâ”€â”€ requirements.txtâ””â”€â”€ README.md

Core Implementation Files[â€‹](#core-implementation-files "Direct link to Core Implementation Files")
---------------------------------------------------------------------------------------------------

**1\. requirements.txt**

**Purpose**: Declares project dependencies for installing all necessary packages quickly with `pip install -r requirements.txt`.

Create a file called `requirements.txt` at the root of your project. Example contents:

    langchain==0.1.0langchain-core==0.1.10langgraph==0.0.10fetchai-sdk==0.16.3flask==2.2.5flask-cors==3.0.10python-dotenv==1.0.0tavily-python==0.2.6qdrant-client==1.7.0tiktoken==0.5.2

Install everything at once via:

    pip install -r requirements.txt

**2\. State Management (src/graph/state.py)**

**Purpose**: Defines the state structure and management for the research team's workflow.

Place `state.py` in the `src/graph` directory. It contains:

    from typing import TypedDict, List, Annotatedfrom langchain_core.messages import BaseMessageimport operatorclass ResearchTeamState(TypedDict):    """State structure for research team coordination."""    messages: Annotated[List[BaseMessage], operator.add]  # Conversation history    team_members: List[str]                              # Available agents    next: str                                           # Next agent to act    information_needed: List[str]                       # Required information    reasoning: str                                      # Decision reasoningdef create_initial_state(query: str) -> ResearchTeamState:    """Create initial state from user query."""    return {        "messages": [HumanMessage(content=query)],        "team_members": ["Search", "SECAnalyst"],        "next": "",        "information_needed": [],        "reasoning": ""    }def update_state(state: ResearchTeamState, agent_response: dict) -> ResearchTeamState:    """Update state with agent response."""    new_state = state.copy()    new_state["messages"].extend(agent_response["messages"])    return new_state

**3\. Tools Implementation (src/tools)**

**Purpose**: Implements specialized tools for market research and SEC filing analysis.

#### A. Search Tool (search.py)[â€‹](#a-search-tool-searchpy "Direct link to A. Search Tool (search.py)")

    from typing import Annotatedfrom langchain_core.tools import toolfrom tavily import TavilyClientimport os@tooldef tavily_search(query: str) -> str:    """Search for real-time market information."""    try:        client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))        result = client.search(query)        return str(result)    except Exception as e:        return f"Error performing search: {str(e)}"

#### B. Analysis Tool (analysis.py)[â€‹](#b-analysis-tool-analysispy "Direct link to B. Analysis Tool (analysis.py)")

    from typing import Annotatedfrom langchain_core.tools import toolfrom ..rag.chain import create_rag_chain@tooldef retrieve_information(query: Annotated[str, "query to analyze financial documents"]) -> str:    """Analyze SEC filings using RAG."""    try:        rag_chain = create_rag_chain()        return rag_chain.invoke(query)    except Exception as e:        return f"Error analyzing documents: {str(e)}"

**4\. RAG Implementation (src/rag)**

**Purpose**: Handles document processing and retrieval for SEC filing analysis.

#### A. Document Loader (loader.py)[â€‹](#a-document-loader-loaderpy "Direct link to A. Document Loader (loader.py)")

    class DocumentLoader:    def __init__(self, file_path: str):        self.file_path = file_path        @staticmethod    def tiktoken_len(text):        tokens = tiktoken.encoding_for_model("gpt-4").encode(text)        return len(tokens)        def load_and_split(self):        """Load and chunk documents for processing."""        docs = PyMuPDFLoader(self.file_path).load()        splitter = RecursiveCharacterTextSplitter(            chunk_size=300,            chunk_overlap=0,            length_function=self.tiktoken_len        )        return splitter.split_documents(docs)

#### B. RAG Chain (chain.py)[â€‹](#b-rag-chain-chainpy "Direct link to B. RAG Chain (chain.py)")

    def create_rag_chain(file_path: str = "data/raw/apple_10k.pdf"):    """Create RAG chain for SEC filing analysis."""    # Initialize document processing    loader = DocumentLoader(file_path)    chunks = loader.load_and_split()        # Set up vector store    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")    vectorstore = Qdrant.from_documents(        chunks,        embeddings,        location=":memory:",        collection_name="sec_filings"    )        # Create retrieval chain    template = """Use the context to answer financial questions.    Context: {context}    Question: {question}    Answer with specific numbers and data when available."""        prompt = ChatPromptTemplate.from_template(template)    chain = (        {"context": vectorstore.as_retriever(), "question": RunnablePassthrough()}        | prompt        | ChatOpenAI(model="gpt-4-turbo-preview")        | StrOutputParser()    )        return chain

**5\. Helper Functions (src/utils/helpers.py)**

**Purpose**: Provides utility functions for agent creation and node management.

    def create_agent(llm: ChatOpenAI, tools: list, system_prompt: str) -> AgentExecutor:    """Create a specialized agent with tools and prompt."""    try:        system_prompt += (            "\nWork autonomously using your tools."            " Do not ask for clarification."            " Your team members will help with their specialties."        )                prompt = ChatPromptTemplate.from_messages([            ("system", system_prompt),            MessagesPlaceholder(variable_name="messages"),            MessagesPlaceholder(variable_name="agent_scratchpad"),        ])                agent = create_openai_functions_agent(llm, tools, prompt)        return AgentExecutor(agent=agent, tools=tools)            except Exception as e:        logger.error(f"Error creating agent: {e}")        raisedef agent_node(state, agent, name):    """Create agent node for the graph."""    try:        if "information_needed" in state:            message_content = f"""Information needed:            {', '.join(state['information_needed'])}            Query: {state['messages'][-1].content}"""            state['messages'][-1] = HumanMessage(content=message_content)        result = agent.invoke(state)        return {            "messages": [                HumanMessage(content=result["output"], name=name)            ]        }    except Exception as e:        logger.error(f"Error in agent node {name}: {e}")        raise

Financial Analysis Agent Implementation[â€‹](#financial-analysis-agent-implementation "Direct link to Financial Analysis Agent Implementation")
---------------------------------------------------------------------------------------------------------------------------------------------

### A. SEC Analysis Agent (src/agents/sec\_agent.py)[â€‹](#a-sec-analysis-agent-srcagentssec_agentpy "Direct link to A. SEC Analysis Agent (src/agents/sec_agent.py)")

A specialized agent that processes SEC filings and financial documents. It uses RAG to analyze documents and extract relevant financial metrics.

    # src/agents/sec_agent.pyfrom langchain.chat_models import ChatOpenAIfrom ..tools.analysis import retrieve_informationdef create_sec_agent(llm: ChatOpenAI):    """Creates an agent specialized in SEC filings analysis."""        system_prompt = """You are a financial analyst specialized in SEC filings analysis.    After analyzing SEC filings:    1. If you need market context, clearly state what specific market data you need    2. If numbers need industry comparison, explicitly request competitor data    3. Always include specific numbers and trends from the filings    4. If you spot significant changes or unusual patterns, highlight them        Format your response as:    1. Data from SEC Filings: [your findings]    2. Additional Context Needed: [if any]    3. Analysis: [your insights]    """        return create_agent(        llm=llm,        tools=[retrieve_information],        system_prompt=system_prompt    )

### B. Search Agent (src/agents/search\_agent.py)[â€‹](#b-search-agent-srcagentssearch_agentpy "Direct link to B. Search Agent (src/agents/search_agent.py)")

Handles real-time market research and data gathering using external search tools.

    # src/agents/search_agent.pyfrom langchain.chat_models import ChatOpenAIfrom ..tools.search import tavily_searchdef create_search_agent(llm: ChatOpenAI):    """Creates a search agent specialized in market research."""        system_prompt = """You are a research assistant who can search for up-to-date     financial information using the tavily search engine.        When responding:    1. Always cite sources    2. Focus on recent market data and analyst reports    3. If SEC data is mentioned, compare it with current market views    4. Highlight any significant discrepancies with official filings        Format your response as:    1. Market Data: [your findings]    2. Analyst Views: [key opinions]    3. Relevance to SEC Data: [if applicable]    """        return create_agent(        llm=llm,        tools=[tavily_search],        system_prompt=system_prompt    )

### C. Supervisor Agent (src/agents/supervisor.py)[â€‹](#c-supervisor-agent-srcagentssupervisorpy "Direct link to C. Supervisor Agent (src/agents/supervisor.py)")

Coordinates between agents and manages the analysis workflow.

    # src/agents/supervisor.pydef create_supervisor_agent(llm: ChatOpenAI):    """Creates the supervisor agent for coordinating analysis."""        function_def = {        "name": "route",        "description": "Select the next role based on query analysis.",        "parameters": {            "title": "routeSchema",            "type": "object",            "properties": {                "next": {                    "title": "Next",                    "anyOf": [{"enum": ["Search", "SECAnalyst", "FINISH"]}],                },                "reasoning": {                    "title": "Reasoning",                    "type": "string",                    "description": "Explanation for why this agent should act next"                },                "information_needed": {                    "title": "Information Needed",                    "type": "array",                    "items": {"type": "string"},                    "description": "List of specific information needed from this agent"                }            },            "required": ["next", "reasoning", "information_needed"],        },    }    prompt = ChatPromptTemplate.from_messages([        ("system", """You are a financial research team supervisor.        Your role is to:        1. Analyze incoming queries        2. Determine what information is needed        3. Choose the appropriate agent for each task        4. Coordinate between agents        5. Ensure comprehensive analysis"""),        MessagesPlaceholder(variable_name="messages"),        ("system", "Who should act next? Consider available information and agent specialties.")    ])    return (        prompt        | llm.bind_functions(functions=[function_def], function_call="route")        | JsonOutputFunctionsParser()    )

Main Workflow Implementation (main.py)[â€‹](#main-workflow-implementation-mainpy "Direct link to Main Workflow Implementation (main.py)")
---------------------------------------------------------------------------------------------------------------------------------------

The main script that initializes the LangGraph workflow and handles financial analysis requests.

    import osfrom dotenv import load_dotenvfrom src.rag.chain import create_rag_chainfrom src.graph.state import create_research_graphdef init_financial_system():    """Initialize the RAG and research system."""    try:        # Create RAG chain for SEC document analysis        rag_chain = create_rag_chain("data/raw/apple_10k.pdf")                # Initialize research graph with RAG chain        chain = create_research_graph(rag_chain)                return chain    except Exception as e:        logger.error(f"Error initializing system: {e}")        raiseasync def run_financial_analysis(query: str):    """    Process financial analysis queries through the research graph.        Args:        query (str): The financial analysis query to process        Returns:        dict: Analysis results from multiple agents    """    try:        # Initialize state with query        state = {            "messages": [HumanMessage(content=query)],            "team_members": ["Search", "SECAnalyst"],            "information_needed": [],            "reasoning": ""        }                # Process through research chain        result = research_chain.invoke(state)                return {            "status": "success",            "analysis": result.get("messages", [])        }            except Exception as e:        logger.error(f"Analysis error: {e}")        return {            "status": "error",            "message": str(e)        }if __name__ == "__main__":    # Load environment variables    load_dotenv()        # Initialize the system    research_chain = init_financial_system()

This workflow:

1.  Initializes the research system with RAG capabilities
2.  Sets up the specialized agents and their tools
3.  Creates the research graph for coordinated analysis
4.  Processes queries through the team of agents
5.  Returns comprehensive financial analysis results

Agentverse Integration (register.py)[â€‹](#agentverse-integration-registerpy "Direct link to Agentverse Integration (register.py)")
---------------------------------------------------------------------------------------------------------------------------------

The Financial Analysis Agent registers itself with Agentverse and handles incoming analysis requests:

    import osimport loggingfrom flask import Flask, request, jsonifyfrom uagents_core.crypto import Identityfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentfrom main import init_financial_system# Configure logginglogging.basicConfig(level=logging.INFO)logger = logging.getLogger(__name__)# Initialize Flask appapp = Flask(__name__)# Global variablesfinancial_identity = Noneresearch_chain = Nonedef init_agent():    """Initialize and register the agent with agentverse"""    global financial_identity, research_chain    try:        # Initialize the research chain        research_chain = init_financial_system()                # Initialize identity and register with Agentverse        financial_identity = Identity.from_seed("Financial Analysis Agent", 0)                # Register with detailed capabilities description        register_with_agentverse(            identity=financial_identity,            url="http://localhost:5008/webhook",            agentverse_token=os.getenv("AGENTVERSE_API_KEY"),            agent_title="Financial Analysis Agent",            readme = """                <description>A comprehensive financial analysis agent that combines                 SEC filing analysis with real-time market data.</description>                <use_cases>                    <use_case>Analyze company financial metrics from SEC filings</use_case>                    <use_case>Research market trends and analyst opinions</use_case>                    <use_case>Compare financial performance with competitors</use_case>                </use_cases>                <payload_requirements>                    <payload>                        <requirement>                            <parameter>query</parameter>                            <description>What would you like to know about the company's financials?</description>                        </requirement>                    </payload>                </payload_requirements>            """        )        logger.info("Financial Analysis Agent registered successfully!")    except Exception as e:        logger.error(f"Error initializing agent: {e}")        raise@app.route('/webhook', methods=['POST'])async def webhook():    """Handle incoming requests from other agents"""    try:        data = request.get_data().decode('utf-8')        message = parse_message_from_agent(data)        query = message.payload.get("request", "")        agent_address = message.sender        if not query:            return jsonify({"status": "error", "message": "No query provided"}), 400        # Process query using research chain        result = research_chain.invoke({            "messages": [HumanMessage(content=query)],            "team_members": ["Search", "SECAnalyst"]        })        # Format response        formatted_result = {            "analysis": [                {                    "role": msg.type if hasattr(msg, 'type') else "message",                    "content": msg.content,                    "name": msg.name if hasattr(msg, 'name') else None                }                for msg in result.get('messages', [])            ]        }        # Send response back through Agentverse        send_message_to_agent(            financial_identity,            agent_address,            {'analysis_result': formatted_result}        )        return jsonify({"status": "analysis_sent"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"status": "error", "message": str(e)}), 500def run_agent():    """Initialize and start the agent"""    try:        init_agent()        app.run(host="0.0.0.0", port=5008, debug=True)    except Exception as e:        logger.error(f"Error running agent: {e}")        raise

Step-by-Step Execution[â€‹](#step-by-step-execution "Direct link to Step-by-Step Execution")
------------------------------------------------------------------------------------------

1.  **Environment Setup**
    
        # Create and activate virtual environmentpython -m venv venvsource venv/bin/activate  # On Windows: venv\Scripts\activate# Install requirementspip install -r requirements.txt
    
2.  **Configure Environment Variables** Create `.env` file with required API keys:
    
        OPENAI_API_KEY=<your_openai_api_key>TAVILY_API_KEY=<your_tavily_api_key>AGENTVERSE_API_KEY=<your_agentverse_api_key>
    
3.  **Start the Financial Analysis Agent**
    
    This will:
    
    *   Initialize the RAG system
    *   Register the agent with Agentverse
    *   Start the Flask server on port 5008
4.  **Verify Agent Registration**
    
    *   Check logs for successful registration message
    *   Verify agent appears in Agentverse registry

User Agent Script (user\_agent.py)[â€‹](#user-agent-script-user_agentpy "Direct link to User Agent Script (user_agent.py)")
-------------------------------------------------------------------------------------------------------------------------

The **User Agent** serves as a client. It registers itself with Agentverse, can search for existing agents, and forwards financial analysis queries to the Financial Analysis Agent. Afterwards, it retrieves the response and processes it.

### Script Breakdown[â€‹](#script-breakdown "Direct link to Script Breakdown")

**Importing Required Libraries**

The script imports libraries for Flask-based HTTP handling, Fetch.ai's identity and communication utilities, and environment variable management:

    from flask import Flask, request, jsonifyfrom flask_cors import CORSfrom uagents_core.crypto import Identityfrom fetchai.registration import register_with_agentversefrom fetchai.communication import parse_message_from_agent, send_message_to_agentfrom fetchai import fetchimport loggingimport osfrom dotenv import load_dotenv

**Setting Up Flask and Logging**

Logging and Flask are initialised to handle web requests and log agent activities:

    logging.basicConfig(level=logging.DEBUG)logger = logging.getLogger(__name__)app = Flask(__name__)CORS(app, resources={r"/api/*": {'origins': 'http://localhost:5174'}})# Global variables for client identity and responsesprimary_agent = None

**Initialising the User Agent**

The PrimaryAgent class handles initialization and registration with Fetch.ai's Agentverse:

    class PrimaryAgent:    def __init__(self):        self.identity = None        self.latest_response = None        def initialize(self):        try:            self.identity = Identity.from_seed(os.getenv("PRIMARY_AGENT_KEY"), 0)                        register_with_agentverse(                identity=self.identity,                url="http://localhost:5001/webhook",                agentverse_token=os.getenv("AGENTVERSE_API_KEY"),                agent_title="Financial Query Router",                readme="<description>Routes queries to Financial Analysis Agent</description>"            )            logger.info("Primary agent initialized successfully!")                        except Exception as e:            logger.error(f"Initialization error: {e}")            raise

**Searching for Agents**

The /api/search-agents endpoint allows the User Agent to search for agents in Agentverse based on a query:

    @app.route('/api/search-agents', methods=['GET'])def search_agents():    """Search for available agents based on the financial query"""    try:        query = request.args.get('query', '')        if not query:            return jsonify({"error": "Query parameter 'query' is required."}), 400        logger.info(f"Searching for agents with query: {query}")        available_ais = fetch.ai(query)        agents = available_ais.get('ais', [])                extracted_data = [            {                'name': agent.get('name'),                'address': agent.get('address')            }            for agent in agents        ]                logger.info(f"Found {len(extracted_data)} agents matching the query")        return jsonify(extracted_data), 200    except Exception as e:        logger.error(f"Error finding agents: {e}")        return jsonify({"error": str(e)}), 500

**Sending Data to Financial Analysis Agent**

The /api/send-request endpoint forwards financial analysis queries to the selected agent:

    @app.route('/api/send-request', methods=['POST'])def send_request():    try:        data = request.json        payload = data.get('payload', {})        user_input = payload.get('request')        agent_address = data.get('agentAddress')                if not user_input:            return jsonify({"error": "No input provided"}), 400                send_message_to_agent(            primary_agent.identity,            agent_address,            {                "request": user_input            }        )                return jsonify({            "status": "request_sent",             "agent_address": agent_address,             "payload": payload        })            except Exception as e:        logger.error(f"Error processing request: {e}")        return jsonify({"error": str(e)}), 500

**Retrieving the Agent's Response**

The /api/get-response endpoint retrieves the analysis from the Financial Analysis Agent:

    @app.route('/api/get-response', methods=['GET'])def get_response():    try:        if primary_agent.latest_response:            response = primary_agent.latest_response            primary_agent.latest_response = None            return jsonify(response)        return jsonify({"status": "waiting"})    except Exception as e:        logger.error(f"Error getting response: {e}")        return jsonify({"error": str(e)}), 500

**Webhook Endpoint**

The webhook endpoint processes messages sent by the Financial Analysis Agent:

    @app.route('/webhook', methods=['POST'])def webhook():    try:        data = request.get_data().decode("utf-8")        message = parse_message_from_agent(data)        primary_agent.latest_response = message.payload        return jsonify({"status": "success"})    except Exception as e:        logger.error(f"Error in webhook: {e}")        return jsonify({"error": str(e)}), 500

**Running the User Agent**

The Flask server runs on port 5001 to handle requests:

    if __name__ == "__main__":    load_dotenv()    primary_agent.initialize()    app.run(host="0.0.0.0", port=5001)

### Interacting with the Agents[â€‹](#interacting-with-the-agents "Direct link to Interacting with the Agents")

**Search for Financial Analysis Agents**

To list agents in the Agentverse:

    curl -X GET "http://localhost:5001/api/search-agents?query=Analyze%20Apple's%20supply%20chain%20risks"

**Sample Output**

    [    {        "name": "Financial Analysis Agent",        "address": "agent1qvkq8x25tvuzclq6v34skqryzsfmqdd6snv8apu3yutc4fylkm4u5e9rsxf"    },    {        "name": "Dashboard Analytics Frontend Client",        "address": "agent1qthka4n7q0m7zwegq0qg5p3aaw5x309e8swc2nttnf3pxd3tusdwzch6ncn"    }]

**Send Analysis Request**

Send a financial analysis query:

    curl -X POST "http://localhost:5001/api/send-request" \-H "Content-Type: application/json" \-d '{    "payload": {        "request": "What are Apple'\''s recent revenue trends and market performance?"    },    "agentAddress": "agent1qvkq8x25tvuzclq6v34skqryzsfmqdd6snv8apu3yutc4fylkm4u5e9rsxf"}'

**Sample Output**

    {    "status": "request_sent",    "agent_address": "agent1qvkq8x25tvuzclq6v34skqryzsfmqdd6snv8apu3yutc4fylkm4u5e9rsxf",    "payload": {        "request": "What are Apple's recent revenue trends and market performance?"    }}

**Retrieve the Analysis Response**

Fetch the analysis response:

    curl -X GET "http://localhost:5001/api/get-response"

**Sample Output**

    {    "analysis_result": {        "analysis": [            {                "content": "Information needed:\n        Historical revenue trends for Apple over the last few quarters\n        \n        Query: What are Apple's recent revenue trends and market performance?",                "name": null,                "role": "human"            }        ]    }}

You now have a **LangGraph-based Financial Analysis Agent** integrated with the **Fetch.ai Agentverse**, handling complex financial analysis tasks through a team of specialized agents (Search Agent and SEC Analyst). Feel free to adapt the LangGraph workflow, state management, and agent interactions to suit your own requirementsâ€”whether you're adding new analysis capabilities, expanding the agent team, or customizing the financial analysis patterns.</content>
</page>